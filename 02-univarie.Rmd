---
output:
  pdf_document: default
  html_document: default
---
# Statistiques descriptives univariées

Dans ce chapitre, nous décrirons la notion de variable, permettant l’opérationnalisation d’un concept. Comprendre les différents types de variables est essentiel en statistiques. En effet, en fonction du type de variable à l'étude, les tests d’hypothèse et les méthodes de statistique inférentielle que l’on pourra appliquer seront différents. Nous distinguerons ainsi cinq types de variables : nominale, ordinale, discrète, continue et semi-quantitative. Nous présenterons ensuite les différentes statistiques descriptives univariées qui peuvent s’appliquer à ces types de variables.

::: {.bloc_package .bloc_package_png data-latex="{blocs/package}"}
Dans cette section, nous utiliserons principalement les packages suivants (À MODIFIER PLUS TARD) : 

* Pour créer des graphiques :
  - **ggplot2**, le seul, l'unique
  - **ggpubr**, pour combiner des graphiques et réaliser des diagrammes quantiles-quantiles
* Pour manipuler des données : 
  - **dplyr**, avec les fonctions *group_by*, *summarize* et les pipes *%>%*
* Pour les corrélations ((section \@ref(sect311))) : 
  - **correlation**, de l'ensemble de package **easy_stats**, offrant une large gamme de méthodes de corrélations
  - **boot** pour réaliser des corrélations avec *bootstrap* 
  - **Hmisc** pour calculer des corrélations de Pearson et Spearman
  - **ppcor**, notamment pour des corrélations partielles
  - **psych** pour obtenir une matrice de  corrélation (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de p.
  - **stargazer** pour créer des beaux tableaux d’une matrice de corrélation en Html ou en LaTeX ou en ASCII.
  - **corrplot**, pour créer des graphiques de matrices de corrélation
* Pour le tableau de contignence ((section \@ref(sect312))) :
  - **gmodels**, pour construire des tableaux de contingence et * Calculer les test *t* et ses différentes variantes ((section \@ref(sect313))) :
* Pour les test *t* : 
  - **sjstats** pour réaliser des test *t* pondérés
  - **effectsize**, pour calculer les tailles d'effet de tests de *t*
* Pour la section sur les ANOVA ((section \@ref(sect314))) : 
  - **car**, pour les ANOVA classiques
  - *lmtest* pour le test de Breusch-Pagan d'homogénéité des variances 
  - **rstatix**, intégrant de nombreux tests classiques (comme le test de Shapiro) avec **tidyverse**

:::


## Notion de variable {#sect21}

### La variable : l'opérationnalisation d'un concept {#sect211}

Une variable permet d'opérationnaliser un concept, soit une « idée générale et abstraite que se fait l'esprit humain d'un objet de pensée concret ou abstrait, et qui lui permet de rattacher à ce même objet les diverses perceptions qu'il en a, et d'en organiser les connaissances » ([Larousse](https://www.larousse.fr/dictionnaires/francais/concept/17875?q=concept#17749){target="_blank"}). Pour valider un modèle théorique, il convient alors d'opérationnaliser ses différentes concepts et d'établir les relations qu'ils partagent. L'opérationnalisation d'un concept nécessite soit de mesurer (dans un intervalle de valeurs, c'est-à-dire de manière quantitative), soit de qualifier (avec plusieurs catégories, c'est-à-dire de manière qualitative) un phénomène. 

Selon [Statistique Canada](https://www.statcan.gc.ca/fra/concepts/variable){target="_blank"}, « une variable est une caractéristique d'une unité statistique que l'on observe et pour laquelle une valeur numérique ou une catégorie d'une classification peut être attribuée ». Il convient alors de bien saisir à quelle unité statistique s'applique les valeurs d'une variable : des personnes, des ménages, des municipalités, etc. 

Prenons deux exemples concrets tirées du Recensement de 2016 de Statistique Canada :

* Le concept **famille de recensement** est défini comme étant « un couple marié et les enfants, le cas échéant, du couple et/ou de l'un ou l'autre des conjoints; un couple en union libre et les enfants, le cas échéant, du couple et/ou de l'un ou l'autre des partenaires; ou un parent seul, peu importe son état matrimonial, habitant avec au moins un enfant dans le même logement et cet ou ces enfants. Tous les membres d'une famille de recensement particulière habitent le même logement. Un couple peut être de sexe opposé ou de même sexe. Les enfants peuvent être des enfants naturels, par le mariage, par l'union libre ou par adoption, peu importe leur âge ou leur état matrimonial, du moment qu'ils habitent dans le logement sans leur propre conjoint marié, partenaire en union libre ou enfant. Les petits-enfants habitant avec leurs grands-parents, alors qu'aucun des parents n'est présent, constituent également une famille de recensement » ([Statistique Canada](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/fam004-fra.cfm){target="_blank"}). À partir de cette définition, les familles de recensement peuvent être qualifiées selon plusieurs modalités : couples mariés sans enfant, couples mariés avec enfants, couples en union libre sans enfant, couples en union libre avec enfant, famille monoparentale (avec un parent de sexe féminin), famille monoparentale (avec un parent de sexe masculin).
* Le concept de **revenu d'emploi** est défini comme étant « tous les revenus reçus sous forme de traitements, salaires et commissions d'un travail rémunéré ou le revenu net d'un travail autonome dans une entreprise agricole ou non agricole non constituée en société et/ou dans l'exercice d'une profession au cours de la période de référence. Pour le Recensement de 2016, la période de référence est l'année civile 2015 pour toutes les variables de revenu » ([Statistique Canada](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop027-fra.cfm){target="_blank"}). Il est donc mesurée en dollars pour chaque individu de 15 ans et plus. Pour l'ensemble de la population de 15 ans et plus, il est être ensuite classer en déciles de revenu d'emploi, soit en dix groupes ([Statistique Canada](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop204-fra.cfm"}).

::: {.bloc_attention .bloc_attention_png data-latex="{blocs/attention}"}
**Maîtriser la définition des variables que vous utilisez : un enjeu crucial ! **

Nous avons vu qu'une variable est l'opérationnalisation d'un concept. Par conséquent, ne pas maîtriser la définition d'une variable revient à ne pas bien saisir le concept sous-jacent qu'elle tente de mesurer. Si vous exploitez des données secondaires – par exemple, issues d'un recensement de population ou d'une enquête longitudinale ou transversale –, il faut impérativement lire les définitions des variables que vous souhaiteriez utiliser. Ne pas le faire, risque d'aboutir à :

* une mauvaise opérationnalisation de votre modèle théorique, même si votre analyse est bien menée statistiquement parlant. Autrement dit, vous risquez de ne pas sélectionner les bonnes variables. Prenons un exemple concret. Vous avez construit un modèle théorique dans lequel vous souhaitez inclure une un concept sur la langue des personnes. Dans le recensement canadien de 2016, plusieurs variables relatives à langue sont disponibles : [connaissance des langues officielles](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop055-fra.cfm){target="_blank"},
[langue parlée à la maison](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop042-fra.cfm){target="_blank"}, [langue maternelle](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop095-fra.cfm){target="_blank"}, [première langue officielle parlée](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop034-fra.cfm){target="_blank"},  [connaissance des langues non officielles](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop054-fra.cfm){target="_blank"} et [langue de travail](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop059-fra.cfm){target="_blank"} ([Statistique Canada, 2019](https://www12.statcan.gc.ca/census-recensement/2016/ref/guides/003/98-500-x2016003-fra.cfm){target="_blank"}). La sélection de l'une de ces variables doit être faite de manière rigoureuse, c'est-à-dire en lien avec votre cadre théorique et suite une bonne compréhension des définitions des variables. Dans une étude sur le marché du travail, on sélectionnerait probablement la variable *sur la connaissance des langues officielles du Canada*, afin d'évaluer son effet sur l'employablité, toutes choses étant égales par ailleurs. Dans une autre étude portant sur la réussite ou la performance scolaire, il est probable qu'on utilise plutôt la *langue maternelle*.

* une mauvaise interprétation et discussion de vos résultats en lien avec votre cadre théorique.
* une mauvaise identification des pistes de recherche.

Finalement, la définition d'une variable peut évoluer à travers plusieurs recensements de population : la société évolue, les variables aussi ! Par conséquent, si vous comptez utiliser plusieurs années de recensement dans un même étude, assurez-vous que les définitions des variables soient similaires d'un jeu de données à l'autre et qu'elles mesurent ainsi la même chose. 

**Comprendre les variables utilisées dans un article scientifique : un exercice indispensable dans l'élaboration d'une revue de littérature**

Une lecture rigoureuse d'un article scientifique suppose, entre autres, de bien comprendre les concepts et variables mobilisés. Il convient alors de lire attentivement la section méthodologique (pas uniquement la section des résultats ou pire le résumé), sans quoi vous risquez d'aboutir à une revue de littérature approximative. 
Ayez aussi un **regard critique** sur les variables visant à opérationnaliser les  concepts clés de l'étude. Certains concepts sont très difficiles à traduire en variables; leurs opérationalisations (mesures) peuvent ainsi faire l'objet de vifs débats parmi les chercheurs. Très succinctement, c'est notamment le cas du concept de capital social. D'une part, les définitions et ancrages sont biens différents selon Bourdieu (sociologue, ancrage au niveau des individus) et Putman (politologue, ancrage au niveau des collectivités). D'autre part, aucun consensus ne semble clairement se dégager quant à la définition de variables permettant de le mesurer efficacement (de manière quantitative).   

**Variable de substitution (*proxy* variable en anglais)**

À mes débuts en recherche (premier auteur du livre), une brillante collègue géographe, renommée dans son champ de recherche, m'a affirmée : « on fait la moins pire des recherches ! ». Secoué par cette affirmation, elle m'expliqua ensuite que les données disponibles sont parfois imparfaites pour répondre avec précision à une question de recherche; on peut toujours les exploiter, tout en signalant honnêtemment leurs faiblesses et limites, et ce, tant pour les données que les variables utilisées. Ce fut le début d'une longue collaboration débouchant sur une multitude de *moins pires recherches*.

* Des bases de données peuvent être en effet imparfaites. Par exemple, en criminologie, des chercheur.e.s exploitant des données policières signalent habituellement la limite du **chiffre noir** : les données policières comprennent uniquement les crimes et délits découverts par la police et occultent ainsi les crimes non-découverts; ils ne peuvent ainsi refléter la criminalité réelle sur un territoire donné.

* Des variables peuvent aussi être imparfaites. Dans un jeu de données, il est fréquent qu'une variable opérationnalisant un concept précis ne soit pas disponible ou qu'elle n'est tout simplement pas été  mesurée. On cherchera alors une variable de substitution (*proxy*) pour la remplacer. Prenons un exemple concret portant sur l'exposition des cyclistes à la pollution atmosphérique ou au bruit routier. L'une des principaux facteurs d'exposition à ces pollutions est le trafic routier : plus ce dernier est élevé, plus le cycliste risque de rouler dans un environnement bruyant et pollué. Toutefois, il est rare de disposer de mesures du trafic en temps réel qui nécessitent des comptages de véhicules pendant le trajet des cyclistes (par exemple, à partir de vidéos captées par une caméra fixée sur le guidon). Pour pallier à l'absence de mesures directes, plusieurs auteurs utilisent des variables de substitution de la densité du trafic, comme la typologie des types d'axes (primaire, secondaire, tertiaire, rue locale, etc.), supposant ainsi qu'un axe primaire supporte un volume de véhicules supérieur à un axe secondaire...
:::

### Les types de variables {#sect212}
On distingue habituellement les variables qualitatives (nominale ou ordinale) des variables quantitatives (discrète ou continue). Tel qu'illustré à la figure \@ref(fig:figunivarie1), l'opérationnalisation du concept en variable est réalisée par différents mécanismes visant à qualifier, classer, compter ou mesurer afin de caractériser les unités statistiques (observations) d'une population ou d'un échantillon.

```{r figunivarie1, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Les types de variables",  out.width='70%'}
knitr::include_graphics('images/univariee/figure1.jpg', dpi = NA)
```

#### Les variables qualitatives {#sect2121}

**Une variable nominale** permet de **qualifier** des observations (individus) à partir de plusieurs catégories, dénommées modalités. Par exemple, la variable _couleur des yeux_ pourrait comprendre les modalités _bleu_, _marron_, _vert_ tandis que le *types de familles* compendrait les modalités _couple marié_, _couple en union libre_ et _famille monoparentale_.

**Une variable ordinale** permet de **classer** des observations à partir de plusieurs modalités hiérarchisées. L'exemple le plus connu est certainement l'échelle de Likert très utilisée dans les sondages évaluant le degré d'accord d'une personne à une affirmation avec les modalités suivantes : _tout à fait d'accord_, _d'accord_, _ni en désaccord ni d'accord_, _pas d'accord_ et _pas du tout d'accord_. Une multitude de variantes sont toutefois possibles pour classer la fréquence d'un phénomène (_Très souvent_, _souvent_, _parfois_, _rarement_, _jamais_), l'importance accordée à un phénomène (_Pas du tout important_, _peu important_, _plus ou moins important_, _Important_, _très important_) ou la proximité perçue d'un lieu (_Très éloigné_, _loin_, _plus ou moins proche_, _proche_, _très proche_).

En fonction du nombre de modalités qu'elle comprend, une variable qualitative (nominale ou ordinale) est soit **dichtomique (binaire)** (deux modalités), soit **polytomique** (plus de deux modalités). Par exemple, dans le recensement canadien, le *sexe* est une variable binaire (avec les modalités *sexe masculin*, *sexe féminin*), tandis que le *genre* est une variable polytomique (avec les modalités *genre masculin*, *genre féminin* et *diverses identités de genre*).

::: {.bloc_attention .bloc_attention_png data-latex="{blocs/attention}"}
Les variables nominales et ordinales sont habituellement encodées avec des valeurs numériques entières (par exemple, 1 pour _couples marié_, 2 pour _couple en union libre_ et 3 pour _famille monoparentale_). Toutefois, aucune opération arithmétique (moyenne ou écart-type par exemple) n'est possible sur ces valeurs. Dans ![](images/Rlogo.png), on utilisera un facteur pour attribuer un intitulé à chacune des valeurs numériques de la variable qualitative :

`df$Famille <- factor(df$Famille, c(1,2,3), labels = c("couples marié","couple en union libre", "famille monoparentale"))`

On calculera toutefois les fréquences des différentes modalités pour une variable nominale ou ordinale. Il est aussi possible de calculer la médiane sur une variable ordinale.
:::

#### Les variables quantitatives {#sect2122}

**Une variable discrète** permet de **compter** un phénomène dans un ensemble fini de valeurs, comme le nombre d'accidents impliquant un cycliste à une intersection sur une période cinq ans ou encore le nombre de vélos en libre service disponibles à une station. Il existe ainsi une variable binaire sous-jacente : la présence ou non d'un accident à l'intersection ou d'un vélo ou non à la station pour laquelle on opère un comptage. Habituellement, une variable discrète ne peut prendre que des valeurs entières (sans décimales), comme le nombre de personnes fréquentant un parc.

**Une variable continue** permet de **mesurer** un phénomène avec un nombre infini de valeurs réelles (avec décimales) dans un intervalle donné. Par exemple, une variable relative à la distance de dépassement d'un cycliste par un véhicule motorisé pourrait varier de 0 à 5 mètres ($X \in \left[0,5\right]$); toutefois cette distance peut être de 0,759421 ou de 4,785612 mètres. Le nombre de décimales de la valeur réelle dépendra de la précision et de la fiabilité de la mesure. Pour un capteur de distance de dépassement, le nombre de décimales dépendra de la précision du lidar ou du sonar de l'appareil; aussi, l'utilisation de trois décimales – soit une précision au millimètre – risque d'être largement suffisant pour mesurer la distance de dépassement. Une variable continue est soit une variable d'intervalle, soit une variable de rapport. Les **variables d'intervalle** ont une échelle relative, c'est-à-dire que les intervalles entre les valeurs de la variables ne sont pas constants; elles n'ont pas de vrai zéro. Ses valeurs peuvent être manipulées uniquement par addition et soustraction et non par multiplication et soustraction. La variable d'intervalle la plus connue est certainement celle de la température. S'il fait 10 degrés Celsius à Montréal et 30°C à Mumbai (soit 50 et 86 degrés en Fahrenheit), on peut affirmer qu'il y a 20°C ou 36°F d'écart entre les deux villes, mais on ne peut pas affirmer qu'il fait trois fois plus chaud à Mumbai. Presque toutes les mesures statistiques sur une variable d'intervalle peuvent être calculées, excepté le coefficient de variation et la moyenne géométrique puisqu'il n'y a pas de vrai zéro et d'intervalles constants entre les valeurs.  À l'inverse, les **variables de rapport** ont une échelle absolue, c'est-à-dire que les intervalles entre les valeurs sont constants et elles ont un vrai zéro. Elles peuvent ainsi manipuler par addition, soutraction, multiplication et division. Par exemple, le prix d'un produit exprimé dans une unité monétaire ou la distance exprimée dans le système métrique sont des variables de rapport. Un vélo dont le prix affiché est de 1000$ est bien deux fois plus cher qu'un autre à 500$, une piste cyclable hors rue à 25 mètres d'un tronçon routier le plus proche est bien quatre fois plus proche qu'un autre à 100 mètres.

**Une variable semi-quantitative**, appelée aussi variable quantitative ordonnée, est une variable discrète ou continue dont les valeurs ont été regroupées en classes hiérarchisées. Par exemple, l'âge est une variable continue pouvant être transformée avec les groupes d'âge ordonnés suivants : *moins 25 ans*, *25 à 44 ans*, *45 à 64 ans* et *65 ans et plus*.


## Les types de données {#sect22}

Différentes types de données sont utilisées en sciences sociales. L'objectif ici n'est pas de les décrire en détail, mais plutôt de donner quelques courtes définitions. En fonction de votre question de recherche et des bases de données disponibles ou non, il s'agira de sélectionner le ou les types de données les plus appropriés à votre sujet.

### Données secondaires *versus* données primaires {#sect221}

Les données secondaires sont des données qui existent déjà au début de votre projet de recherche : pas besoin de les collecter, il suffit de les exploiter! Une multitude de données de recensements ou d'enquêtes de Statistique Canada sont disponibles et largement exploitées en sciences sociales (par exemple, l'enquête nationale auprès des ménages – ENM, l'enquête sur la dynamique du marché du travail et du revenu – EDTR, l'enquête longitudinale auprès des immigrants – ELIC, etc.). 
  
::: {.bloc_notes .bloc_notes_png data-latex="{blocs/notes}"}
Au Canada, les chercheurs (étudiants et professeurs) ont accès aux microdonnées des enquêtes de Statistique Canada dans les Centres de données de recherche (CDR). Vous pouvez consulter le moteur de recherche du ([CRDCN](https://crdcn.org/fr/donn%C3%A9es){target="_blank"}) afin d'explorer les différentes enquêtes disponibles.

Au Québec, l'accès à ces enquêtes est possible dans les différentes antennes du Centre interuniversitaire québécois de statistiques sociales de Statistique Canada ([CIQSS](https://www.ciqss.org/){target="_blank"}).
:::

Par opposition, les données primaires n'existent pas quand vous démarrez votre projet : vous devez les collecter spécifiquement pour votre étude! Par exemple, une chercheure souhaitant analyser l'exposition des cyclistes au bruit et à la pollution dans une ville donnée devra réaliser une collecte de données avec idéalement plusieurs participants (équipés de différents capteurs), et ce, sur plusieurs jours. 
Une collecte de données primaires est peut être aussi réalisée avec une enquête par sondage. Brièvement, réaliser une collecte de données primaires nécessite différentes phases complexes comme la définition de la méthode de collecte, de la population à l'étude, l’estimation de la taille de l'échantillon, la validation des outils de collecte avec une phase de test, la réalisation de la collecte, la structuration, la gestion et l'exploitation de données collectées. Finalement, dans le milieu académique, une collecte de données primaires auprès d'individus doit être approuvée par le comité d'éthique de la recherche de l'université auquel est affilié le responsable du projet de recherche (qu'il soit professeur, chercheur ou étudiant).

###  Données longitudinales *versus* données transversales  {#sect222}
 
### Données aspatiales versus données spatiales {#sect223}

DÉCOUPAGE GÉOGRAPHIQUE ET UNIVERS GÉOGRAPHIQUE
Notion d'erreur écologique

### Données individuelles *versus* données agrégées  {#sect224}


::: {.bloc_notes .bloc_notes_png data-latex="{blocs/notes}"}
LES TYPES CI-DESSUS NE SONT PAS EXCLUSIFS

par exemple, données OSM : données spatiales longitidunales car numérisées depuis le début (historique des numérisations)
:::


::: {.bloc_astuce .bloc_astuce_png data-latex="{blocs/astuce}"}
FUSION DE DONNÉES
appariement de données administratives

:::

## Retour sur la statistique descriptive et la statistique inférentielle (ENCADRÉ)  {#sect23}

* **Population versus échantillon**


### TEST ET INFÉRENCE
### La notion de bootstrap : 
## LA NOTION DE VALEUR DE P ou la mettre
mesures sur populations complètes ou Échantillons



## La notion de distribution (JÉRÉMY) {#sect24}



## Statistiques descriptives sur des variables quantitatives {#sect25}

### Les paramètres de tendance centrale {#sect251}

Trois mesures de tendance centrale permettent de résumer rapidement une variable quantitative :

* la **moyenne arithmétique** est simplement la somme des données d'une variable divisée par le nombre d'observations ($n$), soit $\frac{\sum_{i=1}^n x_i}{n}$ notée $\mu$ (prononcez *mu*) pour des données pour une population et $\bar{x}$ (prononcez *x barre*) pour un échantillon.
* la **médiane** est la valeur qui coupe la distribution d'une variable d'une population ou d'un échantillon en deux parties égales. Autrement dit, 50% des valeurs des observations lui sont supérieures et 50% lui sont inférieures.
* le **mode** est la valeur la plus fréquente parmi un ensemble d'observations pour une variable. Il s'applique ainsi des variables discrètes (avec un nombre fini de valeurs discrètes dans un intervalle donné) et non à des variables continues (avec un nombre infini de valeurs réelles dans un intervalle donné). Prenons deux variables, l'une discrète relative au nombre d'accidents par intersection (avec $X \in \left[0,20\right]$) et l'autre continue relative à la distance de dépassement (en mètres) d'un cycliste par un véhicule motorisé (avec $X \in \left[0,5\right]$). Pour la première, le mode – la valeur la plus fréquente – est certainement 0. Pour la seconde, identifier le mode n'est pas pertinent puisqu'il peut y un nombre infini de valeurs entre 0 et 5 mètres.

Il convient de ne pas confondre moyenne et médiane ! Dans le tableau \@ref(tab:tableRevMoyMed), nous avons reporté les valeurs moyennes et médianes des revenus ménages pour les municipalités de l'île de Montréal en 2015. Par exemple, les 8685 ménages résidant à Wesmount disposaient en moyenne d'un revenu de 295099\$; la moitié de ces 8685 ménages avaient un revenu inférieur à 100153\$  et l'autre moitié un revenu supérieur à cette valeur (médiane). Cela démontre clairement que la moyenne peut être grandement affectée par des valeurs extrêmes (faibles ou fortes); autrement dit, plus l'écart entre les valeurs de la moyenne et la médiane est importante, plus les données de la variable sont inégalement réparties. À Westmount, soit la municipalité la plus nantie de l'île de Montréal, les valeurs extrêmes sont des ménages avec des revenus très élevés tirant fortement la moyenne vers le haut. À l'inverse, le faible écart entre les valeurs moyenne et médiane dans la municipalité de Montréal-Est (58594\$ versus 50318\$) soulignent que les revenus des ménages sont plus également répartis. Cela explique que pour comparer les revenus totaux ou d'emploi entre différents groupes (selon le sexe, le groupe d'âge, le niveau d'éducation, la municipalité ou région métropolitaine, etc.), on prévilégie habituellement l'utilisation des revenus médians.

```{r tableRevMoyMed, echo=FALSE, message=FALSE, warning=FALSE}
df <-  read.csv("data/univariee/revenu.csv")
df2 <- df[, c("Muni","NMenages","RevMoyM","RevMedM")]

knitr::kable(df2,
  format.args = list(decimal.mark = ",", big.mark = " "),   
  col.names = c("Municipalité","Nombre de ménages", "Revenu moyen","Revenu médian"),
  caption = "Revenus moyens et médians des ménages en dollars, municipalités de l'île de Montréal, 2015",
  booktabs = TRUE, valign = 't', row.names = FALSE)
```

### Les paramètres de position {#sect252}

Les paramètres de position permettent de diviser une distributions en _n_ parties égales.

* Les **quartiles** qui divisent une distribution en quatre parties (25%) :
  + Q1 (25%), soit le quartile inférieur ou premier quartile;
  + Q2 (50%), soit la médiane;
  + Q3 (75%), soit le quartile supérieur ou troisième quartile.
* Les **quintiles** qui divisent une distribution en cinq parties égales (20%).
* Les **déciles** (de D1 à D9) qui divisent une distribution en dix parties égales (10%).
* Les **centiles** (de C1 à C99) qui divisent une distribution en cent parties égales (1%).

En cartographie, les quartiles et les quintiles sont souvent utilisés pour discrétiser une variable quantitative (continue ou discrète) en quatre ou cinq classes et plus rarement, en huit ou dix classes. Avec les quartiles, les bornes des classes qui comprendront chacune 25% des untiés spatiales seront ainsi définies comme suit : [Min à Q1], [Q1 à Q2], [Q2 à Q3] et [Q3 à Max]. La méthode discrétisation selon les quartiles ou quintiles permet alors de repérer, en un coup d'œil, à quelle tranche de 25% ou 20% des données appartient chacune des unités spatiales. Cette méthode de discrétisation est aussi utile pour comparer plusieurs cartes et vérifier si deux phénomènes sont ou non colocalisés [@pumain1994]. En guise d'exemple, les pourcentages de personnes à faible revenu et de locataires par secteur de recensement ont clairement des distributions spatiales très semblables dans la région métropolitaine de Montréal en 2016 (figure \@ref(fig:figunivarie2)).



```{r figunivarie2, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Exemples de cartographie avec une discrétisation selon les quantiles",  out.width='85%'}
knitr::include_graphics('images/univariee/figure2.jpg', dpi = NA)
```

Une lecture attentive des valeurs des centiles permet de repérer la présence de valeurs extrêmes voire aberrantes dans un jeu de données. Il n'est donc pas rare de les voir reportées dans un tableau de statistiques descriptives d'un article scientifique, et ce, afin de décrire succinctement les variables à l'étude. Par exemple, dans une étude récente comparant les niveaux d'exposition au bruit des cyclistes dans trois villes  [@2020_1], les auteurs reportent à la fois les valeurs moyennes et celles de plusieurs centiles. Globalement, la lecture des valeurs moyennes permet de constater que, sur la base des données collectées, Paris est bien plus bruyante que Montréal et Copenhague (73,4 dB(A) contre 70,7 et 68,4, tableau \@ref(tab:tableCentiles)). Compte tenu de l'échelle logarithmique du bruit et du taux de change de 3, la différence de 5 dB(A) entre les valeurs moyennes du bruit de Copenhague et de Paris peut être considérée comme une multiplication de l'énergie sonore par plus de 3. Pour Paris, l'analyse des quartiles montre que durant 25% du temps des trajets à vélo (plus de 63 heures de collecte), les participants ont été exposés à des niveaux de bruit soit inférieurs à 69,1 dB(A) (premier quartile), soit supérieurs à 74 dB(A). Quant à l'analyse des centiles, elle permet de constater que durant 5% et 10% du temps, les participants étaient exposés à des niveaux de bruit très élevés, dépassant 77 dB(A) (C90=76 et C90=77,2).

```{r tableCentiles, echo=FALSE, message=FALSE, warning=FALSE}
options(knitr.kable.NA = '')
df <- data.frame(
  Centiles = c("N","Moyenne de bruit","Centiles","1","5","10", "25 (premier quartile)", "50 (médiane)", "75 (troisième quartile)", "90", "95", "99"),
  C = c(6212,68.4,NA,57.5,59.1,60.3,62.7,66.0,69.2,71.9,73.3,76.5),
  M = c(4723,70.7,NA,59.2,61.1,62.3,64.5,67.7,71.0,73.7,75.2,78.9),
  P = c(3793,73.4,NA,62.3,65.0,66.5,69.1,71.6,74.0,76.0,77.2,81.0))

knitr::kable(df,
  format.args = list(decimal.mark = ",", big.mark = " "),   
  col.names = c("Statistiques","Copenhague", "Montréal","Paris"),
  caption = "Stastistiques descriptives de l'exposition au bruit des cyclistes par minute dans trois villes (dB(A), Laeq 1min)",
  booktabs = TRUE, valign = 't', row.names = FALSE)
```


### Les paramètres de dispersion {#sect253}
Cinq principales mesures de dispersion permettent d'évaluer la variabilité des valeurs d'une variable quantitative : l'étendue, l'écart interquartile, la variance, l'écart-type et le coefficient de variation. Notez d'emblée que cette dernière mesure ne s'applique pas à des variables d'intervalle (section \@ref(sect2122)).

* **L'étendue** ou est la différence entre les valeurs minimale et maximale d'une variable, soit l'intervalle de variation des valeurs dans laquelle elle a été mesurée. Il convient d'analyser avec prudence cette mesure puisqu'elle inclut dans son calcul des valeurs potentiellement extrêmes voire aberrantes (faibles ou fortes).

* **L'intervalle ou écart interquartile** est la différence entre les troisième et premier quartiles ($Q3 − Q1$). Il représente ainsi une mesure de la dispersion des valeurs de 50% des observations centrales de la distribution. Plus la valeur de l'écart interquartile est élevée, plus la dispersion des 50% des observations centrales est forte. Contrairement à l'étendue, cette mesure élimine l'influence des valeurs extrêmes puisqu'elle ne tient compte que des 25% des observations les plus faibles [Min à Q1] et des 25% observations les plus fortes [Q3 à Max]. Graphiquement, l'intervalle interquartile est représenté à l'aide d'une boîte à moustache (*boxplot* en anglais) : plus l'intervalle interquartile sera grand, plus la boîte sera allongée (figure \@ref(fig:figunivarie3))

```{r figunivarie3, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Graphique en violon, boîte à moustache et intervalle interquartile",  out.width='30%'}
knitr::include_graphics('images/univariee/figure3.jpg', dpi = NA)
```

* **La variance** est la somme des déviations à la moyenne au carré (numérateur) divisée par le nombre d'observations pour une population ($\sigma^2$) ou divisée par le nombre d'observations moins une ($s^2$) pour un échantillon (eq. \@ref(eq:variance)). Puisque les déviations à la moyenne sont mises au carré, la valeur de variance (tout comme celle de l'écart-type) sera toujours positive. Plus sa valeur est élevée, plus les observations sont dispersées autour de la moyenne. La variance représente ainsi l'écart au carré moyen des observations à la moyenne. 

\begin{equation} 
\sigma^2=\frac{\sum_{i=1}^n (x_{i}-\mu)^2}{n} \text{ ou } s^2=\frac{\sum_{i=1}^n (x_{i}-\bar{x})^2}{n-1}
(\#eq:variance)
\end{equation}

* **L'écart-type** est la racine carrée de la variance (eq. \@ref(eq:ecartype)). Rappelez-vous que la variance est calculée à partir des déviations à la moyenne mises au carré. Étant donné que l'écart-type est la racine carré de la variance, il est donc évalué dans les mêmes unités que la variable, contrairement à la variance. Bien entendu, comme pour la variance, plus la valeur de l'écart-type est élevé, plus la distribution des observations autour de la moyenne est dispersée.

\begin{equation} 
\sigma=\sqrt{\sigma^2}=\sqrt{\frac{\sum_{i=1}^n (x_{i}-\mu)^2}{n}} \text{ ou } s=\sqrt{s^2}=\sqrt{\frac{\sum_{i=1}^n (x_{i}-\bar{x})^2}{n-1}}
(\#eq:ecartype)
\end{equation}

::: {.bloc_notes .bloc_notes_png data-latex="{blocs/notes}"}
Les formules des variances et des écart-types pour une population et un échantillon sont très similaires : seul le dénominateur change avec $n$ *versus* $n-1$ observations. Par conséquent, plus le nombre d'observations de votre jeu de données sera important, plus l'écart entre ces deux mesures de dispersion pour une population et une échantillon sera minime.

Comme dans la plupart des logiciels de statistique, les fonctions de base `var` et `sd` de ![](images/Rlogo.png) calculent la variance et l'écart-type pour un échantillon ($n-1$ au dénominateur). Si vous souhaitez les calculer pour une population, adaptez la syntaxe ci-dessous dans laquelle `df$var1` représente la variable intitulée `var1` présente dans un *dataframe* nommé `df`.

`var.p <- mean((df$var1 - mean(df$var1))^2)` 

`sd.p <- sqrt(mean((df$var1 - mean(df$var1))^2))` 
:::


* **Le coefficient de variation (CV)** est le rapport entre l'écart-type et la moyenne, représentant ainsi une standardisation de l'écart-type, ou en d'autres termes, une mesure de dispersion relative (eq. \@ref(eq:cv)). L'écart-type étant exprimé dans l'unité de mesure de la variable, il ne peut pas être utilisé pour comparer les dispersions de variables exprimées des unités de mesure différentes (par exemple, en pourcentage, en kilomètres, en dollars, etc.). Pour y remédier, on utilisera le coefficient de variation : une variable est plus dispersée qu'une autre si la valeur de son CV est plus élevée. Certains préfèreront multiplier la valeur du CV par 100 : l'écart-type est alors exprimée en pourcentage de la moyenne.


\begin{equation} 
CV=\frac{\sigma}{\mu} \text{ ou } CV=\frac{s^2}{\bar{x}}
(\#eq:cv)
\end{equation}


Illustrons comment calculer les cinq mesures de dispersion précédemment décrites à partir de valeurs fictives pour huit observations (colonne intitulée $x_i$ au tableau \@ref(tab:datavar)). Les différentes statistiques reportées dans ce tableau sont calculées comme suit :

* La **moyenne** est la somme divisée par le nombre d'observations, soit $248/8=31$.
* L'**étendue** est la différence entre les valeurs maximale et minimale, soit $40-22=30$.
* Les quartiles coupent la distribution en quatre parties égales. Avec huit observations triées par ordre croissant, **le premier quartile** est égale la valeur de la 2^e^ observation (soit 25), la **médiane** à celle de la 4^e^ (30), le **troisième quartile** à celle de la 6^e^ (35).
* **L'écart interquartile** est la différence entre Q3 et Q1, soit $35-25=10$.

* La seconde colonne du tableau est l'écart à la moyenne ($x_i-\bar{x}$), soit $22 - 31 = -9$ pour l'observation *1*; la somme de ces écarts est toujours égale à 0. La troisième colonne est cette déviation mise au carré ($(x_i-\bar{x})^2$), soit $-9^2 = 81$ toujours pour l'observation *1*. La somme de ces déviations à la moyenne au carré ($268$) représente le numérateur de la variance (eq. \@ref(eq:variance)). En divisant cette somme par le nombre d'observations, on obtient la **variance pour une population** ($268/8=33,5$) tandis que la **variance d'un échantillon** est égale à $268/(8-1)=38,29$.

* L'écart-type est la racine carrée de la variance (eq. \@ref(eq:ecartype)), soit $\sigma=\sqrt{33,5}=5,79$ et $s=\sqrt{38,29}=6,19$.

* Finalement, les valeurs des coefficients de variation (eq. \@ref(eq:cv)) sont de $5,79/31=0,19$ pour une population et $6,19/31=0,20$ pour un échantillon. 


```{r datavar, echo=FALSE, message=FALSE, warning=FALSE}
a <- c(22,25,27,30,32,35,37,40)
a <- sort(a)
n <- length(a)
df <- data.frame(
    id = as.character(c(1:n)),
    x = round(a,2),
    xi_mean = a - mean(a),
    numer = (a - mean(a))^2
)

df[n+1,1] <- "**Statistiques**"
n <- n+1
df[n+1,1] <- "N"
df[n+2,1] <- "Somme"
df[n+3,1] <- "Moyenne ($\\bar{x}$ ou $\\mu$)"
df[n+1,2] <- length(a)
df[n+2,2] <- sum(a)
df[n+2,4] <- sum(df$numer, na.rm = TRUE)
df[n+2,3] <- sum(df$xi_mean, na.rm = TRUE)

df[n+3,2] <- mean(a)
df[n+3,3] <- sum(df$xi_mean, na.rm = TRUE)/n
df[n+3,4] <- round(mean((a - mean(a))^2),2)
df[n+4,1] <- "Étendue"
df[n+4,2] <- max(a)-min(a)

df[n+5,1] <- "Premier quartile"
df[n+6,1] <- "Troisième quartile"
df[n+5,2] <- quantile(a, type = 1)[2]
df[n+6,2] <- quantile(a, type = 1)[4]

df[n+7,1] <- "Intervalle interquartile"
df[n+7,2] <- quantile(a, type = 1)[4]-quantile(a, type = 1)[2]

df[n+8,1] <- "Variance (population, $\\sigma^2$)"
df[n+9,1] <- "Écart-type (population, $\\sigma$)"
df[n+10,1] <- "Variance (échantillon, $s^2$)"
df[n+11,1] <- "Écart-type (échantillon, $s$)"
df[n+8,2] <- round(mean((a - mean(a))^2),2)
df[n+9,2] <- round(sqrt(mean((a - mean(a))^2)),2)
df[n+10,2] <- round(var(a),2)
df[n+11,2] <- round(sd(a),2)

df[n+12,1] <- "Coefficient de variation ($\\sigma / \\mu$)"
df[n+13,1] <- "Coefficient de variation ($s / \\bar{x}$)"

df[n+12,2] <- round(sqrt(mean((a - mean(a))^2))/mean(a),2)
df[n+13,2] <- round(sd(a)/mean(a),2)

opts <- options(knitr.kable.NA = "")
knitr::kable(df, 
  format.args = list(decimal.mark = ",", big.mark = " "),           
  col.names = c("Observation","$x_i$","$x_i-\\bar{x}$","$(x_i-\\bar{x})^2$"),
  caption = "Calcul des mesures de dispersion sur des données fictives",
  booktabs = TRUE, valign = 't', row.names = FALSE)
``` 

Le tableau \@ref(tab:datavar2) vise à démontrer à partir de trois variables comment certaines mesures de dispersion sont sensibles à l'unité de mesure et/ou aux valeurs extrêmes. 

Concernant **l'unité de mesure**, nous avons créé deux variables *A* et *B*, avec *B* étant simplement *A* multiplié par 10. Pour *A*, les valeurs de la moyenne, l'étendue et l'intervalle interquartile sont respectivement de 31, 18 et 10. Sans surprise, celles de B sont multipliées par 10 (310, 180, 100). La variance étant la moyenne des déviations à la moyenne au carré, elle est égale à 33,50 pour *A* et donc à $33,50\times10^2=3350$ pour *B*; l'écart-type de *B* est égal à celui de *A* multiplié par 10. Cela démontre que l'étendue, l'intervalle interquartile, la variance et l'écart-type sont des mesures de dispersion dépendantes de l'unité de mesure. Par contre, le coefficient de variation (CV) étant le rapport de l'écart-type avec la moyenne, il a la même valeur pour *A* et *B*, ce qui démontre que CV est bien une mesure de dispersion relative permettant de comparer des variables exprimées dans des unités de mesure différentes.

Concernant **la sensibilité aux valeurs extrêmes**, nous avons créé la variable *C* pour laquelle seule la huitième observation a une valeur différente (40 pour *A* et *105* pour B). Cette valeur de 105 pourrait être, soit une valeur extrême positive mesurée, soit une valeur aberrante (par exemple, si l'unité de mesure était un pourcentage variant de 0 à 100%). Cette valeur a un impact important sur la moyenne (31 contre 39,12) et l'étendue (18 contre 83) et corollairement sur la variance (33,50 contre 641,86), l'écart-type (5,79 contre 25,33) et le coefficient de variation (0,19 contre 0,65). Par contre, l'intervalle interquartile étant calculé sur 50% des observations centrales ($Q3-Q1$), il n'est pas affecté par cette valeur extrême.


```{r datavar2, echo=FALSE, message=FALSE, warning=FALSE}
a <- c(22,25,27,30,32,35,37,40)
c <- c(22,25,27,30,32,35,37,105)
b <- a*10
var.a <- round(mean((a - mean(a))^2),2)
var.b <- round(mean((b - mean(b))^2),2)
var.c <- round(mean((c - mean(c))^2),2)
sd.a <- round(sqrt(mean((a - mean(a))^2)),2)
sd.b <- round(sqrt(mean((b - mean(b))^2)),2)
sd.c <- round(sqrt(mean((c - mean(c))^2)),2)
a <- sort(a)
b <- sort(b)
c <- sort(c)

n <- length(a)
df <- data.frame(
  id = as.character(c(1:n)),
  A = round(a,2),
  B = round(b,2),
  C = round(c,2)
)

df[n+1,1] <- "**Statistiques**"
n <- n+1
df[n+1,1] <- "Moyenne ($\\mu$)"
df[n+2,1] <- "Étendue"
df[n+3,1] <- "Intervalle interquartile"
df[n+4,1] <- "Variance (population, $\\sigma^2$)"
df[n+5,1] <- "Écart-type (population, $\\sigma$)"
df[n+6,1] <- "Coefficient de variation ($\\sigma / \\mu$)"

df[n+1,2] <- mean(a)
df[n+2,2] <- max(a)-min(a)
df[n+3,2] <- quantile(a, type = 1)[4]-quantile(a, type = 1)[2]
df[n+4,2] <- round(var.a,2)
df[n+5,2] <- round(sd.a,2)
df[n+6,2] <- round(sd.a/mean(a),2)

df[n+1,3] <- mean(b)
df[n+2,3] <- max(b)-min(b)
df[n+3,3] <- quantile(b, type = 1)[4]-quantile(b, type = 1)[2]
df[n+4,3] <- round(var.b,2)
df[n+5,3] <- round(sd.b,2)
df[n+6,3] <- round(sd.b/mean(b),2)

df[n+1,4] <- round(mean(c),2)
df[n+2,4] <- max(c)-min(c)
df[n+3,4] <- quantile(c, type = 1)[4]-quantile(c, type = 1)[2]
df[n+4,4] <- round(var.c,2)
df[n+5,4] <- round(sd.c,2)
df[n+6,4] <- round(sd.c/mean(c),2)

opts <- options(knitr.kable.NA = "")
knitr::kable(df, 
  format.args = list(decimal.mark = ",", big.mark = " "),           
  col.names = c("Observation","A","B","C"),
  caption = "Illustration de la sensibilité des mesures de dispersion à l'unité de mesure et aux valeurs extrêmes",
  booktabs = TRUE, valign = 't', row.names = FALSE)
```



```{r resume, echo=FALSE, message=FALSE, warning=FALSE}
a <- c("Moyenne", "Étendue",
       "Intervalle interquartile",
       "Variance", "Écart-type", "Coefficient de variation")

b <- c("X","X","X","X","X","")
c <- c("X","X","","X","X","X")

df <- data.frame(
  Stat = a,
  Unite = b,
  Outlier = c
)

opts <- options(knitr.kable.NA = "")
knitr::kable(df, 
  col.names = c("Statistique","Unité de mesure","Valeurs extrêmes"),
  caption = "Résumé de la sensibilité de la moyenne et des mesures de dispersion",
  booktabs = TRUE, valign = 't', row.names = FALSE)
```


### Les paramètres de forme {#sect254}

#### Vérifier la normalité d'une variable quantitative

::: {.bloc_objectif .bloc_objectif_png data-latex="{blocs/objectif}"}
De nombreux méthodes statistiques qui seront abordées dans les chapitres suivants – entre autres, la corrélation de Pearson, les test *t* et l'analyse de variance, les régressions simple et multiple – requiert que la variable quantitative suive une **distribution normale** (nommée aussi **distribution gaussienne**).

Dans cette sous-section, nous décrirons trois démarches pour vérifier si la distribution d'une variables est normale : les coefficients d'asymétrie et d'applatissement (*skewness* et *kurtosis* en anglais), les graphiques (histogramme avec courbe normale, diagramme quantile-quantile), les tests de normalité (tests de Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors, Anderson-Darling et Jarque-Bera).

**Il est vivement recommander de réaliser les trois démarches !**
:::

Une distribution est normale quand elle est symétrique et mésokurtique (figure \@ref(fig:figFormeDistr)).

```{r figFormeDistr, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Formes d'une distribution et les coefficients d'asymétrie et d'aplatissement",  out.width='70%'}
knitr::include_graphics('images/univariee/FormeDistribution.jpg', dpi = NA)
```

##### Vérifier la normalité avec les coefficients d'asymétrie et d'applatissement

**Une distribution est dite symétrique** quand la moyenne arithmétique est au centre de la distribution, c'est-dire que les observations sont bien réparties de part et d'autre la moyenne qui sera alors égale à la médiane et au mode (on utilisera uniquement le mode pour une variable discrète et non pour une variable continue). Pour évaluer l'asysmétrie, on utilise habituellement le coefficient d'assymétrie (*skewness* en anglais). 

Sachez toutefois qu'il existe trois façons (formules) pour le calculer [@joanes1998comparing] : $g_1$ est la formule classique (eq. \@ref(eq:SkewType1), disponible dans ![](images/logos/Rlogo.png) avec la fonction `skewness` du *package* **moments**), $G_1$ est une version ajustée (eq. \@ref(eq:SkewType2), utilisée dans les logiciels SAS et SPSS notamment) et $b_1$ est une autre version ajustée (eq. \@ref(eq:SkewType3), utilisée par les logiciels MINITAB et BMDP). Nous verrons qu'avec les *packages* **DescTools** ou **e1071**, il possible de calculer ces trois méthodes. Aussi, pour des grands échantillons ($n>100$), il y a très peu de différences entre les résultats produits par ces trois formules [@joanes1998comparing]. Quel que soit la formule utilisée, le coefficient d'assymétrie s'interprète comme suit (figure \@ref(fig:asymetrie)) :

* quand la valeur du *skewness* est négative, la **distribution est asymétrique négative**. La distribution est alors tirée à gauche par des valeurs extrêmes faibles, mais peu nombreuses. On emploie souvent l'expression la queue de distribution est étirée vers la gauche. La moyenne est alors inférieure à la médiane.
* quand la valeur du *skewness* est égale à 0, **la distribution est symétrique** (la médiane sera égale à la moyenne). Pour une variable discrète, les valeurs du mode, de la moyenne et de la médiane seront égales.
* quand la valeur du *skewness* est positive, la **distribution est symétrique positive**. La distribution est alors tirée à droite par des valeurs extrêmes fortes, mais peu nombreuses. On emploie souvent l'expression la queue de distribution est étirée vers la droite. La moyenne est alors supérieure à la médiane. En sciences sociales, les variables de revenu (totaux ou d'emploi, des individus ou des ménages) ont souvent des distributions asysmétriques positives : la moyenne est affectée par quelques observations avec des valeurs de revenu très élevées et est ainsi supérieure à la médiane. En études urbaines, la densité de population pour des unités géographiques d'une métropole donnée (secteur de recensement par exemple) a aussi souvent une distribution asymétrique positive : quelques secteurs de recensement au centre de la métropole caractérisés par des valeurs de densité très élevées tirent la distribution vers la droite.

\begin{equation} 
g_1=\frac{ \frac{1}{n} \sum_{i=1}^n(x_i-\bar{x})^3} { \left[\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\right] ^\frac{3}{2}}
(\#eq:SkewType1)
\end{equation}

\begin{equation} 
G_1= \frac{\sqrt{n(n-1)}}{n-2} g_1
(\#eq:SkewType2)
\end{equation}

\begin{equation} 
b_1= \left( \frac{n-1}{n} \right) ^\frac{3}{2} g_1
(\#eq:SkewType3)
\end{equation}


```{r asymetrie, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Asymétrie d'une distribution", out.width='100%'}
library(DescTools)
library(ggplot2)
library(ggpubr)
library(SimDesign)

# Générer des distributions asymétriques
df <- data.frame(
  Normale  = rnorm(1500,0,1),
  Skewed_L = rValeMaurelli(1500, mean=0, sigma=1, skew=-1.4, kurt=3),
  Skewed_R = rValeMaurelli(1500, mean=0, sigma=1, skew=1.4, kurt=3),
  L = rValeMaurelli(1500, mean=0, sigma=1, skew=0, kurt=7),
  P = rValeMaurelli(1500, mean=0, sigma=1, skew=0, kurt=-1)
)

statsL <- c(mean(df$Skewed_L),median(df$Skewed_L),Skew(df$Skewed_L),Kurt(df$Skewed_L))
statsR <- c(mean(df$Skewed_R),median(df$Skewed_R),Skew(df$Skewed_R),Kurt(df$Skewed_R))
statsN <- c(mean(df$Normale),median(df$Normale),Skew(df$Normale),Kurt(df$Normale))

CaptionL <- paste("Moyenne = ",  round(statsL[1],2), 
                  "\nMédiane = ",  round(statsL[2],2),  
                  "\nSkewness = ",  round(statsL[3],2), 
                  sep="")

CaptionR <- paste("Moyenne = ",  round(statsR[1],2), 
                  "\nMédiane = ",  round(statsR[2],2),  
                  "\nSkewness = ",  round(statsR[3],2), 
                  sep="")

CaptionN <- paste("Moyenne = ",  round(statsN[1],2), 
                  "\nMédiane = ",  round(statsN[2],2),  
                  "\nSkewness = ",  round(statsN[3],2), 
                  sep="")

Gl <- ggplot(data = df, mapping = aes(x=Skewed_L))+
      geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
      stat_function(fun = dnorm, args = list(mean = mean(df$Skewed_L), sd = sd(df$Skewed_L)), color="black",size=1)+
      labs(title ="a. Asymétrie négative",
           subtitle = "Moyenne < Médiane",
           x="", 
           y="Densité",
           caption = CaptionL)+
      geom_vline(xintercept = statsL[1],color="cadetblue4", size=.8)+
      geom_vline(xintercept = statsL[2],color="coral4", size=.8)+
      theme(
      plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
      plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
      )
  
Gr <- ggplot(data = df, mapping = aes(x=Skewed_R))+
      geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
      stat_function(fun = dnorm, args = list(mean = mean(df$Skewed_R), sd = sd(df$Skewed_R)), color="black",size=1)+
      labs(title ="b. Asymétrie positive",
           subtitle = "Moyenne > Médiane",
           x="", y="",
           caption = CaptionR)+
      geom_vline(xintercept = statsR[1],color="cadetblue4", size=.8)+
      geom_vline(xintercept = statsR[2],color="coral4", size=.8)+
      theme(
      plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
      plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
      )

Gn <- ggplot(data = df, mapping = aes(x=Normale))+
      geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
      stat_function(fun = dnorm, args = list(mean = mean(df$Normale), sd = sd(df$Normale)), color="black",size=1)+
      labs(title ="c. Asymétrie nulle", 
           subtitle = "Moy. et méd. très semblables",
           x="", y="",
           caption = CaptionN)+
      geom_vline(xintercept = statsN[1],color="cadetblue4", size=.8)+
      geom_vline(xintercept = statsN[2],color="coral4", size=.8)+
      theme(
      plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
      plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
      )

ggarrange(Gl, Gn, Gr, ncol = 2, nrow=2)
```


**Pour évaluer l'applatissement d'une distribution**, on utilisera le coefficient d’aplatissement (*kurtosis* en anglais). Là encore, il existe trois formules pour le calculer (eq. \@ref(eq:KurtType1), \@ref(eq:KurtType2), \@ref(eq:KurtType3)) qui renverront des valeurs très sembables pour de grands échantillons [@joanes1998comparing]. Cette mesure s'interprète comme suit (figure \@ref(fig:asymetrie)) :

* quand la valeur du *kurtosis* est négative, la **distribution est platikurtique**. La distribution est dite plate, c'est-dire que la valeur de l'écart-type est importante (comparativement à une distribution normale), signalant une grande dispersion des valeurs de par et d'autre la moyenne.
* quand la valeur du *kurtosis* est égale à 0, **la distribution est mésokurtique**, ce qui est typique d'une distribution normale.
* quand la valeur du *kurtosis* est positive, la **distribution est leptokurtique**, signalant que l'écart-type (la dispersion des valeurs) est plutôt faible. Autrement dit, la dispersion des valeurs autour de la moyenne est faible.

\begin{equation} 
g_2=\frac{\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^4} {\left( \frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\right)^2}-3
(\#eq:KurtType1)
\end{equation}

\begin{equation} 
G_2 = \frac{n-1}{(n-2)(n-3)} \{(n+1) g_2 + 6\}
(\#eq:KurtType2)
\end{equation}

\begin{equation} 
b_2 = (g_2 + 3) (1 - 1/n)^2 - 3
(\#eq:KurtType3)
\end{equation}

```{r kurtosis, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Applatissement d'une distribution", out.width='100%'}
library(DescTools)
library(ggplot2)
library(ggpubr)
library(SimDesign)

# Générer des distributions asymétriques
statsM <- c(mean(df$Normale),median(df$Normale),Skew(df$M),Kurt(df$Normale))
statsL <- c(mean(df$L),median(df$L),Skew(df$L),Kurt(df$L))
statsP <- c(mean(df$P),median(df$P),Skew(df$P),Kurt(df$P))

CaptionN <- paste("\nKurtosis = ",  as.character(round(Kurt(df$Normale),2)), sep="")
CaptionL <- paste("\nKurtosis = ",  as.character(round(Kurt(df$L),2)), sep="")
CaptionP <- paste("\nKurtosis = ",  as.character(round(Kurt(df$P),2)), sep="")

Gl <- ggplot(data = df, mapping = aes(x=L))+
  geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$L), sd = sd(df$L)), color="black",size=1)+
  labs(title ="a. Distribution leptokurtique",
       x="",
       y="Densité",
       subtitle = CaptionL)+
  geom_vline(xintercept = statsL[1],color="cadetblue4", size=.8)+
  theme(
    plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
    plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
  )

Gm <- ggplot(data = df, mapping = aes(x=Normale))+
  geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$Normale), sd = sd(df$Normale)), color="black",size=1)+
  labs(title ="b. Distribution mésokurtique",
       x="", y="",
       subtitle = CaptionN)+
  geom_vline(xintercept = statsM[1],color="cadetblue4", size=.8)+
  theme(
    plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
    plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
  )

Gp <- ggplot(data = df, mapping = aes(x=P))+
  geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$P), sd = sd(df$P)), color="black",size=1)+
  labs(title ="c. Distribution platikurtique",
       x="", y="",
       subtitle = CaptionP)+
  geom_vline(xintercept = statsN[1],color="cadetblue4", size=.8)+
  theme(
    plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
    plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
  )

ggarrange(Gp, Gm, Gl, ncol = 2, nrow=2)
```

::: {.bloc_attention .bloc_attention_png data-latex="{blocs/attention}"}
Regardez attentivement les équations \@ref(eq:KurtType1), \@ref(eq:KurtType2), \@ref(eq:KurtType3); vous remarquez que pour $g_2$ et $b_2$, il y a une soustraction de $-3$ et une addition $+6$ pour $G_2$. On parle alors de *kurtosis* normalisé (*excess kurtosis* en anglais). Pour une distribution normale, il prendra la valeur de 0, comparativement à la valeur de 3 pour *kurtosis* non normalisé. Par conséquent, avant de calculer du *kurtosis*, il convient de s'assurer que la fonction que vous utilisez implémente une méthode de calcul normalisée (donnant une valeur de 0 pour une distribution normale). Par exemple, la fonction `Kurt` du *package* **DescTools** calcule les trois formules normalisées tandis que la fonction `kurtosis` du *package* **moments** renvoie un *kurtosis* non normalisée.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(DescTools)
library(moments)

#Générer une variable normalement distribuée avec 1000 observations
Normale <- rnorm(1500,0,1)
round(DescTools::Kurt(Normale),3)
round(moments::kurtosis(Normale),3)
```

:::

##### Vérifier la normalité avec des graphiques

Les graphiques sont un excellent moyen de vérifier visuellement si une distribution est normale ou pas. Bien entendu, les histogrammes, que nous avons déjà largement utilisés, sont un incontournable; à titre de rappel, ils permettent de vérifier les valeurs de la variable sont également réparties autour de la moyenne  (figure \@ref(fig:CourbeNormale)). Un autre type de graphique intéressant est le **diagramme  quantile-quantile** (*Q-Q plot* en anglais) qui permet de comparer la distribution d'une variable avec une distribution gaussienne (normale). Trois éléments composent ce graphique tel qu'illustré à la figure \@ref(fig:qqplot) :

* les points de la variable
* la distribution gaussienne (normale) représentée par une ligne
* l'intervalle de confiance à 5% de la distribution normale (en orange sur la figure).

Quand la variable est normale distribuée, les points seront situés le long de la ligne. Plus les points localisés en dehors de l'intervalle de confiance (bande orange) seront nombreux, plus la variable sera alors anormalement distribuée.


```{r CourbeNormale, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Distributions et courbe normale", out.width='90%'}
library(ggplot2)
library(reshape2)
library(qqplotr)

melted_dist <- melt(df,
                    distributions = c("Normale", "Skewed_L",
                                      "Skewed_R","student", "L", "P"))

names(melted_dist) <- c("distribution", "valeur")
melted_dist$distribution <- factor(melted_dist$distribution,
                          levels = c("Normale","Skewed_L","Skewed_R","L","P"),
                          labels = c("Normale",
                                     "Asymétrie négative",
                                     "Asymétrie positive",
                                     "Leptokurtique",
                                     "Platikurtique"))

ggplot(data = melted_dist, mapping = aes(x=valeur))+
  labs(caption = paste0("Skewness", "Kurtosis", sep=""))+
  geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(melted_dist$valeur), sd = sd(melted_dist$valeur)), color="black",size=1)+
  geom_vline(xintercept = mean(melted_dist$valeur),color="cadetblue4", size=.8)+
  labs(y="Densité", x="")+
  facet_wrap(vars(distribution), ncol=2, scales = "free")
```

```{r qqplot, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Diagramme quantile-quantile", out.width='90%'}
ggplot(data = melted_dist, aes(sample=valeur))+
    stat_qq_band(fill="bisque3")+
    stat_qq_line(color="black", size=.3) +
    stat_qq(color="black", size=1)+
    labs(y="Échantillon", x="théorique")+
  facet_wrap(vars(distribution), ncol=2, scales = "free")
```

##### Vérifier la normalité avec des tests de normalité 

Cinq principaux tests d'hypothèse permettent de vérifier la normalité d'une variable : les tests de **Kolmogorov-Smirnov** (KS), **Lilliefors** (LF), **Shapiro-Wilk** (SW), **Anderson-Darling**, et de **Jarque-Bera** (JB); sachez toutefois qu'il y en a d'autres non discutés ici (tests de D’Agostino–Pearson, Cramer–von Mises, e Ryan-Joiner, Shapiro–Francia, etc.). Pour les formules et une description détaillée de ces tests, on pourra consulter Razali et al. [-@razali2011power] ou Yap et Sim [-@yap2011comparisons]. **Quel test choisir ?** Plusieurs auteurs ont comparé ces différents tests à partir de plusieurs échantillons, et ce, en faisant varier la forme de la distribution et le nombre d'observations [@razali2011power;@yap2011comparisons]. Selon Razali et al. [-@razali2011power], le meilleur test semble être celui de Shapiro-Wilk, puis ceux de Anderson-Darling, Lilliefors et Kolmogorov-Smirnov. Yap et Sim [-@yap2011comparisons] concluent aussi que le Shapiro-Wilk semble être le plus performant.

Quoi qu'il en soit, ces cinq tests postulent que la variable suit une distribution gaussienne (hypothèse nulle, h<sub>0</sub>). Cela signifie que si la valeur de P associée à la valeur de chacun des tests est supérieure au seuil alpha choisi (habituellement $\alpha=0,05$), la distribution est normale. À l'inverse, si $P<0,05$, on choisit l'hypothèse alternative (h<sub>1</sub>), c'est-à-dire que la distribution est anormale.

```{r testnormalites, echo=FALSE, message=FALSE, warning=FALSE}
options(knitr.kable.NA = '')
df <- data.frame(
  Test = c("Kolmogorov-Smirnov",
           "Lilliefors",
           "Shapiro-Wilk",
           "Anderson-Darling",
           "Jarque-Bera"),
  Interpretation =
          c("Plus sa valeur est proche de zéro, plus la distribution est normale.  L'avantage de ce test est qu'il peut être utilisé pour vérifier si une variable si la distribution de n'importe quelle loi (autre que la loi normale).",
           "Ce test une adaptation du test de Kolmogorov-Smirnov. Plus sa valeur est proche de zéro, plus la distribution est normale.",
           "Si la valeur de la statistique de Shapiro-Wilk est proche de 1, alors la distribution est normale; et anormale quand elle est inférieure à 1.",
           "Ce test est une modification du test de Cramer-von Mises (CVM). Il peut être aussi utilisé pour tester d'autres distributions (uniforme, log-normale, exponentielle, Weibull, distribution de pareto généralisée, logistique, etc.).",
           "Basé sur un test du type multiplicateur de Lagrange, il utilise dans son calcul les valeurs du *Skewness* et du *Kurtosis*. Plus sa valeur s'approche de 0, plus la distribution est normale. Ce test est surtout utilisé pour vérifier si les résidus d'un modèle de régression linéaire sont normalement distribués, nous y reviendrons dans le chapitre sur la régression multiple. Il s'écrit $JB=\\frac{1}{6} \\left({g_1}^2+\\frac{{g_1}^2}{4} \\right)$ avec $g_1$ et $g_2$ sont respectivement les valeurs du *skewness* et du *kurtosis* de la variable (voir plus haut les équations \\@ref(eq:SkewType1) et \\@ref(eq:KurtType1)."),
  Fonction= c("`ks.test` du *package* **stats**",
           "`lillie.test` du *package* **nortest**",
           "`shapiro.test` du *package* **stats**",
           "`ad.test` du *package* **stats**",
           "`JarqueBeraTest` du *package* **DescTools**")
  )

knitr::kable(df,
  format.args = list(decimal.mark = ",", big.mark = " "),   
  col.names = c("Test","Propriétés et interprétation", "Fonction R"),
  caption = "Les différents tests d'hypothèse pour la normalité",
  booktabs = TRUE, valign = 't', row.names = FALSE)
```

Dans le tableau ci-dessous sont reportées les valeurs des différents tests pour les cinq types de distribution générées à la figure \@ref(fig:CourbeNormale). Sans surprise, pour l'ensemble des tests $P>0,05$ pour la distribution normale.

```{r calcultestnormalites, echo=FALSE, message=FALSE, warning=FALSE}
library(DescTools)
library(nortest)

df <- data.frame(
  Normale  = rnorm(500,0,1),
  Skewed_L = rValeMaurelli(500, mean=0, sigma=1, skew=1.4, kurt=3),
  Skewed_R = rValeMaurelli(500, mean=0, sigma=1, skew=-1.4, kurt=3),
  L = rValeMaurelli(500, mean=0, sigma=1, skew=0, kurt=7),
  P = rValeMaurelli(500, mean=0, sigma=1, skew=0, kurt=-1)
)

vars <- names(df)
S <- c()
K <- c()

KS <- c()
KS.p <- c()

LF <- c()
LF.p <- c()

SW <- c()
SW.p <- c()

AD <- c()
AD.p <- c()

JB <- c()
JB.p <- c()


i <- 1
for (e in vars){
  ksmirnov <- ks.test(df[[e]], "pnorm", mean=mean(df[[e]]), sd=sd(df[[e]]))
  lillie <- lillie.test(df[[e]])
  shapiro <- shapiro.test(df[[e]])
  AndDarling <- ad.test(df[[e]])
  JarqueB <- JarqueBeraTest(df[[e]], robust = TRUE)
  
  S[i] <- Skew(df[[e]])
  K[i] <- Kurt(df[[e]])
  
  KS[i] <- ksmirnov$statistic
  KS.p[i] <- ksmirnov$p.value 
  
  LF[i] <- lillie$statistic
  LF.p[i] <- lillie$p.value  
  
  SW[i] <- shapiro$statistic
  SW.p[i] <- shapiro$p.value

  AD[i] <- AndDarling$statistic
  AD.p[i] <- AndDarling$p.value
  
  JB[i] <- JarqueB$statistic
  JB.p[i] <- JarqueB$p.value

  i <- i+1
}

Tests <- data.frame(
  "S" = round(S,3),
  "K" = round(K,3),
  "KS" = round(KS,3),
  "LF" = round(LF,3),
  "SW" = round(SW,3),
  "AD" = round(AD,3),
  "JB" = round(JB,3),

 "KS.p" = round(KS.p,3),
 "LF.p" = round(LF.p,3),
 "SW.p" = round(SW.p,3),
 "AD.p" = round(AD.p,3),
 "JB.p" = round(JB.p,3)
)

Tests <- t(Tests)
row.names(Tests) <- c("Skewness","Kurtosis",
                      "Kolmogorov-Smirnov (KS)",
                      "Lilliefors (LF)",
                      "Shapiro-Wilk (SW)",
                      "Anderson-Darling (AD)",
                      "Jarque-Bera (JB)",
                      "KS (valeur p)",
                      "LF (valeur p)",
                      "SW (valeur p)",
                      "AD (valeur p)",
                      "JB (valeur p)")

knitr::kable(Tests,
             format.args = list(decimal.mark = ",", big.mark = " "),
             col.names = c("Normale","Asymétrie négative",
                           "Asymétrie positive","Leptokurtique", "Platikurtique"),
             caption = "Calculs des tests de normalité pour différentes distributions",
             booktabs = TRUE, valign = 't', row.names = TRUE)
```

::: {.bloc_attention .bloc_attention_png data-latex="{blocs/attention}"}
**Attention** ! La plupart des auteurs s'entendent sur le fait que ces tests sont très restrictifs : plus la taille de votre échantillon ($n$) est importante, plus les tests risquent de vous signaler que vos distributions sont anormales (à la lecture des valeurs de P).

Certains conseillent même de ne pas les utiliser quand $n>200$ et de vous fier uniquement aux graphiques (histogramme et diagramme Q-Q) !
:::

::: {.bloc_astuce .bloc_astuce_png data-latex="{blocs/astuce}"}
Bref, vérifier la normalité d'une variable n'est pas une tâche si simple. De nouveau, nous vous conseillons vivement de :

* construire les graphiques pour analyser visuellement la forme de la distribution (histogramme avec courbe normale et diagramme Q-Q)
* calculer le *skewness* et le *kurtosis*, 
* calculer plusieurs tests (minimalement Shapiro-Wilk et Kolmogorov-Smirnov)
* accorder une importance particulière aux graphiques lorsque vous traitez des grands échantillons ($n>200$).
:::


#### Vérifier d'autres formes de distribution

**A FAIRE PAR JÉRÉMY** AVEC les tests de Kolmogorov-Smirnov, Lilliefors et Anderson-Darling !!

### La transformation des variables {#sect255}

#### Les transformations visant à atteindre la normalité  {#sect2551}

Comme énoncé au début de cette section, plusieurs méthodes statistiques nécessitent que la variable quantitative soit normalement distribuée. C'est notamment le cas de l'analyse de variance et des tests T (abordés dans les chapitres suivants) qui fourniront des résultats plus robustes lorsque la variable normalement distribuée. Plusieurs transformations sont possibles, les plus courantes étant la racine carrée, le logarithme et l'inverse de la variable. Selon plusieurs auteurs (notamment, Tabacknick et *et al.* [-@tabachnick2007, p. 89], en fonction du type (positive ou négative) et du degré d'asymétrie les transformations suivantes sont possibles afin d'améliorer la normalité de la variable :

* Asymétrie positive modérée : la racine carrée de la variable *X* avec la fonction `sqrt(df$x)`. 
* Asymétrie positive importante : le logarithme de la variable avec `log10(df$x)`
* Asymétrie positive sévère : l'inverse de la variable avec `1/(df$x)`

::: {.bloc_astuce .bloc_astuce_png data-latex="{blocs/astuce}"}
Attention, pour une valeur égale ou inférieure à 0, on ne peut pas calculer une racine carrée ou un logarithme. Par conséquent, il convient de décaler simplement la distribution vers la droite afin de s'assurer qu'il n'y ait plus de valeurs négative ou égale à 0 :

* `sqrt(df$x - min(df$x+1))`  avec pour une asymétrie positive avec des valeurs négatives ou égales à 0
* `log(df$x - min(df$x+1))` pour une asymétrie positive avec des valeurs négatives ou égales à 0

Par exemple, si la valeur minimale de la variable est égale de -10, la valeur minimale de variable décalée sera ainsi de 1.
::: 

* Asymétrie négative modérée : `sqrt(max(df$x+1) - df$x)`. 
* Asymétrie négative importante :`log(max(df$x+1) - df$x)`
* Asymétrie négative sévère : `1/(max(df$x+1) - df$x)`


::: {.bloc_attention .bloc_attention_png data-latex="{blocs/attention}"}

**Transformation des variables pour atteindre la normalité : ce n'est pas toujours la panacée !**

La transformation des données fait et fera encore longtemps débat à la fois parmi les statisticiens que les débutants et utilisateurs avancés des méthodes quantitatives. Field et al. [-@field2012discovering, pp. 193] résument le tout avec humour : « To transform or not transform, that is the question ».

**Avantages de la transformation**

* L'obtention de *résultats plus robustes*.
* Dans une régression multiple linéaire, la transformation de la variable dépendante ou encore d'une ou plusieurs variables indépendantes permet souvent de *remédier au non-respect des hypothèses de base liées à la régression* (linéarité et homoscédasticité des erreurs, absence des valeurs aberrantes, etc.).

**Inconvénients de la transformation**

* *Une variable transformée est plus difficile à interpréter* puisque cela change l'unité de mesure de la variable. Prenons un exemple concret : vous souhaitez comparer les moyennes de revenu de deux groupes *A* et *B*. Vous obtenez une différence de 15000$, soit une valeur facile à interpréter. Par contre, si la variable a été préalablement transformée en logarithme, il est possible que vous obteniez une différence de 9, ce qui beaucoup moins parlant. Aussi, en transformant la variable en *log*, vous ne comparez plus les moyennes arithmétiques des deux groupes, mais plutôt leurs moyennes géométriques [@field2012discovering, pp. 193].

* *Pourquoi perdre la forme initiale de la distribution du phénomène à expliquer ?* Par exemple, on pourrait transformer une variable avec une distribution de type *student* afin qu'elle ait une distribution plus normale et l'introduire ensuite comme variable dépendante dans un modèle de régression linéaire. Or, il existe des modèles linéaires généralisés (GLM) qui permettent justement de modéliser une variable indépendante de type *student*.

**Démarche à suivre avant et après la transformation**

* *La transformation est-elle nécessaire ?* Ne transformer jamais une variable sans avoir analyser rigoureusement sa forme (histogramme avec courbe normale, *skewness* et *kurtosis* et test de normalité).

* *D'autres options à la transformation d'une variable dépendante (VD) sont-elles envisageables ?* Identifier la forme de la distribution de la VD et utilisez au besoin un modèle GLM adapté à cette distribution. Autrement dit, ne transformer automatiquement pour simplement pouvoir l'introduire dans une régression linéaire multiple.

* *La transformation a-t-elle un apport significatif ?* Premièrement, vérifier si la transformation utilisée (logarithme, racine carrée, inverse, etc.) améliore la normalité de la variable. Ce n'est toujours le cas, pourquoi c'est pire ! Prenez soin de comparer les histogrammes, les valeurs de *skewness*, *kurtosis* et des différents tests de normalité avant et après la transformation. Deuxièmement, comparer les résultats de vos analyses statistiques sans et avec transformation, et ce, dans une démarche coût-avantage. Vos résultats sont-ils bien plus robustes? Par exemple, un R^2^ qui passe de 0,597 à 0,602 avant et après la transformation des variables avec des associations significatives similaires, mais plus difficiles à interpréter (du fait des transformations), n'est pas forcément un gain significatif. La modélisation en sciences sociales ne vise à prédire la trajectoire d'un satellite ou l'atterrissage d'un engin sur Mars ! La précision à quatrième décimale n'est pas une condition ! Par conséquent, un modèle un peu moins robuste, mais plus facile à interpréter afin de valider est parfois préférable.
:::



#### Autres types de transformation  {#sect2552}

Les trois transformations les plus couramment utilisées sont :

* **La côte $z$** (*z score* en anglais) qui consiste à soustraire à chaque valeur sa moyenne (soit un centrage), puis à la diviser par son écart-type (soit une réduction) (eq. \@ref(eq:scorez)). Par conséquent, on parle aussi de variable centrée-réduite qui a comme propriétés intéressantes une moyenne égale à 0 et un écart-type égale à 1 (ainsi que la variance puisque 1^2^=1). Nous verrons que cette transformation est largement utilisée dans les méthodes de classification (REF PLUS TARD) et factorielles (REF PLUS TARD).

\begin{equation} 
z= \frac{x_i-\mu}{\sigma}
(\#eq:scorez)
\end{equation}

* **La transformation en rangs** qui consiste simplement à trier une variable en ordre croissant, puis à affecter le rang de chaque observation de 1 à $n$. Cette transformation est très utilisée quand la variable est très anormalement distribuée, notamment pour calculer le coefficient de corrélation de Speaman et certains tests non-paramétriques (RAPPEL SECTION).

* **La transformation sur une échelle de 0 à 1** (ou de 0 à 100) qui consiste à soustraite à chaque observation la valeur minimale et à diviser le tout par l'étendue (eq. \@ref(eq:t01)). 

\begin{equation} 
X_{\in\lbrack0-1\rbrack}= \frac{x_i-max}{max-min} \text{ ou } X_{\in\lbrack0-100\rbrack}= \frac{x_i-min}{max-min}\times100
(\#eq:t01)
\end{equation}


```{r AutresTransformation, echo=FALSE, message=FALSE, warning=FALSE}
a <- c(22,27,25,30,37,32,35,40)
b <- (a-mean(a))/sd(a)
d <- (a-min(a))/(max(a)-min(a))

df <- data.frame(
  id = as.character(c(1:length(a))),
  A = round(a,2),
  B = round(b,2),
  C = rank(a),
  D = round(d,2)
)

df[9,1] <- "Moyenne"
df[10,1] <- "Écart-type"
df[9,2] <- round(mean(a),2)
df[10,2] <- round(sd(a),2)
df[9,3] <- round(mean(b),2)
df[10,3] <- round(sd(b),2)

opts <- options(knitr.kable.NA = "")
knitr::kable(df, 
  format.args = list(decimal.mark = ",", big.mark = " "),           
  col.names = c("Observation","$x_i$","Côte $z$","Rang","0 à 1"),
  caption = "Illustration des trois tranformations",
  booktabs = TRUE, valign = 't', row.names = FALSE)
```


::: {.bloc_aller_loin .bloc_aller_loin_png data-latex="{blocs/aller_loin}"}
Ces trois transformations sont parfois utilisées pour générer un indice composite à partir de plusieurs variables ou encore dans une analyse de sensibilité avec les indices de Sobol [-@Sobol1993].
:::


### Mise en œuvre dans ![](images/Rlogo.png) {#sect256}

## Statistiques descriptives sur des variables qualitatives et semi-qualitatives {#sect26}

### Les fréquences {#sect261}

En guise de rappel, les variables nominales, ordinales et semi-quantitatives comprennent plusieurs modalités pour lesquelles plusieurs types de fréquences sont généralement calculées. Pour illustrer le tout, nous avons extrait du recensement de 2016 de Statistique Canada les effectifs des modalités de la variable sur le principal mode de transport utilisé pour les déplacements domicile-travail, et ce, pour la subdivision de recensement (MRC) de l'île de Montréal (tableau \@ref(tab:Frequences)). Les différents types de fréquences sont les suivantes :

* les fréquences absolues simples (**FAS**) ou fréquences observées représentent le nombre d'observations pour chacune des modalités. Par exemple, sur 857540 navetteurs domicile-travail (ligne total), 427530 utilisent un véhicule motorisé comme conducteur (automobile, camion ou fourgonnette) comme mode de transport principal, contre uniquement 30645 le vélo.

* les fréquences relatives simples (**FRS**) sont les proportions de chaque modalité sur le total ($30645/857540=0,036$); leur somme est égale 1. Elles peuvent bien entendu être exprimées en pourcentage ($30645/857540 \times 100=3,57$); leur somme est alors égale à 100%. Par exemple, 3,7% des navetteurs utilisent le vélo comme mode de transport principal.

* les fréquences absolues cumulées (**FAC**) représentent la fréquence observée (FAS) de la modalité auxquelles sont additionnées celles qui la précèdent. La valeur de la FAC pour la dernière est donc égale au total.

* À partir des fréquences absolues cumulées (FAC), il est alors possible de calculer les fréquences relatives cumulées (**FRC**) en proportion ($453930 / 857540 = 0,529$) et en pourcentage ($453930 / 857540 \times 100= 52,93$). Par exemple, plus de la moitié des navetteurs utilisent l'automobile comme mode de transport principal (passager ou conducteur).

```{r Frequences, echo=FALSE, message=FALSE, warning=FALSE}
M <- c("Véhicule motorisé (conducteur)",
       "Véhicule motorisé (passager)",
       "Transport en comun",
       "À pied",
       "Bicyclette",
       "Autre moyen")
Freq <- c(427530, 26400,295860,69410,30645,7695)

df <- data.frame(
  Mode = M,
  FAS = Freq
)

# Somme des fréquences aboslues simples
sumFAS <-  sum(df$FA)

# Fréquences relatives simples
df$FRS <- round(df$FAS / sum(df$FAS),3)
sumFRS <-  sum(df$FAS / sum(df$FAS))

# Fréquences relatives simples en pourcentages
df$FRSpct <- round(df$FAS / sum(df$FAS)*100,2)
sumFRSpct <-  sum(df$FAS / sum(df$FAS)*100)

# Fréquences absolues cumulées
df$FAC <- cumsum(df$FAS)

# Fréquences relatives cumulées
df$FRC <- round(cumsum(df$FAS)/sum(df$FAS),3)

# Fréquences relatives cumulées en pourcentages
df$FRCpct <- round(cumsum(df$FAS)/sum(df$FAS)*100,2)

n <- nrow(df)
df[n+1,1] <- "Total"
df[n+1,2] <-  sumFAS
df[n+1,3] <-  sumFRS
df[n+1,4] <-  sumFRSpct

opts <- options(knitr.kable.NA = "")
knitr::kable(df, 
  format.args = list(decimal.mark = ",", big.mark = " "),           
  col.names = c("Mode de transport","FAS","FRS","FRS (%)","FAC", "FRC", "FRC (%)"),
  caption = "Les différents types de fréquences sur une variable qualitative ou semi-qualitative",
  booktabs = TRUE, valign = 't', row.names = FALSE)
```


::: {.bloc_attention .bloc_attention_png data-latex="{blocs/attention}"}
**Les fréquences cumulées : peu pertinentes pour les variables nomimales**

Le calcul et l'analyse des fréquences cumulées (absolues et relatives) est très souvent inutiles pour les variables nominales.

Par exemple, au tableau \@ref(tab:Frequences), la fréquence cumulée relative (en %) est de 87,43% pour la troisième ligne. Cela signifie que 87,43% des navetteurs se déplacent en véhicule motorisé (conducteur ou passager) ou en transport en commun. Par contre, si la troisième modalité avait été *à pied*, le pourcentage aurait été de 61,02 ($52,93+8,09$). Si vous souhaitez calculer les fréquences cumulées sur une variable nominale, assurez-vous que l'ordre des modalités vous convient et de le modifier au besoin. Sinon, abstenez-vous de les calculer!


**Les fréquences cumulées : très utiles pour l'analyse pour des variables ordinales ou semi-quantitatives**

Pour des modalités hiérarchisées (variable ordinale ou semi-quantitative), l'analyse des fréquences cumulées (absolues et relatives) est par contre très intéressante. Par exemple, au tableau \@ref(tab:Frequences2), elles permet de constater rapidement que sur l'île de Montréal, un peu moins du très de la population à moins de 25 ans (35,95%) et 83,33% moins de 65 ans.
:::

```{r Frequences2, echo=FALSE, message=FALSE, warning=FALSE}
M <- c("0 à 14 ans",
       "15 à 24 ans",
       "25 à 44 ans",
       "45 à 64 ans",
       "65 à 84 ans",
       "85 ans et plus")
Freq <- c(304470,237555,582150,494205,271560,52100)

df <- data.frame(
  Mode = M,
  FAS = Freq
)
# Somme des fréquences aboslues simples
sumFAS <-  sum(df$FA)

# Fréquences relatives simples
df$FRS <- round(df$FAS / sum(df$FAS),3)
sumFRS <-  sum(df$FAS / sum(df$FAS))

# Fréquences relatives simples en pourcentages
df$FRSpct <- round(df$FAS / sum(df$FAS)*100,2)
sumFRSpct <-  sum(df$FAS / sum(df$FAS)*100)

# Fréquences absolues cumulées
df$FAC <- cumsum(df$FAS)

# Fréquences relatives cumulées
df$FRC <- round(cumsum(df$FAS)/sum(df$FAS),3)

# Fréquences relatives cumulées en pourcentages
df$FRCpct <- round(cumsum(df$FAS)/sum(df$FAS)*100,2)

n <- nrow(df)
df[n+1,1] <- "Total"
df[n+1,2] <-  sumFAS
df[n+1,3] <-  sumFRS
df[n+1,4] <-  sumFRSpct

opts <- options(knitr.kable.NA = "")
knitr::kable(df, 
  format.args = list(decimal.mark = ",", big.mark = " "),           
  col.names = c("Groupes d'âge","FAS","FRS","FRS (%)","FAC", "FRC", "FRC (%)"),
  caption = "Les différents types de fréquences sur une variable semi-qualitative",
  booktabs = TRUE, valign = 't', row.names = FALSE)
```

Différents graphiques peuvent être construits pour illustrer la répartition des observations : les graphiques en barres (verticales et horizontales) avec les fréquences absolues,  les diagrammes circulaires ou en anneau pour les fréquences relatives (figure \@ref(fig:GraphiquesFreq1)). Ces graphiques seront présentés plus en détail dans le chapitre suivant.

```{r GraphiquesFreq1, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Différents graphiques pour représenter les fréquences absolues et relatives", out.width='75%'}

library(dplyr)

Code <- c("A","B","C","D","E","F")
M <- c("0 à 14 ans",
       "15 à 24 ans",
       "25 à 44 ans",
       "45 à 64 ans",
       "65 à 84 ans",
       "85 ans et plus")
Freq <- c(304470,237555,582150,494205,271560,52100)

df <- data.frame(
  Groupe = M,
  FAS = Freq
)

df <- data.frame(
  Code = Code,
  Groupe = M,
  FAS = Freq
)
df$FRSpct <- round(df$FAS / sum(df$FAS),3)*100


mycols <- c("#0c2c84", "#67000d",  "#99000d", "#ef3b2c","#006d2c", "#41ab5d")

# BARRRES
options(scipen = 999)

G1 <- ggplot(data = df)+
  geom_bar(aes(x = Code, weight = FAS, fill=Groupe))+
  scale_fill_manual(values = mycols) +
  labs(x = "Groupe d'âge",
       y = 'Navetteurs')


# BARRRES

G2 <- ggplot(data = df)+
  geom_bar(aes(y = Code, weight = FAS, fill=Groupe))+
  scale_fill_manual(values = mycols) +
  labs(x = "Groupe d'âge",
       y = 'Navetteurs')

# DIAGRAMME CIRCULAIRE

# Ajouter la position de l'étiquette
df <- df %>%
  arrange(desc(Groupe)) %>%
  mutate(ypos = cumsum(FRSpct) - 0.5*FRSpct)

G3 <- ggplot(df, aes(x="", y=FRSpct, fill=Groupe)) +
  geom_bar(width = 1, stat = "identity", color = "white") +
  coord_polar("y", start=0) +
  theme_void() +
  geom_text(aes(y = ypos, label = FRSpct), color = "white", size=3) +
  scale_fill_manual(values = mycols)


# ANNEAU
df$ymax <- cumsum(df$FRSpct)
df$ymin <-  c(0, head(df$ymax, n=-1))

G4 <- ggplot(df, aes(ymax=ymax, ymin=ymin,
                        xmax=4, xmin=3,
                        y=FRSpct, fill=Groupe)) +
  geom_rect(stat="identity", color="white") +
  coord_polar("y", start=0) +
  theme_void() +
  geom_text(aes(x = 3.5,y = ypos, label = FRSpct), color = "white", size=3) +
  scale_fill_manual(values = mycols) +
  xlim(c(2,4))


list_plot <- list(G1, G2, G3, G4)
ggarrange(plotlist = list_plot, ncol = 2, nrow=2,
                      common.legend = TRUE, legend = "bottom")

options(scipen = 0)
```

### Mise en œuvre dans ![](images/Rlogo.png) {#sect262}

La syntaxe ci-dessous permet de calculer les différentes fréquences présentées au tableau \@ref(tab:Frequences2). Notez que pour les fréquences cumulées, nous utilisons la fonction `cumsum`.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Vecteur pour le noms des modalités
Modalite <- c("0 à 14 ans",
             "15 à 24 ans",
             "25 à 44 ans",
             "45 à 64 ans",
             "65 à 84 ans",
             "85 ans et plus")

# Vecteur pour les fréquences absolues simples (FAS)
Navetteurs <- c(304470,237555,582150,494205,271560,52100)

# Somme des FAS
sumFAS <-  sum(Navetteurs)

# Construction du dataframe avec les deux vecteurs
df <- data.frame(
  GroupeAge = Modalite, 
  FAS = Navetteurs,
  FRS = Navetteurs / sumFAS, 
  FRSpct = Freq / sumFAS * 100,
  FAC = cumsum(Freq),
  FRC = cumsum(Freq) / sumFAS,
  FRCpct = cumsum(Freq) / sumFAS * 100
  )
df
```

## Pour aller plus loin : les statistiques descriptives pondérées

