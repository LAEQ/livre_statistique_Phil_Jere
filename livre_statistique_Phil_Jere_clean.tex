% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  11pt,
  french,
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmainfont[]{Palatino Linotype}
  \setmonofont[Scale=0.8]{Source Code Pro}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Introduction aux méthodes quantitatives en sciences sociales avec R},
  pdfauthor={Philippe Apparicio et Jérémy Gelb},
  pdflang={fr},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{float}
\usepackage[bf,singlelinecheck=off]{caption}
\usepackage[justification=centering]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}
\definecolor{shadebluecolor}{RGB}{224,244,255}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\let\oldalign=\align
\renewcommand{\align}{\small\oldalign\normalsize}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\newenvironment{kframev}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframev{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframev{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadebluecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframev}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}

% Definition des environnements pour les blocks pdf---------------------------
\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframev}
  \small
  \item
  }
  {
  \end{kframev}
  \end{itemize}
  }


\newenvironment{bloc_notes}
  {\begin{rmdblock}{notes}}
  {\end{rmdblock}}


\newenvironment{bloc_aller_loin}
  {\begin{rmdblock}{aller_loin}}
  {\end{rmdblock}}


\newenvironment{bloc_astuce}
  {\begin{rmdblock}{astuce}}
  {\end{rmdblock}}


\newenvironment{bloc_attention}
  {\begin{rmdblock}{attention}}
  {\end{rmdblock}}


\newenvironment{bloc_package}
  {\begin{rmdblock}{package}}
  {\end{rmdblock}}

\newenvironment{bloc_objectif}
  {\begin{rmdblock}{objectif}}
  {\end{rmdblock}}


% -------------------------------------------------------------------

% -------------------modifying existing environments-----------------

% changing font size of code chunks
\let\oldShaded\Shaded
\def\Shaded{\oldShaded\small}

%insuring adding a line break for level 5
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
   {-3.25ex\@plus -1ex \@minus -.2ex}%
   {1.5ex \@plus .2ex}%
   {\normalfont\normalsize\bfseries}}
\makeatother

% -------------------------------------------------------------------

\makeatother
\usepackage{flafter}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifxetex
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{french}
\else
  \usepackage[shorthands=off,main=french]{babel}
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Introduction aux méthodes quantitatives en sciences sociales avec R}
\author{Philippe Apparicio et Jérémy Gelb}
\date{2021-08-30}

\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
%\thispagestyle{empty}
%\begin{center}
%\Large{To Jung Jae-sung (1982 -- 2018),}

%\large{a remarkably hard-working badminton player with a remarkably simple playing style}
%\includegraphics{images/dedication.pdf}
%\end{center}

%\setlength{\abovedisplayskip}{-5pt}
%\setlength{\abovedisplayshortskip}{-5pt}

\renewcommand*\contentsname{Table des matières}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{pruxe9face}{%
\chapter*{Préface}\label{pruxe9face}}
\addcontentsline{toc}{chapter}{Préface}

\hypertarget{comment-lire-ce-livre}{%
\section*{Comment lire ce livre}\label{comment-lire-ce-livre}}
\addcontentsline{toc}{section}{Comment lire ce livre}

Si vous googlez l'expression « comment lire un livre ? », vous trouverez une multitude de conseils et astuces. Pour ce livre, nous conseillons de le lire de gauche à droite et page par page. Plus sérieusement, il comprend plusieurs types de blocs de texte qui, on l'espère, faciliteront la lecture.

\begin{bloc_package}
\textbf{Bloc packages}: habituellement localisé en début du chapitre, il comprend la liste des \emph{packages} R utilisés pour un chapitre.

\end{bloc_package}

\begin{bloc_objectif}
\textbf{Bloc objectif}: comprend une description des objectifs d'une section.

\end{bloc_objectif}

\begin{bloc_notes}
\textbf{Bloc notes}: comprend une information secondaire sur une notion, un élément, une idée abordée dans une section.

\end{bloc_notes}

\begin{bloc_aller_loin}
\textbf{Bloc pour aller plus loin} : peut comprendre des références ou des extensions d'une méthode statistique abordée dans une section.

\end{bloc_aller_loin}

\begin{bloc_astuce}
\textbf{Bloc astuce}: décrit un élément qui vous facilera le vie : une propriété statistique, un \emph{package}, une fonction, une syntaxe R.

\end{bloc_astuce}

\begin{bloc_attention}
\textbf{Bloc attention}: comprend une notion ou un élément important à bien maîtriser.

\end{bloc_attention}

\hypertarget{pourquoi-faut-il-programmer-en-sciences-sociales}{%
\section*{Pourquoi faut-il programmer en sciences sociales ?}\label{pourquoi-faut-il-programmer-en-sciences-sociales}}
\addcontentsline{toc}{section}{Pourquoi faut-il programmer en sciences sociales ?}

Il est vrai que la programmation n'est pas la compétence qui vient tout de suite à l'esprit lorsque l'on pense aux sciences sociales. Pourtant, cette compétence est de plus en plus importante, et ce pour plusieurs raisons :

\begin{itemize}
\tightlist
\item
  Une part toujours plus grande des phénomènes sociaux se produisent ou peuvent s'observer au travers d'environnements numériques. Être capable d'exploiter efficacement ces outils permet d'extraire des données riches sur des phénomènes complexes, tels qu'en témoigne des études récentes sur la propagation de la désinformation sur les réseaux sociaux \citep{allcott2017social}, la migration des personnes \citep{spyratos2019quantifying}, la propagation et les risques de contamination de la COVID19 \citep{boulos2020geographical}, etc. Le plus souvent, les interfaces (API par exemple) permettant d'accéder à ces données nécessitent une base en programmation.
\item
  La quantité de données numériques ouvertes et accessibles en ligne croit chaque année sur des sujets très divers. La plupart des villes et des gouvernements ont maintenant leurs portails de données ouvertes auxquels s'ajoutent les données produites par des projets collaboratifs comme \href{https://www.openstreetmap.org}{OpenStreetMap} ou \href{https://noise-planet.org/map_noisecapture/index.html}{NoisePlanet}. Récupérer ces données et les structurer pour les utiliser à des fins de recherche nécessite le plus souvent des compétences en programmation.
\item
  Les méthodes d'analyse quantitative connaissent également un développement très important. Les logiciels propriétaires peinent à suivre la cadence de ce développement contrairement aux logiciels à code source ouvert qui permettent d'avoir accès aux dernières méthodes. Il est souvent long et coûteux de développer une interface graphique pour un logiciel, ce qui explique que la plupart de ces programmes en sont dépourvus et nécessitent alors de savoir programmer pour les utiliser.
\item
  Savoir programmer donne une liberté considérable en recherche. Cette compétence permet notamment de ne plus être limité aux fonctionnalités proposées par des logiciels spécifiques. Il devient possible d'innover tant en matière de structuration, d'analyse que de représentation des résultats en écrivant vos propres fonctions. Cette flexibilité contribue directement à la production d'une recherche de meilleure qualité et plus diversifiée.
\item
  Programmer permet également d'automatiser des tâches qui autrement seraient extrêmement répétitives. Déplacer et renommer une centaine de fichiers ? Retirer les lignes inutiles dans un ensemble de fichiers CSV et les compiler dans une seule base de données ? Tester parmi des milliers d'adresses lesquelles sont valides ? Récupérer chaque jour les messages postés sur un forum ? Autant de tâches faciles à automatiser si l'on sait programmer.
\item
  Dans un logiciel avec une interface graphique, il est compliqué de conserver un historique des opérations effectuées. Programmer permet au contraire de garder une trace de l'ensemble des actions effectuées au cours d'un projet de recherche. En effet, le code utilisé reste disponible et permet de reproduire la méthode et les résultats obtenus ce qui est essentiel dans le monde de la recherche. À cela s'ajoute le fait que chaque ligne de code que vous écrivez vient s'ajouter à un capital de code que vous possédez, car elles pourront être réutilisées dans d'autres projets !
\end{itemize}

\hypertarget{structure-du-livre}{%
\section*{Structure du livre}\label{structure-du-livre}}
\addcontentsline{toc}{section}{Structure du livre}

À écrire plus tard.

\hypertarget{remerciements}{%
\section*{Remerciements}\label{remerciements}}
\addcontentsline{toc}{section}{Remerciements}

Note au beau Cargo (chien Mira) qui nous supporte dans l'écriture du livre !

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/Cargo} 

}

\caption{Cargo, le plus beau}\label{fig:cargo}
\end{figure}

\hypertarget{auteurs}{%
\chapter*{À propos des auteurs}\label{auteurs}}
\addcontentsline{toc}{chapter}{À propos des auteurs}

\textbf{Philippe Apparicio} (\url{http://www.ucs.inrs.ca/philippe-apparicio}) est professeur titulaire au Centre Urbanisation Culture Société de l'INRS (\url{http://www.ucs.inrs.ca/}). Il enseigne au programme de maîtrise en études urbaines (\url{http://www.ucs.inrs.ca/ucs/etudier/programmes/etudes-urbaines}) les cours \emph{méthodes quantitatives appliquées aux études urbaines} et \emph{analyses spatiales appliquées aux études urbaines}. Il a aussi créé et enseigné, il y a plusieurs années, le cours \emph{systèmes d'information géographique appliqués aux études urbaines}. Durant les dernières années, il a offert plusieurs formations aux Écoles d'été du Centre interuniversitaire québécois de statistiques sociales (CIQSS, \url{https://www.ciqss.org/}). Titulaire de la Chaire de recherche du Canada (niveau 2) sur l'équité environnementale et la ville, il est le directeur du \textbf{laboratoire d'équité environnementale} (\url{http://laeq.ucs.inrs.ca}). Géographe de formation, ses intérêts de recherche actuels incluent la justice et l'équité environnementale, la pollution atmosphérique, le bruit et le vélo en ville. Il a publié une centaine d'articles scientifiques dans différents domaines des études urbaines et de la géographie.

\textbf{Jérémy Gelb} est candidat au doctorat en études urbaines à l'INRS (sous la supervision de Philippe Apparicio) et membre du \textbf{laboratoire d'équité environnementale} (\url{http://laeq.ucs.inrs.ca}). Son sujet de thèse porte sur l'exposition des cyclistes aux pollutions atmosphériques et sonores en milieu urbain. Il utilise quotidiennement des systèmes d'information géographique (SIG) et est tombé dans la marmite de l'\emph{open source} avec le triptyque QGIS, R et Python au début de sa maîtrise. Il a récemment développé deux packages R : \textbf{geocmeans} et \textbf{spNetwork}, permettant respectivement d'effectuer des analyses de classification floue non-supervisée pondérée spatialement et des estimations de densité par kernel sur réseau.

Philippe et Jérémy travaillent étroitement ensemble depuis déjà plusieurs années. Avec d'autres collègues, ils ont copublié plusieurs articles \citetext{\citealp{2021_1}; \citealp{2021_2}; \citealp{2021_3}; \citealp[;][]{2021_4}; \citealp{2020_1}; \citealp{2020_2}; \citealp{2020_3}; \citealp{2019_1}; \citealp{2019_2}; \citealp{2019_3}; \citealp{2020_1}; \citealp{2020_2}; \citealp{2018_1}; \citealp{2017_1}; \citealp{2016_1}}. Tous deux s'intéressent à l'exposition des cyclistes à la pollution atmosphérique et sonore dans plusieurs villes à travers le monde : Philippe ayant une préférence pour les collectes dans les villes des Suds (notamment indiennes, africaines et latino-américaines) et Jérémy dans les villes du Nord (européennes et nord-américaines).

\hypertarget{part-duxe9couverte-de-r}{%
\part{Découverte de R}\label{part-duxe9couverte-de-r}}

\hypertarget{chap01}{%
\chapter{Prise en main de R}\label{chap01}}

Dans ce chapitre, nous reviendrons brièvement sur l'histoire de R et la philosophie qui entoure le logiciel. Nous donnerons quelques conseils pour son installation et la mise en place d'un environnement de développement. Nous présenterons les principaux objets qui sous-tendent le travail effectué avec R (\emph{dataframe}, vecteur, matrice, etc.) et comment les manipuler avec des exemples appliqués. Enfin, nous terminerons cette section avec un tour d'horizon des capacités graphiques de R. Si vous maîtrisez déjà R, nullement besoin de lire ce chapitre !

\begin{bloc_package}

Dans ce chapitre, nous utiliserons principalement les \emph{packages} suivants :

\begin{itemize}
\tightlist
\item
  Pour importer des fichiers externes :

  \begin{itemize}
  \tightlist
  \item
    \textbf{foreign} pour entre autres les fichiers \emph{dbase} et ceux des logiciels SPSS et Stata
  \item
    \textbf{sas7bdat} pour les fichiers du logiciel SAS
  \item
    \textbf{xlsx} pour les fichiers Excel
  \end{itemize}
\item
  Pour manipuler des chaînes de caractères et des dates :

  \begin{itemize}
  \tightlist
  \item
    \textbf{stringr} pour les chaînes de caractères
  \item
    \textbf{lubridate} pour les dates
  \end{itemize}
\item
  Pour manipuler des données :

  \begin{itemize}
  \tightlist
  \item
    \textbf{dplyr } du \textbf{tidyverse} propose une grammaire pour manipuler et structurer des données.
  \end{itemize}
\end{itemize}


\end{bloc_package}

\hypertarget{sect011}{%
\section{Histoire et philosophie de R}\label{sect011}}

R est à la fois un langage de programmation et un logiciel libre (sous la licence publique générale GNU) dédié à l'analyse statistique et soutenu par une fondation : \emph{R foundation for Statistical computing}. Il est principalement écrit en C et Fortran.

R a été créé par Ross Ihaka et Robert Gentleman à l'Université d'Auckland en Nouvelle-Zélande. Si vous avez un jour l'occasion de passer dans le coin, une plaque est affichée dans le département de statistique de l'université, ça mérite le détour (figure \ref{fig:fig01}). Une version expérimentale a été publiée en 1996, mais la première version stable ne date que de 2000, il s'agit donc d'un logiciel relativement récent si on le compare à ses concurrents SPSS (1968) , SAS (1976) et Stata (1984).

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{images/introduction/plaque} 

}

\caption{Lieu de pélerinage de R}\label{fig:fig01}
\end{figure}

R a cependant réussi à s'imposer tant dans la milieu de la recherche que dans le secteur privé. Pour s'en convaincre, il suffit de lire l'excellent article concernant la popularité des logiciels d'analyse de données tiré du site \href{http://r4stats.com/articles/popularity}{r4stats.com} (figure \ref{fig:fig02}).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/introduction/r_citations} 

}

\caption{Nombre d'articles trouvés sur Google Scholar (Source : Robert A. Muenchen)}\label{fig:fig02}
\end{figure}

Les nombreux atouts de R justifient largement sa popularité sans cesse croissante :

\begin{itemize}
\tightlist
\item
  R est un logiciel à code source ouvert (\emph{open source}) et ainsi accessible à tous gratuitement.
\item
  Le développement du langage R est centralisé, mais la communauté peut créer et partager facilement des \emph{packages}. Les nouvelles méthodes sont ainsi rapidement implémentées comparativement aux logiciels propriétaires.
\item
  R est un logiciel multi-plateforme, fonctionnant sur Linux, Unix, Windows et Mac.
\item
  Comparativement à ses concurrents, R dispose d'excellentes solutions pour manipuler des données et réaliser des graphiques.
\item
  R dispose de nombreuses interfaces lui permettant de communiquer, notamment avec des systèmes de bases de données SQL et non SQL (MySQL, PostgresSQL, MongoDB, etc.), avec des systèmes de \emph{big data} (Spark, Hadoop), avec des systèmes d'information géographique (QGIS, ArcGIS) et même avec des services en ligne comme Microsoft Azure ou Amazon AWS.
\item
  R est un langage de programmation à part entière, ce qui lui donne plus de flexibilité que ses concurrents dans le domaine privé (SPSS, SAS, STATA). Avec R, vous pouvez accomplir des tâches aussi variées que : monter un site web, créer un robot collectant des données en ligne, combiner des fichiers PDF, composer des diapositives pour une présentation ou même éditer un livre (comme celui-ci), mais aussi et surtout réaliser des analyses statistiques.
\end{itemize}

Un des principaux attrait de R est la quantité astronomique de \emph{packages} actuellement disponibles. \textbf{Un \emph{package} est un ensemble de nouvelles fonctionnalités développées par un·e ou plusieurs utilisateurs·trices de R et mises à disposition de l'ensemble de la communauté}. Par exemple, le \emph{package} \textbf{ggplot2} est dédié à la réalisation de graphiques; les \emph{packages} \textbf{data.table} et \textbf{plyr} permettent de manipuler des tableaux de données; le \emph{package} \textbf{car} apporte de nombreux outils pour faciliter l'analyse de modèles de régressions, etc. Ce partage des \emph{packages} rend accessible à tous des méthodes d'analyses complexes et récentes et favorise grandement la reproductibilité de la recherche. Cependant, ce fonctionnement implique quelques désavantages :

\begin{itemize}
\tightlist
\item
  il existe généralement plusieurs \emph{packages} pour effectuer le même type d'analyse, ce qui peut devenir une source de confusion;
\item
  certains \emph{packages} cessent d'être mis à jour au fil des années, ce qui nécessite de leur trouver d'autres alternatives (et ainsi apprendre la syntaxe des nouveaux \emph{packages});
\item
  il est impératif de s'assurer de la fiabilité des \emph{packages} que vous souhaitez utiliser, car n'importe qui peut proposer un \emph{package}.
\end{itemize}

Il nous semble important de relativiser d'emblée la portée du dernier point. Il est rarement nécessaire de lire et analyser le code source d'un \emph{package} pour s'assurer de sa fiabilité. Nous ne sommes pas des spécialistes de tous les sujets et il peut être extrêmement ardu de comprendre la logique d'un code écrit par une autre personne. Nous vous recommandons donc de privilégier l'utilisation de \emph{packages} qui :

\begin{itemize}
\tightlist
\item
  ont fait l'objet d'une publication dans une revue à comité de lecture ou qui ont déjà été cités dans des études ayant fait l'objet d'une publication revue par les pairs;
\item
  font partie de projets comme \href{https://ropensci.github.io/reproducibility-guide/sections/introduction/}{ROpensci} prônant la vérification par les pairs ou subventionnés par des organisations comme \href{https://www.r-consortium.org/}{R Consortium}.
\item
  sont disponibles sur l'un des deux principaux répertoires de \emph{packages} R, soit \href{https://cran.r-project.org/}{CRAN} et \href{https://www.bioconductor.org/}{Bioconductor}.
\end{itemize}

Toujours pour nuancer notre propos, il convient de distinguer \emph{package} de \emph{package}! Certains d'entre eux sont des ensembles très complexes de fonctions permettant de réaliser des analyses poussées alors que d'autres sont des projets plus modestes dont l'objectif principal est de simplifier le travail des utilisateurs·trices. Ces derniers ressemblent à des petites boites à outils et font généralement moins l'objet d'une vérification intensive.

Pour conclure cette section, l'illustration partagée sur Twitter par Darren L Dahly résume avec humour la force du logiciel R et de sa communauté
(figure \ref{fig:fig03}) : R apparait clairement comme une communauté hétéroclyte, mais diversifiée et adaptable.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/introduction/softwares_and_cars} 

}

\caption{Métaphore sur les langages et programmes d'analyse statistique}\label{fig:fig03}
\end{figure}

Dans ce livre, nous détaillerons les \textbf{packages} utilisés dans chaque section avec un encadré spécifique, accompagné de l'icône présenté à la figure \ref{fig:fig04}.

\begin{figure}

{\centering \includegraphics[width=0.2\linewidth]{css/images/package} 

}

\caption{Icône des encadrés dédiés aux packages}\label{fig:fig04}
\end{figure}

\hypertarget{sect012}{%
\section{Environnement de travail}\label{sect012}}

Dans cette section, nous vous proposons une visite de l'environnement de travail classique R.

\hypertarget{sect0121}{%
\subsection{Installer R}\label{sect0121}}

La première étape pour travailler avec R est bien sûr de l'installer. Pour ce faire, il suffit de visiter le site web de \href{https://cran.r-project.org/}{CRAN} et de télécharger la dernière version de R en fonction de votre système d'exploitation : Windows, Linux ou Mac. Une fois installé, si vous démarrez R immédiatement, vous aurez alors accès à une console, plutôt rudimentaire, attendant sagement vos instructions (figure \ref{fig:fig05}).

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{images/introduction/r_console} 

}

\caption{La console de base de R}\label{fig:fig05}
\end{figure}

Notez que vous pouvez aussi télécharger des version plus anciennes de R en allant sur ce \href{https://cran.r-project.org/bin/windows/base/old/}{lien}. Ceci peut être intéressant lorsque vous voulez reproduire des résultats d'une autre étude ou que certains \emph{packages} ne sont plus disponibles dans les nouvelles versions.

\hypertarget{sect0122}{%
\subsection{L'environnement RStudio}\label{sect0122}}

Rares sont les utilisateurs·trices de R qui préfèrent travailler directement avec la console classique. Nous vous recommandons vivement d'utiliser RStudio, soit un environnement de développement dédié à R, offrant une intégration très intéressante d'une console, d'un éditeur de texte, d'une fenêtre de visualisation des données, d'une autre pour les graphiques, d'un accès à la documentation, etc. En d'autres termes, si R est un vélo minimaliste, RStudio permet d'y rajouter des freins, des vitesses, un porte-bagage, des gardes-boues et une selle confortable. Vous pouvez \href{https://rstudio.com/products/rstudio/download}{télécharger} et installer RStudio sur Windows, Linux et Mac. La version de base est gratuite, mais l'entreprise qui développe ce logiciel propose aussi des versions commerciales du logiciel qui assurent essentiellement un support technique. Il existe d'autres environnements de développement pour travailler avec R (VisualStudio, Jupyter, Tinn-R, Radiant, RIDE, etc.), mais RStudio offre à ce jour la meilleure option en terme de facilité d'installation, de prise en main et de fonctionnalités proposées (voir l'interface de RStudio à la figure \ref{fig:fig06}).

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{images/introduction/r_studio_01} 

}

\caption{Environnement de base de RStudio}\label{fig:fig06}
\end{figure}

Avant d'aller plus loin, notez que :

\begin{itemize}
\tightlist
\item
  La console actuellement ouverte dans RStudio vous informe de la version de R que vous utilisez. Vous pouvez en effet avoir plusieurs versions de R installées sur votre ordinateur et passer de l'une à l'autre avec RStudio. Pour cela, naviguez dans l'onglet \emph{Tools / Global Options} et dans le volet \emph{General}, vous pouvez sélectionner la version de R que vous souhaitez utiliser.
\item
  L'aspect de RStudio peut être modifié en navigant dans l'onglet \emph{Tools / Global Options} et dans le volet \emph{Appearance}. Nous avons une préférence pour le mode sombre avec le style \emph{pastel on dark}, mais libre à chacun de choisir le style qui lui convient.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{images/introduction/r_studio_02} 

}

\caption{RStudio avec le style pastel on dark}\label{fig:fig07}
\end{figure}

Une fois ces détails réglés, vous pouvez ouvrir votre première feuille de code en allant dans l'onglet \emph{File / New File / R Script}. Votre environnement est maintenant découpé en quatre fenêtres (figure \ref{fig:fig08}) :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  L'éditeur de code, vous permettant d'écrire le script que vous voulez exécuter et permettant de garder une trace de votre travail. Ce script peut être enregistré sur votre ordinateur avec l'extension \textbf{.R}, mais ce n'est qu'un simple fichier texte.
\item
  La console vous permettant d'exécuter votre code R et de voir les résultats s'afficher au fur et à mesure.
\item
  La fenêtre d'environnement vous montrant les objets, fonctions et jeux de données actuellement disponibles dans votre session (chargés dans la mémoire vive).
\item
  La fenêtre de l'aide, des graphiques et de l'explorateur de fichiers. Vous pouvez accéder ici à la documentation de R et des \emph{packages} que vous utilisez, aux sorties graphiques que vous produisez et aux dossiers de votre environnement de travail.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{images/introduction/r_studio_03} 

}

\caption{Les quatre fenêtres de RStudio}\label{fig:fig08}
\end{figure}

Prenons un bref exemple, tapez la syntaxe suivante dans l'éditeur de code (fenêtre 1 à la figure \ref{fig:fig08}) :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ma_somme <-}\StringTok{ }\DecValTok{4}\OperatorTok{+}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

Sélectionnez ensuite cette syntaxe (mettre en surbrillance avec la souris), quand vous utilisez le raccourci \emph{Ctrl+Enter} ou cliquez sur le bouton \emph{Run} (avec la flèche verte), cette syntaxe est envoyée à la console qui l'exécute immédiatement. Notez que rien ne se passe tant que le code n'est pas envoyé à la console. Il s'agit donc de deux étapes distinctes : écrire son code, puis l'envoyer à la console. Vous constaterez également qu'un objet \emph{ma\_somme} est apparu dans votre environnement et que sa valeur est bien 8. Votre console se ``souvient'' de cette valeur, elle est actuellement stockée dans votre mémoire vive sous le nom de \emph{ma\_somme} (figure \ref{fig:fig09}).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/introduction/r_studio_04} 

}

\caption{Les quatre fenêtres de RStudio}\label{fig:fig09}
\end{figure}

Pour conclure cette section, nous vous invitons à enregistrer votre première syntaxe R (\emph{File / Save As}) dans un fichier \textbf{.R} que vous pouvez appeler par exemple ``mon\_premier\_script.R''. Fermez ensuite RStudio, redémarrez le et ouvrez (\emph{File / Open File}) votre fichier ``mon\_premier\_script.R''. Vous pouvez constater que votre code est toujours présent, mais que votre environnement est vide tant que vous n'exécutez pas votre syntaxe. En effet, lorsque vous fermez RStudio, l'environnement est vidé pour libérer de la mémoire vive. Ceci peut poser problème lorsque certains codes sont très longs à exécuter, nous verrons donc plus tard comment enregistrer l'environnement en cours pour le recharger par la suite.

\hypertarget{sect0123}{%
\subsection{\texorpdfstring{Installer et charger un \emph{package}}{Installer et charger un package}}\label{sect0123}}

Dans la section sur la Philosophie de R, nous avons souligné la place centrale jouée par les \emph{packages}. Notez que les termes \textbf{paquet} et plus rarement \emph{librarie} sont parfois utilisés en français. Voyons ensemble comment installer un \emph{package} intitulé \textbf{lubridate}, qui nous permettra plus tard de manipuler des données temporelles.

\hypertarget{sect01231}{%
\subsubsection{\texorpdfstring{Installer un \emph{package} depuis CRAN}{Installer un package depuis CRAN}}\label{sect01231}}

Pour installer un \emph{package}, vous devez être connecté à Internet puisque R va accéder au répertoire de \emph{packages} \emph{CRAN} pour télécharger le \emph{package} et l'installer sur votre machine. Cette opération est réalisée avec la fonction \texttt{install.packages}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"lubridate"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Notez qu'une fois que le \emph{package} est installé, vous n'aurez plus besoin de le refaire. Le \emph{package} est disponible localement sur votre ordinateur, à moins de le désinstaller explicitement avec la fonction \texttt{remove.packages}.

\hypertarget{sect01232}{%
\subsubsection{\texorpdfstring{Installer un \emph{package} depuis GitHub}{Installer un package depuis GitHub}}\label{sect01232}}

\emph{CRAN} est le répertoire officiel des \emph{packages} de R. Vous pouvez cependant télécharger des \emph{packages} provenant d'autres sources. Très souvent, les \emph{packages} sont disponibles sur le site web \href{https://github.com/}{GitHub} et l'on peut même y trouver des versions en développement avec des fonctionnalités encore non intégrées dans la version sur \emph{CRAN}. Reprenons le cas de \textbf{lubridate}, sur GitHub, il est disponible à la page \href{https://github.com/tidyverse/lubridate}{suivante}. Pour l'installer nous devons d'abord installer un autre \emph{package} appelé \textbf{remotes} (depuis \emph{CRAN}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"remotes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Maintenant que nous disposons de \textbf{remotes}, nous pouvons utiliser la fonction d'installation \texttt{remotes::install\_github} pour directement télécharger \textbf{lubridate} depuis GitHub.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{remotes}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"tidyverse/lubridate"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sect01233}{%
\subsubsection{\texorpdfstring{Charger un \emph{package}}{Charger un package}}\label{sect01233}}

Maintenant que \textbf{lubridate} est installé, nous pouvons le charger dans notre session actuelle de R et accéder aux fonctions qu'il propose. Pour cela, suffit d'utiliser la fonction \texttt{library}. Conventionnellement, l'appel des \emph{packages} se fait au tout début du script que vous rédigez. Rien ne vous empêche de le faire au fur et à mesure de votre code, mais ce dernier perd alors en lisibilité. Notez qu'à chaque nouvelle session (redémarrage de R), il faudra recharger les \emph{packages} dont vous avez besoin.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lubridate)}
\end{Highlighting}
\end{Shaded}

Si vous obtenez un message d'erreur du type :

{Error in library(mon\_package) : aucun \emph{package} nommé `mon\_package' n'est trouvé.}

Cela signifie que le \emph{package} que vous tentez de charger n'est pas encore installé sur votre ordinateur. Dans ce cas, réessayer de l'installer avec la fonction \texttt{install.packages}. Si le problème persiste, vérifiez que vous n'avez pas fait une faute de frappe dans le nom du \emph{package.} Vous pouvez également redémarrer RStudio et réessayer d'installer le \emph{package}.

\hypertarget{obtenir-de-laide}{%
\subsection{Obtenir de l'aide}\label{obtenir-de-laide}}

Lorsque vous installez des \emph{packages} dans R, vous téléchargez aussi leur documentation. Tous les \emph{packages} de \emph{CRAN} disposent d'une documentation, ce n'est pas forcément vrai pour \emph{GitHub}. Dans RStudio, vous pouvez accéder à la documentation des \emph{packages} dans l'onglet \textbf{Packages} (figure \ref{fig:fig010}). Vous pouvez utiliser la barre de recherche pour retrouver rapidement un \emph{package} installé. Si vous cliquez sur le nom du \emph{package}, vous accédez directement à sa documentation dans cette fenêtre.

\begin{figure}

{\centering \includegraphics[width=0.45\linewidth]{images/introduction/rstudio_packages} 

}

\caption{Description des packages}\label{fig:fig010}
\end{figure}

Vous pouvez également accéder à ces informations en utilisant la syntaxe suivante dans votre console :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{help}\NormalTok{(}\DataTypeTok{package =} \StringTok{'lubridate'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Souvent, vous aurez besoin d'accéder à la documentation d'une fonction spécifique d'un \emph{package}. Affichons la documentation de la fonction \texttt{now} de \textbf{lubridate} :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{help}\NormalTok{(now, }\DataTypeTok{package =} \StringTok{'lubridate'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

ou plus simplement :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?lubridate}\OperatorTok{::}\NormalTok{now}
\end{Highlighting}
\end{Shaded}

Vous pouvez aussi utiliser le raccourci suivant :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?now}
\end{Highlighting}
\end{Shaded}

Si vous connaissez le nom d'une fonction, mais vous ne vous souvenez plus à quel \emph{package} elle appartient, lancez une recherche en utilisant un double point d'interrogation :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{??now}
\end{Highlighting}
\end{Shaded}

Vous découvrirez ainsi que la fonction \texttt{now} n'existe pas que dans \textbf{lubridate}, ce qui souligne l'importance de bien connaître les \emph{packages} que l'on installe et que l'on charge dans notre session !

Maintenant que nous avons fait le tour de l'environnement de travail, nous pouvons passer aux choses sérieuses, soit les bases du langage R.

\hypertarget{sect013}{%
\section{Les bases du langage R}\label{sect013}}

R est un langage de programmation. Il vous permet de communiquer avec votre ordinateur pour lui donner des tâches à accomplir. Dans cette section, nous aborderons les bases du langage. Ce type de section introductive à R est présente dans tous les manuels sur R ; elle est donc incontournable. À la première lecture, elle vous semblera probablement aride, et ce, d'autant plus que nous ne réalisons pas d'analyse à proprement parler. Gardez en tête que l'analyse de données requiert au préalable une phase de structuration de ces dernières, opération qui nécessite la maîtrise des notions abordées dans cette section. Nous vous recommandons une première lecture de ce chapitre pour comprendre quelles manipulations que vous pouvez effectuer avec R, la lecture des chapitres suivants dédiés aux statistiques, puis de consulter à nouveau cette section au besoin. Notez aussi que la maîtrise des différents objets et opérations de base de R ne s'acquiert qu'en pratiquant. Vous gagnerez cette expertise au fil de vos prochains codes R, période durant laquelle vous pourrez consulter ce chapitre tel un guide de référence des différents objets et notions fondamentales de R.

\hypertarget{sect0131}{%
\subsection{Hello World !}\label{sect0131}}

Une introduction à un langage de programmation se doit de commencer par le rite de passage \textbf{Hello World}. Il s'agit d'une forme de tradition consistant à montrer aux nouveaux utilisateurs·trices comment afficher le message ``Hello World'' à l'écran avec le langage en question.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\StringTok{"Hello World"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Hello World"
\end{verbatim}

Bravo ! Vous venez officiellement de faire votre premier pas dans R !

\hypertarget{sect0132}{%
\subsection{Objets et expressions}\label{sect0132}}

Dans R, nous passons notre temps à manipuler des \textbf{objets} à l'aide d'\textbf{expressions}. Prenons un exemple concret, si vous tapez la syntaxe \texttt{4\ +\ 3}, vous manipulez deux objets (4 et 3) au travers d'une expression indiquant que vous souhaitez obtenir la somme des deux objets.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{4} \OperatorTok{+}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7
\end{verbatim}

Cette expression est correcte, R comprend vos indications et effectue le calcul.

Il est possible d'enregistrer le résultat d'une expression et de la conserver dans un nouvel objet. On appelle cette opération déclarer une variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ma_somme <-}\StringTok{ }\DecValTok{4} \OperatorTok{+}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

Concrètement, nous venons de demander à R d'enregistrer le résultat de \texttt{4\ +\ 3} dans un espace spécifique de notre mémoire vive. Si vous regardez dans votre fenêtre \textbf{Environment}, vous verrez en effet qu'un objet appelé ma\_somme est actuellement en mémoire et a pour valeur 7.

Notez ici que le nom des variables ne peut être composé que de lettres, de chiffres, de points (.) et de tiret bas (\_) et doit commencer par une lettre. R est sensible à la casse, en d'autre termes, les variables \texttt{Ma\_somme}, \texttt{ma\_sommE}, \texttt{ma\_SOMME}, et \texttt{MA\_SOMME} renvoient toutes à un objet différent. Attention donc aux fautes de frappe. Si vous déclarez une variable en utilisant le nom d'une variable existante, la première est écrasée par la seconde :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age <-}\StringTok{ }\DecValTok{35}
\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 35
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age <-}\StringTok{ }\DecValTok{45}
\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 45
\end{verbatim}

Portez alors attention aux noms de variables que vous utilisez et réutilisez. Réutilisons notre objet \texttt{ma\_somme} dans une nouvelle expression :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ma_somme2 <-}\StringTok{ }\NormalTok{ma_somme }\OperatorTok{+}\StringTok{ }\NormalTok{ma_somme}
\end{Highlighting}
\end{Shaded}

Avec cette nouvelle expression, nous indiquons à R que nous souhaitons déclarer une nouvelle variable appelée \texttt{ma\_somme2}, et que cette variable aura pour valeur \texttt{ma\_somme\ +\ ma\_somme}, soit \texttt{7\ +\ 7}. Sans surprise, \texttt{ma\_somme2} a pour valeur 14.

Notez que la mémoire vive (l'environnement) est vidée lorsque vous fermez R. En d'autres termes, R perd complètement la mémoire lorsque vous le fermez. Vous pouvez bien sûr recréer vos objets en relançant les mêmes syntaxes. C'est pourquoi vous devez conserver vos feuilles de codes et ne pas seulement travailler dans la console. La console ne garde aucune trace de votre travail. Pensez donc à bien enregistrer votre code !

Nous verrons dans un autre chapitre comment sauvegarder des objets et les recharger dans une session ultérieure de R (LIEN SECTION). Ce type d'opération est pertinent quand le temps de calcul nécessaire à la production de certains objets est très long.

\hypertarget{sect0_133}{%
\subsection{Fonctions et arguments}\label{sect0_133}}

Dans R, nous manipulons le plus souvent nos objets avec des \textbf{fonctions}. Une fonction est elle-même un objet, mais qui a la particularité de pouvoir effectuer des opérations sur d'autres objets. Par exemple, déclarons l'objet \texttt{taille} avec une valeur de 175.897 :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{taille <-}\StringTok{ }\FloatTok{175.897}
\end{Highlighting}
\end{Shaded}

Nous allons utiliser la fonction \texttt{round} dont l'objectif est d'arrondir un nombre à virgule pour obtenir un nombre entier.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(taille)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 176
\end{verbatim}

Pour effectuer leurs opérations, les fonctions ont généralement besoin d'\textbf{arguments}. Ici, \texttt{taille} est un argument passé à la fonction \texttt{round}. Si nous regardons la documentation de \texttt{round} avec \texttt{help(round)}, nous constatons que cette fonction prend en réalité deux argments : \emph{x} et \emph{digits}. Le premier est le nombre que nous souhaitons arrondir et le second le nombre de décimales à conserver. On peut lire dans la documentation que la valeur par défaut de \emph{digits} est 0, ce qui explique que \texttt{round(taille)} a produit le résultat de 176.

\begin{figure}

{\centering \includegraphics[width=0.45\linewidth]{images/introduction/help_round} 

}

\caption{Arguments de la fonction round}\label{fig:fig011}
\end{figure}

Réutilisons maintenant la fonction \texttt{round} mais en gardant une décimale :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(taille, }\DataTypeTok{digits =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 175.9
\end{verbatim}

Il est aussi possible que certaines fonctions ne requièrent pas d'arguments. Par exemple, la fonction \texttt{now} va indiquer la date précise (avec l'heure) et n'a besoin d'aucun argument pour le faire :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{now}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2021-08-30 16:02:25 EDT"
\end{verbatim}

Par contre, si nous essayons de lancer la fonction \texttt{round} sans argument, nous obtiendrons une erreur :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

{Erreur : 0 arguments passed to `round' which requires 1 or 2 arguments}

Le message est très clair, \texttt{round} a besoin d'au moins un argument pour fonctionner. Si au lieu d'un nombre, nous avions donné du texte à la fonction \texttt{round}, nous aurions aussi obtenu une erreur :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\StringTok{"Hello World"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

{Error in round(``Hello World'') :
non-numeric argument to mathematical function}

À nouveau le message est très explicite : nous avons passé un argument non-numérique à une fonction mathématique. Lisez toujours vos messages d'erreurs qui vous permettront de repérer les coquilles et de corriger votre code !

Une fonction essentielle est la fonction \texttt{print} qui permet d'afficher la valeur d'une variable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(ma_somme)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7
\end{verbatim}

\hypertarget{sect0134}{%
\subsection{Principaux types de données}\label{sect0134}}

Depuis le début de ce chapitre, nous avons déclaré plusieurs variables et essentiellement des données numériques. Dans R, il existe trois principaux types de données de base :

\begin{itemize}
\tightlist
\item
  Les données numériques, qui peuvent être des nombres entiers (appelés \emph{integers}), ou des nombres décimaux (appelés \emph{floats}), par exemple \texttt{15} et \texttt{15.3}.
\item
  Les données de type texte, qui sont des chaînes de caratères (appelées \emph{strings}) et déclarées entre guillemets \texttt{"abcdefg"}.
\item
  Les données booléennes (\emph{booleans}) avec deux valeurs, soit vrai (\texttt{TRUE}) ou faux (\texttt{FALSE}).
\end{itemize}

Déclarons une variable pour chacun de ces types :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age <-}\StringTok{ }\DecValTok{35}
\NormalTok{taille <-}\StringTok{ }\FloatTok{175.5}
\NormalTok{adresse <-}\StringTok{ '4225 rue de la gauchetiere'}
\NormalTok{proprietaire <-}\StringTok{ }\OtherTok{TRUE}
\end{Highlighting}
\end{Shaded}

Si vous avez un doute sur le type de données stockées dans une variable, vous pouvez utiliser la fonction \texttt{typeof}. Par exemple, cela permet de repérer si des données qui sont supposées être numériques sont en fait stockées sous forme de texte comme dans l'exemple ci-dessous.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "double"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(taille)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "double"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tailletxt <-}\StringTok{ "175.5"}
\KeywordTok{typeof}\NormalTok{(tailletxt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

Notez également qu'il existe des types pour représenter l'absence de données :

\begin{itemize}
\tightlist
\item
  pour représenter un objet vide, on utilisera l'objet \texttt{NULL},
\item
  pour représenter une donnée manquante, on utilisera l'objet \texttt{NA},
\item
  pour représenter un texte vide, on utilisera une chaîne de caractère de longueur 0, soit \texttt{""}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age2 <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{taille2 <-}\StringTok{ }\OtherTok{NA}
\NormalTok{adresse2 <-}\StringTok{ ''}
\end{Highlighting}
\end{Shaded}

\hypertarget{sect0135}{%
\subsection{Opérateurs}\label{sect0135}}

Nous avons vu que les fonctions permettent de manipuler des objets. Nous pouvons également effectuer un grand nombre d'opérations avec des opérateurs.

\hypertarget{sect01351}{%
\subsubsection{Opérateurs mathématiques}\label{sect01351}}

Les opérateurs mathématiques permettent d'effectuer du calcul avec des données de type numérique.

\begin{table}

\caption{\label{tab:tableOperateurMath}Opérateurs mathématiques}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lllr}
\toprule
Opérateur & Description & Syntaxe & Résultat\\
\midrule
+ & Addition & 4 + 4 & 8,0\\
- & Soustraction & 4 - 3 & 1,0\\
* & Multiplication & 4 * 3 & 12,0\\
/ & Division & 12 / 4 & 3,0\\
\textasciicircum{} & Exponentiel & 4 \textasciicircum{} 3 & 64,0\\
\addlinespace
** & Exponentiel & 4 ** 3 & 64,0\\
\%\% & Reste de division & 15.5 \%\% 2 & 1,5\\
\%/\% & Division entière & 15.5 \%/\% 2 & 7,0\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{sect01352}{%
\subsubsection{Opérateurs relationnels}\label{sect01352}}

Les opérateurs relationnels permettent de vérifier des conditions dans R. Ils renvoient un booléen, \texttt{TRUE} si la condition est vérifiée et \texttt{FALSE} si ce n'est pas le cas.

\begin{table}

\caption{\label{tab:tableOperateurRela}Opérateurs relationnels}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{llll}
\toprule
Opérateur & Description & Syntaxe & Résultat\\
\midrule
== & Égalité & 4 == 4 & TRUE\\
!= & Différence & 4 != 4 & FALSE\\
> & Est supérieur & 5 > 4 & TRUE\\
< & Est inférieur & 5 < 4 & FALSE\\
>= & Est supérieur ou égal & 5 >= 4 & TRUE\\
\addlinespace
<= & Est inférieur ou égal & 5 <= 4 & FALSE\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{sect01353}{%
\subsubsection{Opérateurs logiques}\label{sect01353}}

Les opérateurs logiques permettent de combiner plusieurs conditions :

\begin{itemize}
\item
  L'opérateur \textbf{ET} permet de vérifier que deux conditions (l'une ET l'autre) sont TRUE. Si l'une des deux est FALSE, il renvoie FALSE.
\item
  L'opérateur \textbf{OU} permet de vérifier que l'une des deux conditions est TRUE (l'une OU l'autre). Si les deux sont FALSE, alors il renvoit FALSE.
\item
  L'opérateur \textbf{NOT} permet d'inverser une condition. Ainsi NOT TRUE est FALSE et NOT FALSE est TRUE.
\end{itemize}

\begin{table}

\caption{\label{tab:tableOperateurLogi}Opérateurs logiques}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{llll}
\toprule
Opérateur & Description & Syntaxe & Résultat\\
\midrule
\& & ET & TRUE \& FALSE & FALSE\\
| & OU & TRUE | FALSE & TRUE\\
! & NOT & ! TRUE & FALSE\\
\bottomrule
\end{tabular}
\end{table}

Prenons le temps pour un rapide exemple :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A <-}\StringTok{ }\DecValTok{4} 
\NormalTok{B <-}\StringTok{ }\DecValTok{10}
\NormalTok{C <-}\StringTok{ }\DecValTok{-5}

\CommentTok{# produit TRUE car A est bien plus petit que B et C est bien plus petit que A}
\NormalTok{A }\OperatorTok{<}\StringTok{ }\NormalTok{B }\OperatorTok{&}\StringTok{ }\NormalTok{C }\OperatorTok{<}\StringTok{ }\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# produit FALSE car si A est bien plus petit que B, }
\CommentTok{# B est en revanche plus grand que c}
\NormalTok{A }\OperatorTok{<}\StringTok{ }\NormalTok{B }\OperatorTok{&}\StringTok{ }\NormalTok{B }\OperatorTok{<}\StringTok{ }\NormalTok{C}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# produit TRUE car la seconde condition est inversée}
\NormalTok{A }\OperatorTok{<}\StringTok{ }\NormalTok{B }\OperatorTok{&}\StringTok{  }\OperatorTok{!}\StringTok{ }\NormalTok{B }\OperatorTok{<}\StringTok{ }\NormalTok{C}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# produit TRUE car au moins une des deux conditions est juste}
\NormalTok{A }\OperatorTok{<}\StringTok{ }\NormalTok{B }\OperatorTok{|}\StringTok{  }\NormalTok{B }\OperatorTok{<}\StringTok{ }\NormalTok{C}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Notez que l'opérateur \textbf{ET} est prioritaire sur l'opérateur \textbf{OU} et que les parenthèses sont prioritaires sur tous les opérateurs :

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# produit TRUE car on commence par tester A < B puis B < C ce qui donne FALSE}
\CommentTok{# on obtient ensuite}
\CommentTok{# FALSE |  A > C}
\CommentTok{# enfin, A est bien supérieur à C, donc l'une des deux conditions est vraie}
\NormalTok{A }\OperatorTok{<}\StringTok{ }\NormalTok{B }\OperatorTok{&}\StringTok{ }\NormalTok{B }\OperatorTok{<}\StringTok{ }\NormalTok{C }\OperatorTok{|}\StringTok{  }\NormalTok{A }\OperatorTok{>}\StringTok{ }\NormalTok{C}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Notez qu'en arrière-plan, les opérateurs sont en réalité des fonctions déguisées. Il est donc possible de définir de nouveau comportements pour les opérateurs. Il est par exemple possible d'additionner ou comparer des objets spéciaux comme des dates, des géométries, des graphes, etc.

\hypertarget{sect0136}{%
\subsection{Structures de données}\label{sect0136}}

Jusqu'à présent, nous avons utilisé des objets ne comprenant qu'une seule valeur. Or, des analyses statistiques nécessitent de travailler à des volumes de données bien plus grands. Pour stocker plusieurs valeurs, nous allons travailler avec plusieurs structures de données : les vecteurs, les matrices, les tableaux de données et les listes.

\hypertarget{sect01361}{%
\subsubsection{Vecteurs}\label{sect01361}}

Les vecteurs sont la brique élémentaire de R. Ils permettent de stocker une série de valeurs du même type dans une seule variable. Pour déclarer un vecteur, on utilise la fonction \emph{c()} :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ages <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{35}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{72}\NormalTok{,}\DecValTok{56}\NormalTok{,}\DecValTok{62}\NormalTok{)}
\NormalTok{tailles <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{175.5}\NormalTok{,}\FloatTok{180.3}\NormalTok{,}\FloatTok{168.2}\NormalTok{,}\FloatTok{172.8}\NormalTok{,}\FloatTok{167.6}\NormalTok{)}
\NormalTok{adresses <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'4225 rue de la gauchetiere'}\NormalTok{,}
              \StringTok{'4223 rue de la gauchetiere'}\NormalTok{,}
              \StringTok{'4221 rue de la gauchetiere'}\NormalTok{,}
              \StringTok{'4219 rue de la gauchetiere'}\NormalTok{,}
              \StringTok{'4217 rue de la gauchetiere'}\NormalTok{)}
\NormalTok{proprietaires <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Nous venons ainsi de déclarer quatre nouvelles variables étant chacune un vecteur de longueur cinq (comprenant chacun cinq valeurs). Ces vecteurs représentent, par exemple, les réponses de plusieurs répondants à un questionnaire.

\begin{bloc_attention}

Il existe dans R une subtilité à l'origine de nombreux malentendus : la distinction entre un vecteur de type texte et un vecteur de type facteur. Dans l'exemple précédent, le vecteur \emph{adresses} est un vecteur de type texte. Chaque nouvelle valeur ajoutée dans le vecteur peut être n'importe quelle nouvelle adresse. Déclarons un nouveau vecteur qui contiendrait cette fois-ci la couleur des yeux de personnes ayant répondu au questionnaire.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{couleurs_yeux <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'marron'}\NormalTok{,}\StringTok{'marron'}\NormalTok{,}\StringTok{'bleu'}\NormalTok{,}\StringTok{'bleu'}\NormalTok{,}\StringTok{'marron'}\NormalTok{,}\StringTok{'vert'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Contrairement aux adresses, il y a un nombre limité de couleurs que nous pouvons mettre dans ce vecteur. Il serait intéressant de fixer les valeurs possibles du vecteur pour s'assurer que de nouvelles ne soient pas ajoutées par erreur. Pour cela, nous pouvons convertir ce vecteur texte en vecteur de type facteur avec la fonction \texttt{as.factor}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{couleurs_yeux_facteur <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(couleurs_yeux)}
\end{Highlighting}
\end{Shaded}

Notez qu'à présent, nous pouvons ajouter une nouvelle couleur dans le premier vecteur, mais pas dans le second.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{couleurs_yeux[}\DecValTok{7}\NormalTok{] <-}\StringTok{ "rouge"}
\NormalTok{couleurs_yeux_facteur[}\DecValTok{7}\NormalTok{] <-}\StringTok{ "rouge"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in `[<-.factor`(`*tmp*`, 7, value = "rouge"): invalid factor level, NA
## generated
\end{verbatim}

Le message d'erreur nous informe que nous avons tenté d'introduire une valeur invalide dans le facteur.

Les facteurs peuvent sembler restrictifs et très régulièrement, on préfère travailler avec de simples vecteurs de type texte plutôt que des facteurs. Cependant, de nombreuses fonctions d'analyse nécessitent d'utiliser des facteurs car ils assurent une certaine cohérence dans les données. Il est donc essentiel de savoir passer du texte au facteur avec la fonction \texttt{as.factor}. À l'inverse, il est parfois nécessaire de revenir à une variable de type texte avec la fonction \texttt{as.character}.

Notez que des vecteurs numériques peuvent aussi être convertis en facteurs :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tailles_facteur <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(tailles)}
\end{Highlighting}
\end{Shaded}

Cependant, si vous souhaitez reconvertir ce facteur en format numérique, il faudra passer dans un premier temps par le format texte :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(tailles_facteur)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 5 2 3 1
\end{verbatim}

Comme vous pouvez le voir, convertir un facteur en valeur numérique renvoie des nombres entiers. Ceci est dû au fait que les valeurs dans un facteur sont recodées sous forme de nombres entiers, chaque nombre correspondant à une des valeurs originales (appelées niveaux). Si on convertit un facteur en valeurs numériques, on obtient donc ces nombres entiers.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(tailles_facteur))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 175.5 180.3 168.2 172.8 167.6
\end{verbatim}

Morale de l'histoire, ne confondez pas les données de type texte et de type facteur. Dans le doute, vous pouvez demander à R quel est le type d'un vecteur avec la fonction \texttt{class}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(tailles)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(tailles_facteur)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(couleurs_yeux)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(couleurs_yeux_facteur)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}


\end{bloc_attention}

Quasiment toutes les fonctions utilisent des vecteurs. Par exemple, on pourrait calculer la moyenne du vecteur \emph{ages} en utilisant la fonction \emph{mean} présente de base dans R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(ages)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 54
\end{verbatim}

Cela démontre bien que le vecteur est la brique élémentaire de R ! Toutes les variables que nous avons déclarées dans les sections précédentes sont aussi des vecteurs, mais de longueur 1 !

\hypertarget{sect01362}{%
\subsubsection{Matrices}\label{sect01362}}

Il est possible de combiner des vecteurs pour former des matrices. Une matrice est un tableau en deux dimensions (colonnes et lignes) généralement utilisé pour représenter certaines structures de données comme des images (pixels), effectuer du calcul matriciel ou plus simplement présenter des matrices de corrélations. Vous aurez rarement à travailler directement avec des matrices, mais il est bon de savoir ce qu'elles sont. Créons deux matrices à partir de nos précédents vecteurs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matrice1 <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(ages,tailles)}
\CommentTok{# afficher la matrice 1}
\KeywordTok{print}\NormalTok{(matrice1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      ages tailles
## [1,]   35   175.5
## [2,]   45   180.3
## [3,]   72   168.2
## [4,]   56   172.8
## [5,]   62   167.6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# afficher les dimensions de la matrice 1 (1er chiffre : lignes; 2e chiffre : colonnes)}
\KeywordTok{print}\NormalTok{(}\KeywordTok{dim}\NormalTok{(matrice1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matrice2 <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(ages, tailles)}
\CommentTok{# afficher la matrice 2}
\KeywordTok{print}\NormalTok{(matrice2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]  [,2]  [,3]  [,4]  [,5]
## ages     35.0  45.0  72.0  56.0  62.0
## tailles 175.5 180.3 168.2 172.8 167.6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# afficher les dimensions de la matrice 2}
\KeywordTok{print}\NormalTok{(}\KeywordTok{dim}\NormalTok{(matrice2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 5
\end{verbatim}

Comme vous pouvez le constater, la fonction \texttt{cbind} permet de concaténer des vecteurs comme s'ils étaient les colonnes d'une matrice, alors que \texttt{rbind} les combine comme s'ils étaient des lignes d'une matrice. La figure \ref{fig:fig012} présente graphiquement le passage du vecteur à la matrice.

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{images/introduction/vecteur_to_matrix} 

}

\caption{Du vecteur à la matrice}\label{fig:fig012}
\end{figure}

Notez que vous pouvez transposer une matrice avec la fonction \texttt{t}. Si nous essayons maintenant de comparer la matrice 1 et la matrice 2 nous allons avoir une erreur car elles n'ont pas les mêmes dimensions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matrice1 }\OperatorTok{==}\StringTok{ }\NormalTok{matrice2}
\end{Highlighting}
\end{Shaded}

{Error in matrice1 == matrice2 : non-conformable arrays}

En revanche, on pourrait transposer la matrice 1 et refaire cette comparaison :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t}\NormalTok{(matrice1) }\OperatorTok{==}\StringTok{ }\NormalTok{matrice2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         [,1] [,2] [,3] [,4] [,5]
## ages    TRUE TRUE TRUE TRUE TRUE
## tailles TRUE TRUE TRUE TRUE TRUE
\end{verbatim}

Le résultat souligne bien que l'on a les mêmes valeurs dans les deux matrices. Il est aussi possible de construire des matrices directement avec la fonction \texttt{matrix}, ce que nous montrons dans la prochaine section.

\hypertarget{sect01363}{%
\subsubsection{\texorpdfstring{\emph{Arrays}}{Arrays}}\label{sect01363}}

S'il est rare de travailler avec des matrices, il est encore plus rare de manipuler des \emph{arrays}. Un \emph{array} est une matrice spéciale qui peut avoir plus que deux dimensions. Un cas simple serait un \emph{array} en trois dimensions : lignes, colonnes, profondeur, que l'on pourrait se représenter comme un cube divisé en sous cubes. Au delà de trois dimensions, il devient difficile de se les représenter. Cette structure de données peut être utilisée pour représenter les différentes bandes spectrales d'une image satellitaire. Les lignes et les colonnes délimiteraient les pixels de l'image, la profondeur quant à elle délimiterait les différents bandes composant l'image (figure \ref{fig:fig012}).

\begin{figure}

{\centering \includegraphics[width=0.15\linewidth]{images/introduction/array} 

}

\caption{Un array avec trois dimension}\label{fig:fig013}
\end{figure}

Créons un array en combinant trois matrices avec la fonction \texttt{array}. Chacune de ces matrices sera composée respectivement de 1, de 2 et de 3 et aura une dimension de 5 x 5. L'array final aura donc des dimensions de 5 x 5 x 3.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{5}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{5}\NormalTok{)}
\NormalTok{mat2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{5}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{5}\NormalTok{)}
\NormalTok{mat3 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{5}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{5}\NormalTok{)}

\NormalTok{mon_array <-}\StringTok{ }\KeywordTok{array}\NormalTok{(}\KeywordTok{c}\NormalTok{(mat1, mat2, mat3), }\DataTypeTok{dim =} \KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{))}

\KeywordTok{print}\NormalTok{(mon_array)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## , , 1
## 
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    1    1    1    1
## [2,]    1    1    1    1    1
## [3,]    1    1    1    1    1
## [4,]    1    1    1    1    1
## [5,]    1    1    1    1    1
## 
## , , 2
## 
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    2    2    2    2    2
## [2,]    2    2    2    2    2
## [3,]    2    2    2    2    2
## [4,]    2    2    2    2    2
## [5,]    2    2    2    2    2
## 
## , , 3
## 
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    3    3    3    3    3
## [2,]    3    3    3    3    3
## [3,]    3    3    3    3    3
## [4,]    3    3    3    3    3
## [5,]    3    3    3    3    3
\end{verbatim}

\hypertarget{sect01364}{%
\subsubsection{\texorpdfstring{\emph{DataFrames}}{DataFrames}}\label{sect01364}}

S'il est rare de manipuler des matrices et des \emph{arrays}, le \emph{DataFrame} (tableau de données en français) est la structure de données la plus souvent utilisée. Dans cette structure, chaque ligne du tableau représente un individu et chaque colonne représente une caractéristique de ces individus. Ces colonnes ont des noms, ce qui permet facilement d'accéder à leurs valeurs. Créons un \emph{DataFrame} à partir de nos quatres vecteurs et de la fonction \texttt{data.frame}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \StringTok{"age"}\NormalTok{ =}\StringTok{ }\NormalTok{ages,}
  \StringTok{"taille"}\NormalTok{ =}\StringTok{ }\NormalTok{tailles,}
  \StringTok{"adresse"}\NormalTok{ =}\StringTok{ }\NormalTok{adresses,}
  \StringTok{"proprietaire"}\NormalTok{ =}\StringTok{ }\NormalTok{proprietaires}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:tabfirsttable}Un premier DataFrame}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{rrll}
\toprule
age & taille & adresse & proprietaire\\
\midrule
35 & 175,5 & 4225 rue de la gauchetiere & TRUE\\
45 & 180,3 & 4223 rue de la gauchetiere & TRUE\\
72 & 168,2 & 4221 rue de la gauchetiere & FALSE\\
56 & 172,8 & 4219 rue de la gauchetiere & TRUE\\
62 & 167,6 & 4217 rue de la gauchetiere & TRUE\\
\bottomrule
\end{tabular}
\end{table}

Dans Rstudio, vous pouvez visualiser votre tableau de données avec la fonction \texttt{View(df)}. Comme vous pouvez le constater, chaque vecteur est devenu une colonne de votre tableau de données \emph{df}. La figure \ref{fig:fig014} résume ce passage d'une simple donnée à un DataFrame en passant par un vecteur.

\begin{figure}

{\centering \includegraphics[width=0.25\linewidth]{images/introduction/vecteur_to_dataframe} 

}

\caption{De la donnée au DataFrame}\label{fig:fig014}
\end{figure}

Plusieurs fonctions de base de R fournissent des informations importantes sur un \emph{DataFrame} :

\begin{itemize}
\tightlist
\item
  \texttt{names} renvoie les noms des colonnes du DataFrame;
\item
  \texttt{nrow} renvoie le nombre de lignes;
\item
  \texttt{ncol} renvoie le nombre de colonnes.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "age"          "taille"       "adresse"      "proprietaire"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ncol}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

Vous pouvez accéder à chaque colonne de \emph{df} en utilisant le symbole \texttt{\$} ou \texttt{{[}{[}"nom\_de\_la\_colonne"{]}{]}}. Recalculons ainsi la moyenne des âges :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(df}\OperatorTok{$}\NormalTok{age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 54
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(df[[}\StringTok{"age"}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 54
\end{verbatim}

\hypertarget{sect01365}{%
\subsubsection{Listes}\label{sect01365}}

La dernière structure de données à connaître est la liste. Elle ressemble à un vecteur, au sens où elle permet de stocker un ensemble d'objets les uns à la suite des autres. Cependant, une liste peut contenir n'importe quel type d'objets. Vous pouvez ainsi construire des listes de matrices, des listes d'\emph{arrays}, des listes mixant des vecteurs, des graphiques, des \emph{DataFrames}, des listes de listes\ldots{}

Créons ensemble une liste qui va contenir des vecteurs et des matrices à l'aide de la fonction \texttt{list.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ma_liste <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{),}
                 \KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{5}\NormalTok{),}
                 \KeywordTok{matrix}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{7}\NormalTok{),}
                 \StringTok{'A'}
\NormalTok{                 )}
\end{Highlighting}
\end{Shaded}

Il est possible d'accéder aux éléments de la liste par leur position dans cette dernière en utilisant les doubles crochets \texttt{{[}{[}\ {]}{]}}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(ma_liste[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(ma_liste[[}\DecValTok{4}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "A"
\end{verbatim}

Il est aussi possible de donner des noms aux éléments de la liste et d'utiliser le symbole \texttt{\$} pour y accéder. Créons une nouvelle liste de vecteurs et donnons leurs des noms avec la fonction \texttt{names}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liste2 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{35}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{72}\NormalTok{,}\DecValTok{56}\NormalTok{,}\DecValTok{62}\NormalTok{), }
               \KeywordTok{c}\NormalTok{(}\FloatTok{175.5}\NormalTok{,}\FloatTok{180.3}\NormalTok{,}\FloatTok{168.2}\NormalTok{,}\FloatTok{172.8}\NormalTok{,}\FloatTok{167.6}\NormalTok{),}
               \KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{)}
\KeywordTok{names}\NormalTok{(liste2) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{,}\StringTok{'taille'}\NormalTok{,}\StringTok{'proprietaire'}\NormalTok{)}

\KeywordTok{print}\NormalTok{(liste2}\OperatorTok{$}\NormalTok{age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 35 45 72 56 62
\end{verbatim}

Si vous avez bien suivi, vous devez avoir compris qu'un \emph{DataFrame} n'est en fait rien d'autre qu'une liste de vecteurs avec des noms !

Bravo ! Vous venez de faire le tour des bases du langage R. Vous allez apprendre désormais à manipuler des données dans des \emph{DataFrames} !

\hypertarget{sect014}{%
\section{Manipuler des données}\label{sect014}}

Dans cette section, vous apprendrez à charger et manipuler des \emph{DataFrames} en vue d'effectuer des opérations classiques de gestion de données.

\hypertarget{sect0141}{%
\subsection{\texorpdfstring{Charger un \emph{DataFrame} depuis un fichier}{Charger un DataFrame depuis un fichier}}\label{sect0141}}

Il sera rarement nécessaire de créer vos \emph{DataFrames} manuellement comme réalisé dans la section précédente. Le plus souvent, vous disposerez de fichiers contenant vos données et utiliserez des fonctions pour les importer dans R sous forme d'un \emph{DataFrame}. Les formats à importer les plus répandus sont :

\begin{itemize}
\tightlist
\item
  \emph{.csv}, soit un fichier texte dont chaque ligne représente une ligne du tableau de données et dont les colonnes sont séparées par un délimiteur (généralement une virgule ou un point-virgule).
\item
  \emph{.dbf}, ou fichier \emph{dBase}, souvent associés à des fichiers d'information géographique au format \emph{ShapeFile}.
\item
  \emph{.xls} et \emph{.xlsx}, soit des fichiers générés par Excel.
\item
  \emph{.json}, soit un fichier texte utilisant la norme d'écriture propre au langage JavaScript.
\end{itemize}

Plus rarement, il se peut que vous ayez à charger des fichiers provenant de logiciels propriétaires :

\begin{itemize}
\tightlist
\item
  \emph{.sas7bdat} (SAS),
\item
  \emph{.sav} (SPSS) et
\item
  \emph{.dta} (STATA).
\end{itemize}

Pour lire la plupart de ces fichiers, nous allons utiliser le \emph{package} \textbf{foreign} dédié à l'importation d'une multitude de formats. Commencez donc par l'installer (\texttt{install.packages("foreign")}). Nous allons charger cinq fois le même jeu de données enregistré dans des formats différents (\emph{csv}, \emph{dbf}, \emph{dta}, \emph{sas7bdat} et \emph{xlsx}). Aussi, nous mesurerons le temps nécessaire pour importer chacun de ces fichiers avec la fonction \texttt{Sys.time}.

\hypertarget{sect01411}{%
\subsubsection{\texorpdfstring{Lire un fichier \emph{csv}}{Lire un fichier csv}}\label{sect01411}}

Pour le format \emph{csv}, il n'y a pas besoin d'utiliser un \emph{package} puisque R dispose d'une fonction de base pour lire ce format.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t1 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{df1 <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/priseenmain/SR_MTL_2016.csv"}\NormalTok{, }
         \DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{, }\DataTypeTok{dec =} \StringTok{"."}\NormalTok{,}
         \DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{d1 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{difftime}\NormalTok{(t2,t1,}\DataTypeTok{units=}\StringTok{"secs"}\NormalTok{))}

\KeywordTok{cat}\NormalTok{(}\StringTok{"le dataframe df1 a "}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(df1),}\StringTok{' observations'}\NormalTok{,}
    \StringTok{'et '}\NormalTok{,}\KeywordTok{ncol}\NormalTok{(df1),}\StringTok{"colonnes}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## le dataframe df1 a  951  observations et  48 colonnes
\end{verbatim}

Rien de bien compliqué ! Notez tout de même que :

\begin{itemize}
\tightlist
\item
  Lorsque vous chargez un fichier \emph{csv}, vous devez connaître le \textbf{séparateur}, soit le caractère utilisé pour délimiter les colonnes. Dans le cas présent, il s'agit d'une virgule (spécifiez avec l'argument \texttt{sep\ =\ ","}), mais il pourrait tout aussi bien être un point virgule (\texttt{sep\ =\ ";"}) une tabulation (\texttt{sep\ =\ "\ \ \ \ "}), etc.
\item
  Vous devez également spécifier le caractère utilisé comme séparateur de décimales. Le plus souvent, ce sera le point (\texttt{dec\ =\ "."}), mais certains logiciels avec des paramètres régionaux de langue française (notamment Excel) exportent des fichiers \emph{csv} avec des virgules comme séparateur de décimales (utilisez alors \texttt{dec\ =\ ","}).
\item
  L'argument \texttt{header} indique si la première ligne (l'entête) du fichier comprend ou non les noms des colonnes du jeu de données (avec les valeurs \texttt{TRUE} ou \texttt{FALSE}). Il arrive que certains fichiers \emph{csv} soient fournis sans entête et que les noms et descriptions des colonnes soient fournis dans un autre fichier.
\item
  L'argument \emph{stringsAsFactors} permet d'indiquer à R que les colonnes comportant du texte doivent être chargées comme des vecteurs de type texte et nom de type facteur.
\end{itemize}

\hypertarget{sect01412}{%
\subsubsection{\texorpdfstring{Lire un fichier \emph{dbase}}{Lire un fichier dbase}}\label{sect01412}}

Pour lire un fichier \emph{dbase} (.dbf), nous utilisons la fonction \texttt{read.dbf} du \emph{package} \textbf{foreign} installé précédemment :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(foreign)}

\NormalTok{t1 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{read.dbf}\NormalTok{(}\StringTok{"data/priseenmain/SR_MTL_2016.dbf"}\NormalTok{)}
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{d2 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{difftime}\NormalTok{(t2,t1,}\DataTypeTok{units=}\StringTok{"secs"}\NormalTok{))}

\KeywordTok{cat}\NormalTok{(}\StringTok{"le dataframe df2 a "}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(df2),}\StringTok{" observations"}\NormalTok{,}
    \StringTok{"et "}\NormalTok{,}\KeywordTok{ncol}\NormalTok{(df2),}\StringTok{"colonnes}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## le dataframe df2 a  951  observations et  48 colonnes
\end{verbatim}

Comme vous pouvez le constater, nous obtenons les mêmes résultats qu'avec le fichier \emph{csv}.

\hypertarget{sect01413}{%
\subsubsection{\texorpdfstring{Lire un fichier \emph{dta} (Stata)}{Lire un fichier dta (Stata)}}\label{sect01413}}

Si vous travaillez avec des collègues utilisant le logiciel Stata, il se peut que ces derniers vous partagent des fichiers \emph{dta}. Toujours en utilisant le \emph{package} \textbf{foreign}, vous serez en mesure de les charger directement dans R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t1 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{df3 <-}\StringTok{ }\KeywordTok{read.dta}\NormalTok{(}\StringTok{"data/priseenmain/SR_MTL_2016.dta"}\NormalTok{)}
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{d3 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{difftime}\NormalTok{(t2,t1,}\DataTypeTok{units=}\StringTok{"secs"}\NormalTok{))}

\KeywordTok{cat}\NormalTok{(}\StringTok{"le dataframe df3 a "}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(df3),}\StringTok{" observations"}\NormalTok{,}
    \StringTok{"et "}\NormalTok{,}\KeywordTok{ncol}\NormalTok{(df3),}\StringTok{"colonnes}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## le dataframe df3 a 951 observationset 48colonnes
\end{verbatim}

\hypertarget{sect01414}{%
\subsubsection{\texorpdfstring{Lire un fichier \emph{sav} (SPSS)}{Lire un fichier sav (SPSS)}}\label{sect01414}}

Pour importer un fichier \emph{sav} provenant du logiciel statistique SPSS, utilisez la fonction \texttt{read.spss} du \emph{package} \textbf{foreign}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t1 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{df4 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{read.spss}\NormalTok{(}\StringTok{"data/priseenmain/SR_MTL_2016.sav"}\NormalTok{))}
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{d4 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{difftime}\NormalTok{(t2,t1,}\DataTypeTok{units=}\StringTok{"secs"}\NormalTok{))}

\KeywordTok{cat}\NormalTok{(}\StringTok{"le dataframe df4 a "}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(df4),}\StringTok{" observations"}\NormalTok{,}
    \StringTok{"et "}\NormalTok{,}\KeywordTok{ncol}\NormalTok{(df4),}\StringTok{"colonnes}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## le dataframe df4 a 951 observationset 48colonnes
\end{verbatim}

\hypertarget{sect01415}{%
\subsubsection{\texorpdfstring{Lire un fichier \emph{sas7bdat} (SAS)}{Lire un fichier sas7bdat (SAS)}}\label{sect01415}}

Pour importer un fichier \emph{sas7bdat} provenant du logiciel statistique SAS, utilisez la fonction \texttt{read.sas7bdat} du \emph{package} \textbf{sas7bdat}. Installez préalablement le \emph{package} (\texttt{install.packages("sas7bdat")}) et chargez le (\texttt{library(sas7bdat)}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(sas7bdat)}

\NormalTok{t1 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{df5 <-}\StringTok{ }\KeywordTok{read.sas7bdat}\NormalTok{(}\StringTok{"data/priseenmain/SR_MTL_2016.sas7bdat"}\NormalTok{)}
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{d5 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{difftime}\NormalTok{(t2,t1,}\DataTypeTok{units=}\StringTok{"secs"}\NormalTok{))}

\KeywordTok{cat}\NormalTok{(}\StringTok{"le dataframe df5 a "}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(df5),}\StringTok{" observations"}\NormalTok{,}
    \StringTok{"et "}\NormalTok{,}\KeywordTok{ncol}\NormalTok{(df5),}\StringTok{"colonnes}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{sep =}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## le dataframe df5 a 951 observationset 48colonnes
\end{verbatim}

\hypertarget{sect01416}{%
\subsubsection{\texorpdfstring{Lire un fichier \emph{xlsx} (Excel)}{Lire un fichier xlsx (Excel)}}\label{sect01416}}

Lire un fichier Excel dans R n'est pas toujours une tâche facile. Généralement, nous recommandons d'exporter les fichiers en question au format \emph{csv} dans un premier temps, puis de le lire avec la fonction \texttt{read.csv} dans un second temps (section \ref{sect01411}).
Il est néanmoins possible de lire directement un fichier \emph{xlsx} avec le \emph{package} \textbf{xlsx}. Ce dernier requiert que le logiciel JAVA soit installé sur votre ordinateur (Windows, Mac ou Linux). Si vous utilisez la version 64 bit de R, vous devrez télécharger et installer la version 64 bit de JAVA. Une fois que ce logiciel tiers est installé, il ne vous restera plus qu'à installer (\texttt{install.packages("xlsx")}) et charger (\texttt{library(xlsx)}) le \emph{package} \textbf{xlsx}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(xlsx)}

\NormalTok{t1 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{df6 <-}\StringTok{ }\KeywordTok{read.xlsx}\NormalTok{(}\DataTypeTok{file=}\StringTok{"data/priseenmain/SR_MTL_2016.xlsx"}\NormalTok{,}
                 \DataTypeTok{sheetIndex =} \DecValTok{1}\NormalTok{,}
                 \DataTypeTok{as.data.frame =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{d6 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{difftime}\NormalTok{(t2,t1,}\DataTypeTok{units=}\StringTok{"secs"}\NormalTok{))}

\KeywordTok{cat}\NormalTok{(}\StringTok{"le dataframe df6 a "}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(df6),}\StringTok{" observations"}\NormalTok{,}
    \StringTok{"et "}\NormalTok{,}\KeywordTok{ncol}\NormalTok{(df6),}\StringTok{"colonnes}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## le dataframe df6 a 951 observationset 48colonnes
\end{verbatim}

Il est possible d'accélérer significativement la vitesse de lecture d'un fichier \emph{xlsx} en utilisant la fonction \texttt{read.xlsx2}. Il faut cependant indiquer à cette dernière le type de données de chaque colonne. Dans le cas présent, les cinq premières colonnes contiennent des données de type texte (\texttt{character}), alors que les 43 autres sont des données numériques (\texttt{numeric}). Nous utilisons la fonction \texttt{rep} afin de ne pas avoir à écrire plusieurs fois \emph{character} et \emph{numeric}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(xlsx)}

\NormalTok{t1 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{df7 <-}\StringTok{ }\KeywordTok{read.xlsx2}\NormalTok{(}\DataTypeTok{file=}\StringTok{"data/priseenmain/SR_MTL_2016.xlsx"}\NormalTok{,}
                  \DataTypeTok{sheetIndex =} \DecValTok{1}\NormalTok{, }
                  \DataTypeTok{as.data.frame =} \OtherTok{TRUE}\NormalTok{,}
                  \DataTypeTok{colClasses =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"character"}\NormalTok{,}\DecValTok{5}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\StringTok{"numeric"}\NormalTok{,}\DecValTok{43}\NormalTok{))}
\NormalTok{                  )}
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{d7 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{difftime}\NormalTok{(t2,t1,}\DataTypeTok{units=}\StringTok{"secs"}\NormalTok{))}

\KeywordTok{cat}\NormalTok{(}\StringTok{"le dataframe df6 a "}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(df7),}\StringTok{" observations"}\NormalTok{,}
    \StringTok{"et "}\NormalTok{,}\KeywordTok{ncol}\NormalTok{(df7),}\StringTok{"colonnes}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## le dataframe df6 a 951 observationset 48colonnes
\end{verbatim}

Si l'on compare les temps d'exécution (tableau \ref{tab:tableduration}), on constate que la lecture des fichiers \emph{xlsx} peut être extrêmement longue si l'on ne spécifie pas le type des colonnes, ce qui peut devenir problématique pour des fichiers volumineux. Notez également que la lecture des fichiers \emph{csv} devient de plus en plus laborieuse à mesure que la taille du fichier \emph{csv} augmente. Si vous devez un jour charger des fichiers \emph{csv} de plusieurs gigaoctets, nous vous recommandons vivement d'utiliser la fonction \texttt{fread} du \emph{package} \textbf{data.table} qui est beaucoup plus rapide.

\begin{table}

\caption{\label{tab:tableduration}Temps nécessaire pour lire les données en fonction du type de fichiers}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{rl}
\toprule
Durée (s) & fonction\\
\midrule
0,06 & read.csv\\
0,06 & read.dbf\\
0,02 & read.spss\\
0,03 & read.dta\\
0,86 & read.sas7bdat\\
\addlinespace
20,09 & read.xlsx\\
0,53 & read.xlsx2\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{sect0142}{%
\subsection{\texorpdfstring{Manipuler un \emph{DataFrame}}{Manipuler un DataFrame}}\label{sect0142}}

Une fois le \emph{DataFrame} chargé, voyons comment il est possible de le manipuler.

\hypertarget{sect01421}{%
\subsubsection{\texorpdfstring{Un petit mot sur le \textbf{tidyverse}}{Un petit mot sur le tidyverse}}\label{sect01421}}

\textbf{Tidyverse} est un ensemble de \emph{packages} conçus pour faciliter la structuration et la manipulation des données dans R. Avant d'aller plus loin, il est important d'aborder brièvement un débat actuel dans la Communauté R. Entre 2010 et 2020, l'utilisation du \textbf{tidyverse} s'est peu à peu répandue. Développé et maintenu par Hadley Wickham, \textbf{tidyverse} introduit une philosophie et une grammaire spécifiques qui diffèrent du langage R traditionnel. Une partie de la communauté a pour ainsi dire complètement embrassé le \textbf{tidyverse} et de nombreux \emph{packages} en dehors du \textbf{tidyverse} ont adopté sa grammaire et sa philosophie. À l'inverse, une autre partie de la communauté est contre cette évolution (\href{https://blog.ephorie.de/why-i-dont-use-the-tidyverse}{voir l'article du blogue suivant}). Les arguments pour et contre \textbf{tidyverse} sont résumés dans le tableau suivant.

\begin{table}

\caption{\label{tab:tableTidyverse}Avantages et inconvénients du tidyverse}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{7cm}>{\raggedright\arraybackslash}p{7cm}}
\toprule
Avantage du tidyverse & Problème posé par le tidyverse\\
\midrule
Simplicité d'écriture et d'apprentissage & Nouvelle syntaxe à apprendre\\
Ajout de l'opérateur \%>\% permettant d'enchaîner les traitements & Perte de lisibilité avec l'opérateur ->\\
La meilleure librairie pour réaliser des graphiques : \textbf{ggplot2} & Certaines fonctions de base sont remplacées par \textbf{tidyverse} lors de son chargement, pouvant créer des erreurs.\\
Crée un écosystème cohérent & Ajoute une dépendance dans le code\\
Package en développement et de plus en plus utilisé & Philosophie d'évolution agressive, aucune assurance de rétro-compatibilité\\
\bottomrule
\end{tabular}
\end{table}

Le dernier point est probablement le plus problématique. Dans sa volonté d'évoluer au mieux et sans restriction, le \emph{package} \textbf{tidyverse} n'offre aucune garantie de rétro-comptatibilité. En d'autre termes, des changements importants peuvent être introduits d'une version à l'autre rendant potentiellement obsolète votre propre code. Nous n'avons pas d'opinion tranchée sur le sujet : \textbf{tidyverse} est un outil très intéressant dans de nombreux cas; nous évitons simplement de l'utiliser systématiquement et préférons charger directement des sous-packages (comme \textbf{dplyr} ou \textbf{ggplot2}) du \textbf{tidyverse}. Notez que le \emph{package} \textbf{data.table} offre une alternative au \textbf{tidyverse} dans la manipulation de données. Au prix d'une syntaxe généralement un peu plus complexe, le package \textbf{data.table} offre une vitesse de calcul bien supérieure au \textbf{tidyverse} et assure une bonne rétro-compatibilité.

\hypertarget{sect01422}{%
\subsubsection{\texorpdfstring{Gérer les colonnes d'un \emph{DataFrame}}{Gérer les colonnes d'un DataFrame}}\label{sect01422}}

Repartons du \emph{DataFrame} que nous avions chargé précédemment en important un fichier \emph{csv}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\DataTypeTok{file=}\StringTok{"data/priseenmain/SR_MTL_2016.csv"}\NormalTok{, }
               \DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{, }\DataTypeTok{dec =} \StringTok{"."}\NormalTok{,}
               \DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sect014221}{%
\paragraph{Sélectionner une colonne}\label{sect014221}}

Pour rappel, il est possible d'accéder aux colonnes dans ce \emph{DataFrame} en utilisant le symbole dollar \texttt{\$ma\_colonne} ou les doubles crochets \texttt{{[}{[}"ma\_colonne"{]}{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calcul de la superficie totale de l'Île de Montréal}
\KeywordTok{sum}\NormalTok{(df}\OperatorTok{$}\NormalTok{KM2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4680.543
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(df[[}\StringTok{"KM2"}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4680.543
\end{verbatim}

\hypertarget{sect014222}{%
\paragraph{Sélectionner plusieurs colonnes}\label{sect014222}}

Il est possible de sélectionner plusieurs colonnes d'un \emph{DataFrame} et filtrer ainsi les colonnes inutiles. Pour cela, on peut utiliser un vecteur contenant soit les positions des colonnes (1 pour la première colonne, 2 pour la seconde et ainsi de suite), soit les noms des colonnes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Conserver les 5 premières colonnes}
\NormalTok{df2 <-}\StringTok{ }\NormalTok{df[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{]}

\CommentTok{# Conserver les colonnes 1, 5, 10 et 15}
\NormalTok{df3 <-}\StringTok{ }\NormalTok{df[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{)]}

\CommentTok{# Cela peut aussi être utilisé pour changer l'ordre des champs}
\NormalTok{df3 <-}\StringTok{ }\NormalTok{df[}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{)]}

\CommentTok{# Conserver les colonnes 1 à 5, 7 à 12, 17 et 22}
\NormalTok{df4 <-}\StringTok{ }\NormalTok{df[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{7}\OperatorTok{:}\DecValTok{12}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{22}\NormalTok{)]}

\CommentTok{# Conserver les colonnes avec leurs noms}
\NormalTok{df5 <-}\StringTok{ }\NormalTok{df[}\KeywordTok{c}\NormalTok{(}\StringTok{"SRIDU"}\NormalTok{,}\StringTok{"KM2"}\NormalTok{,}\StringTok{"Pop2016"}\NormalTok{,}\StringTok{"MaisonIndi"}\NormalTok{,}\StringTok{"LoyerMed"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\hypertarget{sect014223}{%
\paragraph{Supprimer des colonnes}\label{sect014223}}

Il est parfois plus intéressant et rapide de supprimer directement des colonnes plutôt que de recréer un nouveau \emph{DataFrame}. Pour ce faire, on attribue la valeur \texttt{NULL} à ces colonnes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Supprimer le colonnes 2, 3 et 5}
\NormalTok{df3[}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{)] <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\OtherTok{NULL}\NormalTok{)}

\CommentTok{# Supprimer une colonne avec son nom}
\NormalTok{df4}\OperatorTok{$}\NormalTok{OID <-}\StringTok{ }\OtherTok{NULL}

\CommentTok{# Supprimer plusieurs colonnes par leur nom}
\NormalTok{df5[}\KeywordTok{c}\NormalTok{(}\StringTok{"SRIDU"}\NormalTok{,}\StringTok{"LoyerMed"}\NormalTok{)] <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\OtherTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Notez que si vous supprimez une colonne, vous ne pouvez pas revenir en arrière. Il faudra recharger votre jeu de données ou éventuellement relancer les calculs qui avaient produit cette colonne.

\hypertarget{sect014224}{%
\paragraph{Renommer des colonnes}\label{sect014224}}

Il est possible de changer le nom d'une colonne. Cette opération est importante pour faciliter la lecture du \emph{DataFrame} ou encore s'assurer que l'exportation du \emph{DataFrame} dans un format ne posera pas de problème.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Voici les noms des colonnes}
\KeywordTok{names}\NormalTok{(df5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "KM2"        "Pop2016"    "MaisonIndi"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Renommer toutes les colonnes}
\KeywordTok{names}\NormalTok{(df5) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'superficie_km2'}\NormalTok{,}\StringTok{'population_2016'}\NormalTok{, }\StringTok{'maison_individuelle_prt'}\NormalTok{)}
\KeywordTok{names}\NormalTok{(df5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "superficie_km2"          "population_2016"        
## [3] "maison_individuelle_prt"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Renommer avec dplyr}
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{df4 <-}\StringTok{ }\KeywordTok{rename}\NormalTok{(df4, }\StringTok{"population_2016"}\NormalTok{ =}\StringTok{ "Pop2016"}\NormalTok{,}
              \StringTok{"prs_moins_14ans_prt"}\NormalTok{ =}\StringTok{ "A014"}\NormalTok{,}
              \StringTok{"prs_15_64_ans_prt"}\NormalTok{ =}\StringTok{ "A1564"}\NormalTok{,}
              \StringTok{"prs_65plus_ans_prt"}\NormalTok{ =}\StringTok{ "A65plus"}
\NormalTok{              )}
\end{Highlighting}
\end{Shaded}

\hypertarget{sect01423}{%
\subsubsection{Calculer de nouvelles variables}\label{sect01423}}

Il est possible d'utiliser les colonnes de type numérique pour calculer de nouvelles colonnes en utilisant les opérateurs mathématiques vus dans la section \ref{sect0135}. Prenons un exemple concret : calculons la densité de population par secteur de recensement dans notre \emph{DataFrame}, puis affichons un résumé de cette nouvelle variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calcul de la densité}
\NormalTok{df}\OperatorTok{$}\NormalTok{pop_density_}\DecValTok{2016}\NormalTok{ <-}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{Pop2016 }\OperatorTok{/}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{KM2}

\CommentTok{# Statistiques descriptives}
\KeywordTok{summary}\NormalTok{(df}\OperatorTok{$}\NormalTok{pop_density_}\DecValTok{2016}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##    17.45  1946.96  3700.50  5465.03  7918.39 48811.79
\end{verbatim}

Nous pouvons aussi calculer le ratio entre le nombre de maisons et le nombre d'appartements.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calcul du ratio}
\NormalTok{df}\OperatorTok{$}\NormalTok{total_maison <-}\StringTok{ }\NormalTok{(df}\OperatorTok{$}\NormalTok{MaisonIndi }\OperatorTok{+}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{MaisJumule }\OperatorTok{+}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{MaisRangee }\OperatorTok{+}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{AutreMais)}
\NormalTok{df}\OperatorTok{$}\NormalTok{total_apt <-}\StringTok{ }\NormalTok{(df}\OperatorTok{$}\NormalTok{AppDuplex }\OperatorTok{+}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{App5Moins }\OperatorTok{+}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{App5Plus)}
\NormalTok{df}\OperatorTok{$}\NormalTok{ratio_maison_apt <-}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{total_maison }\OperatorTok{/}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{total_apt}
\end{Highlighting}
\end{Shaded}

Retenez ici que R va appliquer le calcul à chaque ligne de votre jeu de données et stocker le résultat dans une nouvelle colonne. Cette opération est du calcul vectoriel : toute la colonne est calculée en une seule fois. R est d'ailleurs optimisé pour le calcul vectoriel.

\hypertarget{sect01424}{%
\subsubsection{Fonctions mathématiques}\label{sect01424}}

R propose un ensemble de fonctions de base pour effectuer du calcul. Voici une liste non-exhaustive des principales fonctions :

\begin{itemize}
\tightlist
\item
  \texttt{abs} calcule les valeurs absolues des valeurs d'un vecteur
\item
  \texttt{sqrt} calcule les racines carrées des valeurs d'un vecteur
\item
  \texttt{log} calcule les logarithmes des valeurs d'un vecteur
\item
  \texttt{exp} calcule les exponentielles des valeurs d'un vecteur
\item
  \texttt{factorial} calcule la factorielle des valeurs d'un vecteur
\item
  \texttt{round} arrondit les valeurs d'un vecteur
\item
  \texttt{ceiling}, \texttt{floor} arrondit à l'unité supérieure ou inférieure les valeurs d'un vecteur
\item
  \texttt{sin}, \texttt{asin}, \texttt{cos}, \texttt{acos}, \texttt{tan}, \texttt{atan} sont des fonctions de trigonométrie
\item
  \texttt{cumsum} calcule la somme cumulative des valeurs d'un vecteur.
\end{itemize}

Ces fonctions sont des fonctions vectorielles puisqu'elles s'appliquent à tous les éléments d'un vecteur. Si votre vecteur en entrée comprend cinq valeurs, le vecteur en sortie comprendra aussi cinq valeurs.

À l'inverse, les fonctions suivantes s'appliquent directement à l'ensemble d'un vecteur et ne vont renvoyer qu'une seule valeur :

\begin{itemize}
\tightlist
\item
  \texttt{sum} calcule la somme des valeurs d'un vecteur
\item
  \texttt{prod} calcule le produit des valeurs d'un vecteur
\item
  \texttt{min}, \texttt{max} renvoient les valeurs maximale et minimale d'un vecteur
\item
  \texttt{mean}, \texttt{median} renvoient la moyenne et la médiane d'un vecteur
\item
  \texttt{quantile} renvoit les percentiles d'un vecteur.
\end{itemize}

\hypertarget{sect01425}{%
\subsubsection{Fonctions pour manipuler des chaînes de caractères}\label{sect01425}}

Outre les données numériques, vous aurez à travailler avec des données de type texte (\texttt{string}). Le \textbf{tidyverse} avec le \emph{package} \textbf{stringr} offre des fonctions très intéressantes pour manipuler ce type de données. Pour un aperçu de toutes les fonctions offertes par \textbf{stringr}, référer-vous à sa \href{https://github.com/rstudio/cheatsheets/blob/master/strings.pdf}{\emph{Cheat Sheet}}. Commençons avec un \emph{DataFrame} assez simple comprenant des adresses et des noms de personnes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stringr)}

\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{noms =} \KeywordTok{c}\NormalTok{(}\StringTok{"Jérémy Toutanplace"}\NormalTok{,}\StringTok{"constant Tinople"}\NormalTok{,}\StringTok{"dino Resto"}\NormalTok{,}\StringTok{"Luce tancil"}\NormalTok{),}
  \DataTypeTok{adresses =} \KeywordTok{c}\NormalTok{(}\StringTok{'15 rue Levy'}\NormalTok{, }\StringTok{'413 Blvd Saint-Laurent'}\NormalTok{, }\StringTok{'3606 rue Duké'}\NormalTok{, }\StringTok{'2457 route St Marys'}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sect014251}{%
\paragraph{Majuscules et minuscules}\label{sect014251}}

Pour harmoniser ce \emph{dataframe}, nous allons dans un premier temps mettre des majuscules au premier caractère des prénoms et noms des individus avec la fonction \texttt{str\_to\_title}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\OperatorTok{$}\NormalTok{noms_corr <-}\StringTok{ }\KeywordTok{str_to_title}\NormalTok{(df}\OperatorTok{$}\NormalTok{noms)}
\KeywordTok{print}\NormalTok{(df}\OperatorTok{$}\NormalTok{noms_corr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Jérémy Toutanplace" "Constant Tinople"   "Dino Resto"        
## [4] "Luce Tancil"
\end{verbatim}

On pourrait également tout mettre en minuscules ou tout en majuscules.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\OperatorTok{$}\NormalTok{noms_min <-}\StringTok{ }\KeywordTok{tolower}\NormalTok{(df}\OperatorTok{$}\NormalTok{noms)}
\NormalTok{df}\OperatorTok{$}\NormalTok{noms_maj <-}\StringTok{ }\KeywordTok{toupper}\NormalTok{(df}\OperatorTok{$}\NormalTok{noms)}
\KeywordTok{print}\NormalTok{(df}\OperatorTok{$}\NormalTok{noms_min)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "jérémy toutanplace" "constant tinople"   "dino resto"        
## [4] "luce tancil"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(df}\OperatorTok{$}\NormalTok{noms_maj)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "JÉRÉMY TOUTANPLACE" "CONSTANT TINOPLE"   "DINO RESTO"        
## [4] "LUCE TANCIL"
\end{verbatim}

\hypertarget{sect014252}{%
\paragraph{Remplacer du texte}\label{sect014252}}

Les adresses comprennent des caractères accentués. Ce type de caractères pose régulièrement des problèmes d'encodage. Nous pourrions alors décider de les remplacer par des caractères simples avec la fonction \texttt{str\_replace\_all}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{str_replace_all}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses,}\StringTok{'é'}\NormalTok{,}\StringTok{'e'}\NormalTok{)}
\KeywordTok{print}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "15 rue Levy"            "413 Blvd Saint-Laurent" "3606 rue Duke"         
## [4] "2457 route St Marys"
\end{verbatim}

La même fonction peut être utilisée pour remplacer les \emph{St} par Saint et les \emph{Blvd} par Boulevard.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{str_replace_all}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{1}\NormalTok{,}\StringTok{' St '}\NormalTok{,}\StringTok{' Saint '}\NormalTok{)}
\NormalTok{df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{str_replace_all}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{2}\NormalTok{,}\StringTok{' Blvd '}\NormalTok{,}\StringTok{' Boulevard '}\NormalTok{)}
\KeywordTok{print}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "15 rue Levy"                 "413 Boulevard Saint-Laurent"
## [3] "3606 rue Duke"               "2457 route Saint Marys"
\end{verbatim}

\hypertarget{sect014253}{%
\paragraph{Découper du texte}\label{sect014253}}

Il est parfois nécessaire de découper du texte pour en extraire des éléments. On doit alors choisir un caractère de découpage. Dans notre exemple, on pourrait vouloir extraire les numéros civiques des adresses, en utilisant le premier espace comme caractère de découpage, en utilisant la fonction \texttt{str\_split\_fixed}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\OperatorTok{$}\NormalTok{num_civique <-}\StringTok{ }\KeywordTok{str_split_fixed}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{, }\StringTok{' '}\NormalTok{,}\DataTypeTok{n=}\DecValTok{2}\NormalTok{)[,}\DecValTok{1}\NormalTok{]}
\KeywordTok{print}\NormalTok{(df}\OperatorTok{$}\NormalTok{num_civique)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "15"   "413"  "3606" "2457"
\end{verbatim}

Pour être exact, sachez que pour notre exemple, la fonction \texttt{str\_split\_fixed} renvoie deux colonnes de texte : une avec le texte avant le premier espace, soit le numéro civique, et une avec le reste du texte. Le nombre de colonnes est contrôlé par l'argument \texttt{n}. Si \texttt{n\ =\ 1}, la fonction ne fait aucun découpage, avec \texttt{n\ =\ 2} la fonction va découper en deux parties le texte avec la première occurence du délimiteur, et ainsi de suite. En ajoutant \texttt{{[},1{]}} à la fin, nous indiquons que l'on souhaite garder seulement la première des deux colonnes.

Il est également possible d'extraire des parties de texte et de ne garder par exemple que les \emph{N} premiers caractères ou les \emph{N} derniers caractères :

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ne garder que les 5 premiers caractères}
\KeywordTok{substr}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{,}\DataTypeTok{start =} \DecValTok{1}\NormalTok{, }\DataTypeTok{stop =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "15 ru" "413 B" "3606 " "2457 "
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ne garder que les 5 derniers caractères}
\NormalTok{n_caract <-}\StringTok{ }\KeywordTok{nchar}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{)}
\KeywordTok{substr}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{, }\DataTypeTok{start =}\NormalTok{ n_caract}\DecValTok{-4}\NormalTok{, }\DataTypeTok{stop =}\NormalTok{ n_caract)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] " Levy" "urent" " Duke" "Marys"
\end{verbatim}

Notez que les paramètres \texttt{start} et \texttt{stop} de la fonction \texttt{substr} peuvent accepter un vecteur de valeurs. Il est ainsi possible d'appliquer une sélection de texte différente à chaque chaîne de caractères dans notre vecteur en entrée. On pourrait par exemple vouloir récupérer tout le texte avant le second espace pour garder uniquement le numéro civique et le type de rue.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# étape 1 : récupérer les positions des espaces pour chaque adresses}
\NormalTok{positions <-}\StringTok{ }\KeywordTok{str_locate_all}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{, }\StringTok{" "}\NormalTok{)}

\CommentTok{# étape 2 : récupérer les positions des seconds espaces}
\NormalTok{sec_positions <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(positions, }\ControlFlowTok{function}\NormalTok{(i)\{}
\NormalTok{  i[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{\})}

\CommentTok{# étape 3 : appliquer le découpage}
\KeywordTok{substr}\NormalTok{(df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{, }\DataTypeTok{start =} \DecValTok{1}\NormalTok{, }\DataTypeTok{stop =}\NormalTok{ sec_positions}\DecValTok{-1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "15 rue"        "413 Boulevard" "3606 rue"      "2457 route"
\end{verbatim}

\hypertarget{sect014254}{%
\paragraph{Coller du texte}\label{sect014254}}

À l'inverse du découpage, il est parfois nécessaire de concaténer des éléments de texte, ce qu'il est possible de réaliser avec la fonction \texttt{paste}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\OperatorTok{$}\NormalTok{texte_complet <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(df}\OperatorTok{$}\NormalTok{noms_corr, df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{, }\DataTypeTok{sep =} \StringTok{" : "}\NormalTok{)}
\KeywordTok{print}\NormalTok{(df}\OperatorTok{$}\NormalTok{texte_complet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Jérémy Toutanplace : 15 rue Levy"              
## [2] "Constant Tinople : 413 Boulevard Saint-Laurent"
## [3] "Dino Resto : 3606 rue Duke"                    
## [4] "Luce Tancil : 2457 route Saint Marys"
\end{verbatim}

Le paramètre \texttt{sep} permet d'indiquer le ou les caractères à intercaler entre les éléments à concaténer. Notez qu'il est possible de concaténer plus que deux éléments.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\OperatorTok{$}\NormalTok{ville <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Montreal'}\NormalTok{,}\StringTok{'Montreal'}\NormalTok{,}\StringTok{'Montreal'}\NormalTok{,}\StringTok{'Montreal'}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(df}\OperatorTok{$}\NormalTok{noms_corr, df}\OperatorTok{$}\NormalTok{adresses_}\DecValTok{3}\NormalTok{, df}\OperatorTok{$}\NormalTok{ville, }\DataTypeTok{sep =} \StringTok{", "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Jérémy Toutanplace, 15 rue Levy, Montreal"              
## [2] "Constant Tinople, 413 Boulevard Saint-Laurent, Montreal"
## [3] "Dino Resto, 3606 rue Duke, Montreal"                    
## [4] "Luce Tancil, 2457 route Saint Marys, Montreal"
\end{verbatim}

\hypertarget{sect01426}{%
\subsubsection{Manipuler des colonnes de type date}\label{sect01426}}

Nous avons vu que les principaux types de données dans R sont le numérique, le texte, le booléen et le facteur. Il existe d'autres types introduits par différents \emph{packages}. Nous abordons ici les types date et heure (\emph{date} and \emph{time}). Pour les manipuler, nous privilégions l'utilisation du \emph{package} \textbf{lubridate} du \textbf{tidyverse}. Pour illuster le tout, nous l'appliquerons avec un jeu de données ouvertes de la Ville de Montréal représentant les collisions routières impliquant au moins un cycliste survenues après le 1\textsuperscript{er} janvier 2017.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accidents_df <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\DataTypeTok{file=}\StringTok{"data/priseenmain/accidents.csv"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{)}
\KeywordTok{names}\NormalTok{(accidents_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "HEURE_ACCDN"       "DT_ACCDN"          "NB_VICTIMES_TOTAL"
\end{verbatim}

Nous disposons de trois colonnes représentant respectivement l'heure, la date et le nombre de victimes impliquées dans la collision.

\hypertarget{sect014261}{%
\paragraph{Du texte à la date}\label{sect014261}}

Actuellement, les colonnes \emph{HEURE\_ACCDN} et \emph{DT\_ACCDN} sont au format texte. Nous pouvons afficher quelques lignes du jeu de données avec la fonction \texttt{head} pour visualiser comment elles ont été saisies.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(accidents_df, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         HEURE_ACCDN   DT_ACCDN NB_VICTIMES_TOTAL
## 1 16:00:00-16:59:00 2017/11/02                 0
## 2 06:00:00-06:59:00 2017/01/16                 1
## 3 18:00:00-18:59:00 2017/04/18                 0
## 4 11:00:00-11:59:00 2017/05/28                 1
## 5 15:00:00-15:59:00 2017/05/28                 1
\end{verbatim}

Un peu de ménage s'impose : les heures sont indiquées comme des périodes d'une heure. Nous utilisons la fonction \texttt{str\_split\_fixed} du \emph{package} \textbf{stringr} pour ne garder que la première partie de l'heure (avant le tiret). Nous allons ensuite concaténer l'heure et la date avec la fonction \texttt{paste}, puis nous convertirons ce résultat en un objet \emph{date-time}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lubridate)}

\CommentTok{# Étape 1 : découper la colonne Heure_ACCDN}
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{heure <-}\StringTok{ }\KeywordTok{str_split_fixed}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{HEURE_ACCDN, }\StringTok{"-"}\NormalTok{, }\DataTypeTok{n=}\DecValTok{2}\NormalTok{)[,}\DecValTok{1}\NormalTok{]}

\CommentTok{# Étape 2 : concaténer l'heure et la date}
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{date_heure <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{DT_ACCDN, }
\NormalTok{                                 accidents_df}\OperatorTok{$}\NormalTok{heure,}
                                 \DataTypeTok{sep =} \StringTok{' '}\NormalTok{)}

\CommentTok{# Étape 3 : convertir au format datetime}
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{datetime <-}\StringTok{ }\KeywordTok{as_datetime}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{date_heure,}
                                     \DataTypeTok{format =} \StringTok{"%Y/%m/%d %H:%M:%S"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Pour effectuer la conversion, nous avons utilisé la fonction \texttt{as\_datetime} du package \textbf{lubridate}. Elle prend comme paramètre un vecteur de texte et une indication du format de ce vecteur de texte. Il existe de nombreuses façons de spécifier une date et une heure et l'argument \emph{format} permet d'indiquer celle à utiliser. Dans cet exemple, la date est structurée comme suit :
\texttt{année/mois/jour\ heure:minute:seconde}, ce qui se traduit par le format \texttt{\%Y/\%m/\%d\ \%H:\%M:\%S}.

\begin{itemize}
\tightlist
\item
  \%Y signifie une année indiquée avec quatre caractères : 2017
\item
  \%m signifie un mois, indiqué avec deux caractères : 01, 02, 03, \ldots{} 12
\item
  \%d signifie un jour, indiqué avec deux caractères : 01, 02, 03, \ldots{} 31
\item
  \%H signifie une heure, au format 24 heures avec deux caractères : 00, 02, \ldots{} 23
\item
  \%M signifie des minutes indiquées avec deux caractères : 00, 02, \ldots{} 59
\item
  \%S signifie des secondes, indiquées avec deux caractères : 00, 02, \ldots{} 59
\end{itemize}

Notez que les caractères séparant les années, jours, heures, etc. sont aussi à indiquer dans le format. Dans notre exemple, nous utilisons des \texttt{/} pour séparer les éléments de la date et des \texttt{:} pour l'heure, et un espace pour séparer la date et l'heure.

Il existe d'autres nomenclatures pour spécifier un format \emph{datetime} : par exemple, des mois renseignés par leur nom, l'indication AM-PM, etc. Vous pouvez vous référez à la documentation de la fonction \texttt{strptime} (\texttt{help(strptime)}) pour explorer les différentes nomenclatures et choisir celle qui vous convient. Bien évidemment, il est \textbf{nécessaire} que toutes les dates de votre colonne soient renseignées dans le même format. Sinon, la fonction renverra des valeurs \texttt{NA} aux endroits où elle a échoué à lire le format. Après toutes ces opérations, rejettons un oeil à notre \emph{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(accidents_df, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         HEURE_ACCDN   DT_ACCDN NB_VICTIMES_TOTAL    heure          date_heure
## 1 16:00:00-16:59:00 2017/11/02                 0 16:00:00 2017/11/02 16:00:00
## 2 06:00:00-06:59:00 2017/01/16                 1 06:00:00 2017/01/16 06:00:00
## 3 18:00:00-18:59:00 2017/04/18                 0 18:00:00 2017/04/18 18:00:00
## 4 11:00:00-11:59:00 2017/05/28                 1 11:00:00 2017/05/28 11:00:00
## 5 15:00:00-15:59:00 2017/05/28                 1 15:00:00 2017/05/28 15:00:00
##              datetime
## 1 2017-11-02 16:00:00
## 2 2017-01-16 06:00:00
## 3 2017-04-18 18:00:00
## 4 2017-05-28 11:00:00
## 5 2017-05-28 15:00:00
\end{verbatim}

\hypertarget{sect014262}{%
\paragraph{Extraire des informations d'une date}\label{sect014262}}

À partir de la nouvelle colonne \texttt{datetime}, nous sommes en mesure d'extraire des informations intéressantes comme :

\begin{itemize}
\tightlist
\item
  le nom du jour de la semaine avec la fonction \texttt{weekdays}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{jour <-}\StringTok{ }\KeywordTok{weekdays}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{datetime)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  la période de la journée avec les fonctions \texttt{am} et \texttt{pm}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{AM <-}\StringTok{ }\KeywordTok{am}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{datetime)}
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{PM <-}\StringTok{ }\KeywordTok{pm}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{datetime)}
\KeywordTok{head}\NormalTok{(accidents_df[}\KeywordTok{c}\NormalTok{(}\StringTok{"jour"}\NormalTok{, }\StringTok{"AM"}\NormalTok{, }\StringTok{"PM"}\NormalTok{)], }\DataTypeTok{n=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       jour    AM    PM
## 1    jeudi FALSE  TRUE
## 2    lundi  TRUE FALSE
## 3    mardi FALSE  TRUE
## 4 dimanche  TRUE FALSE
## 5 dimanche FALSE  TRUE
\end{verbatim}

Il est aussi possible d'accéder aux sous-éléments d'un \emph{datetime} comme l'année, le mois, le jour, l'heure, la minute, la seconde avec les fonctions \texttt{year()}, \texttt{month()},\texttt{day()}, \texttt{hour()}, \texttt{minute()} et \texttt{second()}.

\hypertarget{sect014263}{%
\paragraph{\texorpdfstring{Calculer une durée entre deux \emph{datetime}}{Calculer une durée entre deux datetime}}\label{sect014263}}

Une autre utilisation intéressante du format \emph{datetime} est de calculer des différences de temps. Par exemple, nous pourrions utiliser le nombre de minutes écoulées depuis 7h00 le matin comme une variable dans une analyse visant à déterminer le moment critique des collisions routières durant l'heure de pointe du matin.
Pour cela, nous devons créer un \emph{datetime} de référence en concaténant la date de chaque observation, et le temps \texttt{07:00:00} qui sera notre point de départ.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{date_heure_}\DecValTok{07}\NormalTok{ <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{DT_ACCDN, }
                                 \StringTok{'07:00:00'}\NormalTok{,}
                                 \DataTypeTok{sep =} \StringTok{' '}\NormalTok{)}
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{ref_datetime <-}\StringTok{ }\KeywordTok{as_datetime}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{date_heure_}\DecValTok{07}\NormalTok{,}
                                     \DataTypeTok{format =} \StringTok{"%Y/%m/%d %H:%M:%S"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Il ne nous reste plus qu'à calculer la différence de temps entre la colonne \emph{datetime} et notre temps de référence \emph{ref\_datetime}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time <-}\StringTok{ }\KeywordTok{difftime}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{datetime,}
\NormalTok{                                   accidents_df}\OperatorTok{$}\NormalTok{ref_datetime,}
                                   \DataTypeTok{units =} \StringTok{'min'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Notez qu'ici la colonne \emph{diff\_time} est d'un type spécial : une différence temporelle (\emph{difftime}). Il faut encore la convertir au format numérique pour pourvoir l'utiliser avec la fonction \texttt{as.numeric}. Par curiosité, réalisons rapidement un histogramme avec la fonction \texttt{hist} pour analyser rapidement cette variable d'écart de temps !

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{diff_time)}
\KeywordTok{hist}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{diff_time_num, }\DataTypeTok{breaks =} \DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig015-1} 

}

\caption{Répartition temporelle des accidents à vélo}\label{fig:fig015}
\end{figure}

On observe clairement deux pics, un premier entre 0 et 100 ( entre 07h00 08h30 environ) et un second plus important entre 550 et 650 (entre 16h00 et 17h30 environ), ce qui correspond sans surprise aux heures de pointe. Il est intéressant de noter que plus d'accidents se produisent à l'heure de pointe du soir qu'à celle du matin.

\hypertarget{sect014264}{%
\paragraph{Tenir compte du fuseau horaire}\label{sect014264}}

Lorsque l'on travaille avec des données provenant de différents endroits dans le monde ou que l'on doive tenir compte des heures d'été et d'hiver, il convient de tenir compte du fuseau horaire. Pour créer une date avec un fuseau horaire, il est possible d'utiliser le paramètre \texttt{tz} dans la fonction \texttt{as\_datetime} et d'utiliser l'identifiant du fuseau approprié. Dans notre cas, les données d'accident ont été collectées à Montréal qui a un décalage de -5 heures par rapport au temps de référence UTC (+1 heure en été). Le code spécifique de ce fuseau horaire est \emph{EDT}, il est facile de trouver ces codes avec le site web \href{https://www.timeanddate.com/time/map/}{timeanddate.com}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{datetime <-}\StringTok{ }\KeywordTok{as_datetime}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{date_heure,}
                                     \DataTypeTok{format =} \StringTok{"%Y/%m/%d %H:%M:%S"}\NormalTok{,}
                                     \DataTypeTok{tz =} \StringTok{"EDT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sect01427}{%
\subsubsection{Recoder des variables}\label{sect01427}}

Recoder des variables signifie changer la valeur d'une variable selon une condition afin d'obtenir une nouvelle variable. Si nous reprenons le jeu de données précédent sur les accidents à vélo, nous pourrions vouloir créer une nouvelle colonne nous indiquant si la collision a eu lieu en heures de pointe ou non. On obtiendrait ainsi une nouvelle variable avec seulement deux catégories plutôt que la variable numérique originale. Nous pourrions aussi définir quatre catégories avec l'heure de pointe du matin, l'heure de pointe du soir, le reste de la journée et la nuit.

\hypertarget{sect014271}{%
\paragraph{Le cas binaire avec ifelse}\label{sect014271}}

Si l'on ne souhaite créer que deux catégories, le plus simple est d'utiliser la fonction \texttt{ifelse}. Cette fonction va évaluer une condition (section \ref{sect0135}) pour chaque ligne d'un \emph{DataFrame} et produire un nouveau vecteur. Créons donc une variable binaire indiquant si une collision a eu lieu durant les heures de pointe ou hors heures de pointe. Nous devons alors évaluer les conditions suivantes :

Est-ce que l'accident a eu lieu entre 07h00 (0) \textbf{ET} 09h00 (120), \textbf{OU} est ce que la collision a eu lieu entre 16h30 (570) \textbf{ET} 18h30 (690)?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{diff_time_num))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## FALSE  TRUE 
##  2414    40
\end{verbatim}

Notons dans un premier temps que nous avons 40 observations sans valeurs pour la colonne \texttt{diff\_time\_num}. Il s'agit d'observations pour lesquelles nous ne disposions pas de dates au départ.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Cond1 <-}\StringTok{ }\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{>=}\StringTok{ }\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{<=}\StringTok{ }\DecValTok{120}
\NormalTok{Cond2 <-}\StringTok{ }\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{>=}\StringTok{ }\DecValTok{570} \OperatorTok{&}\StringTok{ }\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{<=}\StringTok{ }\DecValTok{690}

\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{moment_bin <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(Cond1 }\OperatorTok{|}\StringTok{ }\NormalTok{Cond2,}
                                  \StringTok{"en heures de pointe"}\NormalTok{,}
                                  \StringTok{"hors heures de pointe"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Comme vous pouvez le constater, la fonction \texttt{ifelse} nécessite trois arguments :

\begin{itemize}
\tightlist
\item
  Une condition, pouvant être \texttt{TRUE} ou \texttt{FALSE},
\item
  La valeur à renvoyer si la condition est \texttt{FALSE}
\end{itemize}

Avec la fonction \texttt{table}, nous pouvons rapidement visualisuer les effectifs des deux catégories ainsi créées :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{moment_bin)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   en heures de pointe hors heures de pointe 
##                   841                  1573
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# vérifier si on a toujours seulement 40 NA}
\KeywordTok{table}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{moment_bin))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## FALSE  TRUE 
##  2414    40
\end{verbatim}

Les heures de pointe représentent quatre heures de la journée, ce qui nous laisse neuf heures hors heures de pointe entre 07h00 et 20h00.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ratio de collisions routières en heures de pointe}
\NormalTok{(}\DecValTok{841} \OperatorTok{/}\StringTok{ }\DecValTok{2414}\NormalTok{) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{4} \OperatorTok{/}\StringTok{ }\DecValTok{13}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.132249
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Ratio de collisions routières hors heure de pointe}
\NormalTok{(}\DecValTok{1573} \OperatorTok{/}\StringTok{ }\DecValTok{2414}\NormalTok{) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{9} \OperatorTok{/}\StringTok{ }\DecValTok{13}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9412225
\end{verbatim}

En rapportant les collisions aux durées des deux périodes, on observe une nette surreprésentation des collisions impliquant un vélo pendant les heures de pointe d'environ 13\% comparativement à la période hors des heures de pointe.

\hypertarget{sect014272}{%
\paragraph{\texorpdfstring{Le cas multiple avec la fonction \emph{case\_when}}{Le cas multiple avec la fonction case\_when}}\label{sect014272}}

Lorsque l'on souhaite créer plus que deux catégories, il est possible soit d'enchaîner plusieurs fonctions \texttt{ifelse} (ce qui produit un code plus long et moins lisible), soit d'utiliser la fonction \texttt{case\_when} du \emph{package} \textbf{dplyr} du \textbf{tidyverse}. Reprenons notre exemple et créons quatre catégories :

\begin{itemize}
\tightlist
\item
  En heures de pointe du matin
\item
  En heures de pointe du soir
\item
  Le reste de la journée (entre 07:00 et 20:00)
\item
  La nuit (entre 21:00 et 07:00)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}

\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{moment_multi <-}\StringTok{ }\KeywordTok{case_when}\NormalTok{(}
\NormalTok{  accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{>=}\StringTok{ }\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{<=}\StringTok{ }\DecValTok{120} \OperatorTok{~}\StringTok{ "pointe matin"}\NormalTok{,}
\NormalTok{  accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{>=}\StringTok{ }\DecValTok{570} \OperatorTok{&}\StringTok{ }\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{<=}\StringTok{ }\DecValTok{690} \OperatorTok{~}\StringTok{ "pointe soir"}\NormalTok{,}
\NormalTok{  accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{>}\StringTok{ }\DecValTok{690} \OperatorTok{&}\StringTok{ }\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{<}\StringTok{ }\DecValTok{780} \OperatorTok{~}\StringTok{ "journee"}\NormalTok{,}
\NormalTok{  accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{>}\StringTok{ }\DecValTok{120} \OperatorTok{&}\StringTok{ }\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{<}\StringTok{ }\DecValTok{570} \OperatorTok{~}\StringTok{ "journee"}\NormalTok{,}
\NormalTok{  accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{<}\StringTok{ }\DecValTok{0} \OperatorTok{|}\StringTok{ }\NormalTok{accidents_df}\OperatorTok{$}\NormalTok{diff_time_num }\OperatorTok{>=}\StringTok{ }\DecValTok{780} \OperatorTok{~}\StringTok{ "nuit"}
\NormalTok{)}

\KeywordTok{table}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{moment_multi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      journee         nuit pointe matin  pointe soir 
##         1155          418          404          437
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#vérifions encore les NA}
\KeywordTok{table}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(accidents_df}\OperatorTok{$}\NormalTok{moment_multi))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## FALSE  TRUE 
##  2414    40
\end{verbatim}

La syntaxe de cette fonction est un peu particulière. Elle accepte un nombre illimité d'arguments. Chaque argument est composé d'une condition et d'une valeur à renvoyer si la condition est vraie; ces deux éléments étant reliés par le symbole \texttt{\textasciitilde{}}. Notez que toutes les évaluations sont effectuées dans l'ordre des arguments. En d'autres termes, la fonction va d'abord tester la première condition et assigner ces valeurs, puis recommencer pour les prochaines conditions. Ainsi, si une observation (ligne du tableau de données) obtient \texttt{TRUE} à plusieurs conditions, elle obtiendra la valeur de la dernière condition qu'elle a validée.

\hypertarget{sect01428}{%
\subsubsection{\texorpdfstring{Sous-sélection d'un \emph{DataFrame}}{Sous-sélection d'un DataFrame}}\label{sect01428}}

Dans cette section, nous verrons comment extraire des sous-parties d'un \emph{DataFrame}. Il est possible de sous-sélectionner des lignes et des colonnes en se basant sur des conditions ou leurs index. Pour cela, nous allons utiliser un jeu de données fourni avec R : le jeu de données \textbf{iris} décrivant des fleurs du même nom.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"iris"}\NormalTok{)}

\CommentTok{# Nombre de lignes et de colonnes}
\KeywordTok{dim}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 150   5
\end{verbatim}

\hypertarget{sect014281}{%
\paragraph{Sous-sélection des lignes}\label{sect014281}}

Sous-sélectionner des lignes par index est relativement simple. Admettons que nous souhaitons sélectionner les lignes 1 à 5, 10 à 25, 37 et 58.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub_iris <-}\StringTok{ }\NormalTok{iris[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{10}\OperatorTok{:}\DecValTok{25}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{58}\NormalTok{),]}
\KeywordTok{nrow}\NormalTok{(sub_iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 23
\end{verbatim}

Sous-sélectionner des lignes avec une condition peut être effectué soit avec une syntaxe similaire, soit en utilisant la fonction \texttt{subset}. Sélectionnons toutes les fleurs de l'espèce Virginica.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_virginica1 <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "virginica"}\NormalTok{,]}
\NormalTok{iris_virginica2 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(iris, iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "virginica"}\NormalTok{)}

\CommentTok{# Vérifions que les deux dataframes ont le même nombre de lignes}
\KeywordTok{nrow}\NormalTok{(iris_virginica1) }\OperatorTok{==}\StringTok{ }\KeywordTok{nrow}\NormalTok{(iris_virginica2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Vous pouvez utiliser dans les deux cas tous les opérateurs vus dans les sections \ref{sect01352} et \ref{sect01353}. L'enjeu est d'arriver à un vecteur booléen final permettant d'identifier les observations à conserver.

\hypertarget{sect014282}{%
\paragraph{Sous-sélectionner des colonnes}\label{sect014282}}

Nous avons déjà vu comment sélectionner des colonnes en utilisant leur nom ou leur index dans la section \ref{sect014221}. Ajoutons ici un cas particulier où nous souhaiterions sélectionner des colonnes selon une condition. Par exemple, nous pourrions vouloir conserver que les colonnes comprenant le mot \emph{Length}. Pour cela, nous utiliserons la fonction \texttt{grepl}, permettant de déterminer si des caractères sont présents dans une chaîne de caractères.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nom_cols <-}\StringTok{ }\KeywordTok{names}\NormalTok{(iris)}
\KeywordTok{print}\NormalTok{(nom_cols)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width"  "Species"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_nom <-}\StringTok{ }\KeywordTok{grepl}\NormalTok{(}\StringTok{"Length"}\NormalTok{,nom_cols, }\DataTypeTok{fixed =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{ok_nom <-}\StringTok{ }\NormalTok{nom_cols[test_nom]}

\NormalTok{iris_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{iris[ok_nom]}
\KeywordTok{print}\NormalTok{(}\KeywordTok{names}\NormalTok{(iris_}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Sepal.Length" "Petal.Length"
\end{verbatim}

Il est possible d'obtenir ce résultat en une seule ligne de code, mais elle est un peu moins lisible.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris2 <-}\StringTok{ }\NormalTok{iris[}\KeywordTok{names}\NormalTok{(iris)[}\KeywordTok{grepl}\NormalTok{(}\StringTok{"Length"}\NormalTok{,}\KeywordTok{names}\NormalTok{(iris), }\DataTypeTok{fixed =} \OtherTok{TRUE}\NormalTok{)]]}
\end{Highlighting}
\end{Shaded}

\hypertarget{sect014283}{%
\paragraph{Sélectionner des colonnes et des lignes}\label{sect014283}}

Nous avons vu qu'avec les crochets, nous pouvons extraire les colonnes et les lignes d'un \emph{DataFrame}. Il est possible de combiner les deux opérations simultanément. Pour ce faire, il faut indiquer en premier les indices ou la condition permettant de sélectionner une ligne, puis les indices ou la condition pour sélectionner les colonnes : \texttt{{[}index\_lignes\ ,\ index\_colonnes{]}}. Sélectionnons cinq premières lignes et les les trois premières colonnes du jeu de données iris :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_5x3 <-}\StringTok{ }\NormalTok{iris[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)]}
\KeywordTok{print}\NormalTok{(iris_5x3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length
## 1          5.1         3.5          1.4
## 2          4.9         3.0          1.4
## 3          4.7         3.2          1.3
## 4          4.6         3.1          1.5
## 5          5.0         3.6          1.4
\end{verbatim}

Combinons nos deux exemples précédents pour sélectionner uniquement les lignes avec des fleurs de l'espèce virginica, et les colonnes avec le mot Length.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_virginica3 <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "virginica"}\NormalTok{,}
                       \KeywordTok{names}\NormalTok{(iris)[}\KeywordTok{grepl}\NormalTok{(}\StringTok{"Length"}\NormalTok{,}\KeywordTok{names}\NormalTok{(iris), }\DataTypeTok{fixed =} \OtherTok{TRUE}\NormalTok{)]]}
\KeywordTok{head}\NormalTok{(iris_virginica3, }\DataTypeTok{n=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Sepal.Length Petal.Length
## 101          6.3          6.0
## 102          5.8          5.1
## 103          7.1          5.9
## 104          6.3          5.6
## 105          6.5          5.8
\end{verbatim}

\hypertarget{sect01429}{%
\subsubsection{\texorpdfstring{Fusionner des \emph{DataFrames}}{Fusionner des DataFrames}}\label{sect01429}}

Terminons cette section avec la fusion de \emph{DataFrames} qu'il est possible de réaliser de deux façons, soit par ajout, soit par jointure.

\hypertarget{sect014291}{%
\paragraph{\texorpdfstring{Fusionner des \emph{DataFrame} par ajout}{Fusionner des DataFrame par ajout}}\label{sect014291}}

Ajouter deux \emph{DataFrames} peut se faire en fonction de leurs colonnes, ou en fonction de leurs lignes. Dans ces deux cas, on utilisera respectivement les fonction \texttt{cbind} et \texttt{rbind}. La figure \ref{fig:fig016} résume graphiquement le fonctionnement des deux fonctions.

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{images/introduction/rbind_cbind} 

}

\caption{Fusion de DataFrames}\label{fig:fig016}
\end{figure}

Pour que \texttt{cbind} fonctionne, il faut que les deux \emph{DataFrames} aient le même nombre de lignes. Pour \texttt{rbind}, les deux \emph{DataFrames} doivent avoir le même nombre de colonnes. Prenons à nouveau comme exemple le jeu de données iris. Nous allons commencer par le séparer en trois sous-jeux de données comprenant chacun une espèce d'iris. Puis, nous fusionnerons deux d'entre eux avec la fonction \texttt{rbind}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris1 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(iris, iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "virginica"}\NormalTok{)}
\NormalTok{iris2 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(iris, iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "versicolor"}\NormalTok{)}
\NormalTok{iris3 <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(iris, iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "setosa"}\NormalTok{)}

\NormalTok{iris_comb <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(iris2,iris3)}
\end{Highlighting}
\end{Shaded}

Nous pourrions aussi extraire dans les deux \emph{DataFrames} les colonnes comprenant le mot \emph{Length} et le mot \emph{Width}, puis les fusionner.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_l <-}\StringTok{ }\NormalTok{iris[}\KeywordTok{names}\NormalTok{(iris)[}\KeywordTok{grepl}\NormalTok{(}\StringTok{"Length"}\NormalTok{,}\KeywordTok{names}\NormalTok{(iris), }\DataTypeTok{fixed =} \OtherTok{TRUE}\NormalTok{)]]}
\NormalTok{iris_w <-}\StringTok{ }\NormalTok{iris[}\KeywordTok{names}\NormalTok{(iris)[}\KeywordTok{grepl}\NormalTok{(}\StringTok{"Width"}\NormalTok{,}\KeywordTok{names}\NormalTok{(iris), }\DataTypeTok{fixed =} \OtherTok{TRUE}\NormalTok{)]]}

\NormalTok{iris_comb <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(iris_l,iris_w)}
\KeywordTok{names}\NormalTok{(iris_comb)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Sepal.Length" "Petal.Length" "Sepal.Width"  "Petal.Width"
\end{verbatim}

\hypertarget{sect014292}{%
\paragraph{\texorpdfstring{Joindre des \emph{DataFrame}}{Joindre des DataFrame}}\label{sect014292}}

Une jointure est une opération un peu plus complexe qu'un simple ajout. L'idée est d'associer des informations de plusieurs \emph{DataFrames} en utilisant une colonne (appelée une clef) présente dans les deux jeux de données. On distingue plusieurs types de jointure :

\begin{itemize}
\tightlist
\item
  Les jointures internes permettant de combiner les éléments communs entre deux \emph{DataFrames} A et B
\item
  La jointure complète permettant de combiner les éléments présents dans A ou B
\item
  La jointure à gauche, permettant de ne conserver que les éléments présents dans A même s'ils ne trouvent pas leur correspondance dans B.
\end{itemize}

Ces trois jointures sont présentées à la figure \ref{fig:fig016}; pour ces trois cas, la colonne commune se nomme \emph{id}.

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{images/introduction/merging} 

}

\caption{Jointure de DataFrames}\label{fig:fig017}
\end{figure}

Vous retiendrez que les deux dernières jointures peuvent produire des valeurs manquantes. Pour réaliser ces opérations, on utilise la fonction \texttt{merge}. Prenons un exemple simple à partir d'un petit jeu de données.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auteurs <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
    \DataTypeTok{name =} \KeywordTok{c}\NormalTok{(}\StringTok{"Tukey"}\NormalTok{, }\StringTok{"Venables"}\NormalTok{, }\StringTok{"Tierney"}\NormalTok{, }\StringTok{"Ripley"}\NormalTok{, }\StringTok{"McNeil"}\NormalTok{, }\StringTok{"Apparicio"}\NormalTok{),}
    \DataTypeTok{nationality =} \KeywordTok{c}\NormalTok{(}\StringTok{"US"}\NormalTok{, }\StringTok{"Australia"}\NormalTok{, }\StringTok{"US"}\NormalTok{, }\StringTok{"UK"}\NormalTok{, }\StringTok{"Australia"}\NormalTok{, }\StringTok{"Canada"}\NormalTok{),}
    \DataTypeTok{retired =} \KeywordTok{c}\NormalTok{(}\StringTok{"yes"}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\StringTok{"no"}\NormalTok{, }\DecValTok{5}\NormalTok{)))}
\NormalTok{livres <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
    \DataTypeTok{aut =} \KeywordTok{c}\NormalTok{(}\StringTok{"Tukey"}\NormalTok{, }\StringTok{"Venables"}\NormalTok{, }\StringTok{"Tierney"}\NormalTok{, }\StringTok{"Ripley"}\NormalTok{, }\StringTok{"Ripley"}\NormalTok{, }\StringTok{"McNeil"}\NormalTok{,}\StringTok{"Wickham"}\NormalTok{),}
    \DataTypeTok{title =} \KeywordTok{c}\NormalTok{(}\StringTok{"Exploratory Data Analysis"}\NormalTok{,}
              \StringTok{"Modern Applied Statistics ..."}\NormalTok{,}
              \StringTok{"LISP-STAT"}\NormalTok{,}
              \StringTok{"Spatial Statistics"}\NormalTok{, }\StringTok{"Stochastic Simulation"}\NormalTok{,}
               \StringTok{"Interactive Data Analysis"}\NormalTok{, }\StringTok{"R for Data Science"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Nous avons donc deux \emph{DataFrames}, le premier décrivant des auteurs et le second des livres. Effectuons une première jointure interne afin de savoir pour chaque livre la nationnalité de son auteur et si ce dernier est à la retraite.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(livres, auteurs, }\CommentTok{#les deux DataFrames }
             \DataTypeTok{by.x =} \StringTok{"aut"}\NormalTok{, }\DataTypeTok{by.y =} \StringTok{'name'}\NormalTok{, }\CommentTok{#les noms des colonnes de jointures}
             \DataTypeTok{all.x =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{all.y =} \OtherTok{FALSE}\NormalTok{)}

\KeywordTok{print}\NormalTok{(df1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        aut                         title nationality retired
## 1   McNeil     Interactive Data Analysis   Australia      no
## 2   Ripley            Spatial Statistics          UK      no
## 3   Ripley         Stochastic Simulation          UK      no
## 4  Tierney                     LISP-STAT          US      no
## 5    Tukey     Exploratory Data Analysis          US     yes
## 6 Venables Modern Applied Statistics ...   Australia      no
\end{verbatim}

Cette jointure est interne car les deux paramètres \emph{all.x} et \emph{all.y} ont pour valeur \texttt{FALSE}. Ainsi, nous indiquons à la fonction que nous ne souhaitons ni garder tous les éléments du premier \emph{DataFrame} ni tous les éléments du second, mais uniquement les éléments présents dans les deux. Vous noterez ainsi que le livre ``R for Data Science'' n'est pas présent dans le jeu de données final car son auteur ``Wickham'' ne fait pas partie du \emph{DataFrame} auteurs. De même, l'auteur ``Apparicio'' n'apparaît pas dans la jointure, car aucun livre dans le \emph{DataFrame} books n'a été écrit par cet auteur.

Pour conserver tous les livres, nous pouvons effectuer une jointure à gauche en renseignant \texttt{all.x\ =\ TRUE}. Nous allons ainsi forcer la fonction à garder tous les livres et mettre des valeurs vides aux informations manquantes des auteurs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(livres, auteurs, }\CommentTok{#les deux DataFrames }
             \DataTypeTok{by.x =} \StringTok{"aut"}\NormalTok{, }\DataTypeTok{by.y =} \StringTok{'name'}\NormalTok{, }\CommentTok{#les noms des colonnes de jointures}
             \DataTypeTok{all.x =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{all.y =} \OtherTok{FALSE}\NormalTok{)}

\KeywordTok{print}\NormalTok{(df2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        aut                         title nationality retired
## 1   McNeil     Interactive Data Analysis   Australia      no
## 2   Ripley            Spatial Statistics          UK      no
## 3   Ripley         Stochastic Simulation          UK      no
## 4  Tierney                     LISP-STAT          US      no
## 5    Tukey     Exploratory Data Analysis          US     yes
## 6 Venables Modern Applied Statistics ...   Australia      no
## 7  Wickham            R for Data Science        <NA>    <NA>
\end{verbatim}

Et pour garder tous les livres et tous les auteurs, nous pouvons faire une jointure complète en indiquant \texttt{all.x\ =\ TRUE} et \texttt{all.y\ =\ TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df3 <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(livres, auteurs, }\CommentTok{#les deux DataFrames }
             \DataTypeTok{by.x =} \StringTok{"aut"}\NormalTok{, }\DataTypeTok{by.y =} \StringTok{'name'}\NormalTok{, }\CommentTok{#les noms des colonnes de jointures}
             \DataTypeTok{all.x =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{all.y =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{print}\NormalTok{(df3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         aut                         title nationality retired
## 1 Apparicio                          <NA>      Canada      no
## 2    McNeil     Interactive Data Analysis   Australia      no
## 3    Ripley            Spatial Statistics          UK      no
## 4    Ripley         Stochastic Simulation          UK      no
## 5   Tierney                     LISP-STAT          US      no
## 6     Tukey     Exploratory Data Analysis          US     yes
## 7  Venables Modern Applied Statistics ...   Australia      no
## 8   Wickham            R for Data Science        <NA>    <NA>
\end{verbatim}

\hypertarget{sect016}{%
\section{Bien structurer un code R}\label{sect016}}

Terminons ici avec quelques conseils sur la rédaction d'un code R. Bien rédiger son code est essentiel pour trois raisons :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  pouvoir relire et réutiliser son code dans le futur.
\item
  permettre à d'autres personnes de réutiliser ou relire son code.
\item
  minimiser les risques d'erreurs.
\end{enumerate}

Ne négligez pas l'importance d'un code bien rédigé et bien documenté, vous vous éviterez ainsi des migraines lorsque vous devrez exhumer du code écrit il y a plusieurs mois.

Voici quelques lignes directrices peu contraignantes, mais qui devraient vous être utiles :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Privilégier la clarté à la concision} : il vaut mieux parfois scinder une ligne de code en plusieurs sous-étapes afin de faciliter la lecture de l'opération réalisée. Par exemple, si l'on reprend une ligne de code d'une section précédente où nous sélectionnions l'ensemble des colonnes du jeu de données \texttt{iris} comprenant le mot \texttt{Length} :
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_l <-}\StringTok{ }\NormalTok{iris[}\KeywordTok{names}\NormalTok{(iris)[}\KeywordTok{grepl}\NormalTok{(}\StringTok{"Length"}\NormalTok{,}\KeywordTok{names}\NormalTok{(iris), }\DataTypeTok{fixed =} \OtherTok{TRUE}\NormalTok{)]]}
\end{Highlighting}
\end{Shaded}

Il serait possible de simplifier la lecture de ce code en détaillant les différentes étapes comme suit :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noms_cols <-}\StringTok{ }\KeywordTok{names}\NormalTok{(iris)}
\NormalTok{sel_noms <-}\StringTok{ }\NormalTok{noms_cols[}\KeywordTok{grepl}\NormalTok{(}\StringTok{"Length"}\NormalTok{,noms_cols, }\DataTypeTok{fixed =} \OtherTok{TRUE}\NormalTok{)]}
\NormalTok{iris_l <-}\StringTok{ }\NormalTok{iris[sel_noms]}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Documenter et commenter son code le plus possible} : il est possible de rajouter du texte dans un code R qui ne sera pas exécuté, ce qu'on appelle des commentaires. Typiquement, une ligne commençant par un \texttt{\#} ne sera pas interprétée par le logiciel. Utilisez des commentaires le plus souvent possible pour décrire les actions que vous souhaitez effectuer avec votre code. Il sera ainsi plus facile de le relire, de naviguer dans votre code, mais également de repérer d'éventuelles erreurs. Si l'on reprend l'exemple précédent :
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# récupération du nom des colonnes dans le dataframe iris}
\NormalTok{noms_cols <-}\StringTok{ }\KeywordTok{names}\NormalTok{(iris)}

\CommentTok{# sélection des colonnes avec les caractères "Length"}
\NormalTok{sel_noms <-}\StringTok{ }\NormalTok{noms_cols[}\KeywordTok{grepl}\NormalTok{(}\StringTok{"Length"}\NormalTok{,noms_cols, }\DataTypeTok{fixed =} \OtherTok{TRUE}\NormalTok{)]}

\CommentTok{# extraction des colonnes sélectionnées dans un nouveau dataframe}
\NormalTok{iris_l <-}\StringTok{ }\NormalTok{iris[sel_noms]}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  \textbf{Éviter le code à rallonge\ldots{}} : typiquement, essayez de vous limiter à des lignes de code d'une longueur maximale de 80 caractères. Au-delà de ce seuil, il est judicieux de découper votre code en plusieurs lignes.
\item
  \textbf{Adopter une convention d'écriture} : une convention d'écriture est un ensemble de règles strictes définissant comment un code doit être rédigé. À titre d'exemple, il est parfois recommandé d'utiliser le lowerCamelCase, le UpperCamelCase, ou encore de séparer les mots par des tirets bas upper\_camel\_case. Un mélange de ces différentes conventions peut être utilisé pour distinguer les variables, les fonctions et les classes. Il peut être difficile de réellement arrêter une telle convention, car les différents packages dans R utilisent des conventions différentes. Dans vos propres codes, il est surtout important d'avoir une certaine cohérence et ne pas changer de convention au fil de votre code.
\item
  \textbf{Indenter son code} : l'indentation du code permet de le rendre beaucoup plus lisible. Indenter son code signifie insérer au début de chaque ligne de code un certain nombre d'espaces permettant d'indiquer à quel niveau de profondeur on se situe. Typiquement, lorsque des accolades ou des parenthèses sont ouvertes dans une fonction, une boucle ou une condition, on rajoute deux ou quatre espaces en début de ligne. Prenons un exemple très concret, admettons que nous écrivons une fonction affichant un résumé statistique à chaque colonne d'un jeu de données si cette colonne est de type numérique. L'indentation dans cette fonction va jouer un rôle crucial dans sa lisibilité.
  Sans indentation et sans respecter la règle des 80 caractères, on obtient ceci :
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary_all_num_cols <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(dataset)\{}\ControlFlowTok{for}\NormalTok{(col }\ControlFlowTok{in} \KeywordTok{names}\NormalTok{(dataset))\{}\ControlFlowTok{if}\NormalTok{(}\KeywordTok{class}\NormalTok{(dataset[[col]] }\OperatorTok{==}\StringTok{ "numeric"}\NormalTok{))\{}\KeywordTok{print}\NormalTok{(}\KeywordTok{summary}\NormalTok{(dataset[[col]]))\}\}\}}
\end{Highlighting}
\end{Shaded}

Avec de l'indentation et des commentaires, la syntaxe est beaucoup plus lisible puisqu'elle permet de repérer facilement trois niveaux / paliers dans le code :

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#définition d'une fonction}
\NormalTok{summary_all_num_cols <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(dataset)\{}
  \CommentTok{#Itération sur chaque colonne de la fonction}
  \ControlFlowTok{for}\NormalTok{(col }\ControlFlowTok{in} \KeywordTok{names}\NormalTok{(dataset))\{}
    \CommentTok{# A chaque itération, testons si la colonne est de type numérique}
    \ControlFlowTok{if}\NormalTok{(}\KeywordTok{class}\NormalTok{(dataset[[col]] }\OperatorTok{==}\StringTok{ "numeric"}\NormalTok{))\{}
      \CommentTok{# Si oui, on affiche un résumé statistique pour cette colonne}
      \KeywordTok{print}\NormalTok{(}\KeywordTok{summary}\NormalTok{(dataset[[col]]))}
\NormalTok{    \} }\CommentTok{# ici on sort de la condition (niveau 3)}
\NormalTok{  \} }\CommentTok{# ici on sort de la boucle (niveau 2)}
\NormalTok{\}}\CommentTok{# ici on sort de la fonction (niveau 1)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Adopter une structure globale pour vos scripts} : un code R peut être comparé à une recette de cuisine. Si tous les éléments sont dans le désordre et sans structure globale, la recette risque d'être très difficile à suivre. Cette structure risque de changer quelque peu en fonction de la recette ou de l'auteur·e, mais les principaux éléments restent les mêmes. Dans un code R, on peut distinguer plusieurs éléments récurrents que nous vous recommandons d'organiser de la façon suivante :
\item
  Charger les différents packages \textbf{utilisés} par le script. Cela permet dès le début du code de savoir quelles sont les fonctions et méthodes qui seront employées dans le script. On limite aussi les risques d'oublier des packages qui seraient chargés plus loin dans le code.
\item
  Définir les fonctions dont vous aurez besoin en plus de celles présentes dans les packages. Idem, placer ses fonctions en début de code évite d'oublier de les charger ou de les chercher quand on en a besoin.
\item
  Définir le répertoire de travail avec la fonction \texttt{setwd} et charger les données nécessaires.
\item
  Effectuer au besoin les opérations de manipulations sur les données.
\item
  Effectuer les analyses nécessaires en scindant si possibles les différentes étapes.
\end{enumerate}

Notez également que l'étape de définition des fonctions complémentaires peut être effectuée dans une feuille de code séparée, et l'ensemble de ces fonctions chargées à l'aide de la fonction \texttt{source}. De même, si la manipulation des données est conséquente, il est recommandé de l'effectuer avec un code à part, d'enregistrer les données structurées, puis de le charger directement au début de votre code dédié à l'analyse.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  \textbf{Exploiter les commentaires délimitant les sections dans Rstudio} : Dans RStudio, il est possible d'écrire des commentaires d'une certaine façon pour que l'IDE les détecte comme des délimiteurs de sections. L'intérêt principal est que l'on peut ensuite facilement naviguer entre ces sections en utilisant Rstudio comme montré à la figure @ref(fig:sections\_rstudio), mais aussi masquer des sections afin de faciliter la lecture du reste du code. Pour délimiter une section, il suffit d'ajouter une ligne de commentaire comprenant quatre fois les caractères \texttt{-}, \texttt{=} ou \texttt{\#} à la suite :
\end{enumerate}

\begin{verbatim}

# Voici ma section 1 ----------------------------------

# Voici ma section 2 ==================================

# Voici ma section 3 ##################################

# Autre exemple pour mieux marquer la rupture dans un code : 

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#### Titre de ma section 4 ####
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  \textbf{Adopter une structure globale pour vos projets} : au-delà du script, il est nécessaire de bien structurer vos projets. Le plus important étant d'utiliser une structure commune à chaque projet pour vous faciliter le travail. Nous proposons ci-dessous un exemple de structure assez général pouvant être utilisé dans la plupart des cas. Elle sépare notamment les données originales des données structurées, ainsi que les fonctions complémentaires et la structuration des données du principal bloc d'analyse.
\end{enumerate}

\textbf{Ne négligez jamais l'importance d'un code bien écrit et documenté !}

\hypertarget{sect016}{%
\section{Conclusion et ressources pertinentes}\label{sect016}}

Voilà qui conclut ce chapitre sur les bases du langage R. Vous avez maintenant les connaissances nécessaires pour commencer à travailler. N'hésitez pas à revenir sur les différentes sous-sections au besoin ! Pour aller plus loin dans l'apprentissage du langage, vous pouvez également vous plonger dans le chapitre R AVANCÉ. Cependant, nous vous recommandons de faire vos premiers pas avec cette base avant de vous lancer dans cette partie davantage orientée programmation. Quelques ressources pertinentes qui pourraient vous être utiles sont aussi reportées au tableau ci-dessous.

\begin{table}

\caption{\label{tab:tableRessources}Ressources pertinente pour en apprendre plus sur R}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{l>{\raggedright\arraybackslash}p{6cm}>{\raggedright\arraybackslash}p{6cm}}
\toprule
Ressource & Description & Url\\
\midrule
Rbloggers & Un recueil de nombreux blogues sur R : parfait pour être tenu au courant des nouveautés et faire des découvertes. & https://www.r-bloggers.com\\
CRAN packages by date & Les derniers packages publiés sur CRAN : cela permet de garder un oeil sur les nouvelles fonctionnalités de ses packages préférés. & https://cran.r-project.org/web/packages\\
Introduction à R et au TidyVerse & Une excellente ressource en français pour en apprendre plus sur le tidyverse. & https://juba.github.io/tidyverse\\
Numyard & Une chaîne YouTube pour revoir les bases de R en vidéo. & https://www.youtube.com/user/TheLearnR\\
Cheasheets & Des feuilles de triche résumant les fonctionnalités de nombreux packages. & https://rstudio.com/resources/cheatsheets\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{part-analyses-univariuxe9es}{%
\part{Analyses univariées}\label{part-analyses-univariuxe9es}}

\hypertarget{chap02}{%
\chapter{Statistiques descriptives univariées}\label{chap02}}

Dans ce chapitre, nous décrirons la notion de variable, permettant l'opérationnalisation d'un concept. Comprendre les différents types de variables est essentiel en statistiques. En effet, en fonction du type de variable à l'étude, les tests d'hypothèse et les méthodes de statistique inférentielle que l'on pourra appliquer seront différents. Nous distinguerons ainsi cinq types de variables : nominale, ordinale, discrète, continue et semi-quantitative. Aussi, nous abordons un concept central de la statistique : les distributions. Nous présenterons ensuite les différentes statistiques descriptives univariées qui peuvent s'appliquer à ces types de variables.

\begin{bloc_package}

Dans ce chpaitre, nous utiliserons principalement les packages suivants (À MODIFIER PLUS TARD) :

\begin{itemize}
\tightlist
\item
  Pour créer des graphiques :

  \begin{itemize}
  \tightlist
  \item
    \textbf{ggplot2}, le seul, l'unique
  \item
    \textbf{ggpubr} pour combiner des graphiques et réaliser des diagrammes
  \end{itemize}
\item
  Pour créer des distribution :

  \begin{itemize}
  \tightlist
  \item
    \textbf{fitdistrplus} pour générer différentes distributions
  \item
    \textbf{actuar} pour la fonction de densité de Pareto
  \item
    \textbf{gamlss.dist} pour des distributions de Poisson
  \end{itemize}
\item
  Pour les statistiques descriptives :

  \begin{itemize}
  \tightlist
  \item
    \textbf{stats} pour les statistiques descriptives
  \item
    \textbf{nortest} pour le test de Kolmogorov-Smirnov
  \item
    \textbf{DescTools} pour les tests de Lilliefors, Shapiro-Wilk, Anderson-Darling et Jarque-Bera
  \end{itemize}
\item
  Autres \emph{packages} :

  \begin{itemize}
  \tightlist
  \item
    \textbf{Hmisc} et \textbf{Weighted.Desc.Stat} pour les statistiques descriptives pondérées
  \item
    \textbf{foreign} pour importer des fichiers externes
  \end{itemize}
\end{itemize}


\end{bloc_package}

\hypertarget{sect021}{%
\section{Notion de variable}\label{sect021}}

\hypertarget{sect0211}{%
\subsection{La variable : l'opérationnalisation d'un concept}\label{sect0211}}

Une variable permet d'opérationnaliser un concept, soit une « idée générale et abstraite que se fait l'esprit humain d'un objet de pensée concret ou abstrait, et qui lui permet de rattacher à ce même objet les diverses perceptions qu'il en a, et d'en organiser les connaissances » (\href{https://www.larousse.fr/dictionnaires/francais/concept/17875?q=concept\#17749}{Larousse}). Pour valider un modèle théorique, il convient alors d'opérationnaliser ses différentes concepts et d'établir les relations qu'ils partagent. L'opérationnalisation d'un concept nécessite soit de mesurer (dans un intervalle de valeurs, c'est-à-dire de manière quantitative), soit de qualifier (avec plusieurs catégories, c'est-à-dire de manière qualitative) un phénomène.

Selon \href{https://www.statcan.gc.ca/fra/concepts/variable}{Statistique Canada}, « une variable est une caractéristique d'une unité statistique que l'on observe et pour laquelle une valeur numérique ou une catégorie d'une classification peut être attribuée ». Il convient alors de bien saisir à quelle unité statistique (ou unité d'observation) s'applique les valeurs d'une variable : des personnes, des ménages, des municipalités, etc.

Prenons deux exemples concrets tirées du Recensement de 2016 de Statistique Canada :

\begin{itemize}
\tightlist
\item
  Le concept \textbf{famille de recensement} est défini comme étant « un couple marié et les enfants, le cas échéant, du couple et/ou de l'un ou l'autre des conjoints; un couple en union libre et les enfants, le cas échéant, du couple et/ou de l'un ou l'autre des partenaires; ou un parent seul, peu importe son état matrimonial, habitant avec au moins un enfant dans le même logement et cet ou ces enfants. Tous les membres d'une famille de recensement particulière habitent le même logement. Un couple peut être de sexe opposé ou de même sexe. Les enfants peuvent être des enfants naturels, par le mariage, par l'union libre ou par adoption, peu importe leur âge ou leur état matrimonial, du moment qu'ils habitent dans le logement sans leur propre conjoint marié, partenaire en union libre ou enfant. Les petits-enfants habitant avec leurs grands-parents, alors qu'aucun des parents n'est présent, constituent également une famille de recensement » (\href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/fam004-fra.cfm}{Statistique Canada}). À partir de cette définition, les familles de recensement peuvent être qualifiées selon plusieurs modalités : couples mariés sans enfant, couples mariés avec enfants, couples en union libre sans enfant, couples en union libre avec enfant, famille monoparentale (avec un parent de sexe féminin), famille monoparentale (avec un parent de sexe masculin).
\item
  Le concept de \textbf{revenu d'emploi} est défini comme étant « tous les revenus reçus sous forme de traitements, salaires et commissions d'un travail rémunéré ou le revenu net d'un travail autonome dans une entreprise agricole ou non agricole non constituée en société et/ou dans l'exercice d'une profession au cours de la période de référence. Pour le Recensement de 2016, la période de référence est l'année civile 2015 pour toutes les variables de revenu » (\href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop027-fra.cfm}{Statistique Canada}). Il est donc mesurée en dollars pour chaque individu de 15 ans et plus. Pour l'ensemble de la population de 15 ans et plus, il peut ensuite être classé en déciles de revenu d'emploi, soit en dix groupes (\href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop204-fra.cfm\%22\%7D}{Statistique Canada}).
\end{itemize}

\begin{bloc_attention}

\textbf{Maîtriser la définition des variables que vous utilisez : un enjeu crucial ! }

Nous avons vu qu'une variable est l'opérationnalisation d'un concept. Par conséquent, ne pas maîtriser la définition d'une variable revient à ne pas bien saisir le concept sous-jacent qu'elle tente de mesurer. Si vous exploitez des données secondaires -- par exemple, issues d'un recensement de population ou d'une enquête longitudinale ou transversale --, il faut impérativement lire les définitions des variables que vous souhaiteriez utiliser. Ne pas le faire risque d'aboutir à :

\begin{itemize}
\item
  Une mauvaise opérationnalisation de votre modèle théorique, même si votre analyse est bien menée statistiquement parlant. Autrement dit, vous risquez de ne pas sélectionner les bonnes variables. Prenons un exemple concret. Vous avez construit un modèle théorique dans lequel vous souhaitez inclure un concept sur la langue des personnes. Dans le recensement canadien de 2016, plusieurs variables relatives à la langue sont disponibles : \href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop055-fra.cfm}{connaissance des langues officielles},
  \href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop042-fra.cfm}{langue parlée à la maison}, \href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop095-fra.cfm}{langue maternelle}, \href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop034-fra.cfm}{première langue officielle parlée}, \href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop054-fra.cfm}{connaissance des langues non officielles} et \href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop059-fra.cfm}{langue de travail} (\href{https://www12.statcan.gc.ca/census-recensement/2016/ref/guides/003/98-500-x2016003-fra.cfm}{Statistique Canada, 2019}). La sélection de l'une de ces variables doit être faite de manière rigoureuse, c'est-à-dire en lien avec votre cadre théorique et suite à une bonne compréhension des définitions des variables. Dans une étude sur le marché du travail, on sélectionnerait probablement la variable \emph{sur la connaissance des langues officielles du Canada}, afin d'évaluer son effet sur l'employabilité, toutes choses étant égales par ailleurs. Dans une autre étude portant sur la réussite ou la performance scolaire, il est probable qu'on utilise plutôt la \emph{langue maternelle}.
\item
  Une mauvaise interprétation et discussion de vos résultats en lien avec votre cadre théorique.
\item
  Une mauvaise identification des pistes de recherche.
\end{itemize}

Finalement, la définition d'une variable peut évoluer à travers plusieurs recensements de population : la société évolue, les variables aussi ! Par conséquent, si vous comptez utiliser plusieurs années de recensement dans une même étude, assurez-vous que les définitions des variables soient similaires d'un jeu de données à l'autre et qu'elles mesurent ainsi la même chose.

\textbf{Comprendre les variables utilisées dans un article scientifique : un exercice indispensable dans l'élaboration d'une revue de littérature}

Une lecture rigoureuse d'un article scientifique suppose, entre autres, de bien comprendre les concepts et variables mobilisés. Il convient alors de lire attentivement la section méthodologique (pas uniquement la section des résultats ou pire le résumé), sans quoi vous risquez d'aboutir à une revue de littérature approximative.
Ayez aussi un \textbf{regard critique} sur les variables visant à opérationnaliser les concepts clés de l'étude. Certains concepts sont très difficiles à traduire en variables; leurs opérationalisations (mesures) peuvent ainsi faire l'objet de vifs débats parmi les chercheurs. Très succinctement, c'est notamment le cas du concept de capital social. D'une part, les définitions et ancrages sont biens différents selon Bourdieu (sociologue, ancrage au niveau des individus) et Putman (politologue, ancrage au niveau des collectivités); d'autre part, aucun consensus ne semble clairement se dégager quant à la définition de variables permettant de le mesurer efficacement (de manière quantitative).

\textbf{Variable de substitution (\emph{proxy variable} en anglais)}

On fait la moins pire des recherches ! En effet, les données disponibles sont parfois imparfaites pour répondre avec précision à une question de recherche; on peut toujours les exploiter, tout en signalant honnêtemment leurs faiblesses et limites, et ce, tant pour les données que les variables utilisées.

\begin{itemize}
\item
  Des bases de données peuvent être en effet imparfaites. Par exemple, en criminologie, des chercheur·e·s exploitant des données policières signalent habituellement la limite du \textbf{chiffre noir} : les données policières comprennent uniquement les crimes et délits découverts par la police et occultent ainsi les crimes non-découverts; ils ne peuvent ainsi refléter la criminalité réelle sur un territoire donné.
\item
  Des variables peuvent aussi être imparfaites. Dans un jeu de données, il est fréquent qu'une variable opérationnalisant un concept précis ne soit pas disponible ou qu'elle n'ait tout simplement pas été mesurée. On cherchera alors une variable de substitution (\emph{proxy}) pour la remplacer. Prenons un exemple concret portant sur l'exposition des cyclistes à la pollution atmosphérique ou au bruit environnemental. L'un des principaux facteurs d'exposition à ces pollutions est le trafic routier : plus ce dernier est élevé, plus les cyclistes risquent de rouler dans un environnement bruyant et pollué. Toutefois, il est rare de disposer de mesures du trafic en temps réel qui nécessitent des comptages de véhicules pendant le trajet des cyclistes (par exemple, à partir de vidéos captées par une caméra fixée sur le guidon). Pour pallier à l'absence de mesures directes, plusieurs auteurs utilisent des variables de substitution de la densité du trafic, comme la typologie des types d'axes (primaire, secondaire, tertiaire, rue locale, etc.), supposant ainsi qu'un axe primaire supporte un volume de véhicules supérieur à un axe secondaire.
\end{itemize}


\end{bloc_attention}

\hypertarget{sect0212}{%
\subsection{Les types de variables}\label{sect0212}}

On distingue habituellement les variables qualitatives (nominale ou ordinale) des variables quantitatives (discrète ou continue). Tel qu'illustré à la figure \ref{fig:figunivarie1}, l'opérationnalisation du concept en variable est réalisée par différents mécanismes visant à qualifier, classer, compter ou mesurer afin de caractériser les unités statistiques (observations) d'une population ou d'un échantillon.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/univariee/figure1} 

}

\caption{Les types de variables}\label{fig:figunivarie1}
\end{figure}

\hypertarget{sect02121}{%
\subsubsection{Les variables qualitatives}\label{sect02121}}

\textbf{Une variable nominale} permet de \textbf{qualifier} des observations (individus) à partir de plusieurs catégories dénommées modalités. Par exemple, la variable \emph{couleur des yeux} pourrait comprendre les modalités \emph{bleu}, \emph{marron}, \emph{vert}, \emph{noir} tandis que les \emph{types de familles} compendrait les modalités \emph{couple marié}, \emph{couple en union libre} et \emph{famille monoparentale}.

\textbf{Une variable ordinale} permet de \textbf{classer} des observations à partir de plusieurs modalités hiérarchisées. L'exemple le plus connu est certainement l'échelle de Likert, très utilisée dans les sondages évaluant le degré d'accord d'une personne à une affirmation avec les modalités suivantes : \emph{tout à fait d'accord}, \emph{d'accord}, \emph{ni en désaccord ni d'accord}, \emph{pas d'accord} et \emph{pas du tout d'accord}. Une multitude de variantes sont toutefois possibles pour classer la fréquence d'un phénomène (\emph{Très souvent}, \emph{souvent}, \emph{parfois}, \emph{rarement}, \emph{jamais}), l'importance accordée à un phénomène (\emph{Pas du tout important}, \emph{peu important}, \emph{plus ou moins important}, \emph{important}, \emph{très important}) ou la proximité perçue d'un lieu (\emph{très éloigné}, \emph{loin}, \emph{plus ou moins proche}, \emph{proche}, \emph{très proche}).

En fonction du nombre de modalités qu'elle comprend, une variable qualitative (nominale ou ordinale) est soit \textbf{dichtomique (binaire)} (deux modalités), soit \textbf{polytomique} (plus de deux modalités). Par exemple, dans le recensement canadien, le \emph{sexe} est une variable binaire (avec les modalités \emph{sexe masculin}, \emph{sexe féminin}), tandis que le \emph{genre} est une variable polytomique (avec les modalités \emph{genre masculin}, \emph{genre féminin} et \emph{diverses identités de genre}).

\begin{bloc_attention}
Les variables nominales et ordinales sont habituellement encodées avec des valeurs numériques entières (par exemple, 1 pour \emph{couple marié}, 2 pour \emph{couple en union libre} et 3 pour \emph{famille monoparentale}). Toutefois, aucune opération arithmétique (moyenne ou écart-type par exemple) n'est possible sur ces valeurs. Dans R, on utilisera un facteur pour attribuer un intitulé à chacune des valeurs numériques de la variable qualitative :

\texttt{df\$Famille\ \textless{}-\ factor(df\$Famille,\ c(1,2,3),\ labels\ =\ c("couple\ marié","couple\ en\ union\ libre",\ "famille\ monoparentale"))}

On calculera toutefois les fréquences des différentes modalités pour une variable nominale ou ordinale. Il est aussi possible de calculer la médiane sur une variable ordinale.

\end{bloc_attention}

\hypertarget{sect02122}{%
\subsubsection{Les variables quantitatives}\label{sect02122}}

\textbf{Une variable discrète} permet de \textbf{compter} un phénomène dans un ensemble fini de valeurs, comme le nombre d'accidents impliquant un·e cycliste à une intersection sur une période de cinq ans ou encore le nombre de vélos en libre service disponibles à une station. Il existe ainsi une variable binaire sous-jacente : la présence ou non d'un accident à l'intersection ou d'un vélo ou non à la station pour laquelle on opère un comptage. Habituellement, une variable discrète ne peut prendre que des valeurs entières (sans décimales), comme le nombre de personnes fréquentant un parc.

\textbf{Une variable continue} permet de \textbf{mesurer} un phénomène avec un nombre infini de valeurs réelles (avec décimales) dans un intervalle donné. Par exemple, une variable relative à la distance de dépassement d'un·e cycliste par un véhicule motorisé pourrait varier de 0 à 5 mètres (\(X \in \left[0,5\right]\)); toutefois cette distance peut être de 0,759421 ou de 4,785612 mètres. Le nombre de décimales de la valeur réelle dépendra de la précision et de la fiabilité de la mesure. Pour un capteur de distance de dépassement, le nombre de décimales dépendra de la précision du lidar ou du sonar de l'appareil; aussi, l'utilisation de trois décimales -- soit une précision au millimètre -- est largement suffisant pour mesurer la distance de dépassement. Une variable continue est soit une variable d'intervalle, soit une variable de rapport. Les \textbf{variables d'intervalle} ont une échelle relative, c'est-à-dire que les intervalles entre les valeurs de la variables ne sont pas constants; elles n'ont pas de vrai zéro. Ces valeurs peuvent être manipulées uniquement par addition et soustraction et non par multiplication et division. La variable d'intervalle la plus connue est certainement celle de la température. S'il fait 10 degrés Celsius à Montréal et 30°C à Mumbai (soit 50 et 86 degrés en Fahrenheit), on peut affirmer qu'il y a 20°C ou 36°F d'écart entre les deux villes, mais on ne peut pas affirmer qu'il fait trois fois plus chaud à Mumbai. Presque toutes les mesures statistiques sur une variable d'intervalle peuvent être calculées, exceptés le coefficient de variation et la moyenne géométrique puisqu'il n'y a pas de vrai zéro et d'intervalles constants entre les valeurs. À l'inverse, les \textbf{variables de rapport} ont une échelle absolue, c'est-à-dire que les intervalles entre les valeurs sont constants et elles ont un vrai zéro. Elles peuvent ainsi être manipulées par addition, soustraction, multiplication et division. Par exemple, le prix d'un produit exprimé dans une unité monétaire ou la distance exprimée dans le système métrique sont des variables de rapport. Un vélo dont le prix affiché est de 1000\$ est bien deux fois plus cher qu'un autre à 500\$, une piste cyclable hors rue à 25 mètres du tronçon routier le plus proche est bien quatre fois plus proche qu'une autre à 100 mètres.

\textbf{Une variable semi-quantitative}, appelée aussi variable quantitative ordonnée, est une variable discrète ou continue dont les valeurs ont été regroupées en classes hiérarchisées. Par exemple, l'âge est une variable continue pouvant être transformée avec les groupes d'âge ordonnés suivants : \emph{moins 25 ans}, \emph{25 à 44 ans}, \emph{45 à 64 ans} et \emph{65 ans et plus}.

\hypertarget{sect022}{%
\section{Les types de données}\label{sect022}}

Différents types de données sont utilisés en sciences sociales. L'objectif ici n'est pas de les décrire en détail, mais plutôt de donner quelques courtes définitions. En fonction de votre question de recherche et des bases des données disponibles ou non, il s'agira de sélectionner le ou les types de données les plus appropriés à votre sujet.

\hypertarget{sect0221}{%
\subsection{\texorpdfstring{Données secondaires \emph{versus} données primaires}{Données secondaires versus données primaires}}\label{sect0221}}

Les \textbf{données secondaires} sont des données qui existent déjà au début de votre projet de recherche : pas besoin de les collecter, il suffit de les exploiter! Une multitude de données de recensements ou d'enquêtes de Statistique Canada sont disponibles et largement exploitées en sciences sociales (par exemple, l'enquête nationale auprès des ménages -- ENM, l'enquête sur la dynamique du marché du travail et du revenu -- EDTR, l'enquête longitudinale auprès des immigrants -- ELIC, etc.).

\begin{bloc_notes}
Au Canada, les chercheurs (étudiants et professeurs) ont accès aux microdonnées des enquêtes de Statistique Canada dans les Centres de données de recherche (CDR). Vous pouvez consulter le moteur de recherche du (\href{https://crdcn.org/fr/donn\%C3\%A9es}{RCCDR}) afin d'explorer les différentes enquêtes disponibles.

Au Québec, l'accès à ces enquêtes est possible dans les différentes antennes du Centre interuniversitaire québécois de statistiques sociales de Statistique Canada (\href{https://www.ciqss.org/}{CIQSS}).

\end{bloc_notes}

Par opposition, les \textbf{données primaires} n'existent pas quand vous démarrez votre projet : vous devez les collecter spécifiquement pour votre étude! Par exemple, un·e chercheur·e souhaitant analyser l'exposition des cyclistes au bruit et à la pollution dans une ville donnée devra réaliser une collecte de données avec idéalement plusieurs participants (équipés de différents capteurs), et ce, sur plusieurs jours.
Une collecte de données primaires peut aussi être réalisée avec une enquête par sondage. Brièvement, réaliser une collecte de données primaires nécessite différentes phases complexes comme la définition de la méthode de collecte, de la population à l'étude, l'estimation de la taille de l'échantillon, la validation des outils de collecte avec une phase de test, la réalisation de la collecte, la structuration, la gestion et l'exploitation de données collectées. Finalement, dans le milieu académique, une collecte de données primaires auprès d'individus doit être approuvée par le comité d'éthique de la recherche de l'université à laquelle est affilié·e le ou la responsable du projet de recherche (qu'il soit professeur·e, chercheur·e ou étudiant·e).

\hypertarget{sect0222}{%
\subsection{\texorpdfstring{Données transversales \emph{versus} données longitudinales}{Données transversales versus données longitudinales}}\label{sect0222}}

Les \textbf{données transversales} sont des mesures pour une période relativement courte. L'exemple classique est un jeu de données constitué des variables extraites d'un recensement de population pour une année donnée (comme celui 2016 de Statistique Canada).

Les \textbf{données longitudinales}, appelées aussi données par panel, sont des mesures répétées pour plusieurs observations au cours du temps (\emph{N} observations pour \emph{T} dates). Par exemple, des observations pourraient être des pays, les dates pourraient être différentes années (de 1990 à 2019) pour lesquelles différentes variables seraient disponibles (population totale, taux d'urbanisation, produit intérieur brut par habitant, émissions de gaz à effet de serre par habitant, etc).

\hypertarget{sect0223}{%
\subsection{Données spatiales versus données aspatiales}\label{sect0223}}

Les observations des \textbf{données spatiales} sont des unités spatiales géoréférencées (points, lignes, polygones ou encore pixels d'une image). Elles peuvent être par exemple :

\begin{itemize}
\tightlist
\item
  des points \emph{(x,y)} ou \emph{(lat-long)} représentant des entreprises avec plusieurs variables (adresse, date de création, nombre d'employés, secteurs d'activité, etc.);
\item
  les lignes représentant des tronçons de rues pour lesquels plusieurs variables sont disponibles (types d'axe, longueur en mètres, nombre de voies, débit journalier moyen annuel, etc.);
\item
  des polygones délimitant des régions ou des arrondissements pour lesquels une multitude de variables sociodémographiques et socioéconomiques sont disponibles.
\end{itemize}

À l'inverse, aucune information spatiale n'est disponible pour des \textbf{données aspatiales}.

\hypertarget{sect0224}{%
\subsection{\texorpdfstring{Données individuelles \emph{versus} données agrégées}{Données individuelles versus données agrégées}}\label{sect0224}}

Comme son nom l'indique, pour des \textbf{données individuelles}, chaque observation correspond à un individu. Les microdonnées de recensement ou d'enquêtes, par exemple, sont des données individuelles pour lesquelles toute une série de variables est disponible. Une étude analysant les caractéristiques de chaque arbre d'un quartier nécessite aussi des données individuelles : l'information doit être disponible pour chaque arbre. Pour les microdonnées des recensements canadiens, « chaque enregistrement au niveau de la personne comprend des identifiants (comme les identifiants du ménage et de la famille), des variables géographiques et des variables directes et dérivées tirées du questionnaire » (\href{https://www150.statcan.gc.ca/n1/pub/12-002-x/2012001/article/11642-fra.htm}{Statistique Canada}). Comme signalé plus haut, ces microdonnées de recensement ou d'enquêtes sont uniquement accessibles dans les Centres de données de recherche (CDR).

Les données individuelles peuvent être \textbf{agrégées} à un niveau supérieur. Prenons le cas de microdonnées d'un recensement. Les informations disponibles pour chaque individu sont agrégées par territoire géographique (province, région économique, division de recensement, subdivision de recensement, région et agglomération de recensement, secteurs de recensement, aires de diffusion, etc.) en fonction du lieu de résidence des individus. Des sommaires statistiques -- basés sur la moyenne, la médiane, la somme ou la proportion de chacune des variables mesurées au niveau individuel (âge, sexe, situation familiale, revenu, etc.) -- sont alors construits pour ces différents découpages géographiques (\href{https://www.statcan.gc.ca/fra/idd/trousse/section5\#a4}{Statistique Canada}).

L'agrégation n'est pas nécessairement géographique. En éducation, il est fréquent de travailler avec des données concernant les élèves, mais agrégées au niveau des écoles. La figure \ref{fig:figunivarie1b} donne un exemple simple d'agrégation de données individuelles.

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{images/univariee/aggregation} 

}

\caption{Exemple d'agrégation de données individuelles}\label{fig:figunivarie1b}
\end{figure}

Pour le cas de l'agrégation géographique, il convient alors de bien comprendre la hiérarchie des régions géographiques délimitées par l'organisme ou l'agence ayant la responsabilité de produire, gérer et diffuser les données des recensements et des enquêtes, puis de sélectionner le découpage géographique qui répond le mieux à votre question de recherche.

\begin{bloc_astuce}

Pour le recensement de 2016 de Statistique Canada vous pourrez consulter :

\begin{itemize}
\item
  la \href{https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/figures/f1_1-fra.cfm}{hiérarchie des régions géographiques normalisées pour la diffusion}
\item
  le \href{https://www150.statcan.gc.ca/n1/pub/92-195-x/92-195-x2016001-fra.htm}{glossaire illustré} des régions géographiques
\item
  les différents \href{https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/prof/details/download-telecharger/comp/page_dl-tc.cfm?Lang=F}{profils du recensement de 2016} à télécharger pour les différentes régions géographiques.
\end{itemize}


\end{bloc_astuce}

\begin{bloc_notes}
Bien entendu, les différents types de données abordés ci-dessus ne sont pas exclusifs. Par exemple, des données pour des régions administratives extraites de plusieurs recensements sont en fait des données secondaires, spatiales, agrégées et longitudinales.

Une collecte de données sur la pollution atmosphérique et sonore réalisée à vélo (avec différents capteurs et un GPS) sont des données spatiales primaires.

\end{bloc_notes}

\hypertarget{sect023}{%
\section{Statistique descriptive et statistique inférentielle}\label{sect023}}

\hypertarget{sect0231}{%
\subsection{Population, échantillon et inférence}\label{sect0231}}

Les notions de \textbf{population} et d'\textbf{échantillon} sont essentielles en statistique puisqu'elles sont le socle de l'inférence statistique.
Un échantillon est un \textbf{sous-ensemble représentatif} d'une population donnée. Prenons un exemple concret. Une chercheure veut comprendre la mobilité des étudiants d'une université. Bien entendu, elle ne pourra interroger l'ensemble des étudiants de son université. Elle devra alors s'assurer d'obtenir un échantillon de taille suffisante et représentatif de la population étudiante. Une fois les données collectées (avec un sondage par exemple), elle pourra utiliser des techniques inférentielles pour analyser la mobilité des étudiants interrogés. Si son échantillon est représentatif, les résultats obtenus pourront être inférés -- c'est-à-dire généralisés, extrapolés -- à l'ensemble de la population.

\begin{bloc_aller_loin}
\textbf{Les méthodes d'échantillonnage}

Nous n'abordons pas ici les méthodes d'échantillonnage. Sachez toutefois qu'il existe plusieurs méthodes probabilistes pour constituer un échantillon, notamment de manière aléatoire, systématique, stratifiée, par grappes (\href{https://www150.statcan.gc.ca/n1/edu/power-pouvoir/ch13/prob/5214899-fra.htm}{voir par exemple cette publique de Statistique Canada}).

\end{bloc_aller_loin}

Autre exemple, une autre chercheure souhaite comprendre les facteurs influençant le sentiment de sécurité des cyclistes dans un quartier. De nouveau, elle ne pourra pas enquêter tous les cyclistes du quartier et devra constituer un échantillon représentatif. Par la suite, la mise en œuvre de techniques inférentielles lui permettra d'identifier les caractéristiques individuelles (âge, sexe, habiletés à vélo, etc.) et de l'environnement urbain (types de voies empruntés, niveaux de trafic, de pollution, de bruit, etc.) ayant des effets significatifs sur le sentiment de sécurité. Si l'échantillon est représentatif, les résultats pourront être généralisés à l'ensemble des cyclistes du quartier.

\hypertarget{sect0232}{%
\subsection{Deux grandes familles de méthodes statistiques}\label{sect0232}}

On distingue deux grandes familles de méthodes statistiques :

\begin{itemize}
\tightlist
\item
  « \textbf{La statistique descriptive et exploratoire} : elle permet, par des résumés et des graphiques plus ou moins élaborés, de décrire des ensembles de données statistiques, d'établir des relations entre les variables sans faire jouer de rôle privilégié à une variable particulière. Les conclusions ne portent dans cette phase de travail que sur les données étudiées, sans être inférées à une population plus large. L'analyse exploratoire s'appuie essentiellement sur des notions élémentaires telles que des indicateurs de moyenne et de dispersion, sur des représentations graphiques. {[}\ldots{]}
\item
  \textbf{La statistique inférentielle et confirmatoire} : elle permet de valider ou d'infirmer, à partir de tests statistiques ou de modèles probabilistes, des hypothèses formulées a priori (ou après une phase exploratoire), et d'extrapoler, c'est-à-dire d'étendre certaines propriétés d'un échantillon à une population plus large. Les conclusions obtenues à partir des données vont au-delà de ces données. La statistique confirmatoire fait surtout appel aux méthodes dites explicatives et prévisionnelles, destinées comme leurs noms l'indiquent, à expliquer puis à prévoir, suivant des règles de décision, une variable privilégiée à l'aide d'une ou plusieurs variables explicatives (régressions multiples et logistiques, analyse de variance, analyse discriminante, segmentation, etc.) » \citep[p.~209]{lebart1995statistique}.
\end{itemize}

\hypertarget{sect024}{%
\section{La notion de distribution}\label{sect024}}

\begin{bloc_objectif}
Dans cette section, nous abordons un concept central de la statistique : les distributions. Prenez le temps de lire cette section à tête reposée et assurez-vous de bien comprendre chaque idée avant de passer à la suivante. N'hésitez pas à y revenir plusieurs fois si nécessaire, car la compréhension de ces concepts est essentielle pour utiliser adéquatement les méthodes que nous abordons dans ce livre.

\end{bloc_objectif}

\hypertarget{duxe9finitions-guxe9nuxe9rales}{%
\subsection{Définitions générales}\label{duxe9finitions-guxe9nuxe9rales}}

En statistique, on s'intéresse aux résultats d'expériences. Lancer un dé, mesurer la pollution atmosphérique, compter le nombre de collisions à une intersection, demander à une personne d'évaluer son sentiment de sécurité sur une échelle de 1 à 10 sont autant d'expériences pouvant produire des résultats.

\textbf{Une distribution est une fonction permettant d'associer pour chaque résultat possible d'une expérience la probabilité d'obtenir ce résultat}. En d'autres termes, il s'agit d'une fonction indiquant par exemple que pour l'expérience : « mesurer la concentration d'ozone à Montréal à 13h en été », la probabilité de mesurer une valeur inférieure à 15 μg/m3 est de seulement 2\%.

Les distributions sont toujours définies dans un intervalle en dehors duquel elles sont indéfinies; les valeurs dans cet intervalle sont appelées \textbf{l'espace d'échantillonnage}. Il s'agit donc des valeurs possibles que peut produire l'expérience. La somme des probabilités de l'ensemble des valeurs de l'espace d'échantillonnage est 1 (100\%). Intuitivement, cela signifie que si l'on réalise l'expérience, on est obligé d'obtenir un résultat, et que cette probabilité totale est répartie entre tous les résultats possibles de l'expérience. En langage mathématique, on dit que l'intégrale des fonctions de distribution est 1 dans leur intervalle de définition.

Prenons un exemple concret avec l'expérience suivante : tirer à pile ou face avec une pièce de monnaie non truquée. Si l'on souhaite décrire la probabilité d'obtenir pile ou face, on peut utiliser une distribution qui aura comme espace d'échantillonnage {[}pile ; face{]} et ces deux valeurs auront chacune comme probabilité 0,5. Il est facile d'étendre cet exemple au cas d'un dé à six faces. La distribution de probabilité décrivant l'expérience « lancer le dé » a pour espace d'échantillonnage {[}1,2,3,4,5,6{]}, chacune de ces valeurs étant associée à la probabilité 1/6.

Les deux distributions précédentes appartiennent à la famille des distributions \textbf{discrètes}. Elles servent à décrire des expériences dont le nombre de valeurs possibles est fini. Par opposition, la seconde famille de distributions regroupe les distributions \textbf{continues}, décrivant des expériences dont le nombre de résultats possibles est infini. Par exemple, mesurer la taille d'une personne adulte sélectionnée au hasard peut produire un nombre infini de valeurs comprises entre 50 cm et 280 cm. Les distributions sont utiles pour décrire les résultats attendus d'une expérience. Reprenons notre exemple du dé. Nous savons que chaque face a une chance sur six d'être tirée au hasard. Nous pouvons représenter cette distribution avec un graphique (figure \ref{fig:fig251}).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig251-1} 

}

\caption{Distribution théorique d'un lancé de dé}\label{fig:fig251}
\end{figure}

Nous avons donc sous les yeux un modèle statistique décrivant le comportement attendu d'un dé, nous l'appelons la distribution \textbf{théorique}. Cependant, si nous effectuons l'expérience 10 fois (nous collectons donc un échantillon), nous obtiendrons une distribution différente de cette distribution théorique (figure \ref{fig:fig252}).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig252-1} 

}

\caption{Distribution empirique d'un lancé de dé (n=10)}\label{fig:fig252}
\end{figure}

Nous appelons cette distribution la distribution \textbf{empirique}. Chaque échantillon aura sa propre distribution empirique. Cependant, comme le prédit la loi des grands nombres (ou théorème de Bernoulli) : si une expérience est répétée un grand nombre de fois, la probabilité empirique d'un résultat se rapproche de la probabilité théorique à mesure que le nombre de répétitions augmente. Pour nous en convaincre, collectons trois échantillons de lancer de dé de respectivement 30, 100 et 1000 observations (figure \ref{fig:fig253}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig253-1} 

}

\caption{Distribution empirique d'un lancé de dé (n=10)}\label{fig:fig253}
\end{figure}

On constate bien qu'au fur et à mesure que la taille de l'échantillon augmente, on tend vers la distribution théorique. Ces dernières sont donc utilisées pour modéliser des phénomènes réels et sont à la base de presque tous les tests statistiques d'inférence fréquentiste ou bayésienne.

En pratique, la question que l'on se pose le plus souvent est : quelle distribution théorique peut le mieux décrire le phénomène empirique à l'étude ? Pour répondre à cette question, deux approches sont possibles :

\begin{itemize}
\tightlist
\item
  Considérant la littérature existante sur le sujet, les connaissances accumulées et la nature de la variable étudiée, il est possible de sélectionner des distributions théoriques pouvant vraisemblablement correspondre à la variable.
\item
  Comparer visuellement ou à l'aide de tests statistiques la distribution empirique de la variable et diverses distributions théoriques pour trouver la plus adaptée.
\end{itemize}

Idéalement, le choix d'une distribution théorique devrait reposer sur ces deux méthodes combinées.

\hypertarget{anatomie-dune-distribution}{%
\subsection{Anatomie d'une distribution}\label{anatomie-dune-distribution}}

Puisqu'une distribution est une fonction, il est possible de la représenter à l'aide d'une formule mathématique (appelée \textbf{fonction de masse} pour les distributions discrètes et \textbf{fonction de densité} pour les distributions continues). Prenons un premier exemple concret avec la distribution théorique associée au lancer de pièce de monnaie : la distribution de \textbf{Bernoulli}. Sa formule est la suivante :

\footnotesize

\begin{equation} f(x ; p)=\left\{\begin{array}{ll}
q=1-p & \text { si } x=0 \\
p & \text { si } x=1
\end{array}\right.
\label{eq:Bernoulli}
\end{equation}
\normalsize

avec \emph{p} la probabilité d'obtenir \(x = 1\) (pile), et \(1 – p\) la probabilité d'avoir \(x = 0\) (face). La distribution de Bernoulli ne dépend que d'un paramètre : \emph{p}. Avec différentes valeurs de \emph{p}, on peut obtenir différentes formes pour la distribution de Bernoulli. Si \emph{p} = 1/2, la distribution de Bernoulli décrit parfaitement l'expérience : obtenir pile à un lancer de pièce de monnaie. Si \emph{p} = 1/6, elle décrit alors l'expérience : obtenir 4 (tout comme n'importe quelle valeur de 1 à 6) à un lancer de dé. Pour un exemple plus appliqué, la distribution de Bernoulli est utilisée en analyse spatiale pour étudier la concentration d'accidents de la route ou de crimes en milieu urbain. En chaque endroit du territoire, il est possible de calculer la probabilité qu'un tel évènement ait lieu ou non en se basant sur les données observées et cette distribution.
La distribution continue la plus simple à décrire est certainement la distribution \textbf{uniforme}. Il s'agit d'une distribution un peu spéciale puisqu'elle attribue la même probabilité à toutes ses valeurs dans son espace d'échantillonnage. Elle est définie sur l'intervalle {[}-Inf ; +Inf{]} et a la fonction de densité suivante :

\footnotesize

\begin{equation} f(x ; \mathrm{a} ; \mathrm{b})=\left\{\begin{array}{cc}
\frac{1}{a-b} & \text { si } a \geq x \geq b \\
0 & \text { sinon }
\end{array}\right.
\label{eq:Uniforme}
\end{equation}
\normalsize

La fonction uniforme a donc deux paramètres, \emph{a} et \emph{b}, représentant respectivement les valeurs maximale et minimale au-delà desquelles les valeurs ont une probabilité 0 d'être obtenues. Pour avoir une meilleure intuition de ce que décrit une fonction de densité, il est intéressant de la représenter avec un graphique (figure \ref{fig:fig254}). Notez que sur ce graphique, l'axe des ordonnées n'indique pas précisément la probabilité associée à chaque valeur car celle-ci serait infinidécimale. Il sert uniquement à représenter la densité de la fonction de distribution.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig254-1} 

}

\caption{Distributions uniformes continues}\label{fig:fig254}
\end{figure}

On observe clairement que toutes les valeurs de \emph{x} entre \emph{a} et \emph{b} ont la même probabilité pour chacune de trois distributions uniformes présentées dans le graphique. Plus l'étendue est grande (\(a-b\)), plus l'espace d'échantillonnage est grand et plus la probabilité totale est répartie dans cet espace. Cette distribution serait donc idéale pour décrire un phénomène pour lequel chaque valeur a autant de chance de se produire qu'une autre. Prenons pour exemple un cas fictif avec un jeu de hasard qui vous proposerait la situation suivante : en tirant sur la manette d'une machine à sous, un nombre est tiré aléatoirement entre -60 et +50. Si le nombre est négatif, vous perdez de l'argent et inversement si le nombre est positif. Nous pouvons représenter cette situation avec une distribution uniforme continue et l'utiliser pour calculer quelques informations essentielles :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Selon cette distribution, quelle est la probabilité de gagner de l'argent lors d'un tirage (x \textgreater{} 0)?
\item
  Quelle est la probabilité de perdre de l'argent ? (x \textless{} 0)?
\item
  Si je perds moins de 30\$ au premier tirage, quelle est la probabilité que ai-je d'au moins récupérer ma mise au second tirage (x \textgreater{} 30)?
\end{enumerate}

Il est assez facile de calculer ces probabilités en utilisant la fonction \texttt{punif} dans R. Concrètement, cela permet de calculer l'intégrale de la fonction de masse sur un intervalle donné.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Probabilité d'obtenir une valeur supérieure ou égale à 0}
\KeywordTok{punif}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DataTypeTok{min =} \DecValTok{-60}\NormalTok{, }\DataTypeTok{max =} \DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5454545
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Probabilité d'obtenir une valeur inférieure à 0}
\KeywordTok{punif}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DataTypeTok{min =} \DecValTok{-60}\NormalTok{, }\DataTypeTok{max =} \DecValTok{50}\NormalTok{, }\DataTypeTok{lower.tail =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4545455
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Probabilité d'obtenir une valeur supérieure à 30}
\KeywordTok{punif}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DataTypeTok{min =} \DecValTok{-60}\NormalTok{, }\DataTypeTok{max =} \DecValTok{50}\NormalTok{,}\DataTypeTok{lower.tail =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1818182
\end{verbatim}

Les paramètres permettent donc d'ajuster la fonction de masse ou de densité d'une distribution afin de lui permettre de prendre des formes différentes. Certains paramètres vont changer la localisation de la distribution (la déplacer vers la droite ou la gauche de l'axe des X), d'autres son degré de dispersion (distribution pointue ou aplatie) ou encore sa forme (symétrie). Les différents paramètres d'une distribution correspondent donc à sa carte d'identité et donnent une idée précise sur sa nature.

\hypertarget{principales-distributions}{%
\subsection{Principales distributions}\label{principales-distributions}}

Il existe un très grand nombre de distributions théoriques et parmi elles, de nombreuses sont en fait des cas spéciaux d'autres distributions. Pour un petit aperçu du bestiaire, vous pouvez faire un saut à la page \href{http://www.math.wm.edu/~leemis/chart/UDR/UDR.html}{Univariate Distribution Relationships}, qui liste près de 80 distributions.

Nous nous concentrons ici sur une sélection de 18 distributions très répandues en sciences sociales. La figure \ref{fig:fig255} présente graphiquement leurs fonctions de masse et de densité présentées dans cette section. Notez que ces graphiques correspondent tous à une forme possible de chaque distribution. En modifiant leurs paramètres, il serait possible de produire une figure très différente. Les distributions discrètes sont représentées avec des graphiques en barres, et les distributions continues avec des graphiques de densité.



\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{images/distributions/all_distributions} 

}

\caption{18 distributions essentielles, design inspiré de \citet{SeanOwendist}}\label{fig:figdistribs}
\end{figure}

\hypertarget{la-distribution-uniforme-discruxe8te}{%
\subsubsection{La distribution uniforme discrète}\label{la-distribution-uniforme-discruxe8te}}

Nous avons déjà abordé cette distribution dans les exemples précédents. Elle permet de décrire un phénomène dont tous les résultats possibles ont exactement la même probabilité de se produire. L'exemple classique est bien sûr un lancer de dé.

\hypertarget{la-distribution-de-bernoulli}{%
\subsubsection{La distribution de Bernoulli}\label{la-distribution-de-bernoulli}}

La distribution de Bernoulli permet de décrire une expérience pour laquelle deux résultats sont possibles. Son espace d'échantillonnage est donc \([0 ; 1]\). Sa fonction de masse est la suivante :

\footnotesize

\begin{equation} f(x ; p)=\left\{\begin{array}{ll}
q=1-p & \text { si } x=0 \\
p & \text { si } x=1
\end{array}\right.
\label{eq:BernoulliB}
\end{equation}
\normalsize

avec \emph{p}, la probabilité d'obtenir \(x = 1\) (réussite) et donc \(1 – p\), la probabilité d'avoir \(x = 0\) (échec). La distribution de Bernoulli ne dépend que d'un paramètre : \emph{p} contrôlant la probabilité de réussite de l'expérience. Notez que si \(p = 1/2\), alors la distribution de Bernoulli est également une distribution uniforme. Un exemple d'application de la distribution de Bernoulli en études urbaines serait la modélisation de la survie d'un·e cycliste (1 pour survie, 0 pour décès) lors d'une collision avec une voiture selon une vitesse donnée.

\hypertarget{la-distribution-binomiale}{%
\subsubsection{La distribution binomiale}\label{la-distribution-binomiale}}

La distribution binomiale est utilisée pour caractériser une somme de distributions de Bernoulli. Un exemple simple serait l'accumulation des lancers d'une pièce de monnaie. Si l'on compte le nombre de fois où l'on fait pile, cette expérience est décrite par une distribution binomiale. Son espace d'échantillonnage est donc \([0 ; +\infty[\) (limité aux nombres entiers). Sa fonction de masse est la suivante :

\footnotesize

\begin{equation} 
    f(x ; n )=\binom{n}{x}p^x(1-p)^{n-x}
\label{eq:Binomial}
\end{equation}
\normalsize

avec \emph{x} le nombre de tirages réussis sur \emph{n} essais avec une probabilité \emph{p} de réussite à chaque tirage. Pour reprendre l'exemple précédent concernant les accidents de la route, une distribution binomiale permettrait de représenter la distribution du nombre de cyclistes survivant·e·s sur dix accidents impliquant une voiture à une intersection.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig256-1} 

}

\caption{La distribution binomiale}\label{fig:fig256}
\end{figure}

\hypertarget{la-distribution-guxe9omuxe9trique}{%
\subsubsection{La distribution géométrique}\label{la-distribution-guxe9omuxe9trique}}

La distribution géométrique permet de représenter le nombre de tirages nécessaires avec une distribution de Bernoulli avant d'obtenir une réussite. Par exemple, avec un lancer de dé, l'idée serait de compter le nombre de lancers nécessaires avant de tomber sur un 6. Son espace d'échantillonnage est donc \([1 ; +\infty[\) (limité aux nombres entiers). Sa distribution de masse est la suivante :

\footnotesize

\begin{equation} f(x ; p)= (1-p)^xp
\label{eq:geometrique}
\end{equation}
\normalsize

avec \emph{x} le nombre de tentatives avant d'obtenir une réussite, \(f(x)\) la probabilité que le premier succès n'arrive qu'après \emph{x} tentatives et \emph{p} la probabilité de réussite à chaque tentative. Cette distribution est notamment utilisée en marketing pour modéliser le nombre d'appels nécessaires avant de réussir une vente.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig257-1} 

}

\caption{La distribution géométrique}\label{fig:fig257}
\end{figure}

\hypertarget{la-distribution-binomiale-nuxe9gative}{%
\subsubsection{La distribution binomiale négative}\label{la-distribution-binomiale-nuxe9gative}}

La distribution binomiale négative est proche de la distribution géométrique. Elle permet de représenter le nombre de tentatives nécessaires afin d'obtenir un nombre \emph{n} de réussites \([1 ; +\infty[\) (limité aux nombres entiers positifs). Sa formule est la suivante :

\footnotesize

\begin{equation} f(x ; n ; p)=\left(\begin{array}{c}
x+n-1 \\
n
\end{array}\right) p^{n}(1-p)^{x}
\label{eq:binomialnegative}
\end{equation}
\normalsize

avec \emph{x} le nombre de tentatives avant d'obtenir \emph{n} réussites et \emph{p} la probabilité d'obtenir une réussite à chaque tentative. Cette distribution pourrait être utilisée pour modéliser le nombre de questionnaires \emph{x} à envoyer pour une enquête si l'on espère au moins \emph{n} réponses, sachant que la probabilité d'une réponse est \emph{p}.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig258-1} 

}

\caption{La distribution binomiale négative}\label{fig:fig258}
\end{figure}

\hypertarget{la-distribution-de-poisson}{%
\subsubsection{La distribution de poisson}\label{la-distribution-de-poisson}}

La distribution de poisson est utilisée pour modéliser des comptages. Son espace d'échantillonnage est donc \([0 ; +\infty[\) (limité aux nombres entiers positifs). Par exemple, il est possible de compter à une intersection le nombre de collisions entre des automobilistes et des cyclistes sur une période donnée. Cet exemple devrait vous faire penser à la distribution binomiale vue plus haut. En effet, il serait possible de noter chaque rencontre entre une voiture et un cycliste et de considérer que leur collision est une « réussite » (0 : pas d'accidents, 1 : accident). Cependant, ce type de données serait fastidieux à collecter comparativement au simple comptage des accidents. La distribution de poisson à une fonction de densité avec un seul paramètre \(\lambda\) (lambda) et est décrite par la formule suivante :

\footnotesize

\begin{equation} f(x ; \lambda)=\frac{\lambda^{x}}{x !} e^{-\lambda}
\label{eq:poisson}
\end{equation}
\normalsize

avec \emph{x} le nombre de cas, \emph{f(x)} la probabilité d'obtenir \emph{x} sachant \(\lambda\). \(\lambda\) peut être vue comme le taux moyen d'occurrences (nombre d'évènements divisé par la durée totale de l'expérience). Il permet à la fois de caractériser le centre et la dispersion de la distribution. Notez également que plus le paramètre \lambda augmente, plus la distribution de poisson tend vers une distribution normale.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig259-1} 

}

\caption{La distribution de poisson}\label{fig:fig259}
\end{figure}

\hypertarget{sectpoissonzero}{%
\subsubsection{La distribution de poisson avec excès de zéros}\label{sectpoissonzero}}

Il arrive régulièrement qu'une variable de comptage mesurée produise un très grand nombre de zéros. Prenons pour exemple le nombre de seringues de drogue injectable par tronçon de rue ramassées sur une période d'un mois. À l'échelle de toute une ville, un très grand nombre de tronçons n'auront tout simplement aucune seringue et dans ce contexte, la distribution classique de poisson n'est pas adaptée. On lui préfère alors sa version avec une inflation de zéros qui inclut un paramètre contrôlant la forte présence de zéros. Sa fonction de densité est la suivante :

\footnotesize

\begin{equation} f(x ; \lambda; p)=(1-p)\frac{\lambda^{x}}{x !} e^{-\lambda}
\label{eq:poissonzi}
\end{equation}
\normalsize

Plus exactement, la distribution de poisson avec excès de zéro (zero-inflated en anglais) est une combinaison de deux processus générant des zéros. En effet, un zéro peut être produit par la distribution de poisson originale (aussi appelé vrai zéro) ou alors par le processus menant à la surreprésentation des 0 dans le jeu de données, capturée par la probabilité \emph{p} (faux zéro). \emph{p} est donc le paramètre contrôlant la probabilité d'obtenir un zéro, indépendamment du phénomène étudié.

\begin{verbatim}
## Loading required package: stats4
\end{verbatim}

\begin{verbatim}
## Loading required package: splines
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'VGAM'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:tidyr':
## 
##     fill
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig259b-1} 

}

\caption{La distribution de poisson avec excès de zéros}\label{fig:fig259b}
\end{figure}

\hypertarget{la-distribution-gaussienne}{%
\subsubsection{La distribution gaussienne}\label{la-distribution-gaussienne}}

Plus communément appelée la distribution normale, la distribution gaussienne est utilisée pour représenter des variables continues centrées sur leur moyenne. Son espace d'échantillonnage est {]}-\(\infty\) ; +\(\infty\){[}. Cette distribution joue un rôle central en statistique. Le théorème central limite stipule que la somme d'un grand nombre de distributions tend généralement vers une distribution normale. Autrement dit, lorsque nous répétons une même expérience et que nous conservons les résultats de ces expériences, la distribution du résultat de ces expériences tend vers la normalité. Ceci s'explique par le fait qu'en moyenne, chaque répétition de l'expérience produit le même résultat, mais qu'un ensemble de petits facteurs aléatoires viennent rajouter de la variabilité dans les données collectées. Prenons un exemple concret, si l'on plante une centaine d'arbres simultanément dans un parc avec un degré d'ensoleillement identique et qu'on leur apporte les mêmes soins pendant dix ans, la distribution de leurs tailles suivra une distribution normale. Un ensemble de facteurs aléatoires (composition du sol, exposition au vent, aléas génétiques, passage de nuages, etc.) auront affecté différemment chaque arbre, ajoutant ainsi un peu de hasard dans leurs tailles finales. Ces dernières seront cependant davantage affectées par des paramètres centraux (espèces, ensoleillement, arrosage, etc.), et seront donc centrées autour d'une moyenne.
La fonction de densité de la distribution normale est la suivante :

\footnotesize

\begin{equation} f(x ; \mu ; \sigma)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}
\label{eq:gaussien}
\end{equation}
\normalsize

avec \emph{x} une valeur dont on souhaite connaître la probabilité, \emph{f(x)} sa probabilité, \(\mu\) (mu) la moyenne de la distribution normale (paramètre de localisation) et \(\sigma\) (sigma) son écart-type (paramètre de dispersion). La courbe normale suit une forme de cloche. Notez que :

\begin{itemize}
\tightlist
\item
  68,2\% de la masse de la distribution normale est comprise dans l'intervalle \([\mu- \sigma≤x≤ \mu+ \sigma]\)
\item
  95,4\% dans l'intervalle \([\mu- 2\sigma≤x≤ \mu+ 2\sigma]\)
\item
  99,7\% dans l'intervalle \([\mu- 3\sigma≤x≤ \mu+ 3\sigma]\)
\end{itemize}

Autrement dit, dans le cas d'une distribution normale, il est très invraisemblable d'observer des données situées à plus de trois écarts types de la moyenne.
Notez ici que lorsque \(\mu = 0\) et \(\sigma = 0\), on obtient la loi normale générale (ou centrée-réduite) (section \ref{sect02552}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig260-1} 

}

\caption{La distribution Gaussienne}\label{fig:fig260}
\end{figure}

\hypertarget{la-distribution-gaussienne-asymuxe9trique}{%
\subsubsection{La distribution gaussienne asymétrique}\label{la-distribution-gaussienne-asymuxe9trique}}

La distribution normale asymétrique (skew-normal) est une extension de la distribution gaussienne permettant de modifier la forme de la distribution normale pour qu'elle ne soit plus symétrique. Son espace d'échantillonnage est donc {]}-\(\infty\) ; +\(\infty\){[}. Sa fonction de densité est la suivante :

\footnotesize

\begin{equation} f(x;\xi;\omega;\alpha) = \frac{2}{\omega \sqrt{2 \pi}} e^{-\frac{(x-\xi)^{2}}{2 \omega^{2}}} \int_{-\infty}^{\alpha\left(\frac{x-\xi}{\omega}\right)} \frac{1}{\sqrt{2 \pi}} e^{-\frac{t^{2}}{2}} d t
\label{eq:skewgaussien}
\end{equation}
\normalsize

avec \(\xi\) (xi) le paramètre de localisation, \(\omega\) (omega) le paramètre de dispersion (ou d'échelle) et \(\alpha\) (alpha) le paramètre de forme (contrôlant le degré de symétrie). Si \(\alpha = 0\), alors la distribution skew-normal est une simple distribution normale. Ce type de distribution est très utile lorsque que l'on souhaite modéliser une variable pour laquelle on sait que des valeurs plus extrêmes s'observeront d'un côté ou de l'autre de la distribution. Les revenus totaux annuels des personnes ou des ménages sont de très bons exemples puisqu'ils sont distribués généralement avec une asymétrie positive : bien qu'une moyenne existe, il y a généralement plus de personnes ou de ménages avec des revenus très faibles, que de personnes ou de ménages avec des revenus très élevés.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig261-1} 

}

\caption{La distribution skew-Gaussienne}\label{fig:fig261}
\end{figure}

\hypertarget{la-distribution-log-normale}{%
\subsubsection{La distribution log-normale}\label{la-distribution-log-normale}}

Au même titre que la distribution skew-normal, la distribution log-normal est une version asymétrique de la distribution normale. Son espace d'échantillonnage est {]}0 ; +\(\infty\){[}. Cela signifie que cette distribution ne peut décrire que des données continues et positives. Sa fonction de densité est la suivante :
\footnotesize
\begin{equation} f(x ; \mu ; \sigma)=\frac{1}{x \sigma \sqrt{2 \pi}} e^{-\left(\frac{(\ln x-\mu)^{2}}{2 \sigma^{2}}\right)}
\label{eq:loggaussien}
\end{equation}
\normalsize

À la différence la distribution skew-normal, la distribution log-normal ne peut avoir qu'une asymétrie positive (étirée vers la droite). Elle est cependant intéressante puisqu'elle ne compte que deux paramètres (\(\mu\) et \(\sigma\)) ce qui la rend plus facile à ajuster. À nouveau, une distribution log-normal pourrait être utilisée pour décrire les revenus totaux annuels des individus ou des ménages ou les revenus d'emploi. Elle est aussi utilisée en économie sur les marchés financiers pour représenter les cours des actions et des biens (ces derniers ne pouvant pas être inférieurs à 0).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig262-1} 

}

\caption{La distribution log-gaussienne}\label{fig:fig262}
\end{figure}

\hypertarget{sect024311}{%
\subsubsection{La distribution de Student}\label{sect024311}}

La distribution de Student joue un rôle important en statistique, elle est par exemple utilisée lors du test \emph{t} pour calculer le degré de significativité du test. Comme la distribution gaussienne, la distribution de Student a une forme de cloche, est centrée sur sa moyenne et définie sur {]}-\(\infty\) ; +\(\infty\){[}. Elle a cependant des « queues plus lourdes » (\emph{heavy tails} en anglais). Entendez par-là que les valeurs extrêmes ont une plus grande probabilité d'occurrence dans une distribution de Student que dans une distribution gaussienne. Sa fonction de densité est la suivante :

\footnotesize

\begin{equation} p(x ; \nu ; \hat{\mu} ; \hat{\sigma})=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right) \sqrt{\pi \nu} \hat{\sigma}}\left(1+\frac{1}{\nu}\left(\frac{x-\hat{\mu}}{\hat{\sigma}}\right)^{2}\right)^{-\frac{\nu+1}{2}}
\label{eq:student}
\end{equation}
\normalsize

avec \(\mu\) le paramètre de localisation, \(\sigma\) le paramètre de dispersion (qui n'est cependant pas un écart-type comme pour la distribution normale) et \(\nu\) le nombre de degré de liberté. Plus \(\nu\) est grand, plus la distribution de Student tend vers une distribution normale. \(\Gamma\) représente la fonction mathématique gamma (à ne pas confondre avec la distribution de Gamma). Un exemple d'application en études urbaines serait l'exposition au bruit environnemental de cyclistes. Cette distribution s'approcherait certainement d'une distribution normale, mais les cyclistes croisent régulièrement des secteurs peu bruyants (parcs, rues résidentielles, etc.) et des secteurs très bruyants (artères majeures, zones industrielles, etc.), ce qui conduit vers une distribution de Student.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig263-1} 

}

\caption{La distribution de Student}\label{fig:fig263}
\end{figure}

\hypertarget{la-distribution-de-cauchy}{%
\subsubsection{La distribution de Cauchy}\label{la-distribution-de-cauchy}}

La distribution de Cauchy est également une distribution symétrique définie sur l'intervalle {]}-\(\infty\) ; +\(\infty\){[}. Elle a comme particularité d'avoir des queues potentiellement plus lourdes que la distribution de Student. Elle est notamment utilisée pour modéliser des phénomènes extrêmes comme les précipitations maximales annuelles, les niveaux d'inondations maximaux annuels ou les \emph{values at risk} pour les portefeuilles financiers. Il est également intéressant de noter que le quotient de deux variables indépendantes normalement distribuées suit une distribution de Cauchy. Sa fonction de densité est la suivante :

\footnotesize

\begin{equation} \frac{1}{\pi \gamma}\left[\frac{\gamma^{2}}{\left(x-x_{0}\right)^{2}+\gamma^{2}}\right]
\label{eq:cauchy}
\end{equation}
\normalsize

Elle dépend donc de deux paramètres : \(x_0\), le paramètre de localisation indiquant le pic de la distribution et \(\gamma\), un paramètre de dispersion.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig264-1} 

}

\caption{La distribution de Cauchy}\label{fig:fig264}
\end{figure}

\hypertarget{la-distribution-du-chi-carruxe9}{%
\subsubsection{La distribution du Chi-carré}\label{la-distribution-du-chi-carruxe9}}

La distribution du Chi2 est utilisée dans de nombreux tests statistiques. Spécifiquement, le test du Chi2 de Pearson est utilisé pour comparer les écarts au carré entre des fréquences attendues et observées de deux variables qualitatives. La distribution du Chi2 décrit donc les sommes des carrés d'un nombre \emph{k} de variables indépendantes normalement distribuées. Il est assez rare de modéliser un phénomène à l'aide d'une distribution du Chi2, mais son omniprésence dans les tests statistiques justifie qu'elle soit mentionnée ici. Cette distribution est définie sur l'intervalle {[}0 ; +\(\infty\){[} et a pour fonction de densité :

\footnotesize

\begin{equation} f(x;k) = \frac{1}{2^{k / 2} \Gamma(k / 2)} x^{k / 2-1} e^{-x / 2}
\label{eq:chi2}
\end{equation}
\normalsize

Cette fonction n'a qu'un paramètre \emph{k}, représentant donc le nombre de variables au carré sommées pour obtenir la distribution du Chi2

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig265-1} 

}

\caption{La distribution du Chi<sup>2</sup>}\label{fig:fig265}
\end{figure}

\hypertarget{la-distribution-exponentielle}{%
\subsubsection{La distribution exponentielle}\label{la-distribution-exponentielle}}

La distribution exponentielle est une version continue de la distribution géométrique. Pour cette dernière, on s'intéresserait au nombre de tentatives nécessaires pour obtenir un résultat positif, soit une dimension discrète. Pour la distribution exponentielle, cette dimension discrète est remplacée par une dimension continue. L'exemple le plus intuitif est sûrement le cas du temps. Dans ce cas, la distribution exponentielle servirait à décrire le temps d'attente nécessaire pour qu'un évènement se produise. Il pourrait aussi s'agir d'une force que l'on applique jusqu'à ce qu'un matériau cède. Cette distribution est donc définie sur l'intervalle {[}0 ; +\(\infty\){[} et a pour fonction de densité :

\footnotesize

\begin{equation} f(x;\lambda) = \lambda e^{-\lambda x}
\label{eq:exponentiel}
\end{equation}
\normalsize

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig266-1} 

}

\caption{La distribution exponentielle}\label{fig:fig266}
\end{figure}

\hypertarget{sect024315}{%
\subsubsection{La distribution de Gamma}\label{sect024315}}

La distribution de Gamma est une généralisation d'un grand nombre de distributions. Elle regroupe ainsi la distribution exponentielle et du Chi2. En d'autres termes, les distributions du chi2 et exponentielles sont des cas particuliers de la distribution de Gamma. Cette distribution est définie sur l'intervalle {]}0 ; +\(\infty\){[} (notez que le 0 est exclu) et sa fonction de densité est la suivante :

\footnotesize

\begin{equation} f(x ; \alpha; \beta)=\frac{\beta^{\alpha} x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)}
\label{eq:gamma}
\end{equation}
\normalsize

Elle comprend donc deux paramètres : \(\alpha\) et \(\beta\). Le premier est le paramètre de forme et le second un paramètre d'échelle (à l'inverse d'un paramètre de dispersion, plus sa valeur est petite, plus la distribution sera dispersée). Notez que cette distribution ne dispose pas d'un paramètre de localisation. Du fait de sa flexibilité, cette distribution est largement utilisée, que ce soit dans la modélisation des temps d'attente avant un évènement, la taille des réclamations d'assurance, les quantités de précipitations, etc.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig267-1} 

}

\caption{La distribution de Gamma}\label{fig:fig267}
\end{figure}

\hypertarget{sect024316}{%
\subsubsection{La distribution de Beta}\label{sect024316}}

La distribution de Beta est définie sur l'intervalle {[}0 ; 1{]}, elle est donc énormément utilisée pour représenter des variables étant des proportions ou des probabilités. Elle a aussi une utilité pratique en statistique, car en combinaison avec d'autres distributions, elle permet de modéliser leurs paramètres de probabilité (distribution beta-binomial, beta-negative-binomial, etc.). Un autre usage plus rare, mais intéressant est la modélisation de la fraction du temps représentée par une tâche dans le temps nécessaire à la réalisation de deux tâches de façon séquentielle. Ceci est dû au fait que la distribution d'une distribution gamma \emph{g1} divisée par la somme de \emph{g1} et d'une autre distribution gamma \emph{g2}, suit une distribution beta. Un exemple concret serait par exemple la fraction du temps effectué à pied dans un déplacement multimodal. La distribution de beta a la fonction de densité suivante :

\footnotesize

\begin{equation} f(x;\alpha;\beta) = \frac{1}{\mathrm{B}(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}
\label{eq:beta}
\end{equation}
\normalsize

Elle a donc deux paramètres \(\alpha\) et \(\beta\) contrôlant tous les deux la forme de la distribution. Cette caractéristique lui permet d'avoir une très grande flexibilité et même d'adopter des formes bimodales. \(B\) correspondant à la fonction mathématique Beta, à ne pas confondre avec la distribution de Beta et le paramètre Beta (\(\beta\)) de cette même distribution.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig268-1} 

}

\caption{La distribution de Beta}\label{fig:fig268}
\end{figure}

\hypertarget{la-distribution-de-weibull}{%
\subsubsection{La distribution de Weibull}\label{la-distribution-de-weibull}}

La distribution de Weibull est directement liée à la distribution exponentielle, cette dernière étant en fait un cas particulier de distribution de Weibull. Elle sert donc à représenter une quantité \emph{x} (souvent le temps) à accumuler pour qu'un évènement se produise. La distribution de Weibull est définie sur l'intervalle {[}0 ; +\(\infty\){[} et a la fonction de densité suivante :

\footnotesize

\begin{equation} f(x;\lambda) = \frac{k}{\lambda} (\frac{x}{\lambda})^{k-1} e^{-(\frac{x}{\lambda})^k}
\label{eq:weibull}
\end{equation}
\normalsize

\(\lambda\) est le paramètre de dispersion (analogue a celui d'une distribution exponentielle classique) et \emph{k} le paramètre de forme. Pour bien comprendre le rôle de \emph{k}, prenons un exemple : la propagation d'un champignon d'un arbre à son voisin. Si \(k<1\), cela signifie que la probabilité que l'évènement modélisé se produise diminue avec le temps. En d'autres termes, dans de nombreux cas la contamination se fait rapidement. Si \(k=1\), alors la probabilité que l'évènement se produise reste stable dans le temps. Si \(k > 1\), alors la probabilité que l'évènement se produisent augmente avec le temps, ce qui signifie une augmentation des risques de contamination à mesure que les deux arbres restent à proximité. La distribution de Weibull est très utilisée en analyse de survie, en météorologie, en ingénierie des matériaux et dans la théorie des valeurs extrêmes.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig269-1} 

}

\caption{La distribution de Weibull}\label{fig:fig269}
\end{figure}

\hypertarget{la-distribution-de-pareto}{%
\subsubsection{La distribution de Pareto}\label{la-distribution-de-pareto}}

La distribution de Pareto est à la distribution exponentielle ce que la distribution log-normal est à la distribution gaussienne : la distribution de l'exponentiel (e) de cette distribution originale. Elle est définie sur l'intervalle \([x_m ; +\infty[\) avec la fonction de densité suivante :

\footnotesize

\begin{equation} f(x;x_m;k) = (\frac{x_m}{x})^k
\label{eq:pareto}
\end{equation}
\normalsize

Elle comprend donc deux paramètres, \(x_m\) étant un paramètre de localisation (décalant la distribution vers la droite ou vers la gauche) et \(k\) un paramètre de forme. Plus \(k\) augmente, plus la probabilité prédite par la distribution décroît rapidement.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/fig270-1} 

}

\caption{La distribution de Pareto}\label{fig:fig270}
\end{figure}

Originalement, le mathématicien Pareto a utilisé cette fonction pour décrire la répartition du capital parmi la population puisqu'une large partie du capital est détenue par une petite fraction de la population. Elle peut également être utilisée pour décrire la répartition de la taille des villes \citep{William_pareto_ville}, \href{https://medium.com/@worstonlinedater/tinder-experiments-ii-guys-unless-you-are-really-hot-you-are-probably-better-off-not-wasting-your-2ddf370a6e9a}{la popularité des hommes sur tinder} ou la taille des fichiers échangés sur internet \citep{William_pareto}. Pour ces trois exemples, nous avons une situation avec : de nombreuses petites villes, profils peu attractifs, petits fichiers échangés et à l'inverse très peu de grandes villes, profils très attractifs, gros fichiers échangés.

\hypertarget{cas-particuliers}{%
\subsubsection{Cas particuliers}\label{cas-particuliers}}

Sachez également qu'il existe des formes « plus exotiques » de distributions que nous n'abordons pas ici, mais auxquelles vous pourriez être confrontés un jour :

\begin{itemize}
\tightlist
\item
  Les distributions sphériques, servant à décrire des données dont le 0 est équivalent à la valeur maximale. Par exemple, des angles puisque 0 et 360 degrés sont identiques.
\item
  Les mixtures de distributions, décrivant des combinaisons de distributions. Par exemple, la distribution de la taille de tous les humains est en réalité un mixte entre deux distributions gaussiennes, une pour chaque sexe, puisque ces deux sous-distributions n'ont pas la même moyenne ni le même écart-type.
\item
  Les distributions multivariées, permettant de décrire des phénomènes multidimensionnels. Par exemple, la réussite des élèves en français et en mathématique pourrait être modélisée comme une distribution gaussienne bivariée plutôt que deux distributions distinctes.
\item
  Les distributions censurées décrivant des variables pour lesquels des valeurs sont possibles au-delà d'une certaine limite mais que l'on est incapable de mesurer. Un bon exemple serait la mesure de la pollution sonore avec un capteur incapable de détecter des niveaux sonores en dessous de 55 décibels. Il arrive parfois en ville que les niveaux sonores soient si faibles, mais les données collectées ne le montrent pas. Dans ce contexte, il est important d'utiliser des versions censurées des distributions présentées précédemment. Les observations au-delà de la limite sont conservées dans l'analyse, mais nous ne disposons que d'une information partielle à leur égard.
\item
  Les distributions tronquées, souvent confondues avec les distributions censurées, décrivent des situations ou des données qui au-delà d'une certaine limite sont retirées simplement de l'analyse.
\end{itemize}

\hypertarget{conclusion-sur-les-distributions}{%
\subsection{Conclusion sur les distributions}\label{conclusion-sur-les-distributions}}

Voilà qui conclut cette exploration des principales distributions à connaître. L'idée n'est bien sûr pas de toutes les retenir par cœur (et encore moins les formules mathématiques), mais plutôt de se rappeler dans quels contextes elles peuvent être utiles; et de revenir au besoin sur ce chapitre. Vous aurez certainement besoin de le relire avant d'aborder le chapitre portant sur les modèles linéaires généralisés (GLM).
Wikipédia dispose d'informations très détaillées sur chaque distribution si vous avez besoin d'informations complémentaires. Pour un tour d'horizon plus exhaustif des distributions, vous pouvez aussi faire un tour sur les projets \href{https://sites.google.com/site/probonto/screenshots}{probonto} et \href{https://blog.wolfram.com/2013/02/01/the-ultimate-univariate-probability-distribution-explorer/}{the ultimate probability distribution explorer}.

\hypertarget{sect025}{%
\section{Statistiques descriptives sur des variables quantitatives}\label{sect025}}

\hypertarget{sect0251}{%
\subsection{Les paramètres de tendance centrale}\label{sect0251}}

Trois mesures de tendance centrale permettent de résumer rapidement une variable quantitative :

\begin{itemize}
\tightlist
\item
  la \textbf{moyenne arithmétique} est simplement la somme des données d'une variable divisée par le nombre d'observations (\(n\)), soit \(\frac{\sum_{i=1}^n x_i}{n}\) notée \(\mu\) (prononcez \emph{mu}) pour des données pour une population et \(\bar{x}\) (prononcez \emph{x barre}) pour un échantillon.
\item
  la \textbf{médiane} est la valeur qui coupe la distribution d'une variable d'une population ou d'un échantillon en deux parties égales. Autrement dit, 50\% des valeurs des observations lui sont supérieures et 50\% lui sont inférieures.
\item
  le \textbf{mode} est la valeur la plus fréquente parmi un ensemble d'observations pour une variable. Il s'applique ainsi à des variables discrètes (avec un nombre fini de valeurs discrètes dans un intervalle donné) et non à des variables continues (avec un nombre infini de valeurs réelles dans un intervalle donné). Prenons deux variables, l'une discrète relative au nombre d'accidents par intersection (avec \(X \in \left[0,20\right]\)) et l'autre continue relative à la distance de dépassement (en mètres) d'un·e cycliste par un véhicule motorisé (avec \(X \in \left[0,5\right]\)). Pour la première, le mode -- la valeur la plus fréquente -- est certainement 0. Pour la seconde, identifier le mode n'est pas pertinent puisqu'il peut y avoir un nombre infini de valeurs entre 0 et 5 mètres.
\end{itemize}

Il convient de ne pas confondre moyenne et médiane ! Dans le tableau \ref{tab:tableRevMoyMed}, nous avons reporté les valeurs moyennes et médianes des revenus des ménages pour les municipalités de l'île de Montréal en 2015. Par exemple, les 8685 ménages résidant à Wesmount disposaient en moyenne d'un revenu de 295099\$; la moitié de ces 8685 ménages avaient un revenu inférieur à 100153\$ et l'autre moitié un revenu supérieur à cette valeur (médiane). Cela démontre clairement que la moyenne peut être grandement affectée par des valeurs extrêmes (faibles ou fortes); autrement dit, plus l'écart entre les valeurs de la moyenne et la médiane est importante, plus les données de la variable sont inégalement réparties. À Westmount, soit la municipalité la plus nantie de l'île de Montréal, les valeurs extrêmes sont des ménages avec des revenus très élevés tirant fortement la moyenne vers le haut. À l'inverse, le faible écart entre les valeurs moyenne et médiane dans la municipalité de Montréal-Est (58594\$ versus 50318\$) soulignent que les revenus des ménages sont plus également répartis. Cela explique que pour comparer les revenus totaux ou d'emploi entre différents groupes (selon le sexe, le groupe d'âge, le niveau d'éducation, la municipalité ou région métropolitaine, etc.), on prévilégie habituellement l'utilisation des revenus médians.

\begin{table}

\caption{\label{tab:tableRevMoyMed}Revenus moyens et médians des ménages en dollars, municipalités de l'île de Montréal, 2015}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lrrr}
\toprule
Municipalité & Nombre de ménages & Revenu moyen & Revenu médian\\
\midrule
Baie-D'Urfé & 1 330 & 171 390 & 118 784\\
Beaconsfield & 6 660 & 187 173 & 123 392\\
Côte-Saint-Luc & 13 490 & 94 570 & 58 935\\
Dollard-Des Ormeaux & 17 210 & 102 104 & 78 981\\
Dorval & 8 390 & 89 952 & 64 689\\
\addlinespace
Hampstead & 2 470 & 250 497 & 122 496\\
Kirkland & 6 685 & 144 676 & 115 381\\
Montréal & 779 805 & 69 047 & 50 227\\
Montréal-Est & 1 730 & 58 594 & 50 318\\
Montréal-Ouest & 1 850 & 159 374 & 115 029\\
\addlinespace
Mont-Royal & 7 370 & 205 309 & 109 540\\
Pointe-Claire & 12 380 & 100 294 & 80 242\\
Sainte-Anne-de-Bellevue & 1 960 & 102 969 & 67 200\\
Senneville & 345 & 203 790 & 116 224\\
Westmount & 8 685 & 295 099 & 100 153\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{sect0252}{%
\subsection{Les paramètres de position}\label{sect0252}}

Les paramètres de position permettent de diviser une distribution en \emph{n} parties égales.

\begin{itemize}
\tightlist
\item
  Les \textbf{quartiles} qui divisent une distribution en quatre parties (25\%) :

  \begin{itemize}
  \tightlist
  \item
    Q1 (25\%), soit le quartile inférieur ou premier quartile;
  \item
    Q2 (50\%), soit la médiane;
  \item
    Q3 (75\%), soit le quartile supérieur ou troisième quartile.
  \end{itemize}
\item
  Les \textbf{quintiles} qui divisent une distribution en cinq parties égales (20\%).
\item
  Les \textbf{déciles} (de D1 à D9) qui divisent une distribution en dix parties égales (10\%).
\item
  Les \textbf{centiles} (de C1 à C99) qui divisent une distribution en cent parties égales (1\%).
\end{itemize}

En cartographie, les quartiles et les quintiles sont souvent utilisés pour discrétiser une variable quantitative (continue ou discrète) en quatre ou cinq classes et plus rarement, en huit ou dix classes. Avec les quartiles, les bornes des classes qui comprendront chacune 25\% des unités spatiales seront ainsi définies comme suit : {[}Min à Q1{]}, {[}Q1 à Q2{]}, {[}Q2 à Q3{]} et {[}Q3 à Max{]}. La méthode de discrétisation selon les quartiles ou quintiles permet alors de repérer, en un coup d'œil, à quelle tranche de 25\% ou 20\% des données appartient chacune des unités spatiales. Cette méthode de discrétisation est aussi utile pour comparer plusieurs cartes et vérifier si deux phénomènes sont ou non colocalisés \citep{pumain1994}. En guise d'exemple, les pourcentages de personnes à faible revenu et de locataires par secteur de recensement ont clairement des distributions spatiales très semblables dans la région métropolitaine de Montréal en 2016 (figure \ref{fig:figunivarie2}).

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{images/univariee/figure2} 

}

\caption{Exemples de cartographie avec une discrétisation selon les quantiles}\label{fig:figunivarie2}
\end{figure}

Une lecture attentive des valeurs des centiles permet de repérer la présence de valeurs extrêmes voire aberrantes dans un jeu de données. Il n'est donc pas rare de les voir reportées dans un tableau de statistiques descriptives d'un article scientifique, et ce, afin de décrire succinctement les variables à l'étude. Par exemple, dans une étude récente comparant les niveaux d'exposition au bruit des cyclistes dans trois villes \citep{2020_1}, les auteurs reportent à la fois les valeurs moyennes et celles de plusieurs centiles. Globalement, la lecture des valeurs moyennes permet de constater que, sur la base des données collectées, les cyclistes sont plus exposés au bruit à Paris qu'à Montréal et Copenhague (73,4 dB(A) contre 70,7 et 68,4, tableau \ref{tab:tableCentiles}). Compte tenu de l'échelle logarithmique du bruit, la différence de 5 dB(A) entre les valeurs moyennes du bruit de Copenhague et de Paris peut être considérée comme une multiplication de l'énergie sonore par plus de 3. Pour Paris, l'analyse des quartiles montre que durant 25\% du temps des trajets à vélo (plus de 63 heures de collecte), les participants ont été exposés à des niveaux de bruit soit inférieurs à 69,1 dB(A) (premier quartile), soit supérieurs à 74 dB(A). Quant à l'analyse des centiles, elle permet de constater que durant 5\% et 10\% du temps, les participants étaient exposés à des niveaux de bruit très élevés, dépassant 77 dB(A) (C90=76 et C90=77,2).

\begin{table}

\caption{\label{tab:tableCentiles}Stastistiques descriptives de l'exposition au bruit des cyclistes par minute dans trois villes (dB(A), Laeq 1min)}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lrrr}
\toprule
Statistiques & Copenhague & Montréal & Paris\\
\midrule
N & 6 212,0 & 4 723,0 & 3 793,0\\
Moyenne de bruit & 68,4 & 70,7 & 73,4\\
Centiles &  &  & \\
1 & 57,5 & 59,2 & 62,3\\
5 & 59,1 & 61,1 & 65,0\\
\addlinespace
10 & 60,3 & 62,3 & 66,5\\
25 (premier quartile) & 62,7 & 64,5 & 69,1\\
50 (médiane) & 66,0 & 67,7 & 71,6\\
75 (troisième quartile) & 69,2 & 71,0 & 74,0\\
90 & 71,9 & 73,7 & 76,0\\
\addlinespace
95 & 73,3 & 75,2 & 77,2\\
99 & 76,5 & 78,9 & 81,0\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{sect0253}{%
\subsection{Les paramètres de dispersion}\label{sect0253}}

Cinq principales mesures de dispersion permettent d'évaluer la variabilité des valeurs d'une variable quantitative : l'étendue, l'écart interquartile, la variance, l'écart-type et le coefficient de variation. Notez d'emblée que cette dernière mesure ne s'applique pas à des variables d'intervalle (section \ref{sect02122}).

\begin{itemize}
\item
  \textbf{L'étendue} est la différence entre les valeurs minimale et maximale d'une variable, soit l'intervalle des valeurs dans lequel elle a été mesurée. Il convient d'analyser avec prudence cette mesure puisqu'elle inclut dans son calcul des valeurs potentiellement extrêmes voire aberrantes (faibles ou fortes).
\item
  \textbf{L'intervalle ou écart interquartile} est la différence entre les troisième et premier quartiles (\(Q3 − Q1\)). Il représente ainsi une mesure de la dispersion des valeurs de 50\% des observations centrales de la distribution. Plus la valeur de l'écart interquartile est élevée, plus la dispersion des 50\% des observations centrales est forte. Contrairement à l'étendue, cette mesure élimine l'influence des valeurs extrêmes puisqu'elle ne tient pas compte des 25\% des observations les plus faibles {[}Min à Q1{]} et des 25\% des observations les plus fortes {[}Q3 à Max{]}. Graphiquement, l'intervalle interquartile est représenté à l'aide d'une boîte à moustaches (\emph{boxplot} en anglais) : plus l'intervalle interquartile sera grand, plus la boîte sera allongée (figure \ref{fig:figunivarie3})
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{images/univariee/figure3} 

}

\caption{Graphique en violon, boîte à moustaches et intervalle interquartile}\label{fig:figunivarie3}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{La variance} est la somme des déviations à la moyenne au carré (numérateur) divisée par le nombre d'observations pour une population (\(\sigma^2\)) ou divisée par le nombre d'observations moins une (\(s^2\)) pour un échantillon (eq. \eqref{eq:variance}). Puisque les déviations à la moyenne sont mises au carré, la valeur de la variance (tout comme celle de l'écart-type) sera toujours positive. Plus sa valeur est élevée, plus les observations sont dispersées autour de la moyenne. La variance représente ainsi l'écart au carré moyen des observations à la moyenne.
\end{itemize}

\footnotesize

\begin{equation}  
\sigma^2=\frac{\sum_{i=1}^n (x_{i}-\mu)^2}{n} \text{ ou } s^2=\frac{\sum_{i=1}^n (x_{i}-\bar{x})^2}{n-1}
\label{eq:variance}
\end{equation}
\normalsize

\begin{itemize}
\tightlist
\item
  \textbf{L'écart-type} est la racine carrée de la variance (eq. \eqref{eq:ecartype}). Rappelez-vous que la variance est calculée à partir des déviations à la moyenne mises au carré. Étant donné que l'écart-type est la racine carrée de la variance, il est donc évalué dans les mêmes unités que la variable, contrairement à la variance. Bien entendu, comme pour la variance, plus la valeur de l'écart-type est élevée, plus la distribution des observations autour de la moyenne est dispersée.
\end{itemize}

\footnotesize

\begin{equation}  
\sigma=\sqrt{\sigma^2}=\sqrt{\frac{\sum_{i=1}^n (x_{i}-\mu)^2}{n}} \text{ ou } s=\sqrt{s^2}=\sqrt{\frac{\sum_{i=1}^n (x_{i}-\bar{x})^2}{n-1}}
\label{eq:ecartype}
\end{equation}
\normalsize

\begin{bloc_notes}
Les formules des variances et des écart-types pour une population et un échantillon sont très similaires : seul le dénominateur change avec \(n\) \emph{versus} \(n-1\) observations. Par conséquent, plus le nombre d'observations de votre jeu de données sera important, plus l'écart entre ces deux mesures de dispersion pour une population et un échantillon sera minime.

Comme dans la plupart des logiciels de statistique, les fonctions de base \texttt{var} et \texttt{sd} de R calculent la variance et l'écart-type pour un échantillon (\(n-1\) au dénominateur). Si vous souhaitez les calculer pour une population, adaptez la syntaxe ci-dessous dans laquelle \texttt{df\$var1} représente la variable intitulée \texttt{var1} présente dans un \emph{dataframe} nommé \texttt{df}.

\texttt{var.p\ \textless{}-\ mean((df\$var1\ -\ mean(df\$var1))\^{}2)}

\texttt{sd.p\ \textless{}-\ sqrt(mean((df\$var1\ -\ mean(df\$var1))\^{}2))}

\end{bloc_notes}

\begin{itemize}
\tightlist
\item
  \textbf{Le coefficient de variation (CV)} est le rapport entre l'écart-type et la moyenne, représentant ainsi une standardisation de l'écart-type ou, en d'autres termes, une mesure de dispersion relative (eq. \eqref{eq:cv}). L'écart-type étant exprimé dans l'unité de mesure de la variable, il ne peut pas être utilisé pour comparer les dispersions de variables exprimées des unités de mesure différentes (par exemple, en pourcentage, en kilomètres, en dollars, etc.). Pour y remédier, on utilisera le coefficient de variation : une variable est plus dispersée qu'une autre si la valeur de son CV est plus élevée. Certains préfèreront multiplier la valeur du CV par 100 : l'écart-type est alors exprimé en pourcentage de la moyenne.
\end{itemize}

\footnotesize

\begin{equation}  
CV=\frac{\sigma}{\mu} \text{ ou } CV=\frac{s^2}{\bar{x}}
\label{eq:cv}
\end{equation}
\normalsize

Illustrons comment calculer les cinq mesures de dispersion précédemment décrites à partir de valeurs fictives pour huit observations (colonne intitulée \(x_i\) au tableau \ref{tab:datavar}). Les différentes statistiques reportées dans ce tableau sont calculées comme suit :

\begin{itemize}
\item
  La \textbf{moyenne} est la somme divisée par le nombre d'observations, soit \(248/8=31\).
\item
  L'\textbf{étendue} est la différence entre les valeurs maximale et minimale, soit \(40-22=30\).
\item
  Les quartiles coupent la distribution en quatre parties égales. Avec huit observations triées par ordre croissant, \textbf{le premier quartile} est égale à la valeur de la 2\textsuperscript{e} observation (soit 25), la \textbf{médiane} à celle de la 4\textsuperscript{e} (30), le \textbf{troisième quartile} à celle de la 6\textsuperscript{e} (35).
\item
  \textbf{L'écart interquartile} est la différence entre Q3 et Q1, soit \(35-25=10\).
\item
  La seconde colonne du tableau est l'écart à la moyenne (\(x_i-\bar{x}\)), soit \(22 - 31 = -9\) pour l'observation \emph{1}; la somme de ces écarts est toujours égale à 0. La troisième colonne est cette déviation mise au carré (\((x_i-\bar{x})^2\)), soit \(-9^2 = 81\), toujours pour l'observation \emph{1}. La somme de ces déviations à la moyenne au carré (\(268\)) représente le numérateur de la variance (eq. \eqref{eq:variance}). En divisant cette somme par le nombre d'observations, on obtient la \textbf{variance pour une population} (\(268/8=33,5\)) tandis que la \textbf{variance d'un échantillon} est égale à \(268/(8-1)=38,29\).
\item
  L'écart-type est la racine carrée de la variance (eq. \eqref{eq:ecartype}), soit \(\sigma=\sqrt{33,5}=5,79\) et \(s=\sqrt{38,29}=6,19\).
\item
  Finalement, les valeurs des coefficients de variation (eq. \eqref{eq:cv}) sont de \(5,79/31=0,19\) pour une population et \(6,19/31=0,20\) pour un échantillon.
\end{itemize}

\begin{table}

\caption{\label{tab:datavar}Calcul des mesures de dispersion sur des données fictives}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lrrr}
\toprule
Observation & $x_i$ & $x_i-\bar{x}$ & $(x_i-\bar{x})^2$\\
\midrule
1 & 22,00 & -9 & 81,0\\
2 & 25,00 & -6 & 36,0\\
3 & 27,00 & -4 & 16,0\\
4 & 30,00 & -1 & 1,0\\
5 & 32,00 & 1 & 1,0\\
\addlinespace
6 & 35,00 & 4 & 16,0\\
7 & 37,00 & 6 & 36,0\\
8 & 40,00 & 9 & 81,0\\
\textbf{Statistique} &  &  & \\
N & 8,00 &  & \\
\addlinespace
Somme & 248,00 & 0 & 268,0\\
Moyenne ($\bar{x}$ ou $\mu$) & 31,00 & 0 & 33,5\\
Étendue & 18,00 &  & \\
Premier quartile & 25,00 &  & \\
Troisième quartile & 35,00 &  & \\
\addlinespace
Intervalle interquartile & 10,00 &  & \\
Variance (population, $\sigma^2$) & 33,50 &  & \\
Écart-type (population, $\sigma$) & 5,79 &  & \\
Variance (échantillon, $s^2$) & 38,29 &  & \\
Écart-type (échantillon, $s$) & 6,19 &  & \\
\addlinespace
Coefficient de variation ($\sigma / \mu$) & 0,19 &  & \\
Coefficient de variation ($s / \bar{x}$) & 0,20 &  & \\
\bottomrule
\end{tabular}
\end{table}

Le tableau \ref{tab:datavar2} vise à démontrer à partir de trois variables comment certaines mesures de dispersion sont sensibles à l'unité de mesure et/ou aux valeurs extrêmes.

Concernant \textbf{l'unité de mesure}, nous avons créé deux variables \emph{A} et \emph{B}, avec \emph{B} étant simplement \emph{A} multiplié par 10. Pour \emph{A}, les valeurs de la moyenne, l'étendue et l'intervalle interquartile sont respectivement de 31, 18 et 10. Sans surprise, celles de B sont multipliées par 10 (310, 180, 100). La variance étant la moyenne des déviations à la moyenne au carré, elle est égale à 33,50 pour \emph{A} et donc à \(33,50\times10^2=3350\) pour \emph{B}; l'écart-type de \emph{B} est égal à celui de \emph{A} multiplié par 10. Cela démontre que l'étendue, l'intervalle interquartile, la variance et l'écart-type sont des mesures de dispersion dépendantes de l'unité de mesure. Par contre, le coefficient de variation (CV) étant le rapport de l'écart-type avec la moyenne, il a la même valeur pour \emph{A} et \emph{B}, ce qui démontre que CV est bien une mesure de dispersion relative permettant de comparer des variables exprimées dans des unités de mesure différentes.

Concernant \textbf{la sensibilité aux valeurs extrêmes}, nous avons créé la variable \emph{C} pour laquelle seule la huitième observation a une valeur différente (40 pour \emph{A} et \emph{105} pour B). Cette valeur de 105 pourrait être soit une valeur extrême positive mesurée, soit une valeur aberrante (par exemple, si l'unité de mesure était un pourcentage variant de 0 à 100\%). Cette valeur a un impact important sur la moyenne (31 contre 39,12) et l'étendue (18 contre 83) et corollairement sur la variance (33,50 contre 641,86), l'écart-type (5,79 contre 25,33) et le coefficient de variation (0,19 contre 0,65). Par contre, l'intervalle interquartile étant calculé sur 50\% des observations centrales (\(Q3-Q1\)), il n'est pas affecté par cette valeur extrême.

\begin{table}

\caption{\label{tab:datavar2}Illustration de la sensibilité des mesures de dispersion à l'unité de mesure et aux valeurs extrêmes}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lrrr}
\toprule
Observation & A & B & C\\
\midrule
1 & 22,00 & 220,00 & 22,00\\
2 & 25,00 & 250,00 & 25,00\\
3 & 27,00 & 270,00 & 27,00\\
4 & 30,00 & 300,00 & 30,00\\
5 & 32,00 & 320,00 & 32,00\\
\addlinespace
6 & 35,00 & 350,00 & 35,00\\
7 & 37,00 & 370,00 & 37,00\\
8 & 40,00 & 400,00 & 105,00\\
\textbf{Statistique} &  &  & \\
Moyenne ($\mu$) & 31,00 & 310,00 & 39,12\\
\addlinespace
Étendue & 18,00 & 180,00 & 83,00\\
Intervalle interquartile & 10,00 & 100,00 & 10,00\\
Variance (population, $\sigma^2$) & 33,50 & 3 350,00 & 641,86\\
Écart-type (population, $\sigma$) & 5,79 & 57,88 & 25,33\\
Coefficient de variation ($\sigma / \mu$) & 0,19 & 0,19 & 0,65\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:resume}Résumé de la sensibilité de la moyenne et des mesures de dispersion}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lll}
\toprule
Statistique & Unité de mesure & Valeurs extrêmes\\
\midrule
Moyenne & X & X\\
Étendue & X & X\\
Intervalle interquartile & X & \\
Variance & X & X\\
Écart-type & X & X\\
\addlinespace
Coefficient de variation &  & X\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{sect0254}{%
\subsection{Les paramètres de forme}\label{sect0254}}

\hypertarget{vuxe9rifier-la-normalituxe9-dune-variable-quantitative}{%
\subsubsection{Vérifier la normalité d'une variable quantitative}\label{vuxe9rifier-la-normalituxe9-dune-variable-quantitative}}

\begin{bloc_objectif}
De nombreuses méthodes statistiques qui seront abordées dans les chapitres suivants -- entre autres, la corrélation de Pearson, les test \emph{t} et l'analyse de variance, les régressions simple et multiple -- requièrent que la variable quantitative suive une \textbf{distribution normale} (nommée aussi \textbf{distribution gaussienne}).

Dans cette sous-section, nous décrirons trois démarches pour vérifier si la distribution d'une variable est normale : les coefficients d'asymétrie et d'applatissement (\emph{skewness} et \emph{kurtosis} en anglais), les graphiques (histogramme avec courbe normale, diagramme quantile-quantile), les tests de normalité (tests de Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors, Anderson-Darling et Jarque-Bera).

\textbf{Il est vivement recommandé de réaliser les trois démarches !}

\end{bloc_objectif}

Une distribution est normale quand elle est symétrique et mésokurtique (figure \ref{fig:figFormeDistr}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/univariee/FormeDistribution} 

}

\caption{Formes d'une distribution et les coefficients d'asymétrie et d'aplatissement}\label{fig:figFormeDistr}
\end{figure}

\hypertarget{vuxe9rifier-la-normalituxe9-avec-les-coefficients-dasymuxe9trie-et-dapplatissement}{%
\paragraph{Vérifier la normalité avec les coefficients d'asymétrie et d'applatissement}\label{vuxe9rifier-la-normalituxe9-avec-les-coefficients-dasymuxe9trie-et-dapplatissement}}

\textbf{Une distribution est dite symétrique} quand la moyenne arithmétique est au centre de la distribution, c'est-à-dire que les observations sont bien réparties de part et d'autre de la moyenne qui sera alors égale à la médiane et au mode (on utilisera uniquement le mode pour une variable discrète et non pour une variable continue). Pour évaluer l'asymétrie, on utilise habituellement le coefficient d'asymétrie (\emph{skewness} en anglais).

Sachez toutefois qu'il existe trois façons (formules) pour le calculer \citep{joanes1998comparing} : \(g_1\) est la formule classique (eq. \eqref{eq:SkewType1}, disponible dans R avec la fonction \texttt{skewness} du \emph{package} \textbf{moments}), \(G_1\) est une version ajustée (eq. \eqref{eq:SkewType2}, utilisée dans les logiciels SAS et SPSS notamment) et \(b_1\) est une autre version ajustée (eq. \eqref{eq:SkewType3}, utilisée par les logiciels MINITAB et BMDP). Nous verrons qu'avec les \emph{packages} \textbf{DescTools} ou \textbf{e1071}, il possible de calculer ces trois méthodes. Aussi, pour des grands échantillons (\(n>100\)), il y a très peu de différences entre les résultats produits par ces trois formules \citep{joanes1998comparing}. Quelle que soit la formule utilisée, le coefficient d'assymétrie s'interprète comme suit (figure \ref{fig:asymetrie}) :

\begin{itemize}
\tightlist
\item
  quand la valeur du \emph{skewness} est négative, la \textbf{distribution est asymétrique négative}. La distribution est alors tirée à gauche par des valeurs extrêmes faibles, mais peu nombreuses. On emploie souvent l'expression \emph{la queue de distribution} est étirée vers la gauche. La moyenne est alors inférieure à la médiane.
\item
  quand la valeur du \emph{skewness} est égale à 0, \textbf{la distribution est symétrique} (la médiane sera égale à la moyenne). Pour une variable discrète, les valeurs du mode, de la moyenne et de la médiane seront égales.
\item
  quand la valeur du \emph{skewness} est positive, la \textbf{distribution est symétrique positive}. La distribution est alors tirée à droite par des valeurs extrêmes fortes, mais peu nombreuses. La queue de distribution est alors étirée vers la droite. La moyenne est alors supérieure à la médiane. En sciences sociales, les variables de revenu (totaux ou d'emploi, des individus ou des ménages) ont souvent des distributions asymétriques positives : la moyenne est affectée par quelques observations avec des valeurs de revenu très élevées et est ainsi supérieure à la médiane. En études urbaines, la densité de population pour des unités géographiques d'une métropole donnée (secteur de recensement par exemple) a aussi souvent une distribution asymétrique positive : quelques secteurs de recensement au centre de la métropole sont caractérisés par des valeurs de densité très élevées qui tirent la distribution vers la droite.
\end{itemize}

\footnotesize

\begin{equation}  
g_1=\frac{ \frac{1}{n} \sum_{i=1}^n(x_i-\bar{x})^3} { \left[\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\right] ^\frac{3}{2}}
\label{eq:SkewType1}
\end{equation}
\normalsize

\footnotesize

\begin{equation}  
G_1= \frac{\sqrt{n(n-1)}}{n-2} g_1
\label{eq:SkewType2}
\end{equation}
\normalsize

\footnotesize

\begin{equation}  
b_1= \left( \frac{n-1}{n} \right) ^\frac{3}{2} g_1
\label{eq:SkewType3}
\end{equation}
\normalsize

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/asymetrie-1} 

}

\caption{Asymétrie d'une distribution}\label{fig:asymetrie}
\end{figure}

\textbf{Pour évaluer l'applatissement d'une distribution}, on utilisera le coefficient d'aplatissement (\emph{kurtosis} en anglais). Là encore, il existe trois formules pour le calculer (eq. \eqref{eq:KurtType1}, \eqref{eq:KurtType2}, \eqref{eq:KurtType3}) qui renverront des valeurs très sembables pour de grands échantillons \citep{joanes1998comparing}. Cette mesure s'interprète comme suit (figure \ref{fig:asymetrie}) :

\begin{itemize}
\tightlist
\item
  quand la valeur du \emph{kurtosis} est négative, la \textbf{distribution est platikurtique}. La distribution est dite plate, c'est-à-dire que la valeur de l'écart-type est importante (comparativement à une distribution normale), signalant une grande dispersion des valeurs de part et d'autre la moyenne.
\item
  quand la valeur du \emph{kurtosis} est égale à 0, \textbf{la distribution est mésokurtique}, ce qui est typique d'une distribution normale.
\item
  quand la valeur du \emph{kurtosis} est positive, la \textbf{distribution est leptokurtique}, signalant que l'écart-type (la dispersion des valeurs) est plutôt faible. Autrement dit, la dispersion des valeurs autour de la moyenne est faible.
\end{itemize}

\footnotesize

\begin{equation}  
g_2=\frac{\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^4} {\left( \frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\right)^2}-3
\label{eq:KurtType1}
\end{equation}
\normalsize

\footnotesize

\begin{equation}  
G_2 = \frac{n-1}{(n-2)(n-3)} \{(n+1) g_2 + 6\}
\label{eq:KurtType2}
\end{equation}
\normalsize

\footnotesize

\begin{equation}  
b_2 = (g_2 + 3) (1 - 1/n)^2 - 3
\label{eq:KurtType3}
\end{equation}
\normalsize

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/kurtosis-1} 

}

\caption{Applatissement d'une distribution}\label{fig:kurtosis}
\end{figure}

\begin{bloc_attention}

Regardez attentivement les équations \eqref{eq:KurtType1}, \eqref{eq:KurtType2}, \eqref{eq:KurtType3}; vous remarquez que pour \(g_2\) et \(b_2\), il y a une soustraction de \(-3\) et une addition \(+6\) pour \(G_2\). On parle alors de \emph{kurtosis} normalisé (\emph{excess kurtosis} en anglais). Pour une distribution normale, il prendra la valeur de 0, comparativement à la valeur de 3 pour un \emph{kurtosis} non normalisé. Par conséquent, avant de calculer du \emph{kurtosis}, il convient de s'assurer que la fonction que vous utilisez implémente une méthode de calcul normalisée (donnant une valeur de 0 pour une distribution normale). Par exemple, la fonction \texttt{Kurt} du \emph{package} \textbf{DescTools} calcule les trois formules normalisées tandis que la fonction \texttt{kurtosis} du \emph{package} \textbf{moments} renvoie un \emph{kurtosis} non normalisé.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DescTools)}
\KeywordTok{library}\NormalTok{(moments)}
\CommentTok{#Générer une variable normalement distribuée avec 1000 observations}
\NormalTok{Normale <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1500}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\KeywordTok{round}\NormalTok{(DescTools}\OperatorTok{::}\KeywordTok{Kurt}\NormalTok{(Normale),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.097
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(moments}\OperatorTok{::}\KeywordTok{kurtosis}\NormalTok{(Normale),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.907
\end{verbatim}


\end{bloc_attention}

\hypertarget{vuxe9rifier-la-normalituxe9-avec-des-graphiques}{%
\paragraph{Vérifier la normalité avec des graphiques}\label{vuxe9rifier-la-normalituxe9-avec-des-graphiques}}

Les graphiques sont un excellent moyen de vérifier visuellement si une distribution est normale ou pas. Bien entendu, les histogrammes, que nous avons déjà largement utilisés, sont un incontournable; à titre de rappel, ils permettent de représenter la forme de la distribution des données (figure \ref{fig:CourbeNormale}). Un autre type de graphique intéressant est le \textbf{diagramme quantile-quantile} (\emph{Q-Q plot} en anglais) qui permet de comparer la distribution d'une variable avec une distribution gaussienne (normale). Trois éléments composent ce graphique tel qu'illustré à la figure \ref{fig:qqplot} :

\begin{itemize}
\tightlist
\item
  les points, représentant les observations de la variable
\item
  la distribution gaussienne (normale), représentée par une ligne
\item
  l'intervalle de confiance à 5\% de la distribution normale (en orange sur la figure).
\end{itemize}

Quand la variable est normale distribuée, les points seront situés le long de la ligne. Plus les points localisés en dehors de l'intervalle de confiance (bande orange) seront nombreux, plus la variable sera alors anormalement distribuée.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/CourbeNormale-1} 

}

\caption{Distributions et courbe normale}\label{fig:CourbeNormale}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/qqplot-1} 

}

\caption{Diagrammes quantile-quantile}\label{fig:qqplot}
\end{figure}

\hypertarget{sect025413}{%
\paragraph{Vérifier la normalité avec des tests de normalité}\label{sect025413}}

Cinq principaux tests d'hypothèse permettent de vérifier la normalité d'une variable : les tests de \textbf{Kolmogorov-Smirnov} (KS), \textbf{Lilliefors} (LF), \textbf{Shapiro-Wilk} (SW), \textbf{Anderson-Darling}, et de \textbf{Jarque-Bera} (JB); sachez toutefois qu'il y en a d'autres non discutés ici (tests de D'Agostino--Pearson, Cramer--von Mises, de Ryan-Joiner, Shapiro--Francia, etc.). Pour les formules et une description détaillée de ces tests, vous pouvez consulter Razali et al.~\citeyearpar{razali2011power} ou Yap et Sim \citeyearpar{yap2011comparisons}. \textbf{Quel test choisir ?} Plusieurs auteurs ont comparé ces différents tests à partir de plusieurs échantillons, et ce, en faisant varier la forme de la distribution et le nombre d'observations \citep{razali2011power, yap2011comparisons}. Selon Razali et al.~\citeyearpar{razali2011power}, le meilleur test semble être celui de Shapiro-Wilk, puis ceux de Anderson-Darling, Lilliefors et Kolmogorov-Smirnov. Yap et Sim \citeyearpar{yap2011comparisons} concluent aussi que le Shapiro-Wilk semble être le plus performant.

Quoi qu'il en soit, ces cinq tests postulent que la variable suit une distribution gaussienne (hypothèse nulle, h0). Cela signifie que si la valeur de P associée à la valeur de chacun des tests est supérieure au seuil alpha choisi (habituellement \(\alpha=0,05\)), la distribution est normale. À l'inverse, si \(P<0,05\), on choisit l'hypothèse alternative (h1), c'est-à-dire que la distribution est anormale.

\begin{table}

\caption{\label{tab:testnormalites}Les différents tests d'hypothèse pour la normalité}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{l>{\raggedright\arraybackslash}p{6cm}>{\raggedright\arraybackslash}p{6cm}}
\toprule
Test & Propriétés et interprétation & Fonction R\\
\midrule
Kolmogorov-Smirnov & Plus sa valeur est proche de zéro, plus la distribution est normale.  L'avantage de ce test est qu'il peut être utilisé pour vérifier si une variable suit la distribution de n'importe quelle loi (autre que la loi normale). & ks.test du \textit{package} \textbf{stats}\\
Lilliefors & Ce test est une adaptation du test de Kolmogorov-Smirnov. Plus sa valeur est proche de zéro, plus la distribution est normale. & lillie.test du \textit{package} \textbf{nortest}\\
Shapiro-Wilk & Si la valeur de la statistique de Shapiro-Wilk est proche de 1, alors la distribution est normale; et anormale quand elle est inférieure à 1. & shapiro.test du \textit{package} \textbf{stats}\\
Anderson-Darling & Ce test est une modification du test de Cramer-von Mises (CVM). Il peut être aussi utilisé pour tester d'autres distributions (uniforme, log-normale, exponentielle, Weibull, distribution de pareto généralisée, logistique, etc.). & ad.test du \textit{package} \textbf{stats}\\
Jarque-Bera & Basé sur un test du type multiplicateur de Lagrange, il utilise dans son calcul les valeurs du \textit{Skewness} et du \textit{Kurtosis}. Plus sa valeur s'approche de 0, plus la distribution est normale. Ce test est surtout utilisé pour vérifier si les résidus d'un modèle de régression linéaire sont normalement distribués, nous y reviendrons dans le chapitre sur la régression multiple. Il s'écrit $JB=\frac{1}{6} \left({g_1}^2+\frac{{g_1}^2}{4} \right)$ avec $g_1$ et $g_2$ qui sont respectivement les valeurs du \textit{skewness} et du \textit{kurtosis} de la variable (voir plus haut les équations \ref{eq:SkewType1} et \ref{eq:KurtType1}). & JarqueBeraTest du \textit{package} \textbf{DescTools}\\
\bottomrule
\end{tabular}
\end{table}

Dans le tableau ci-dessous sont reportées les valeurs des différents tests pour les cinq types de distribution générées à la figure \ref{fig:CourbeNormale}. Sans surprise, pour l'ensemble des tests, la valeur de \emph{P} est inférieur à 0,05 pour la distribution normale.

\begin{table}

\caption{\label{tab:calcultestnormalites}Calculs des tests de normalité pour différentes distributions}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{llllll}
\toprule
 & Normale & Asymétrie négative & Asymétrie positive & Leptokurtique & Platikurtique\\
\midrule
Skewness & 0.012 & 1.35 & -1.331 & -0.571 & 0.035\\
Kurtosis & -0.074 & 2.327 & 2.988 & 4.424 & -0.966\\
Kolmogorov-Smirnov (KS) & 0.018 & 0.093 & 0.086 & 0.074 & 0.047\\
Lilliefors (LF) & 0.018 & 0.093 & 0.086 & 0.074 & 0.047\\
Shapiro-Wilk (SW) & 0.999 & 0.9 & 0.91 & 0.935 & 0.972\\
\addlinespace
Anderson-Darling (AD) & 0.131 & 11.086 & 7.854 & 6.623 & 2.633\\
Jarque-Bera (JB) & 0.036 & 348.162 & 347.491 & 850.708 & 13.97\\
KS (valeur p) & 0.998 & 0 & 0.001 & 0.009 & 0.226\\
LF (valeur p) & 0.966 & 0 & 0 & 0 & 0.011\\
SW (valeur p) & 0.967 & 0 & 0 & 0 & 0\\
\addlinespace
AD (valeur p) & 0.982 & 0 & 0 & 0 & 0\\
JB (valeur p) & 0.982 & 0 & 0 & 0 & 0.001\\
\bottomrule
\end{tabular}
\end{table}

\begin{bloc_attention}
\textbf{Attention} ! La plupart des auteurs s'entendent sur le fait que ces tests sont très restrictifs : plus la taille de votre échantillon (\(n\)) est importante, plus les tests risquent de vous signaler que vos distributions sont anormales (à la lecture des valeurs de P).

Certains conseillent même de ne pas les utiliser quand \(n>200\) et de vous fier uniquement aux graphiques (histogramme et diagramme Q-Q) !

\end{bloc_attention}

\begin{bloc_astuce}

Bref, vérifier la normalité d'une variable n'est pas une tâche si simple. De nouveau, nous vous conseillons vivement de :

\begin{itemize}
\tightlist
\item
  construire les graphiques pour analyser visuellement la forme de la distribution (histogramme avec courbe normale et diagramme Q-Q)
\item
  calculer le \emph{skewness} et le \emph{kurtosis},
\item
  calculer plusieurs tests (minimalement Shapiro-Wilk et Kolmogorov-Smirnov)
\item
  accorder une importance particulière aux graphiques lorsque vous traitez des grands échantillons (\(n>200\)).
\end{itemize}


\end{bloc_astuce}

\hypertarget{sect02adjdistrib}{%
\subsubsection{Vérifier d'autres formes de distributions}\label{sect02adjdistrib}}

Comme nous l'avons vu, la distribution normale n'est que l'une des multiples distributions existantes. Dans de nombreuses situations, elle ne sera pas adaptée pour décrire vos variables. La démarche à adopter pour trouver une distribution adaptée est la suivante :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Définissez la nature de votre variable, identifier si elle est discrète ou continue et l'intervalle dans lequel elle est définie. Une variable dont les valeurs sont positives ou négatives ne pourra pas être décrite avec une distribution Gamma par exemple (à moins de la décaler).
\item
  Explorez votre variable, affichez son histogramme et son graphique de densité pour avoir une vue générale de sa morphologie.
\item
  Présélectionnez un ensemble de distributions candidates compte tenu des observations précédentes. Vous pouvez également vous reporter à la littérature existante sur votre sujet d'étude pour inclure d'autres distributions. Soyez flexible ! Une variable strictement positive pourrait tout de même avoir une forme normale. De même, une variable décrivant des comptages suffisamment grands pourrait être mieux décrite par une distribution normale qu'une distribution de poisson.
\item
  Tentez d'ajuster chacune des distributions retenues à vos données et comparez les qualités d'ajustements pour retenir la plus adaptée.
\end{enumerate}

Pour ajuster une distribution à un jeu de données, il faut trouver les valeurs des paramètres de cette distribution qui lui permettront d'adopter une forme la plus proche possible des données. On appelle cette opération \textbf{ajuster un modèle}, puisque la distribution théorique est utilisée pour modéliser les données. L'ajustement des paramètres est un problème d'optimisation que plusieurs algorithmes sont capables de résoudre (\emph{gradient descent}, \emph{Newton-Raphson method}, \emph{Fisher scoring}, etc.). Dans R, le \emph{package} \textbf{fitdistrplus} permet d'ajuster pratiquement n'importe quelle distribution à des données en offrant plusieurs stratégies d'optimisation grâce à la fonction \texttt{fitdist}. Il suffit de disposer d'une fonction représentant la distribution de densité ou de masse de la distribution en question, généralement noté \texttt{dnomdeladistribution} (\texttt{dnorm}, \texttt{dgamma}, \texttt{dpoisson}, etc.) dans R. Notez que certains \emph{packages} comme \textbf{VGAM} ou \textbf{gamlss.dist} ajoutent un grand nombre de fonctions de densité et de masse à celles déjà disponibles de base dans R.

Pour comparer l'ajustement de plusieurs distributions théoriques à des données, trois approches doivent être combinées :

\begin{itemize}
\tightlist
\item
  Observer graphiquement l'ajustement de la courbe théorique à l'histogramme des données. Cela permet d'éliminer au premier coup d'œil les distributions qui ne correspondent pas.
\item
  Comparer les \emph{loglikelihood}. Le \emph{loglikelihood} est un score d'ajustement des distributions aux données. Pour faire simple, plus le \emph{loglikelihood} est grand, plus la distribution théorique est proche des données. Référez-vous à l'encadré suivant pour une description plus en profondeur du \emph{loglikelihood}.
\item
  Utiliser le test de Kolmogorov-Smirnov pour déterminer si une distribution particulière est mieux ajustée pour les données.
\end{itemize}

\begin{bloc_aller_loin}
\textbf{Qu'est-ce-que le loglikelihood} ?

Le \emph{loglikelihood} est une mesure de l'ajustement d'un modèle à des données. Il est utilisé à peu près partout en statistique. Comprendre sa signification est donc un exercice important pour développer une meilleure intuition du fonctionnement général de nombreuses méthodes. Si les concepts de fonction de densité et de fonction de masse vous semblent encore flous, reportez-vous à la section \ref{sect024} sur les distributions dans un premier temps.

Admettons que nous disposons d'une variable continue \emph{v} que nous avons tenté de modéliser avec une distribution \emph{d} (il peut s'agir de n'importe quelle distribution). \emph{d} a une fonction de densité avec laquelle il est possible de calculer pour chacune des valeurs de \emph{v} sa probabilité d'être observée selon le modèle \emph{d}.

Prenons un exemple concret dans R. Admettons que nous avons une variable comprenant 10 valeurs (oui, c'est un petit échantillon, mais c'est pour faire un exemple simple).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{7}\NormalTok{)}
\NormalTok{moyenne <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(v)}
\NormalTok{ecart_type <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(v)}
\end{Highlighting}
\end{Shaded}

En calculant sa moyenne et son écart type, nous obtenons les paramètres d'une distribution normale que nous pouvons utiliser pour représenter les données observées. En utilisant la fonction \texttt{dnorm} (la fonction de densité de la distribution normale), nous pouvons calculer la probabilité d'observer chacune des valeurs de \emph{v} selon cette distribution normale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probas <-}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(v, moyenne, ecart_type)}
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{valeur =}\NormalTok{ v,}
                 \DataTypeTok{proba =}\NormalTok{ probas)}
\KeywordTok{print}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    valeur      proba
## 1       5 0.11203710
## 2       8 0.19624888
## 3       7 0.22228296
## 4       8 0.19624888
## 5      10 0.06009897
## 6       4 0.04985613
## 7       7 0.22228296
## 8       6 0.18439864
## 9       9 0.12689976
## 10      7 0.22228296
\end{verbatim}

On observe ainsi que les valeurs 7 et 8 sont très probables selon le modèle alors que la valeur 10 est très improbable.

Le \emph{likelihood} est simplement le produit de toutes ces probabilités. Il s'agit donc de \textbf{la probabilité conjointe} d'avoir observé toutes les valeurs de \emph{v} \textbf{sous l'hypothèse} que \emph{d} est la distribution produisant ces valeurs. Si \emph{d} décrit efficacement \emph{v}, alors le \emph{likelihood} est plus grand que si \emph{d} ne décrit pas efficacement \emph{v}. Il s'agit d'une forme de raisonnement par l'absurde : après avoir observé \emph{v}, on calcule la probabilité d'avoir observé \emph{v} (\emph{likelihood}) si notre modèle \emph{d} était vrai. Si cette probabilité est très basse, alors c'est que notre modèle est mauvais puisqu'on a bien observé \emph{v}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{likelihood_norm <-}\StringTok{ }\KeywordTok{prod}\NormalTok{(probas)}
\KeywordTok{print}\NormalTok{(likelihood_norm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.322759e-09
\end{verbatim}

Cependant, multiplier un grand nombre de valeurs inférieures à zéro tend à produire des chiffres infiniment petits et donc à complexifier grandement le calcul. On préfère donc utiliser le \emph{loglikelihood}. L'idée étant transformer les probabilités obtenues avec la fonction \emph{log} puis d'additionner leurs résultats, puisque \(log(xy) = log(x)+log(y)\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loglikelihood_norm <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{log}\NormalTok{(probas))}
\KeywordTok{print}\NormalTok{(loglikelihood_norm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -19.52247
\end{verbatim}

Comparons ce \emph{loglikelihood} a celui d'un second modèle dans lequel nous utilisons toujours la distribution normale, mais avec une moyenne différente (faussée en rajoutant +3) :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probas2 <-}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(v, moyenne}\OperatorTok{+}\DecValTok{3}\NormalTok{, ecart_type)}
\NormalTok{loglikelihood_norm2 <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{log}\NormalTok{(probas2))}
\KeywordTok{print}\NormalTok{(loglikelihood_norm2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -33.53631
\end{verbatim}

Ce second \emph{loglikehood} est plus faible, indiquant clairement que le premier modèle est plus adapté aux données.

\end{bloc_aller_loin}

Passons à la pratique avec deux exemples.

\hypertarget{temps-de-retard-des-bus-de-la-ville-de-toronto}{%
\paragraph{Temps de retard des bus de la ville de Toronto}\label{temps-de-retard-des-bus-de-la-ville-de-toronto}}

Analysons les temps de retard pris par les bus de la ville de Toronto lorsqu'un évènement perturbe la circulation. Ce jeu de données est disponible sur le site de l'\href{https://open.toronto.ca/catalogue/?search=bus\%20delay\&sort=score\%20desc}{Open Data} de la ville de Toronto. Compte tenu de la grande quantité d'observations, nous avons fait le choix de nous concentrer sur les évènements ayant eu lieu durant le mois de janvier 2019. Puisque la variable étudiée est une durée exprimée en minutes, elle est strictement positive (supérieure à 0), car un bus avec zéro minute de retard est à l'heure. Nous considérons également qu'un bus ayant plus de 150 minutes de retard (2h30) n'est tout simplement pas passé (personne ne risque d'attendre 2h30 pour prendre son bus). Commençons par charger les données et observer leur distribution empirique.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# charger le jeu de données}
\NormalTok{data_trt_bus <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'data/univariee/bus-delay-2019_janv.csv'}\NormalTok{, }\DataTypeTok{sep =}\StringTok{';'}\NormalTok{)}
\CommentTok{# retirer les observations aberrantes}
\NormalTok{data_trt_bus <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(data_trt_bus, data_trt_bus}\OperatorTok{$}\NormalTok{Min.Delay }\OperatorTok{>}\StringTok{ }\DecValTok{0} \OperatorTok{&}
\StringTok{                         }\NormalTok{data_trt_bus}\OperatorTok{$}\NormalTok{Min.Delay }\OperatorTok{<}\StringTok{ }\DecValTok{150}\NormalTok{)}
\CommentTok{# représenter la distribution empirique du jeu de données}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ data_trt_bus) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Min.Delay, }\DataTypeTok{y =}\NormalTok{ ..density..), }\DataTypeTok{bins =} \DecValTok{40}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Min.Delay), }\DataTypeTok{color =} \StringTok{'blue'}\NormalTok{, }\DataTypeTok{bw =} \DecValTok{2}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.8}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{'temps de retard (min)'}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{''}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/figbustrt-1} 

}

\caption{Distribution empirique des temps de retard des bus à Toronto en janvier 2019}\label{fig:figbustrt}
\end{figure}

Compte tenu de la forme de la distribution empirique et de sa nature, quatre distributions sont envisageables :

\begin{itemize}
\tightlist
\item
  La distribution Gamma, strictement positive et asymétrique, elle est aussi une généralisation de la distribution exponentielle utilisée pour modéliser des temps d'attente. Pour des raisons similaires, on peut aussi retenir la distribution de Weibull et la distribution log-normale. Nous écartons ici la distribution skew-normale puisque le jeu de données n'a clairement pas une forme normale au départ.
\item
  La distribution de Pareto, strictement positive et permettant de représenter ici le fait que la plupart des retards durent moins de 10 minutes, mais que quelques retards sont également beaucoup plus longs.
\end{itemize}

Commençons par ajuster les quatre distributions avec la fonction \texttt{fitdist} du \emph{package} \textbf{fitdistrplus} et représentons-les graphiquements pour éliminer les moins bons candidats. Nous utilisons également le \emph{package} \textbf{actuar} pour la fonction de densité de Pareto (\texttt{dpareto}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(fitdistrplus)}
\KeywordTok{library}\NormalTok{(actuar)}
\KeywordTok{library}\NormalTok{(ggpubr)}
\CommentTok{# ajustement des modèles}
\NormalTok{model_gamma <-}\StringTok{ }\KeywordTok{fitdist}\NormalTok{(data_trt_bus}\OperatorTok{$}\NormalTok{Min.Delay, }\DataTypeTok{distr =} \StringTok{"gamma"}\NormalTok{)}
\NormalTok{model_weibull <-}\StringTok{ }\KeywordTok{fitdist}\NormalTok{(data_trt_bus}\OperatorTok{$}\NormalTok{Min.Delay, }\DataTypeTok{distr =} \StringTok{"weibull"}\NormalTok{)}
\NormalTok{model_lognorm <-}\StringTok{ }\KeywordTok{fitdist}\NormalTok{(data_trt_bus}\OperatorTok{$}\NormalTok{Min.Delay, }\DataTypeTok{distr =} \StringTok{"lnorm"}\NormalTok{)}
\NormalTok{model_pareto <-}\StringTok{ }\KeywordTok{fitdist}\NormalTok{(data_trt_bus}\OperatorTok{$}\NormalTok{Min.Delay, }\DataTypeTok{distr =} \StringTok{"pareto"}\NormalTok{, }
                        \DataTypeTok{start =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{shape =} \DecValTok{1}\NormalTok{, }\DataTypeTok{scale =} \DecValTok{1}\NormalTok{),}
                        \DataTypeTok{method =} \StringTok{"mse"}\NormalTok{) }\CommentTok{# différentes méthodes d'optimisations}
\CommentTok{# réalisation des graphiques}
\NormalTok{plot1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ data_trt_bus) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Min.Delay, }\DataTypeTok{y =}\NormalTok{ ..density..), }\DataTypeTok{bins =} \DecValTok{40}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dgamma, }\DataTypeTok{color =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.8}\NormalTok{, }
                \DataTypeTok{args =} \KeywordTok{as.list}\NormalTok{(model_gamma}\OperatorTok{$}\NormalTok{estimate))}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{'temps de retard (min)'}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{''}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"modèle Gamma"}\NormalTok{)}
\NormalTok{plot2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ data_trt_bus) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Min.Delay, }\DataTypeTok{y =}\NormalTok{ ..density..), }\DataTypeTok{bins =} \DecValTok{40}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dweibull, }\DataTypeTok{color =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.8}\NormalTok{, }
                \DataTypeTok{args =} \KeywordTok{as.list}\NormalTok{(model_weibull}\OperatorTok{$}\NormalTok{estimate))}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{'temps de retard (min)'}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{''}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"modèle Weibull"}\NormalTok{)}
\NormalTok{plot3 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ data_trt_bus) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Min.Delay, }\DataTypeTok{y =}\NormalTok{ ..density..), }\DataTypeTok{bins =} \DecValTok{40}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dlnorm, }\DataTypeTok{color =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.8}\NormalTok{, }
                \DataTypeTok{args =} \KeywordTok{as.list}\NormalTok{(model_lognorm}\OperatorTok{$}\NormalTok{estimate))}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{'temps de retard (min)'}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{''}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"modèle log-normal"}\NormalTok{)}
\NormalTok{plot4 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ data_trt_bus) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Min.Delay, }\DataTypeTok{y =}\NormalTok{ ..density..), }\DataTypeTok{bins =} \DecValTok{40}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dpareto, }\DataTypeTok{color =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{size =} \FloatTok{0.8}\NormalTok{, }
                \DataTypeTok{args =} \KeywordTok{as.list}\NormalTok{(model_pareto}\OperatorTok{$}\NormalTok{estimate))}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{'temps de retard (min)'}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{''}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"Modèle Pareto"}\NormalTok{)}
\KeywordTok{ggarrange}\NormalTok{(}\DataTypeTok{plotlist =} \KeywordTok{list}\NormalTok{(plot1, plot2, plot3, plot4),}
          \DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/figbustrt2-1} 

}

\caption{Comparaison des distributions ajustées aux données de retard des bus}\label{fig:figbustrt2}
\end{figure}

Visuellement, on constate que la distribution de Pareto est un mauvais choix. Pour les trois autres distributions, la comparaison des \emph{loglikelihood} s'impose.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{model =} \KeywordTok{c}\NormalTok{(}\StringTok{"Gamma"}\NormalTok{,}\StringTok{"Weibull"}\NormalTok{,}
                           \StringTok{"log-normal"}\NormalTok{), }
                 \DataTypeTok{loglikelihood =} \KeywordTok{c}\NormalTok{(model_gamma}\OperatorTok{$}\NormalTok{loglik, }
\NormalTok{                 model_weibull}\OperatorTok{$}\NormalTok{loglik,}
\NormalTok{                 model_lognorm}\OperatorTok{$}\NormalTok{loglik))}
\KeywordTok{show_table}\NormalTok{(df, }
      \DataTypeTok{col.names =} \KeywordTok{c}\NormalTok{(}\StringTok{"Distributon"}\NormalTok{,}\StringTok{"LogLikelihood"}\NormalTok{),}
      \DataTypeTok{caption =} \StringTok{'Comparaison des LogLikekelihood des trois distributions'}\NormalTok{,}
\NormalTok{           )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:tabledistribs}Comparaison des LogLikekelihood des trois distributions}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lr}
\toprule
Distributon & LogLikelihood\\
\midrule
Gamma & -23 062,56\\
Weibull & -23 195,54\\
log-normal & -23 375,74\\
\bottomrule
\end{tabular}
\end{table}

Le plus grand \emph{logLikelihood} est obtenu par la distribution de Gamma qui s'ajuste donc le mieux à nos données. Pour finir, nous pouvons tester formellement avec le test de Kolmogorov-Smirnov si nos données proviennent bien de cette distribution de Gamma.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params <-}\StringTok{ }\KeywordTok{as.list}\NormalTok{(model_gamma}\OperatorTok{$}\NormalTok{estimate)}
\KeywordTok{ks.test}\NormalTok{(data_trt_bus}\OperatorTok{$}\NormalTok{Min.Delay,}
        \DataTypeTok{y =}\NormalTok{ pgamma, }\DataTypeTok{shape =}\NormalTok{ params}\OperatorTok{$}\NormalTok{shape, }\DataTypeTok{rate =}\NormalTok{ params}\OperatorTok{$}\NormalTok{rate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  data_trt_bus$Min.Delay
## D = 0.099912, p-value < 2.2e-16
## alternative hypothesis: two-sided
\end{verbatim}

La valeur de \emph{p} est inférieure à 0,05, on ne peut donc pas accepter l'hypothèse que notre jeu de données suit effectivement un loi de Gamma. Considérant le nombre d'observations et le fait que de nombreux temps d'attente sont identiques (ce à quoi le test est très sensible), ce résultat n'est pas surprenant. La distribution de Gamma reste cependant la distribution qui représente le mieux nos données. Nous pouvons estimer grâce à cette distribution la probabilité qu'un bus ait un retard de plus de 10 minutes de la façon suivante :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pgamma}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DataTypeTok{shape =}\NormalTok{ params}\OperatorTok{$}\NormalTok{shape, }\DataTypeTok{rate =}\NormalTok{ params}\OperatorTok{$}\NormalTok{rate, }\DataTypeTok{lower.tail =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5409424
\end{verbatim}

ce qui correspond à 54\% de chance.

Pour moins de 10 minutes :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pgamma}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DataTypeTok{shape =}\NormalTok{ params}\OperatorTok{$}\NormalTok{shape, }\DataTypeTok{rate =}\NormalTok{ params}\OperatorTok{$}\NormalTok{rate, }\DataTypeTok{lower.tail =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4590576
\end{verbatim}

soit 46\%.

Uun dernier exemple avec la probabilité qu'un retard dépasse 45 minutes :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pgamma}\NormalTok{(}\DecValTok{45}\NormalTok{, }\DataTypeTok{shape =}\NormalTok{ params}\OperatorTok{$}\NormalTok{shape, }\DataTypeTok{rate =}\NormalTok{ params}\OperatorTok{$}\NormalTok{rate, }\DataTypeTok{lower.tail =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01348194
\end{verbatim}

Soit seulement 1,3\%.

Par conséquent, si un matin à Toronto votre bus a plus de 45 minutes de retard, bravo vous êtes tombé sur une des très rares occasions où un tel retard se produit

\hypertarget{les-accidents-de-vuxe9lo-uxe0-montruxe9al}{%
\paragraph{Les accidents de vélo à Montréal}\label{les-accidents-de-vuxe9lo-uxe0-montruxe9al}}

Le second jeu de données représente le nombre d'accidents de la route impliquant un vélo sur les intersections dans les quartiers centraux de Montréal. Le jeu de données complet est disponible sur le site des \href{http://donnees.ville.montreal.qc.ca/dataset/collisions-routieres}{données ouvertes} de la ville de Montréal. Puisque ces données correspondent à des comptages, la première distribution à envisager est la distribution de poisson. Cependant, puisque nous aurons également un grand nombre d'intersections sans accident, il serait judicieux de tester la distribution de poisson avec excès de zéro.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{# charger le jeu de données}
\NormalTok{data_accidents <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'data/univariee/accidents_mtl.csv'}\NormalTok{, }\DataTypeTok{sep =}\StringTok{','}\NormalTok{)}
\NormalTok{counts <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{table}\NormalTok{(data_accidents}\OperatorTok{$}\NormalTok{nb_accident))}
\KeywordTok{names}\NormalTok{(counts) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"nb_accident"}\NormalTok{,}\StringTok{'frequence'}\NormalTok{)}
\NormalTok{counts}\OperatorTok{$}\NormalTok{nb_accident <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(counts}\OperatorTok{$}\NormalTok{nb_accident))}
\NormalTok{counts}\OperatorTok{$}\NormalTok{prop <-}\StringTok{ }\NormalTok{counts}\OperatorTok{$}\NormalTok{frequence }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(counts}\OperatorTok{$}\NormalTok{frequence)}
\CommentTok{# représenter la distribution empirique du jeu de donnée}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ counts) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{nb_accident, }\DataTypeTok{weight =}\NormalTok{ frequence), }\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"nombre d'accidents"}\NormalTok{,}
      \DataTypeTok{y =} \StringTok{'fréquence'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/figaccmtl-1} 

}

\caption{Distribution empirique du nombre d'accidents par intersection impliquant un cycliste à Montréal en 2017 dans les quartiers centraux}\label{fig:figaccmtl}
\end{figure}

Nous avons effectivement de nombreux zéros ici, essayons d'ajuster nos deux distributions à ce jeu de données. Dans le graphique suivant, les barres grises représentent la distribution empirique du jeu de données et les barres rouges les distributions théoriques ajustées. Nous utilisons ici le \emph{package} \textbf{gamlss.dist} pour avoir la fonction de masse d'une distribution de poisson avec excès de zéros.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gamlss.dist)}
\CommentTok{#ajuster le modèle de poisson}
\NormalTok{model_poisson <-}\StringTok{ }\KeywordTok{fitdist}\NormalTok{(data_accidents}\OperatorTok{$}\NormalTok{nb_accident, }\DataTypeTok{distr =} \StringTok{"pois"}\NormalTok{)}
\CommentTok{#ajuster le modèle de poisson avec excès de zéros}
\NormalTok{model_poissonzi <-}\StringTok{ }\KeywordTok{fitdist}\NormalTok{(data_accidents}\OperatorTok{$}\NormalTok{nb_accident, }\StringTok{"ZIP"}\NormalTok{,}
    \DataTypeTok{start =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mu =} \DecValTok{4}\NormalTok{, }\DataTypeTok{sigma =} \FloatTok{0.15}\NormalTok{), }\CommentTok{# valeurs pour faciliter la convergence}
    \DataTypeTok{optim.method =} \StringTok{"L-BFGS-B"}\NormalTok{, }\CommentTok{# méthode d'optimisation recommandée dans la doc}
    \DataTypeTok{lower =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.00001}\NormalTok{, }\FloatTok{0.00001}\NormalTok{),}\CommentTok{# valeurs minimales des deux paramètres}
    \DataTypeTok{upper =} \KeywordTok{c}\NormalTok{(}\OtherTok{Inf}\NormalTok{, }\DecValTok{1}\NormalTok{)}\CommentTok{# valeurs maximales des deux paramètres}
\NormalTok{    )}
\NormalTok{dfpoisson <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{10}\NormalTok{),}
                        \DataTypeTok{y=}\KeywordTok{dpois}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{10}\NormalTok{, model_poisson}\OperatorTok{$}\NormalTok{estimate)}
\NormalTok{                        )}
\NormalTok{plot1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{nb_accident, }\DataTypeTok{weight =}\NormalTok{ prop), }\DataTypeTok{width =} \FloatTok{0.6}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ counts)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{weight =}\NormalTok{ y), }\DataTypeTok{width =} \FloatTok{0.15}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ dfpoisson, }\DataTypeTok{fill =} \StringTok{"red"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{,}\DecValTok{7}\NormalTok{), }\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{7}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{subtitle =} \StringTok{"modèle poisson"}\NormalTok{,}
       \DataTypeTok{x =} \StringTok{"nombre d'accidents"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{""}\NormalTok{)}
\NormalTok{dfpoissonzi <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{10}\NormalTok{),}
                        \DataTypeTok{y=}\KeywordTok{dZIP}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{10}\NormalTok{, model_poissonzi}\OperatorTok{$}\NormalTok{estimate[[}\DecValTok{1}\NormalTok{]],}
\NormalTok{                               model_poissonzi}\OperatorTok{$}\NormalTok{estimate[[}\DecValTok{2}\NormalTok{]])}
\NormalTok{                        )}
\NormalTok{plot2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{nb_accident, }\DataTypeTok{weight =}\NormalTok{ prop), }\DataTypeTok{width =} \FloatTok{0.6}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ counts)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{weight =}\NormalTok{ y), }\DataTypeTok{width =} \FloatTok{0.15}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ dfpoissonzi, }\DataTypeTok{fill =} \StringTok{"red"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.5}\NormalTok{,}\DecValTok{7}\NormalTok{), }\DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{7}\NormalTok{))}\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{subtitle =} \StringTok{"modèle poisson avec excès de zéro"}\NormalTok{,}
       \DataTypeTok{x =} \StringTok{"nombre d'accident"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{""}\NormalTok{)}
\KeywordTok{ggarrange}\NormalTok{(}\DataTypeTok{plotlist =} \KeywordTok{list}\NormalTok{(plot1,plot2), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/figaccmtldist-1} 

}

\caption{Ajustement des distributions de poisson et poisson avec excès de zéros}\label{fig:figaccmtldist}
\end{figure}

Visuellement, le modèle avec excès de zéro semble s'imposer. Nous pouvons vérifier cette impression avec la comparaison des \emph{loglikelihood}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(model_poisson}\OperatorTok{$}\NormalTok{loglik)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -989.83
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(model_poissonzi}\OperatorTok{$}\NormalTok{loglik)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -931.8778
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#afficher les paramètres ajustés}
\NormalTok{model_poissonzi}\OperatorTok{$}\NormalTok{estimate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        mu     sigma 
## 0.6690301 0.7022605
\end{verbatim}

Nous avons donc la confirmation que le modèle de poisson avec excès de zéros est mieux ajusté. Nous apprenons donc que 70\% (sigma = 0,70) des intersections sont en fait exclues du phénomène étudié (probablement parce que très peu de cyclistes les utilisent ou parce qu'elles sont très peu accidentogènes) et que pour les autres, le taux d'accidents par année en 2017 était de 0,67 (mu = 0,669, mu signifiant \(\lambda\) pour le \emph{package} \textbf{gamlss}). À nouveau, nous pouvons effectuer un test formel avec le fonction \texttt{ks.test}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params <-}\StringTok{ }\KeywordTok{as.list}\NormalTok{(model_poissonzi}\OperatorTok{$}\NormalTok{estimate)}
\KeywordTok{ks.test}\NormalTok{(data_accidents}\OperatorTok{$}\NormalTok{nb_accident,}
        \DataTypeTok{y =}\NormalTok{ pZIP, }\DataTypeTok{mu =}\NormalTok{ params}\OperatorTok{$}\NormalTok{mu, }\DataTypeTok{sigma =}\NormalTok{ params}\OperatorTok{$}\NormalTok{sigma)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  data_accidents$nb_accident
## D = 0.85476, p-value < 2.2e-16
## alternative hypothesis: two-sided
\end{verbatim}

Encore une fois, on doit rejeter l'hypothèse selon laquelle le test suit une distribution de poisson avec excès de zéros. Ces deux exemples montrent à quel point ce test est restrictif.

\hypertarget{sect0255}{%
\subsection{La transformation des variables}\label{sect0255}}

\hypertarget{sect02551}{%
\subsubsection{Les transformations visant à atteindre la normalité}\label{sect02551}}

Comme énoncé au début de cette section, plusieurs méthodes statistiques nécessitent que la variable quantitative soit normalement distribuée. C'est notamment le cas de l'analyse de variance et des tests \emph{t} (abordés dans les chapitres suivants) qui fourniront des résultats plus robustes lorsque la variable est normalement distribuée. Plusieurs transformations sont possibles, les plus courantes étant la racine carrée, le logarithme et l'inverse de la variable. Selon plusieurs auteurs (notamment, Tabacknick et \emph{et al.} \citeyearpar[p.~89]{tabachnick2007}), en fonction du type (positive ou négative) et du degré d'asymétrie, les transformations suivantes sont possibles afin d'améliorer la normalité de la variable :

\begin{itemize}
\tightlist
\item
  Asymétrie positive modérée : la racine carrée de la variable \emph{X} avec la fonction \texttt{sqrt(df\$x)}.
\item
  Asymétrie positive importante : le logarithme de la variable avec \texttt{log10(df\$x)}
\item
  Asymétrie positive sévère : l'inverse de la variable avec \texttt{1/(df\$x)}
\end{itemize}

\begin{bloc_astuce}

Attention, pour une valeur égale ou inférieure à 0, on ne peut pas calculer une racine carrée ou un logarithme. Par conséquent, il convient de décaler simplement la distribution vers la droite afin de s'assurer qu'il n'y ait plus de valeurs négative ou égale à 0 :

\begin{itemize}
\tightlist
\item
  \texttt{sqrt(df\$x\ -\ min(df\$x+1))}avec pour une asymétrie positive avec des valeurs négatives ou égales à 0
\item
  \texttt{log(df\$x\ -\ min(df\$x+1))}pour une asymétrie positive avec des valeurs négatives ou égales à 0
\end{itemize}

Par exemple, si la valeur minimale de la variable est égale à -10, la valeur minimale de variable décalée sera ainsi de 11.

\begin{itemize}
\tightlist
\item
  Asymétrie négative modérée : \texttt{sqrt(max(df\$x+1)\ -\ df\$x)}.
\item
  Asymétrie négative importante :\texttt{log(max(df\$x+1)\ -\ df\$x)}
\item
  Asymétrie négative sévère : \texttt{1/(max(df\$x+1)\ -\ df\$x)}
\end{itemize}


\end{bloc_astuce}

\begin{bloc_attention}

\textbf{Transformation des variables pour atteindre la normalité : ce n'est pas toujours la panacée !}

La transformation des données fait et fera encore longtemps débat à la fois parmi les statisticiens, les débutants et utilisateurs avancés des méthodes quantitatives. Field et al.~\citeyearpar[pp.~193]{field2012discovering} résument le tout avec humour : « To transform or not transform, that is the question ».

\textbf{Avantages de la transformation}

\begin{itemize}
\tightlist
\item
  L'obtention de \emph{résultats plus robustes}.
\item
  Dans une régression linéaire multiple, la transformation de la variable dépendante peut \emph{remédier au non-respect des hypothèses de base liées à la régression} (linéarité et homoscédasticité des erreurs, absence des valeurs aberrantes, etc.).
\end{itemize}

\textbf{Inconvénients de la transformation}

\begin{itemize}
\item
  \emph{Une variable transformée est plus difficile à interpréter} puisque cela change l'unité de mesure de la variable. Prenons un exemple concret : vous souhaitez comparer les moyennes de revenu de deux groupes \emph{A} et \emph{B}. Vous obtenez une différence de 15000\$, soit une valeur facile à interpréter. Par contre, si la variable a été préalablement transformée en logarithme, il est possible que vous obteniez une différence de 9, ce qui est beaucoup moins parlant. Aussi, en transformant la variable en \emph{log}, vous ne comparez plus les moyennes arithmétiques des deux groupes, mais plutôt leurs moyennes géométriques \citep[pp.~193]{field2012discovering}.
\item
  \emph{Pourquoi perdre la forme initiale de la distribution du phénomène à expliquer ?} Il est possible pour de nombreuses méthodes de choisir la distribution que l'on souhaite utiliser, il n'est donc pas nécessaire de toujours se limiter à la distribution normale. Par exemple, dans les modèles de régression généralisés (GLM), on pourrait indiquer que notre variable indépendante suit une distribution de \emph{Student} plutôt que de vouloir à tout prix la rendre normale. De même, certains tests non-paramétriques permettent d'analyser des variables ne suivant pas une distribution normale.
\end{itemize}

\textbf{Démarche à suivre avant et après la transformation}

\begin{itemize}
\item
  \emph{La transformation est-elle nécessaire ?} Ne transformez jamais une variable sans avoir analyser rigoureusement sa forme (histogramme avec courbe normale, \emph{skewness} et \emph{kurtosis}, tests de normalité).
\item
  \emph{D'autres options à la transformation d'une variable dépendante (VD) sont-elles envisageables ?} Identifiez la forme de la distribution de la VD et utilisez au besoin un modèle GLM adapté à cette distribution. Autrement dit, ne transformez pas automatiquement votre VD pour simplement pouvoir l'introduire dans une régression linéaire multiple.
\item
  \emph{La transformation a-t-elle un apport significatif ?} Premièrement, vérifiez si la transformation utilisée (logarithme, racine carrée, inverse, etc.) améliore la normalité de la variable. Ce n'est toujours le cas, pourquoi c'est pire ! Prenez soin de comparer les histogrammes, les valeurs de \emph{skewness}, \emph{kurtosis} et des différents tests de normalité avant et après la transformation. Deuxièmement, comparez les résultats de vos analyses statistiques sans et avec transformation, et ce, dans une démarche coût-avantage. Vos résultats sont-ils bien plus robustes? Par exemple, un R\textsuperscript{2} qui passe de 0,597 à 0,602 avant et après la transformation des variables avec des associations significatives similaires, mais plus difficiles à interpréter (du fait des transformations), n'est pas forcément un gain significatif. La modélisation en sciences sociales ne vise pas à prédire la trajectoire d'un satellite ou l'atterrissage d'un engin sur Mars ! La précision à la quatrième décimale n'est pas une condition ! Par conséquent, un modèle un peu moins robuste, mais plus facile à interpréter est parfois préférable.
\end{itemize}


\end{bloc_attention}

\hypertarget{sect02552}{%
\subsubsection{Autres types de transformations}\label{sect02552}}

Les trois transformations les plus couramment utilisées sont :

\begin{itemize}
\tightlist
\item
  \textbf{La côte \(z\)} (\emph{z score} en anglais) qui consiste à soustraire à chaque valeur sa moyenne (soit un centrage), puis à la diviser par son écart-type (soit une réduction) (eq. \eqref{eq:scorez}). Par conséquent, on parle aussi de variable centrée-réduite qui a comme propriétés intéressantes une moyenne égale à 0 et un écart-type égale à 1 (la variance est aussi égale à 1 puisque \(1^2=1\)). Nous verrons que cette transformation est largement utilisée dans les méthodes de classification (chapitre \ref{chap09}) et les méthodes factorielles (chapitre \ref{chap10}).
\end{itemize}

\footnotesize

\begin{equation}  
z= \frac{x_i-\mu}{\sigma}
\label{eq:scorez}
\end{equation}
\normalsize

\begin{itemize}
\item
  \textbf{La transformation en rangs} qui consiste simplement à trier une variable en ordre croissant, puis à affecter le rang de chaque observation de 1 à \(n\). Cette transformation est très utilisée quand la variable est très anormalement distribuée, notamment pour calculer le coefficient de corrélation de Spearman (section \ref{sect04133}) et certains tests non-paramétriques (sections \ref{sect0432} et \ref{sect0442}).
\item
  \textbf{La transformation sur une échelle de 0 à 1} (ou de 0 à 100) qui consiste à soustraite à chaque observation la valeur minimale et à diviser le tout par l'étendue (eq. \eqref{eq:t01}).
\end{itemize}

\footnotesize

\begin{equation}  
X_{\in\lbrack0-1\rbrack}= \frac{x_i-max}{max-min} \text{ ou } X_{\in\lbrack0-100\rbrack}= \frac{x_i-min}{max-min}\times100
\label{eq:t01}
\end{equation}
\normalsize

\begin{table}

\caption{\label{tab:AutresTransformation}Illustration des trois tranformations}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lrrrr}
\toprule
Observation & $x_i$ & Côte $z$ & Rang & 0 à 1\\
\midrule
1 & 22,00 & -1,45 & 1 & 0,00\\
2 & 27,00 & -0,65 & 3 & 0,28\\
3 & 25,00 & -0,97 & 2 & 0,17\\
4 & 30,00 & -0,16 & 4 & 0,44\\
5 & 37,00 & 0,97 & 7 & 0,83\\
\addlinespace
6 & 32,00 & 0,16 & 5 & 0,56\\
7 & 35,00 & 0,65 & 6 & 0,72\\
8 & 40,00 & 1,45 & 8 & 1,00\\
Moyenne & 31,00 & 0,00 &  & \\
Écart-type & 6,19 & 1,00 &  & \\
\bottomrule
\end{tabular}
\end{table}

Pour un \emph{dataframe} nommé \emph{df} comprenant une variable \emph{x}, la syntaxe ci-dessous illustre comment obtenir quatre transformations (côte \(z\), rangs, 0 à 1 et 0 à 100).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{,}\DecValTok{27}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{30}\NormalTok{,}\DecValTok{37}\NormalTok{,}\DecValTok{32}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{40}\NormalTok{))}

\CommentTok{# Transformation centrée-réduite : côte Z}
\NormalTok{df2}\OperatorTok{$}\NormalTok{zx <-}\StringTok{ }\NormalTok{(df2}\OperatorTok{$}\NormalTok{x}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(df2}\OperatorTok{$}\NormalTok{x))}\OperatorTok{/}\KeywordTok{sd}\NormalTok{(df2}\OperatorTok{$}\NormalTok{x)}

\CommentTok{# Transformation en rangs avec la fonction rank}
\NormalTok{df2}\OperatorTok{$}\NormalTok{rz <-}\StringTok{ }\KeywordTok{rank}\NormalTok{(df2}\OperatorTok{$}\NormalTok{x)}

\CommentTok{# Transformation en rangs de 0 à 1}
\NormalTok{df2}\OperatorTok{$}\NormalTok{x01 <-}\StringTok{ }\NormalTok{(df2}\OperatorTok{$}\NormalTok{x}\OperatorTok{-}\KeywordTok{min}\NormalTok{(df2}\OperatorTok{$}\NormalTok{x))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(df2}\OperatorTok{$}\NormalTok{x)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(df2}\OperatorTok{$}\NormalTok{x))}

\CommentTok{# Transformation en rangs de 0 à 100}
\NormalTok{df2}\OperatorTok{$}\NormalTok{x0100 <-}\StringTok{ }\NormalTok{(df2}\OperatorTok{$}\NormalTok{x}\OperatorTok{-}\KeywordTok{min}\NormalTok{(df2}\OperatorTok{$}\NormalTok{x))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(df2}\OperatorTok{$}\NormalTok{x)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(df2}\OperatorTok{$}\NormalTok{x))}\OperatorTok{*}\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{bloc_aller_loin}
Ces trois transformations sont parfois utilisées pour générer un indice composite à partir de plusieurs variables ou encore dans une analyse de sensibilité avec les indices de Sobol \citeyearpar{Sobol1993}.

\end{bloc_aller_loin}

\hypertarget{sect0256}{%
\subsection{Mise en œuvre dans R}\label{sect0256}}

Il existe une multitude de \emph{packages} dédiés au calcul des statistiques descriptives univariées. Par parcimonie, nous en utiliserons uniquement trois : \texttt{DescTools}, \texttt{nortest} et \texttt{stats}. Libre à vous de faire vos recherches sur Internet pour utiliser d'autres \emph{packages} au besoin. Les principales fonctions que nous utilisons ici sont :

\begin{itemize}
\tightlist
\item
  \texttt{summary} : pour obtenir un résumé sommaire des statistiques descriptives (minimum, Q1, Q2 Q3, Maximum)
\item
  \texttt{mean} : moyenne
\item
  \texttt{min} : minimum
\item
  \texttt{max} : maximum
\item
  \texttt{range} : minimum et maximum
\item
  \texttt{quantile} : quartiles
\item
  \texttt{quantile((x,\ probs\ =\ seq(.0,\ 1,\ by\ =\ .2))} : quintiles
\item
  \texttt{quantile((x,\ probs\ =\ seq(.0,\ 1,\ by\ =\ .1))} : déciles
\item
  \texttt{var} : variance
\item
  \texttt{sd} : écart-type
\item
  \texttt{Skew} du \emph{package} \texttt{DescTools} : coefficient d'asymétrie
\item
  \texttt{Kurt} du \emph{package} \texttt{DescTools} : coefficient d'applatissement
\item
  \texttt{ks.test(x,\ "pnorm",\ mean=mean(x),\ sd=sd(x))} du \emph{package} \texttt{nortest} : test de Kolmogorov-Smirnov
\item
  \texttt{shapiro.test} du \emph{package} \texttt{DescTools} : test de Shapiro-Wilk
\item
  \texttt{lillie.test} du \emph{package} \texttt{DescTools} : du package \textbf{nortest} : test de Lilliefors
\item
  \texttt{ad.test} du \emph{package} \texttt{DescTools} : test d'Anderson-Darling
\item
  \texttt{JarqueBeraTest} du \emph{package} \texttt{DescTools} : test de Jarque-Bera
\end{itemize}

\hypertarget{sect02561}{%
\subsubsection{Application à une seule variable}\label{sect02561}}

Admettons que vous voulez obtenir des statistiques pour une seule variable présente dans un \emph{dataframe} (\texttt{dataMTL\$PctFRev}) :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DescTools)}
\KeywordTok{library}\NormalTok{(stats)}
\KeywordTok{library}\NormalTok{(nortest)}

\CommentTok{# Importation du fichier csv dans un dataframe}
\NormalTok{dataMTL <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/univariee/DataSR2016.csv"}\NormalTok{)}
\CommentTok{# Tableau sommaire pour la variable PctFRev}
\KeywordTok{summary}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.846  11.242  15.471  16.822  20.229  68.927
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PARAMÈTRES DE TENDANCE CENTRALE}
\KeywordTok{mean}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)   }\CommentTok{# Moyenne}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 16.82247
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)   }\CommentTok{# Médiane}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 15.471
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PARAMÈTRES DE POSITION}
\CommentTok{# Quartiles}
\KeywordTok{quantile}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      0%     25%     50%     75%    100% 
##  1.8460 11.2420 15.4710 20.2285 68.9270
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Quintiles}
\KeywordTok{quantile}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev, }\DataTypeTok{probs =} \KeywordTok{seq}\NormalTok{(.}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{.2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     0%    20%    40%    60%    80%   100% 
##  1.846 10.294 13.626 16.918 21.756 68.927
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Déciles}
\KeywordTok{quantile}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev, }\DataTypeTok{probs =} \KeywordTok{seq}\NormalTok{(.}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{.1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     0%    10%    20%    30%    40%    50%    60%    70%    80%    90%   100% 
##  1.846  8.402 10.294 12.172 13.626 15.471 16.918 18.868 21.756 26.854 68.927
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Percentiles personnalisés avec apply}
\KeywordTok{quantile}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev, }\DataTypeTok{probs =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{,.}\DecValTok{05}\NormalTok{,}\FloatTok{0.10}\NormalTok{,.}\DecValTok{25}\NormalTok{,.}\DecValTok{50}\NormalTok{,.}\DecValTok{75}\NormalTok{,.}\DecValTok{90}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{99}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      1%      5%     10%     25%     50%     75%     90%     95%     99% 
##  5.2290  7.1470  8.4020 11.2420 15.4710 20.2285 26.8540 31.7530 45.6010
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PARAMÈTRES DE DISPERSION}
\KeywordTok{range}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)  }\CommentTok{# Min et Max}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1.846 68.927
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Étendue}
\KeywordTok{max}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 67.081
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Écart interquartile}
\KeywordTok{quantile}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)[}\DecValTok{4}\NormalTok{]}\OperatorTok{-}\KeywordTok{quantile}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    75% 
## 8.9865
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev) }\CommentTok{# Variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 66.62482
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)  }\CommentTok{# Écart-type}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8.162403
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev) }\OperatorTok{/}\StringTok{ }\KeywordTok{mean}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev) }\CommentTok{# CV}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4852083
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PARAMÈTRES DE FORME}
\KeywordTok{Skew}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)    }\CommentTok{# Skewness}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.67367
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{Kurt}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)    }\CommentTok{# Kurtosis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.858815
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# TESTS D'HYPOTHÈSE SUR LA NORMALITÉ}
\CommentTok{# K-Smirnov}
\KeywordTok{ks.test}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev, }\StringTok{"pnorm"}\NormalTok{, }\DataTypeTok{mean=}\KeywordTok{mean}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev), }\DataTypeTok{sd=}\KeywordTok{sd}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  dataMTL$PctFRev
## D = 0.10487, p-value = 1.646e-09
## alternative hypothesis: two-sided
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Shapiro-Wilk normality test
## 
## data:  dataMTL$PctFRev
## W = 0.88748, p-value < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lillie.test}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Lilliefors (Kolmogorov-Smirnov) normality test
## 
## data:  dataMTL$PctFRev
## D = 0.10487, p-value < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ad.test}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Anderson-Darling normality test
## 
## data:  dataMTL$PctFRev
## A = 21.072, p-value < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{JarqueBeraTest}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Robust Jarque Bera Test
## 
## data:  dataMTL$PctFRev
## X-squared = 2173.1, df = 2, p-value < 2.2e-16
\end{verbatim}

Pour construire un histogramme avec la courbe normale, vous pourez consulter la section \ref{sect03213} ou la syntaxe ci-dessous.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{moyenne <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)}
\NormalTok{ecart_type <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{PctFRev)}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dataMTL) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ PctFRev, }\DataTypeTok{y =}\NormalTok{ ..density..),}
                 \DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\DataTypeTok{color =} \StringTok{"#343a40"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"#a8dadc"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{"densité"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ moyenne, }\DataTypeTok{sd =}\NormalTok{ ecart_type), }
                \DataTypeTok{color =} \StringTok{"#e63946"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{1.2}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/GcourbeNormale-1} 

}

\caption{Histogramme avec courbe normale}\label{fig:GcourbeNormale}
\end{figure}

\hypertarget{sect02562}{%
\subsubsection{Application à plusieurs variables}\label{sect02562}}

Pour obtenir des sorties de statistiques descriptives pour plusieurs variables, nous vous conseillons :

\begin{itemize}
\tightlist
\item
  de créer un vecteur avec les noms de variables (\emph{VarsSelect} dans la syntaxe ci-dessous)
\item
  d'utiliser ensuite les fonctions \texttt{sapply} et \texttt{apply.}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Noms des variables du dataframe}
\KeywordTok{names}\NormalTok{(dataMTL)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "CTNAME"                "PopTotal"              "HabKm2"               
##  [4] "PctFRev"               "TxChomage"             "PctImmigrant"         
##  [7] "PctImgRecent"          "PctMenage1pers"        "PctFamilleMono"       
## [10] "PctLangueMaternelleFR" "PctLangueMaternelleAN" "PctLangueMaternelleAU"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Vecteur pour trois variables}
\NormalTok{VarsSelect <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"HabKm2"}\NormalTok{, }\StringTok{"TxChomage"}\NormalTok{, }\StringTok{"PctFRev"}\NormalTok{ )}

\CommentTok{# Tableau sommaire pour les 3 variables}
\KeywordTok{summary}\NormalTok{(dataMTL[VarsSelect])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      HabKm2        TxChomage         PctFRev      
##  Min.   :   18   Min.   : 1.942   Min.   : 1.846  
##  1st Qu.: 1980   1st Qu.: 5.482   1st Qu.:11.242  
##  Median : 3773   Median : 7.130   Median :15.471  
##  Mean   : 5513   Mean   : 7.743   Mean   :16.822  
##  3rd Qu.: 7916   3rd Qu.: 9.391   3rd Qu.:20.229  
##  Max.   :50282   Max.   :26.882   Max.   :68.927
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PARAMÈTRES DE TENDANCE CENTRALE}
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], mean)   }\CommentTok{# Moyenne}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      HabKm2   TxChomage     PctFRev 
## 5512.830705    7.743329   16.822470
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], median) }\CommentTok{# Médiane}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    HabKm2 TxChomage   PctFRev 
##  3773.000     7.130    15.471
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PARAMÈTRES DE POSITION}
\CommentTok{# Quartiles}
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], quantile)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       HabKm2 TxChomage PctFRev
## 0%      18.0    1.9420  1.8460
## 25%   1980.5    5.4825 11.2420
## 50%   3773.0    7.1300 15.4710
## 75%   7915.5    9.3910 20.2285
## 100% 50282.0   26.8820 68.9270
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Quintiles}
\KeywordTok{apply}\NormalTok{(dataMTL[VarsSelect], }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{quantile}\NormalTok{(x, }\DataTypeTok{probs =} \KeywordTok{seq}\NormalTok{(.}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{.2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      HabKm2 TxChomage PctFRev
## 0%       18     1.942   1.846
## 20%    1525     5.116  10.294
## 40%    2953     6.422  13.626
## 60%    4971     7.973  16.918
## 80%    9509    10.000  21.756
## 100%  50282    26.882  68.927
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Déciles}
\KeywordTok{apply}\NormalTok{(dataMTL[VarsSelect], }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{quantile}\NormalTok{(x, }\DataTypeTok{probs =} \KeywordTok{seq}\NormalTok{(.}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \FloatTok{.1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      HabKm2 TxChomage PctFRev
## 0%       18     1.942   1.846
## 10%     455     4.369   8.402
## 20%    1525     5.116  10.294
## 30%    2298     5.780  12.172
## 40%    2953     6.422  13.626
## 50%    3773     7.130  15.471
## 60%    4971     7.973  16.918
## 70%    6918     8.909  18.868
## 80%    9509    10.000  21.756
## 90%   13055    11.749  26.854
## 100%  50282    26.882  68.927
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Percentiles personnalisés avec apply}
\KeywordTok{apply}\NormalTok{(dataMTL[VarsSelect], }\DecValTok{2}\NormalTok{, }
      \ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{quantile}\NormalTok{(x, }\DataTypeTok{probs =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{,.}\DecValTok{05}\NormalTok{,}\FloatTok{0.10}\NormalTok{,.}\DecValTok{25}\NormalTok{,.}\DecValTok{50}\NormalTok{,.}\DecValTok{75}\NormalTok{,.}\DecValTok{90}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{99}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      HabKm2 TxChomage PctFRev
## 1%     58.5    2.9665  5.2290
## 5%    178.0    3.8980  7.1470
## 10%   455.0    4.3690  8.4020
## 25%  1980.5    5.4825 11.2420
## 50%  3773.0    7.1300 15.4710
## 75%  7915.5    9.3910 20.2285
## 90% 13055.0   11.7490 26.8540
## 95% 15355.0   13.8400 31.7530
## 99% 18578.5   17.1920 45.6010
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PARAMÈTRES DE DISPERSION}
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], range)  }\CommentTok{# Min et Max}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      HabKm2 TxChomage PctFRev
## [1,]     18     1.942   1.846
## [2,]  50282    26.882  68.927
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Étendue}
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], max) }\OperatorTok{-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], min)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    HabKm2 TxChomage   PctFRev 
## 50264.000    24.940    67.081
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Écart interquartile}
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], quantile)[}\DecValTok{4}\NormalTok{,] }\OperatorTok{-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], quantile)[}\DecValTok{2}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    HabKm2 TxChomage   PctFRev 
## 5935.0000    3.9085    8.9865
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], var)    }\CommentTok{# Variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       HabKm2    TxChomage      PctFRev 
## 2.633462e+07 9.880932e+00 6.662482e+01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], sd)     }\CommentTok{# Écart-type}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      HabKm2   TxChomage     PctFRev 
## 5131.726785    3.143395    8.162403
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Coefficient de variation}
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], sd) }\OperatorTok{/}\StringTok{ }\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    HabKm2 TxChomage   PctFRev 
## 0.9308696 0.4059488 0.4852083
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PARAMÈTRES DE FORME}
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], Skew)    }\CommentTok{# Skewness}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    HabKm2 TxChomage   PctFRev 
##  1.967468  1.280216  1.673670
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], Kurt)    }\CommentTok{# Kurtosis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    HabKm2 TxChomage   PctFRev 
##  8.546403  2.892443  4.858815
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# TESTS D'HYPOTHÈSE POUR LA NORMALITÉ}
\CommentTok{# K-Smirnov}
\KeywordTok{apply}\NormalTok{(dataMTL[VarsSelect], }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{ks.test}\NormalTok{(x, }\StringTok{"pnorm"}\NormalTok{, }\DataTypeTok{mean=}\KeywordTok{mean}\NormalTok{(x), }\DataTypeTok{sd=}\KeywordTok{sd}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $HabKm2
## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  x
## D = 0.14899, p-value < 2.2e-16
## alternative hypothesis: two-sided
## 
## 
## $TxChomage
## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  x
## D = 0.080183, p-value = 9.778e-06
## alternative hypothesis: two-sided
## 
## 
## $PctFRev
## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  x
## D = 0.10487, p-value = 1.646e-09
## alternative hypothesis: two-sided
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], shapiro.test)       }\CommentTok{# Shapiro-Wilk}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           HabKm2                        TxChomage                    
## statistic 0.8385086                     0.9235146                    
## p.value   5.648795e-30                  1.451222e-21                 
## method    "Shapiro-Wilk normality test" "Shapiro-Wilk normality test"
## data.name "X[[i]]"                      "X[[i]]"                     
##           PctFRev                      
## statistic 0.8874803                    
## p.value   1.00278e-25                  
## method    "Shapiro-Wilk normality test"
## data.name "X[[i]]"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], lillie.test)       }\CommentTok{# Lilliefors}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           HabKm2                                          
## statistic 0.148988                                        
## p.value   5.689619e-58                                    
## method    "Lilliefors (Kolmogorov-Smirnov) normality test"
## data.name "X[[i]]"                                        
##           TxChomage                                       
## statistic 0.0801829                                       
## p.value   7.758887e-16                                    
## method    "Lilliefors (Kolmogorov-Smirnov) normality test"
## data.name "X[[i]]"                                        
##           PctFRev                                         
## statistic 0.1048704                                       
## p.value   7.43257e-28                                     
## method    "Lilliefors (Kolmogorov-Smirnov) normality test"
## data.name "X[[i]]"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], ad.test)           }\CommentTok{# Anderson-Darling}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           HabKm2                            TxChomage                        
## statistic 36.40276                          14.9237                          
## p.value   3.7e-24                           3.7e-24                          
## method    "Anderson-Darling normality test" "Anderson-Darling normality test"
## data.name "X[[i]]"                          "X[[i]]"                         
##           PctFRev                          
## statistic 21.07194                         
## p.value   3.7e-24                          
## method    "Anderson-Darling normality test"
## data.name "X[[i]]"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], JarqueBeraTest)    }\CommentTok{# Jarque-Bera}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           HabKm2                    TxChomage                
## statistic 4270.113                  639.2741                 
## parameter 2                         2                        
## p.value   0                         0                        
## method    "Robust Jarque Bera Test" "Robust Jarque Bera Test"
## data.name "X[[i]]"                  "X[[i]]"                 
##           PctFRev                  
## statistic 2173.082                 
## parameter 2                        
## p.value   0                        
## method    "Robust Jarque Bera Test"
## data.name "X[[i]]"
\end{verbatim}

\hypertarget{sect02563}{%
\subsubsection{Transformer une variable dans R}\label{sect02563}}

La syntaxe ci-dessous illustre trois exemples de transformation (logarithme, racine carrée et inverse de la variable). Rappelez-vous qu'il faut compare les valeurs de forme (\emph{skewness} et \emph{kurtosis}) et de forme (tests de Shapiro-Wilk) avant et après les transformations pour identifier celle qui est la plus efficace.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggpubr)}

\CommentTok{# Importation du fichier csv dans un dataframe}
\NormalTok{dataMTL <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/univariee/DataSR2016.csv"}\NormalTok{)}

\CommentTok{# Noms des variables du dataframe}
\KeywordTok{names}\NormalTok{(dataMTL)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "CTNAME"                "PopTotal"              "HabKm2"               
##  [4] "PctFRev"               "TxChomage"             "PctImmigrant"         
##  [7] "PctImgRecent"          "PctMenage1pers"        "PctFamilleMono"       
## [10] "PctLangueMaternelleFR" "PctLangueMaternelleAN" "PctLangueMaternelleAU"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Transformations}
\NormalTok{dataMTL}\OperatorTok{$}\NormalTok{HabKm2_log <-}\StringTok{  }\KeywordTok{log10}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2)}
\NormalTok{dataMTL}\OperatorTok{$}\NormalTok{HabKm2_sqrt <-}\StringTok{  }\KeywordTok{sqrt}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2)}
\NormalTok{dataMTL}\OperatorTok{$}\NormalTok{HabKm2_inv <-}\StringTok{  }\DecValTok{1}\OperatorTok{/}\NormalTok{dataMTL}\OperatorTok{$}\NormalTok{HabKm2}

\CommentTok{# Vecteur pour la variable et les trois transformations}
\NormalTok{VarsSelect <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"HabKm2"}\NormalTok{, }\StringTok{"HabKm2_log"}\NormalTok{, }\StringTok{"HabKm2_sqrt"}\NormalTok{, }\StringTok{"HabKm2_inv"}\NormalTok{)}

\CommentTok{# paramètres de forme}
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], Skew)    }\CommentTok{# Skewness}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      HabKm2  HabKm2_log HabKm2_sqrt  HabKm2_inv 
##   1.9674683  -1.2071326   0.4179037   8.2536901
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], Kurt)    }\CommentTok{# Kurtosis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      HabKm2  HabKm2_log HabKm2_sqrt  HabKm2_inv 
##  8.54640302  1.55670769  0.04563433 82.85604898
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# TESTS D'HYPOTHÈSE SUR LA NORMALITÉ}
\KeywordTok{sapply}\NormalTok{(dataMTL[VarsSelect], shapiro.test) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           HabKm2                        HabKm2_log                   
## statistic 0.8385086                     0.9113234                    
## p.value   5.648795e-30                  4.11156e-23                  
## method    "Shapiro-Wilk normality test" "Shapiro-Wilk normality test"
## data.name "X[[i]]"                      "X[[i]]"                     
##           HabKm2_sqrt                   HabKm2_inv                   
## statistic 0.9771699                     0.2530266                    
## p.value   4.638049e-11                  8.324983e-52                 
## method    "Shapiro-Wilk normality test" "Shapiro-Wilk normality test"
## data.name "X[[i]]"                      "X[[i]]"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Histogrammes avec courbe normale}
\NormalTok{Graph1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dataMTL) }\OperatorTok{+}
\StringTok{          }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ HabKm2, }\DataTypeTok{y =}\NormalTok{ ..density..),}
                 \DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\DataTypeTok{color =} \StringTok{"#343a40"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"#a8dadc"}\NormalTok{) }\OperatorTok{+}
\StringTok{          }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\StringTok{"Habitants au km2"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"densité"}\NormalTok{)}\OperatorTok{+}
\StringTok{          }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }
                        \DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2), }
                                    \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2)), }
                        \DataTypeTok{color =} \StringTok{"#e63946"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{1.2}\NormalTok{)}

\NormalTok{Graph2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dataMTL) }\OperatorTok{+}
\StringTok{          }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ HabKm2_log, }\DataTypeTok{y =}\NormalTok{ ..density..),}
                         \DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\DataTypeTok{color =} \StringTok{"#343a40"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"#a8dadc"}\NormalTok{) }\OperatorTok{+}
\StringTok{          }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\StringTok{"habitants au km2 (logarithme)"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"densité"}\NormalTok{)}\OperatorTok{+}
\StringTok{          }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }
                        \DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2_log), }
                                    \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2_log)), }
                        \DataTypeTok{color =} \StringTok{"#e63946"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{1.2}\NormalTok{)}

\NormalTok{Graph3 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dataMTL) }\OperatorTok{+}
\StringTok{          }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ HabKm2_sqrt, }\DataTypeTok{y =}\NormalTok{ ..density..),}
                         \DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\DataTypeTok{color =} \StringTok{"#343a40"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"#a8dadc"}\NormalTok{) }\OperatorTok{+}
\StringTok{          }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\StringTok{"habitants au km2 (racine carrée)"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"densité"}\NormalTok{)}\OperatorTok{+}
\StringTok{          }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }
                        \DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2_sqrt), }
                                    \DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2_sqrt)), }
                        \DataTypeTok{color =} \StringTok{"#e63946"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{1.2}\NormalTok{)}

\NormalTok{Graph4 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ dataMTL) }\OperatorTok{+}
\StringTok{          }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ HabKm2_inv, }\DataTypeTok{y =}\NormalTok{ ..density..),}
                         \DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\DataTypeTok{color =} \StringTok{"#343a40"}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"#a8dadc"}\NormalTok{) }\OperatorTok{+}
\StringTok{          }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x=}\StringTok{"habitants au km2 (inverse)"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"densité"}\NormalTok{)}\OperatorTok{+}
\StringTok{          }\KeywordTok{stat_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }
                        \DataTypeTok{args =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2_inv), }\DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(dataMTL}\OperatorTok{$}\NormalTok{HabKm2_inv)), }
                        \DataTypeTok{color =} \StringTok{"#e63946"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{1.2}\NormalTok{)}

\KeywordTok{ggarrange}\NormalTok{(}\DataTypeTok{plotlist =} \KeywordTok{list}\NormalTok{(Graph1, Graph2, Graph3, Graph4), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/GTranf-1} 

}

\caption{Histogramme des transformations}\label{fig:GTranf}
\end{figure}

La variable \emph{HabKm2} est asymétrique positive et leptokurtique. Tant les valeurs des statistiques de forme, du test de Shapiro-Wilk que les histogrammes semblent démontrer que la transformation la plus efficace est la racine carrée. Si la variable originale est asymétrique positive, sa transformation logarithme est par contre asymétrique négative. Cela démontre que la transformation logarithmique n'est pas toujours la panacée.

\hypertarget{sect026}{%
\section{Statistiques descriptives sur des variables qualitatives et semi-qualitatives}\label{sect026}}

\hypertarget{sect0261}{%
\subsection{Les fréquences}\label{sect0261}}

En guise de rappel, les variables nominales, ordinales et semi-quantitatives comprennent plusieurs modalités pour lesquelles plusieurs types de fréquences sont généralement calculées. Pour illustrer le tout, nous avons extrait du recensement de 2016 de Statistique Canada les effectifs des modalités de la variable sur le principal mode de transport utilisé pour les déplacements domicile-travail, et ce, pour la subdivision de recensement (MRC) de l'île de Montréal (tableau \ref{tab:Frequences}). Les différents types de fréquences sont les suivantes :

\begin{itemize}
\item
  les fréquences absolues simples (\textbf{FAS}) ou fréquences observées représentent le nombre d'observations pour chacune des modalités. Par exemple, sur 857 540 navetteurs domicile-travail (ligne totale), seulement 30 645 optent pour le vélo, alors que 427 530 conduisent un véhicule motorisé (automobile, camion ou fourgonnette) comme principal mode de transport.
\item
  les fréquences relatives simples (\textbf{FRS}) sont les proportions de chaque modalité sur le total (\(30645/857540=0,036\)); leur somme est égale à 1. Elles peuvent bien entendu être exprimées en pourcentage (\(30645/857540 \times 100=3,57\)); leur somme est alors égale à 100\%. Par exemple, 3,7\% des navetteurs utilisent le vélo comme mode de transport principal.
\item
  les fréquences absolues cumulées (\textbf{FAC}) représentent la fréquence observée (FAS) de la modalité auxquelles sont additionnées celles qui la précèdent. La valeur de la FAC pour la dernière est donc égale au total.
\item
  À partir des fréquences absolues cumulées (FAC), il est alors possible de calculer les fréquences relatives cumulées (\textbf{FRC}) en proportion (\(453930 / 857540 = 0,529\)) et en pourcentage (\(453930 / 857540 \times 100= 52,93\)). Par exemple, plus de la moitié des navetteurs utilisent l'automobile comme mode de transport principal (passager ou conducteur).
\end{itemize}

\begin{table}

\caption{\label{tab:Frequences}Les différents types de fréquences sur une variable qualitative ou semi-qualitative}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
Mode de transport & FAS & FRS & FRS (\%) & FAC & FRC & FRC (\%)\\
\midrule
Véhicule motorisé (conducteur) & 427 530 & 0,499 & 49,86 & 427 530 & 0,499 & 49,86\\
Véhicule motorisé (passager) & 26 400 & 0,031 & 3,08 & 453 930 & 0,529 & 52,93\\
Transport en commun & 295 860 & 0,345 & 34,50 & 749 790 & 0,874 & 87,43\\
À pied & 69 410 & 0,081 & 8,09 & 819 200 & 0,955 & 95,53\\
Bicyclette & 30 645 & 0,036 & 3,57 & 849 845 & 0,991 & 99,10\\
\addlinespace
Autre moyen & 7 695 & 0,009 & 0,90 & 857 540 & 1,000 & 100,00\\
Total & 857 540 & 1,000 & 100,00 &  &  & \\
\bottomrule
\end{tabular}
\end{table}

\begin{bloc_attention}
\textbf{Les fréquences cumulées : peu pertinentes pour les variables nomimales}

Le calcul et l'analyse des fréquences cumulées (absolues et relatives) sont très souvent inutiles pour les variables nominales.

Par exemple, au tableau \ref{tab:Frequences}, la fréquence cumulée relative (en \%) est de 87,43\% pour la troisième ligne. Cela signifie que 87,43\% des navetteurs se déplacent en véhicule motorisé (conducteur ou passager) ou en transport en commun. Par contre, si la troisième modalité avait été \emph{à pied}, le pourcentage aurait été de 61,02 (\(52,93+8,09\)). Si vous souhaitez calculer les fréquences cumulées sur une variable nominale, assurez-vous que l'ordre des modalités vous convient et de le modifier au besoin. Sinon, abstenez-vous de les calculer!

\textbf{Les fréquences cumulées : très utiles pour l'analyse pour des variables ordinales ou semi-quantitatives}

Pour des modalités hiérarchisées (variable ordinale ou semi-quantitative), l'analyse des fréquences cumulées (absolues et relatives) est par contre très intéressante. Par exemple, au tableau \ref{tab:Frequences2}, elle permet de constater rapidement que sur l'île de Montréal, un peu moins du très de la population à moins de 25 ans (35,95\%) et 83,33\% moins de 65 ans.

\end{bloc_attention}

\begin{table}

\caption{\label{tab:Frequences2}Les différents types de fréquences sur une variable semi-qualitative}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
Groupes d'âge & FAS & FRS & FRS (\%) & FAC & FRC & FRC (\%)\\
\midrule
0 à 14 ans & 304 470 & 0,157 & 15,68 & 304 470 & 0,157 & 15,68\\
15 à 24 ans & 237 555 & 0,122 & 12,23 & 542 025 & 0,279 & 27,91\\
25 à 44 ans & 582 150 & 0,300 & 29,98 & 1 124 175 & 0,579 & 57,89\\
45 à 64 ans & 494 205 & 0,254 & 25,45 & 1 618 380 & 0,833 & 83,33\\
65 à 84 ans & 271 560 & 0,140 & 13,98 & 1 889 940 & 0,973 & 97,32\\
\addlinespace
85 ans et plus & 52 100 & 0,027 & 2,68 & 1 942 040 & 1,000 & 100,00\\
Total & 1 942 040 & 1,000 & 100,00 &  &  & \\
\bottomrule
\end{tabular}
\end{table}

Différents graphiques peuvent être construits pour illustrer la répartition des observations : les graphiques en barres (verticales et horizontales) avec les fréquences absolues, les diagrammes circulaires ou en anneau pour les fréquences relatives (figure \ref{fig:GraphiquesFreq1}). Ces graphiques seront présentés plus en détails dans le chapitre suivant.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{livre_statistique_Phil_Jere_clean_files/figure-latex/GraphiquesFreq1-1} 

}

\caption{Différents graphiques pour représenter les fréquences absolues et relatives}\label{fig:GraphiquesFreq1}
\end{figure}

\hypertarget{sect0262}{%
\subsection{Mise en œuvre dans R}\label{sect0262}}

La syntaxe ci-dessous permet de calculer les différentes fréquences présentées au tableau \ref{tab:Frequences2}. Notez que pour les fréquences cumulées, nous utilisons la fonction \texttt{cumsum}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Vecteur pour les noms des modalités}
\NormalTok{Modalite <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"0 à 14 ans"}\NormalTok{,}
             \StringTok{"15 à 24 ans"}\NormalTok{,}
             \StringTok{"25 à 44 ans"}\NormalTok{,}
             \StringTok{"45 à 64 ans"}\NormalTok{,}
             \StringTok{"65 à 84 ans"}\NormalTok{,}
             \StringTok{"85 ans et plus"}\NormalTok{)}
\CommentTok{# Vecteur pour les fréquences absolues simples (FAS)}
\NormalTok{Navetteurs <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{304470}\NormalTok{,}\DecValTok{237555}\NormalTok{,}\DecValTok{582150}\NormalTok{,}\DecValTok{494205}\NormalTok{,}\DecValTok{271560}\NormalTok{,}\DecValTok{52100}\NormalTok{)}
\CommentTok{# Somme des FAS}
\NormalTok{sumFAS <-}\StringTok{  }\KeywordTok{sum}\NormalTok{(Navetteurs)}
\CommentTok{# Construction du dataframe avec les deux vecteurs}
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{GroupeAge =}\NormalTok{ Modalite, }
  \DataTypeTok{FAS =}\NormalTok{ Navetteurs,}
  \DataTypeTok{FRS =}\NormalTok{ Navetteurs }\OperatorTok{/}\StringTok{ }\NormalTok{sumFAS, }
  \DataTypeTok{FRSpct =}\NormalTok{ Navetteurs }\OperatorTok{/}\StringTok{ }\NormalTok{sumFAS }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{,}
  \DataTypeTok{FAC =} \KeywordTok{cumsum}\NormalTok{(Navetteurs),}
  \DataTypeTok{FRC =} \KeywordTok{cumsum}\NormalTok{(Navetteurs) }\OperatorTok{/}\StringTok{ }\NormalTok{sumFAS,}
  \DataTypeTok{FRCpct =} \KeywordTok{cumsum}\NormalTok{(Navetteurs) }\OperatorTok{/}\StringTok{ }\NormalTok{sumFAS }\OperatorTok{*}\StringTok{ }\DecValTok{100}
\NormalTok{  )}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        GroupeAge    FAS        FRS    FRSpct     FAC       FRC    FRCpct
## 1     0 à 14 ans 304470 0.15677844 15.677844  304470 0.1567784  15.67784
## 2    15 à 24 ans 237555 0.12232240 12.232240  542025 0.2791008  27.91008
## 3    25 à 44 ans 582150 0.29976211 29.976211 1124175 0.5788629  57.88629
## 4    45 à 64 ans 494205 0.25447725 25.447725 1618380 0.8333402  83.33402
## 5    65 à 84 ans 271560 0.13983234 13.983234 1889940 0.9731725  97.31725
## 6 85 ans et plus  52100 0.02682746  2.682746 1942040 1.0000000 100.00000
\end{verbatim}

\hypertarget{sect027}{%
\section{Pour aller un peu plus loin : les statistiques descriptives pondérées}\label{sect027}}

Dans la section \ref{sect025}, les différentes statistiques descriptives sur des variables quantitatives -- paramètres de tendance centrale, de position, de dispersion et de forme -- ont été largement abordées. Il est possible de calculer ces différentes statistiques en tenant compte d'une pondération. La statistique descriptive pondérée la plus connue est certainement la moyenne arithmétique pondérée. Son calcul est très simple; pour chaque observation, deux valeurs sont disponibles :

\begin{itemize}
\tightlist
\item
  \(x_i\), soit la valeur de la variable \(X\) pour l'observation \(i\)
\item
  \(w_i\), soit la valeur de la pondération pour \(i\).
\end{itemize}

Prenez soin de comparer les deux équations ci-dessous (à gauche, la moyenne arithmétique; à droite, la moyenne arithmétique pondérée). Vous constaterez rapidement qu'il suffit simplement de multiplier chaque observation par sa pondération (numérateur) et de diviser ce produit par la somme des pondérations (dénominateur; et non par \(n\), soit le nombre d'observations comme pour la moyenne arithmétique non pondérée).

\footnotesize

\begin{equation}  
\bar{x}=\frac{\sum_{i=1}^n x_i}{n} \text { versus } \bar{m}=\frac{\sum_{i=1}^n  w_ix_i}{\sum_{i=1}^nw_i}
\label{eq:moypond}
\end{equation}
\normalsize 

\begin{table}

\caption{\label{tab:MoyPondCalcul}Calcul de la moyenne pondérée}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{lrrr}
\toprule
Observation & $x_i$ & $w_i$ & $x_i \times w_i$\\
\midrule
1 & 200 & 20 & 4 000\\
2 & 225 & 80 & 18 000\\
3 & 275 & 50 & 13 750\\
4 & 300 & 200 & 60 000\\
Somme & 1 000 & 350 & 95 750\\
\addlinespace
Moyenne & 250 &  & \\
Moyenne pondérée &  &  & 274\\
\bottomrule
\end{tabular}
\end{table}

\begin{bloc_notes}
\textbf{Calcul d'autres statistiques descriptives pondérées}

Nous n'allons pas reporter ici les formules des versions pondérées de toutes les statistiques descriptives. Retenez toutefois le principe suivant permettant de les calculer à partir de l'exemple du tableau \ref{tab:MoyPondCalcul}. Pour la variable \emph{X}, dupliquons respectivement 20, 80, 50, 200 fois les observations 1 à 4. Si nous calculons la moyenne arithmétique sur ces valeurs dupliquées, alors cette valeur sera identique à la celle de la moyenne arithmétique pondérée. Le même principe reposant sur la duplication des valeurs s'applique à l'ensemble des statistiques descriptives.

\end{bloc_notes}

Dans un article récent, Alvarenga et al.~\citeyearpar{de2018accessibilite} évaluent l'accessibilité aux aires de jeux dans les parcs de la Communauté métropolitaine de Montréal (CMM). Pour les 881 secteurs de recensement de la CMM, ils ont calculé la distance à l'aire de jeux la plus proche à travers le réseau de rues. Ce résultat, cartographié à la figure \ref{fig:FigParcCMM}, permet d'avancer le constat suivant : « la quasi-totalité des secteurs de recensement de l'agglomération de Montréal présente des distances de l'aire de jeux la plus proche inférieures à 500 m, alors que les secteurs situés à plus d'un kilomètre d'une aire de jeux sont très majoritairement localisés dans les couronnes
nord et sud de la CMM » \citep[p.~238]{de2018accessibilite}.

Pour chaque secteur de recensement, Alvarenga et al.~\citeyearpar{de2018accessibilite} disposent des données suivantes :

\begin{itemize}
\tightlist
\item
  \(x_i\), soit la distance à l'aire de jeux la plus proche pour le secteur de recensement \emph{i} et
\item
  \(w_i\), la pondération, soit le nombre d'enfants de moins de dix ans.
\end{itemize}

Il est alors possible de calculer les statistiques descriptives de la proximité à l'aire de jeux la plus proche en tenant compte du nombre d'enfants résidant dans chaque secteur de recensement (tableau \ref{tab:MoyPondParc}). Cet exercice permet de conclure que : « {[}\ldots{]} globalement, les enfants ont une bonne accessibilité aux aires de jeux sur le territoire de la CMM. {[}\ldots{]} Les enfants sont en moyenne à un peu plus de 500 m de l'aire de jeux la plus proche (moyenne = 559 ; médiane = 512). Toutefois, les valeurs percentiles extrêmes signalent que respectivement 10\% et 5\% des enfants résident à près de 800 m et à plus de 1000 m de l'aire de jeux la plus proche » \citeyearpar[p.~236]{de2018accessibilite}.

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{images/bivariee/BivarieeFigureParc} 

}

\caption{Accessibilité aux aires de jeux par secteur de recensement, Communauté métropolitaine de Montréal, 2016}\label{fig:FigParcCMM}
\end{figure}

\begin{table}

\caption{\label{tab:MoyPondParc}Statistiques de l'aire de jeux la plus proche par secteur de recensement pondérées par la population de moins de 10 ans}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{rrrrrrrrr}
\toprule
N & Moyenne & P5 & P10 & Q1 & Médiane & Q3 & P90 & P95\\
\midrule
881 & 559 & 282 & 327 & 408 & 512 & 640 & 799 & 1 006\\
\bottomrule
\end{tabular}
\end{table}

De nombreux \emph{packages} sont disponibles pour calculer des statistiques pondérées, dont notamment \texttt{Weighted.Desc.Stat} et \texttt{Hmisc} utilisés dans la syntaxe ci-dessous.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(foreign)}
\KeywordTok{library}\NormalTok{(Hmisc)}
\KeywordTok{library}\NormalTok{(Weighted.Desc.Stat)}

\NormalTok{df <-}\StringTok{ }\KeywordTok{read.dbf}\NormalTok{(}\StringTok{"data/bivariee/SR_AireJeux_PopMoins10.dbf"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(df, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     SRNOM PopMoins10 AireJeux
## 1 0659.06        380 600.1921
## 2 0410.02        390 324.4396
## 3 0863.01        325 524.3323
## 4 0734.05        875 574.6682
## 5 0073.00        100 352.9505
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# xi (variable) et wi (pondération)}
\NormalTok{x <-}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{AireJeux}
\NormalTok{w <-}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{PopMoins10}

\CommentTok{# Calcul des paramètres de position}
\CommentTok{# Moyenne}
\NormalTok{Hmisc}\OperatorTok{::}\KeywordTok{wtd.mean}\NormalTok{(x, w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 559.8026
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Weighted.Desc.Stat}\OperatorTok{::}\KeywordTok{w.mean}\NormalTok{(x, w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 559.8026
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Quartiles et percentile}
\NormalTok{Hmisc}\OperatorTok{::}\KeywordTok{wtd.quantile}\NormalTok{(x, }\DataTypeTok{weights=}\NormalTok{w, }\DataTypeTok{probs=}\KeywordTok{c}\NormalTok{(.}\DecValTok{05}\NormalTok{, }\FloatTok{.10}\NormalTok{, }\FloatTok{.25}\NormalTok{, }\FloatTok{.50}\NormalTok{, }\FloatTok{.75}\NormalTok{, }\FloatTok{.90}\NormalTok{, }\FloatTok{.95}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        5%       10%       25%       50%       75%       90%       95% 
##  281.3623  327.3056  406.0759  511.5880  639.4813  798.6559 1011.5493
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Paramètres de dispersion avec le package Weighted.Desc.Stat}
\CommentTok{# Variance, écart-type et coefficient de variation}
\KeywordTok{w.var}\NormalTok{(x,w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 82818.18
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{w.sd}\NormalTok{(x,w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 287.7815
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{w.cv}\NormalTok{(x,w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5140767
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Paramètres de forme avec le package Weighted.Desc.Stat}
\CommentTok{# Skewness et kurtosis }
\KeywordTok{w.skewness}\NormalTok{(x, w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.735351
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{w.kurtosis}\NormalTok{(x, w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 41.17146
\end{verbatim}

  \bibliography{book.bib}

\printindex

\end{document}
