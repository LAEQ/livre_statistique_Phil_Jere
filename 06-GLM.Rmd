# Régressions linéaires généralisées (GLM)  {#chap06}

## Introduction de variables explicatives particulières

### Variable indépendante avec une fontion polynomiale 

Dans la section \@ref(sect0411), nous avons vu que la relation entre deux variables continues n'est pas toujours linéaire; elle peut être aussi curvilinéaire. Pour explorer les relations curvilinéaires, on introduit la variable sous une forme polynomiale d'ordre 2. L'équation de régression s'écrit alors :

\begin{equation}\footnotesize 
Y = b_{0} + b_{1}X_{1} + b_{11}X_{1}^2 + b_{2}X_{2} +\ldots+ b_{k}X_{k} + e
(\#eq:regpolyordre2)
\end{equation}

Dans l'équation ci-dessus, la première variable indépendante est introduite dans le modèle de régression à la fois dans sa forme originelle et mise au carré : $b_{1}X_{1} + b_{1}X_{1}^2$. 

La démographie est certainement une des disciplines des sciences sociales qui a le plus recours aux régressions polynomiales. En effet, la variable `âge` est souvent introduite comme variable explicative dans sa forme originale et mise au carré. L'objectif est de vérifier si l'âge partage ou non une relation curvilinéaire avec un phénomène donné : par exemple, il pourrait être associé positivement jusqu'à un certain seuil (45 ans par exemple), puis négativement à partir de ce seuil.


::: {.bloc_aller_loin data-latex=""}
**Régression polynomiale et nombre d'ordres**.

Sachez qu'il est aussi possible de construire des régressions polynomiales avec plus deux ordres. Par exemple, une régression polynomiale d'ordre 3, comprend une variable dans sa forme originelle, puis mise au carré et au cube. Cela a l'inconvénient d'augmenter corrolairement le nombre de coefficients. Nous verrons au chapitre \@ref(chap06) qui existe une solution plus élégante et efficace : le recours aux modèles de régressions linéaires généralisées avec des *splines*. Dans le cadre de cette section, nous limiterons à des régression polynomiale d'ordre 2.

\begin{equation}\footnotesize 
\mbox{Ordre 2 : } Y = b_{0} + b_{1}X_{1} + b_{1}X_{11}^2 + b_{2}X_{2} +\ldots+ b_{k}X_{k} + e
(\#eq:regpolyordre2b)
\end{equation}

\begin{equation}\footnotesize 
\mbox{Ordre 3 : } Y = b_{0} + b_{1}X_{1} + b_{1}X_{11}^2 + b_{1}X_{111}^3 + b_{2}X_{2} +\ldots+ b_{k}X_{k} + e
(\#eq:regpolyordre3)
\end{equation}

\begin{equation}\footnotesize 
\mbox{Ordre 4 : } Y = b_{0} + b_{1}X_{1} + b_{1}X_{11}^2 + b_{1}X_{111}^3 + b_{1}X_{1111}^4 + b_{2}X_{2} +\ldots+ b_{k}X_{k} + e
(\#eq:regpolyordre4)
\end{equation}
:::

Pour construire une régression polynomiale dans R, il est possible d'utiliser deux fonctions de R :

* `I(VI^2)` avec `VI` est la variable indépendante sur laquelle est appliquée la mise au carré.
* `poly(VI,2)` qui utilise une forme polynomiale orthogonale pour éviter les problèmes de corrélation entre les deux termes, c'est-à-dire entre **VI** et **VI^2^**.

Ces deux méthodes produiront les mêmes résultats pour les autres variables dépendantes et pour la qualité d'ajustement du modèle (R^2^, F, etc.). On privilégie la seconde fonction pour éviter ainsi les problèmes de multicolinéarité excessive.

Appliquons cette démarche à la variable `AgeMedian` (âge médian des bâtiments) afin de vérifier si elle partage ou non une relation curvilinéaire avec la couverture végétale de l'îlot. À la lecture des sorties des sorties pour les deux modèles, les constats suivants peuvent être avancées :

* Le R^2^ ajusté passe de 0,4179 à 0,4378 du modèle 1 au modèle 2, ce qui signale un gain de variance expliquée.
* Le F incrémentiel entre les deux modèles s'élève à 362,64 et est significatif (P<0,001). On peut donc en conclure que le second modèle est plus performant que le premier, ce qui signale que la forme curvilinéaire pour `AgeMedian` (modèle 2) est plus efficace que la forme linéaire (modèle 1).
* Dans le premier modèle, le coefficient de régression pour `AgeMedian` n'est pas significatif. L'âge médian des bâtiments n'est donc pas associé linéairement avec la variable dépendante.
* Dans le second modèle, la valeur du coefficient de `poly(AgeMedian, 2)1` est positive et celle de `poly(AgeMedian, 2)2` est négative et significative. Cela indique qu'il existe une relation linéaire en forme de U inversé. Si le premier coefficient avait été négatif et le second positif, on aurait alors conclu que la forme curvilinéaire prend la forme d'un U.


```{r calculRegPoly1, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}

# Régression linéaire
modele1 <- lm(VegPct ~ HABHA+AgeMedian+Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)

# Régression polynomiale
modele2 <- lm(VegPct ~ HABHA+poly(AgeMedian,2)+Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)

summary(modele1)
summary(modele2)
anova(modele1, modele2)

```

Pour visualiser à partir d'un graphiques, la relation linéaire et curvilinéaire, nous vous proposons la syntaxe suivante.

```{r calculRegPoly2, echo=TRUE, fig.align='center', fig.cap="Relations linéaire et curvilinéaire", message=FALSE, warning=FALSE, out.width='75%'}
library(ggplot2)
# Statistique sur la variable AgeMedian qui varie de 0 à 226 ans
summary(DataFinal$AgeMedian)

# Création d'un dataframe temporaire
# remarquez que les autres variables indépendantes sont constantes :
# nous leur avons attribué leur moyenne correspondante
df <- data.frame(
  HABHA = mean(DataFinal$HABHA),
  AgeMedian= seq(0,200, by = 2),
  AgeMedian2 = seq(0,200, by = 2)**2,
  Pct_014= mean(DataFinal$Pct_014),
  Pct_65P= mean(DataFinal$Pct_65P),
  Pct_MV= mean(DataFinal$Pct_MV),
  Pct_FR= mean(DataFinal$Pct_FR)
)

# calcul de la valeur de t pour un intervalle à 95%
n <- length(modele1$fitted.values) 
k <- length(modele1$coefficients)-1
t95 <- qt(p=1 - (0.05/2),  df=n-k-1)

# Calcul des valeurs prédites pour le 1er modèle
# avec l'intervalle de confiance à 95%
predsM1 <- predict(modele1,se = T, newdata = df)
df$predM1 <- predsM1$fit
df$lowerM1 <- predsM1$fit - t95*predsM1$se.fit
df$upperM1 <- predsM1$fit + t95*predsM1$se.fit

# Calcul des valeurs prédites pour le 2e modèle
# avec l'intervalle de confiance à 95%
predsM2 <- predict(modele2,se = T, newdata = df)
df$predM2 <- predsM2$fit
df$lowerM2 <- predsM2$fit - t95*predsM2$se.fit
df$upperM2 <- predsM2$fit + t95*predsM2$se.fit

# Graphique
ggplot(data = df) + 
  geom_ribbon(aes(x = AgeMedian, ymin = lowerM1, ymax = upperM1), 
             fill = rgb(0.1,0.1,0.1,0.4)) + 
  geom_path(aes(x = AgeMedian, y = predM1), color = 'blue', size = 1)+
  
  geom_ribbon(aes(x = AgeMedian, ymin = lowerM2, ymax = upperM2), 
              fill = rgb(0.1,0.1,0.1,0.4)) + 
  geom_path(aes(x = AgeMedian, y = predM2), color = 'red', size = 1)+
  
  labs(title="Effet de l'âge médian des bâtiments sur la couverture végétale des îlots",
       subtitle = "toutes choses étant égales par ailleurs",
       capation = "bleu : relation linéaire; rouge : curvilinéaire",
       x = "Âge médian des bâtiments",
       y = "Couverture végétale (%)")
```

La figure \@ref(fig:calculRegPoly2) démontre bien que la relation linéaire n'est pas significative : la pente est extrêmement faible, ce qui signale que l'effet de l'âge médian est presque nul (B=0,0108, P=0,0902). Par contre, la relation linéaire est plus intéressante : la couverture végétale croit quand l'âge médian des bâtiments dans l'îlot augmente de 0 à 60 ans environ, puis, elle décroît.

### Variable indépendante qualitative dichtonomique

### Variable indépendante qualitative polytomique


## Qu'est qu'un modèle GLM? {#sect061}

### Principes de base des GLM {#sect0611}

### Reformulation d'un modèle OLS sous forme GLM {#sect0612}

## Les principaux modèles GLM utilisées {#sect062}

### Les régressions logistiques {#sect0621}

#### Régression logit et probit simple {#sect06211}

#### Régression logistique multinomiale {#sect06212}

#### Régression logistique ordinale {#sect06213}

### Les régressions de type Poisson {#sect0623}


## D'autres modèles GLM  {#sect063}
Student, Beta, GAMMA, Exponentiel



