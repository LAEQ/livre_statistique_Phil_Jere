# (PART) Analyses exploratoires multivariées{-}

# Méthodes factorielles {#chap12}

Dans le cadre de ce chapitre, nous présentons les trois méthodes factorielles les plus fréquemment utilisées en sciences sociales&nbsp;: l'analyse en composantes principales (ACP, section \@ref(sect122)), l'analyse des factorielles des correspondances (AFC, section \@ref(sect123)) et l'analyse factorielles correspondances multiples (ACM, section \@ref(sect124)). Ces méthodes qui permettent d'explorer et de synthétiser l'information de différents tableaux de données relèvent de la statistique exploratoire multidimensionnelle.

::: {.bloc_package data-latex=""}
Dans ce chapitre, nous utilisons principalement les *packages* suivants&nbsp;: 

* Pour créer des graphiques&nbsp;:
  - `ggplot2`, le seul, l'unique!
  - `ggpubr` pour combiner des graphiques.
* Pour les analyses factorielles&nbsp;:
  - `FactoMineR` pour réaliser des ACP, AFC et ACM.
  - `factoextra` pour réaliser des graphiques à partir des résultats d'une analyse factorielle.
  - `explor` pour les résultats d'une ACP, d'une AFC ou d'une ACM avec une interface Web interactive.
* Autre *package*&nbsp;:
  - `geocmeans` pour un jeu de données utilisé pour calculer une ACP.
  - `ggplot2`, `ggpubr`, `stringr` et `corrplot` pour réaliser des graphiques personnalisés sur les résultats d'une analyse factorielle.
  - `tmap` et `RColorBrewer` pour cartographier les coordonnées factorielles.
  - `Hmisc` pour l'obtention d'une matrice de corrélation.
:::

 
## Aperçu des méthodes factorielles {#sect121}

### Objectifs et types de méthodes factorielles {#sect1211}

::: {.bloc_objectif data-latex=""}
**Réduction de données et identification de variables latentes**

Les méthodes factorielles sont souvent dénommées des **méthodes de réduction de données**, en raison de leur objectif principal, à savoir résumer l'information d'un tableau en de nouvelles variables synthétiques (figure \@ref(fig:AnalysesFactoriellesFig)). Ainsi, elles permettent de réduire l'information d'un tableau volumineux —&nbsp;comprenant par exemple 1000&nbsp;observations et 100&nbsp;variables&nbsp;— en *p* nouvelles variables (par exemple cinq avec toujours avec toujours 1000&nbsp;observations) résumant *X* % de l'information contenue dans le tableau initial. Formulée plus mathématiquement, Lebart et al. [-@lebart1995statistique, pp. 13] signalent qu'avec les méthodes factorielles, «&nbsp;on cherche à réduire les dimensions du tableau de données en représentant les associations entre individus et entre variables dans des espaces de faibles dimensions&nbsp;».

```{r AnalysesFactoriellesFig, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Principede base des analyses factorielles",  out.width='50%'}
knitr::include_graphics('images/analysesfactorielles/AnalysesFactorielles.png', dpi = NA)
```

Ces nouvelles variables synthétiques peuvent alors être considérées comme des **variables latentes** puisqu’elles ne sont directement observées, mais plutôt produites par la méthode factorielle utilisée afin de résumer les relations/associations entre plusieurs variables initialement mesurées.
:::

En analyse factorielle, la nature même des données du tableau à traiter détermine la méthode à employer&nbsp;: l’analyse en composantes principales (ACP) est adaptée aux tableaux avec des variables continues, l’analyse factorielle des correspondances (AFC) s’applique à des tableaux de contingence tandis que l’analyse des correspondances multiples (ACM) permet de résumer des tableaux avec des données qualitatives (tableau \@ref(tab:typesanalysesfactorielles)). Sachez toutefois qu'il existe d'autres méthodes factorielles qui ne seront pas abordées dans ce chapitre, notamment&nbsp;: l'analyse factorielle de données mixtes (AFDM) permettant d'explorer des tableaux avec à la fois des variables continues et des variables qualitatives, l'analyse factorielle multiple hiérarchique (AFMH) permettant de traiter des tableaux avec une structure hiérarchique. Pour s'initier ces deux récentes méthodes factorielles, consultez notamment l'excellent ouvrage de Jérôme Pagès [-@pages2013analyse].

```{r typesanalysesfactorielles, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}

typesaf <- data.frame(Metho= c("Analyse en composantes principales",
                             "Analyse factorielle des correspondances", 
                             "Analyse factorielle des correspondances multiples"), 
                      Abr = c("ACP", "AFC", "ACM"),
                      type = c("Variables continues", "Tableau de contingence", "Variables qualitatives"),
                      dist = c("Distance euclidienne", "Distance du khi-deux", "Distance du khi-deux")
                    )
show_table(typesaf,
           caption = "Trois principales méthodes factorielles",
           col.names=c("Méthode factorielle","Abr.","Type de données", "Type de distance"),
           align= c("l", "l", "l", "l")
           )
```

### Bref historique des méthodes factorielles {#sect1212}

Il existe une longue tradition de l'utilisation des méthodes factorielles dans le monde universitaire francophone puisque plusieurs d'entre elles ont été proposées par des statisticiens et des statisticiennes francophones à partir des années 1960. En effet, l’analyse en composantes principales (ACP) a été proposée dès les années 1930 par le statisticien américain Harold Hotelling [-@hotelling1933analysis]. Par contre, l’analyse des correspondances (AFC) et son extension (l’analyse des correspondances multiples, ACM) a été proposée par le statisticien français Jean-Paul Benzécri [-@benzecri1973analyse], tandis que l’analyse factorielle des données mixtes (AFDM) a été proposée par Brigitte Escofier et Jérôme Pagès [@escofier1979traitement; @pages2002analyse].

Ainsi, plusieurs ouvrages de statistique  sur les méthodes factorielles, désormais classiques, ont été publiés en français [@benzecri1973analyse; @escofier1998analyses; @lebart1995statistique; @pages2013analyse]. Ils méritent grandement d'être consultés, notamment pour mieux comprendre les formulations mathématiques (matricielles et géométriques) de ces méthodes. À cela, s'ajoute plusieurs ouvrages visant à «&nbsp;vulgariser ces méthodes&nbsp;» en sciences sociales; c'est notamment le cas de l'excellent ouvrage de Léna Sanders [-@sanders1989analyse] en géographie.


## Analyses en composantes principales (ACP) {#sect122}

D'emblée, notez qu'il existe deux types d'analyse en composantes principales (ACP) (*Principal Component Analysis, PCA* en anglais)&nbsp;: 

- **l'ACP non normée** dans laquelle les variables quantitatives du tableau sont uniquement centrées (moyenne&nbsp;=&nbsp;0).
- **l'ACP normée** dans laquelle les variables quantitatives du tableau sont préalablement centrées réduites (moyenne&nbsp;=&nbsp;0 et variance&nbsp;=&nbsp;1; section \@ref(sect02552)).

Puisque les variables d'un tableau sont souvent exprimées dans des unités de mesure différentes ou avec des ordres de grandeur différents (intervalles et écart-types bien différents), l'utilisation de l'ACP normée est bien plus courante et d'ailleurs l'option par défaut dans les fonctions R permettant de calculer une ACP. Autrement dit, l'usage de l'ACP non normée est beaucoup rare. Par conséquent, nous expliquons ici uniquement l'ACP normée.

### Recherche d'une simplification {#sect1221}

L’ACP permet d'explorer et de résumer un tableau avec des variables quantitatives (figure \@ref(fig:AnalysesFactoriellesTabACPFig)), et ce, de trois façons&nbsp;: 1) en montrant les ressemblances entre les individus (observations), 2) en révélant les liaisons entre les variables qunatitatives et 3) en résumant l’ensemble des variables du tableau par des variables synthétiques nommées composantes principales.

```{r AnalysesFactoriellesTabACPFig, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Tableau pour une ACP", out.width='60%'}
knitr::include_graphics('images/analysesfactorielles/AnalysesFactoriellesTabACP.png', dpi = NA)
```


**Ressemblance entre les individus**. Concrètement deux individus se ressemblent si leurs valeurs respectives pour les *p* variables du tableau sont similaires. Cette proximité/ressemblance est évaluée à partir de la distance euclidienne&nbsp; (eq. \@ref(eq:ACPdistEuc)).

\footnotesize
\begin{equation}
d^2(a,b) = \sum_{j=1}^p(x_{aj}-x_{bj})^2
(\#eq:ACPdistEuc)
\end{equation}
\normalsize

Prenons un exemple fictif avec trois individus (*i*, *j* et *k*) ayant des valeurs pour trois variables préalablement centrées réduites (V1 à V3) (tableau \@ref(tab:distanceACPindi)). La proximité entre les paires de points est évaluée comme suit&nbsp;: 

$d^2(i,j)=(-\mbox{1,15}-\mbox{0,49})^2+(-\mbox{1,15}-\mbox{0,58})^2+(\mbox{0,83}+\mbox{1,11})^2=\mbox{9,44}$
$d^2(i,k)=(-\mbox{1,15}+\mbox{0,66})^2+(-\mbox{1,15}-\mbox{0,58})^2+(\mbox{0,83}-\mbox{0,28})^2=\mbox{5,98}$
$d^2(j,k)= (\mbox{0,49}+\mbox{0,66})^2+(\mbox{0,58}-\mbox{0,58})^2+(-\mbox{1,11}-\mbox{0,28})^2=\mbox{1,97}$

Nous pouvons en conclure que *i* est plus proche de *k* que de *j*, mais aussi que la paire de points les plus proches est (*i*,*k*).

```{r distanceACPindi, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
library(kableExtra)
df1 <- data.frame(Ind=c("i", "j", "k"),
                  V1=c(100,150,155),
                  V2=c(20,22,22),
                  V3=c(25,18,23))

df1$V1 <- round(scale(df1$V1),2)
df1$V2 <- round(scale(df1$V2),2)
df1$V3 <- round(scale(df1$V3),2)

my_table <- show_table(df1,
             caption = "Données fictives",
             col.names = c("Individu","A","B", "C"),
             align= c("c", "c", "c", "c")
             )
add_header_above(my_table, c(" " = 1, "Variable centrée réduite" = 3))
```


**Liaisons entre les variables**. Dans une ACP normée, les liaisons entre les variables deux à deux sont évaluées avec le coefficient de corrélation (section \@ref(sect0431)), soit la moyenne du produit des deux variables centrée réduites (eq. \@ref(eq:ACPcor)). Notez que dans une ACP non normée, plus rarement utilisée, les liaisons sont alors évaluées avec la covariance puisque les variables sont uniquement centrées (eq. \@ref(eq:ACPcov)).

\footnotesize
\begin{equation}
r_{xy} = \frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{n\sqrt{\sum_{i=1}^n(x_i - \bar{x})^2(y_i - \bar{y})^2}}=\sum_{i=1}^n\frac{Zx_iZy_i}{n}
(\#eq:ACPcor)
\end{equation}
\normalsize

\footnotesize
\begin{equation}
cov(x,y) = \frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{n}
(\#eq:ACPcov)
\end{equation}
\normalsize

**Les valeurs propres et vecteurs propres** (*eigenvalues* et *eigenvectors* en anglais). 

Au chapitre&nbsp;4, nous avons abordé : la corrélation entre deux variables continues (section \@ref(sect043)), qu'il est possible d'illustrer graphiquement à partir d'un nuage de points; la régression linéaire simple (section \@ref(sect044)) permettant de résumer la relation linéaire entre deux variables avec une droite de régression de type $Y=a+bX$. 

Brièvement, plus deux variables sont corrélées (positivement ou négativement), plus le nuage de points sera allongé et plus les points seront proches de cette droite (figure \@ref(fig:liaisons2Vars), partie **a**). À l'inverse, plus la liaison entre les deux variables est faible, plus le nuage prend la forme d'un cercle et plus les points du nuage sont éloignés de la droite de régression (figure \@ref(fig:liaisons2Vars), partie **b**). Puisqu'en ACP normée, les variables sont centrées réduites, le centre de la gravité du nuage de points est (*x*=0, *y*=0) et il est toujours traversé par la droite de régression. Finalement, nous avons vu que la méthode des moindres carrés ordinaires (MCO) permet de déterminer cette droite en minimisant les distances entre les valeurs observées et celles projetées orthogonalement sur cette droite (valeurs prédites). Dans le cas de deux variables uniquement, l'axe factoriel/la composante principale est donc la droite qui résume le mieux la liaison entre les deux variables. Quant aux valeurs de points projetées orthogonalement sur cette droite, elles formes le vecteur propre.

```{r liaisons2Vars, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Corrélation, allongement du nuage de points et axe factoriel", auto_pdf=TRUE, dev="png", dpi=300, out.width='80%'}
library("MASS")
library("ggplot2")
library("ggpubr")
N <- 500      # nombre d'observations
moy_x <- 50   # moyenne de x
moy_y <- 40   # moyenne de y
sd_x <- 10    # écart-type de x
sd_y <- 8     # écart-type de y
rxy <- c(.90,-.85,0.01) # corrélation entre X et Y
 # Matrice de covariance
cov1 <- matrix(c(sd_x^2,  rxy[1]*sd_x*sd_y, rxy[1]*sd_x*sd_y, sd_y^2), nrow=2)
cov2 <- matrix(c(sd_x^2,  rxy[2]*sd_x*sd_y, rxy[2]*sd_x*sd_y, sd_y^2), nrow=2) 
cov3 <- matrix(c(sd_x^2,  rxy[3]*sd_x*sd_y, rxy[3]*sd_x*sd_y, sd_y^2), nrow=2) 
data1 <-  as.data.frame(mvrnorm(N, c(moy_x, moy_y), cov1))
data2 <-  as.data.frame(mvrnorm(N, c(moy_x, moy_y), cov2))
data3 <-  as.data.frame(mvrnorm(N, c(moy_x, moy_y), cov3))

data1$V1 <- scale(data1$V1)
data1$V2 <- scale(data1$V2)
data2$V1 <- scale(data2$V1)
data2$V2 <- scale(data2$V2)
data3$V1 <- scale(data3$V1)
data3$V2 <- scale(data3$V2)

cor1 <- round(cor(data1)[1,2],3)
cor2 <- round(cor(data2)[1,2],3)
cor3 <- round(cor(data3)[1,2],3)

plot1 <- ggplot(data1, aes(x=V1,y=V2))+
  xlim(-3,3)+
  ylim(-3,3)+
  geom_point(size = 1, color="steelblue")+
  ggtitle("a. Forte relation linéaire positive", subtitle = paste0("Corrélation = ", tofr(cor1)))+
  geom_hline(yintercept=0, size = .2, color="black")+
  geom_vline(xintercept=0, size = .2, color="black")+
  xlab("Variable 1")+ylab("Variable 2")+
  stat_ellipse( size = 1, color="black")+
  geom_smooth(method = lm, color = "red",  se=FALSE)+
  coord_fixed()

plot3 <- ggplot(data3, aes(x=V1,y=V2))+
  xlim(-3,3)+
  ylim(-3,3)+
  geom_point(size = 1, color="steelblue")+
  ggtitle("c. Absence de relation linéaire", subtitle = paste0("Corrélation = ", tofr(cor3)))+
  xlab("Variable 1")+ylab("Variable 2")+
  geom_hline(yintercept=0, size = .2, color="black")+
  geom_vline(xintercept=0, size = .2, color="black")+
  stat_ellipse( size = 1, color="black")+
  geom_smooth(method = lm, color = "red",  se=FALSE)+ 
  coord_fixed()
  

ggarrange(plot1, plot3, ncol = 2, nrow = 1)

```

Imaginez maintenant trois variables pour lesquelles vous désirez identifier un axe, une droite qui résume le mieux les liaisons entre elles. Visuellement, vous passez d'un nuage de points en deux dimensions (2D) à trois dimensions (3D). Si les corrélations entre les trois variables sont très faibles, alors le nuage prendra la forme d'un ballon de football (soccer en Amérique du Nord). Par contre, plus ces liaisons seront fortes, plus la forme sera allongée telle celle d'un ballon de rugby (ou football américain) et plus les points seront proches de l'axe traversant le ballon.

**ESSAYER D'AJOUTER ICI DEUX GRAPHIQUES 3D**

Ajouter une autre variable revient alors à ajouter une quatrième dimension qu'il est impossible de visualiser, même pour les plus fervents adaptes de science-fiction. Pourtant le problème reste le même, identifier dans dans un plan en *p* dimensions (variables), les axes factoriels, les composantes principales qui concourent le plus à résumer liaisons entre les variables continues préalablement centrées réduites, et ce, en utilisation la méthode des moindres carrés ordinaires.

::: {.bloc_attention data-latex=""}
Les termes **composantes principales** et **axes factoriels** sont des synomymes employés pour référer aux nouvelles variables synthétiques produites par l'ACP et résumant l'information du tableau intitial.
:::


### Aides à l'interprétation {#sect1222}

Pour illustrer les aides l'interprétation de l'ACP, nous utilisons un jeu de données spatiales tiré d'un article sur l'agglomération lyonnaise en France [@2021_4]. Ce jeu de données comprend dix variables, dont quatre environnementales (EN) et six socioéconomiques (SE),  pour les îlots regroupés pour l'information statistique (IRIS) de l'agglomération lyonnaise (tableau \@ref(tab:dataacp) et figure \@ref(fig:datacartoacp)). Sur ces dix variables, nous calculons une **ACP normée**.


```{r dataacp, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
library(geocmeans)
data(LyonIris)
Data <- LyonIris@data[c("Lden","NO2","PM25","VegHautPrt",
                        "Pct0_14","Pct_65","Pct_Img",
                        "TxChom1564","Pct_brevet","NivVieMed")]

intitule <- c("Bruit routier (Lden dB(A))",
              "Dioxyde d'azote (ug/m^3^)",
              "Particules fines (PM$_{2,5}$)",
              "Canopée (%)",
              "Moins de 15 ans (%)",
              "65 ans et plus (%)",
              "Immigrants (%)",
              "Taux de chômage",
              "Personnes à faible scolarité (%)",
              "Médiane du niveau de vie (Euros)" )

stats <- data.frame(variable = names(Data),
                    nom = intitule,
                    type = c("EN","EN","EN","EN","SE","SE","SE","SE","SE","SE"),
                    moy = round(sapply(Data, mean),2),
                    et = round(sapply(Data, sd),2), 
                    minimum =round(sapply(Data, min),2), 
                    maximum =round(sapply(Data, max),2)
                    )
show_table(stats,
           digits = 1,
            caption = "Statistiques descriptives pour le jeu de données utilisé pour l'ACP",
           col.names=c("Nom","Intitulé","Type","Moy.", "E.-T.", "Min.", "Max."),
           align= c("l","l", "c","r", "r", "r", "r")
           )
```


```{r datacartoacp, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Cartographie des dix variables utilisées pour l'ACP",  out.width='100%'}
knitr::include_graphics('images/analysesfactorielles/Figure3Data.png', dpi = NA)
```

::: {.bloc_objectif data-latex=""}
**L'analyse d'une ACP est réalisée en trois étapes&nbsp;:**

- Interprétation des résultats des valeurs propres pour identifier le nombre d'axes (de composantes principales) à retenir.
- Analyse des résultats pour les variables (coordonnées factorielles, cosinus carrés et contributions sur les axes retenus).
- Analyse des résultats pour les individus (coordonnées factorielles, cosinus carrés et contributions sur les axes retenus).

Les deux dernières étapes permettent de comprendre la signification des axes retenus et de les qualifier.
:::


#### Les résultats de l'ACP pour les valeurs propres {#sect12221}

À titre de rappel, une ACP normée est réalisée sur des variables préalablement centrées réduites (équation \@ref(eq:scorezacpnormee)), ce qui signifie que pour chaque variable&nbsp;:

- Nous soustrayons à chaque valeur la moyenne de la variable correspondante (centrage); la moyenne est donc égale à&nbsp;0.
- Nous divisons cette différence par l’écart-type de la variable corespondante (réduction); la variance est égale à&nbsp;1.
	
\footnotesize
\begin{equation}  
z= \frac{x_i-\mu}{\sigma}
(\#eq:scorezacpnormee)
\end{equation}
\normalsize
	
Par conséquent, la variance totale (ou inertie totale) d’un tableau sur lequel est calculée une ACP normée est égale au nombre de variables qu'il comprend. Puisque nous l'appliquons ici à dix variables, la variance totale du tableau à réduire – c'est-à-dire à résumer en *K* nouvelles variables synthétiques, composantes principales, axes factoriels – est donc égale à 10. Trois mesures reportées au tableau \@ref(tab:dataacpValeurPropres) permettent d'analyser les valeurs propres&nbsp;: 

- $\mbox{VP}_k$, la valeur propre (*eigenvalue* en anglais) de l'axe *k* c'est-à-dire la quantité de variance du tableau initial résumé par l'axe.
- $\mbox{VP}_k / \mbox{P}$ avec *P* étant le nombre de variables que comprend le tableau initial. Cette mesure représente ainsi le pourcentage de la variance totale du tableau résumé par l’axe *k*, autrement dit, la quantité d’informations du tableau initial résumée par l’axe, la composante principale *k*. Cela nous permet ainsi d’évaluer le pouvoir explicatif de l’axe.
- le pourcentage cumulé pour les axes.

```{r dataacpValeurPropres, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
library(FactoMineR)

# Calcul de l'ACP
res.acp <- PCA(Data, ncp=5, scale.unit=TRUE, graph=F)

# Construction d'un dataframe pour les valeurs propres
dfACPvp <- data.frame(res.acp$eig)
names(dfACPvp) <- c("VP","VP_pct","VP_cumupct")
dfACPvp$Composante <- factor(1:nrow(dfACPvp), levels=rev(1:nrow(dfACPvp)))
dfACPvp <- dfACPvp[,c(4,1:3)]

show_table(dfACPvp,
           digits = 3,
            caption = "Résultats de l'ACP pour les valeurs propres",
           col.names=c("Composante","Valeur propre","Pourcentage", "Pourc. cumulé"),
           align= c("r", "r", "r", "r")
           )
```

Avant d'analyser en détail le tableau \@ref(tab:dataacpValeurPropres), notez que la somme des valeurs propres de toutes les composantes de l'ACP esttoujours  égale au nombre de variables du tableau initial. Aussi, la quantité de variance expliquée (les valeurs propres) décroît de de la composante&nbsp;1 à la composante&nbsp;*K*.

**Combien d'axes d'une ACP faut-il retenir?** Pour ce faire, deux approches sont possibles&nbsp;:

- **Approche statistique (avec le critère de Kaiser)**, nous retenons uniquement les composantes qui présentent une valeur propre supérieure à&nbsp;1. Rappelez-vous qu'en ACP normée, les variables sont préalablement centrées réduites et donc que leur variance respective est égale à&nbsp;1. Par conséquent, une composante ayant une valeur propre inférieure à&nbsp;1 a un pouvoir explicatif inférieur à celui d'une variable. À la lecture du tableau, nous retenons les trois premières composantes si nous appliquons ce critère.
- **Approche empirique** basé sur la lecture des pourcentages et des pourcentages cumulés, nous pourrons retenir uniquement les deux premières composantes. En effet, ces deux premiers facteurs résument près des deux tiers de la variance totale du tableau (63,02&nbsp;%). Cela démontre bien que l'ACP comme les autres méthodes factorielles est bien une méthode de réduction de données puisque nous résumons dix variables avec deux nouvelles variables synthétiques (axes, composantes principales). Pour faciliter le choix du nombre d'axes, il est fortement conseillé de construire des histogrammes à partir des valeurs propres, des pourcentages et des pourcentages cumulés (figure \@ref(fig:acpgraphvp)). Or, à la lecture de ces graphiques, nous constatons que la variance expliquée chute drastiquement après les deux premières composantes. Par conséquent, nous pourrons ici retenir uniquement les deux premiers axes.

::: {.bloc_astuce data-latex=""}
**Lecture du diagramme des valeurs propres** 

Plus les variables incluses dans l'ACP sont corrélées entre elles, plus l'ACP sera intéressante&nbsp;: plus les valeurs propres des premiers axes sont fortes et plus il y a des sauts importants dans le diagramme des valeurs propores. À l'inverse, lorsque les variables incluses dans l'ACP sont peu corrélées entre elles, il n'y aura pas de sauts importants dans l'histogramme, autrement dit, les valeurs propres sont uniformément décroissantes.
:::


```{r acpgraphvp, echo=FALSE, fig.align='center', fig.cap="Graphiques personnalisés pour les valeurs propres", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
library(ggplot2)
library(ggpubr)
library(stringr)

dfACPvp <- data.frame(res.acp$eig)
names(dfACPvp) <- c("VP","VP_pct","VP_cumupct")
dfACPvp$Composante <- factor(1:nrow(dfACPvp), levels=rev(1:nrow(dfACPvp)))

couleursAxes <- c("steelblue","skyblue2")
vpsup1 <- round(sum(subset(dfACPvp, VP >= 1)$VP),2)
vpsup1cumul <- round(sum(subset(dfACPvp, VP >= 1)$VP_pct),2)

plotVP1 <- ggplot(dfACPvp,aes(x=VP, y=Composante,fill=VP<1))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=1, linetype="dashed", color = "azure4", size=1)+
  scale_fill_manual(name="Valeur\npropre",values=couleursAxes,labels = c(">= 1","< 1"))+
  labs(x="Valeur propre", y="Composante principale")

plotVP2 <- ggplot(dfACPvp, aes(x=VP_pct, y=Composante,fill=VP<1))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  scale_fill_manual(name="Valeur\npropre",values=couleursAxes,labels = c(">= 1","< 1"))+
  theme(legend.position="none")+
  labs(x="Variance expliquée (%)", y="")

plotVP3 <- ggplot(dfACPvp, aes(x=VP_cumupct, y=Composante,fill=VP<1, group=1))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  scale_fill_manual(name="Valeur\npropre",values=couleursAxes,labels = c(">= 1","< 1"))+
  geom_line(colour="brown", linetype="solid", size=.8) +
  geom_point(size=3, shape=21, color="brown", fill="brown")+
  theme(legend.position="none")+
  labs(x="Variance expliquée (% cumulé)", y="")

  annotate_figure(ggarrange(plotVP1, plotVP2, plotVP3, ncol=2, nrow=2),
                  text_grob("Graphiques pour les valeurs propres", color = "black", face = "bold", size = 12),
                  bottom = text_grob(
                          paste0("Somme des valeurs propres supérieures à 1 : ", vpsup1,
                                 ".\nPourcentage cumulé des valeurs propres supérieures à 1 : ", vpsup1cumul, "%."),
                           color = "black", hjust = 1, x = 1, size = 10))
```

#### Les résultats de l'ACP pour les variables {#sect12222}

Pour qualifier les axes, quatre mesures sont disponibles pour les variables&nbsp;:

- **Les coordonnées factorielles des variables**  sont simplement les coefficients de corrélation de Pearson des variables sur l’axe *k* et varient ainsi de -1 à 1 (relire au besoin la section \@ref(sect043)). Pour qualifier un axe, il convient alors de repérer les variables les plus corrélées positivement et négativement sur l’axe, autrement dit, de repérer les variables situées aux extrémités l'axe. 
- **Les cosinus carrés des variables** (Cos^2^) des variables permettent de repérer le ou les axes qui concourent le plus à donner un sens à la variable. Elles sont en fait les coordonnées des variables mises au carré. La somme des cosinus carrés d’une variable sur tous les axes de l’ACP est donc égale à&nbsp;1 (sommation en ligne).
**La qualité de représentation d'une variable sur les _n_ premiers axes** est simplement la somme des cosinus carrés d'une variable sur les axes retenus.
- **Les contributions des variables** permettent de repérer celles qui participent le plus à la formation d’un axe. Elles s'obtiennent en divisant les cosinus carrés par la valeur propre de l’axe multiplié par 100. La somme des contributions des variables pour un axe donné est donc égale à 100 (sommation en colonne). Par exemple, pour la variable *Lden* la contribution sur le premier axe est égale&nbsp;: $\mbox{0,174} / \mbox{3,543} \times \mbox{100}= \mbox{4,920%}$.

Les résultats de l'ACP pour les variables sont présentés au tableau \@ref(tab:dataacpCoordVars).

```{r dataacpCoordVars, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
library(kableExtra)
nComp <- 3
# Variance expliquée par les axes retenus
vppct <- round(dfACPvp[1:nComp,"VP_pct"],1)
# Dataframe des résultats pour les variables
CoordsVar <- res.acp$var$coord[, 1:nComp]
Cos2Var   <- res.acp$var$cos2[, 1:nComp]
CtrVar   <- res.acp$var$contrib[, 1:nComp]
dfACPVars <- data.frame(Variable =  row.names(res.acp$var$coord[, 1:nComp]),
                        Coord = CoordsVar,
                        Cos2 = Cos2Var,
                        Qualite = rowSums(Cos2Var),
                        Ctr = CtrVar)
row.names(dfACPVars) <- NULL
names(dfACPVars) <- str_replace(names(dfACPVars), ".Dim.", "Comp")

my_table <- show_table(dfACPVars,
                   digits = 2,
                   caption = "Résultats de l'ACP pour les variables",
                   col.names=c("Variable","1","2","3","1","2","3","Qualité","1","2","3"),
                   align= c("l","r","r","r","r","r","r","r","r","r","r")
                   )
add_header_above(my_table, c(" " = 1, "Coordonnées" = 3, "Cosinus carrés" = 4, "Contributions" = 3))
```

**Analyse de la première composante principale (valeur propre de 3,54, 35,43&nbsp;%)**

- À la lecture des contributions, il est clair que quatre variables contribuent grandement à la formation de l'axe&nbsp;1&nbsp;: `NivVieMed` (22,06&nbsp;%), 
`Pct_Img` (21,56&nbsp;%), `TxChom1564` (16,89&nbsp;%) et `Pct_brevet` (14,94&nbsp;%). Il convient alors d'analyser en détail leurs coordonnées factorielles.

- À la lecture des coordonnées factorielles, nous constatons que trois variables socioéconomiques sont fortement corrélées positivement avec l’axe&nbsp;1, soit le *pourcentage d’immigrants* (0,87), le *taux de chômage* (0,77) le *pourcentage de personnes avec une faible scolarité* (0,73). À l’autre extrémité, la *médiane du niveau de vie* (en Euros) est négative corrélée avec l’axe 1. Comment interpréter ce résultat? Premièrement, cela signifie que plus la valeur de l’axe 1 est positive et élevée, plus celles des trois variables (`Pct_Img`,`TxChom1564` et `Pct_brevet`) sont aussi élevées (corrélations positives) et plus la valeur de `NivVieMed` est faible (corrélation négative). Inversement, plus la valeur de l’axe&nbsp;1 est négative et faible, les valeurs de `Pct_Img`, `TxChom1564` et `Pct_brevet` sont faibles et plus celle de `NivVieMed` est forte. Deuxièmement, cela signifie que les trois variables (`Pct_Img`,`TxChom1564` et `Pct_brevet`) sont fortement corrélées positivement entre elles puisqu’elles se situent sur la même extrémité de l’axe et qu’elles sont toutes trois négativement corrélées avec la variable `NivVieMed`. Cela peut être rapidement confirmée avec la matrice de corrélation entre les dix variables (tableau \@ref(tab:dataacpMatriceCorr)). 

```{r dataacpMatriceCorr, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
library(Hmisc)

DataZ <-  scale(Data)

MatriceCorr <- as.data.frame(rcorr(Data %>% as.matrix())$r)
MatriceCorr$Variable <- c("A. Lden",  "B. NO2", "C. PM25", "D. VegHautPrt", "E. Pct0_14", 
                          "F. Pct_65", "G. Pct_Img", "H. TxChom1564", "I. Pct_brevet", "J. NivVieMed")
MatriceCorr <- MatriceCorr[, c(11,1:10)]
names(MatriceCorr) <- c("Variable", "A", "B", "C", "D", "E", "F", "G", "H", "I", "J")

for (e in 2:11){
  MatriceCorr[[e]] <- ifelse(MatriceCorr[[e]] == 1.00, NA, MatriceCorr[[e]])
}

show_table(MatriceCorr,
                   digits = 2,
                   caption = "Matrice de corrélation de Pearson entre les variables utilisées pour l'ACP",
                   align= c("l","r","r","r","r","r","r","r","r","r","r","r")
                   )
```

**Analyse de la deuxième composante principale (valeur propre de 2,76, 27,60&nbsp;%)**

- À la lecture des contributions, trois variables environnementales contribuent à la formation de l'axe&nbsp;1&nbsp;: principalement, celles sur la pollution de l'air (`NO2`&nbsp;=&nbsp;31,07&nbsp;% et `PM25`&nbsp;=&nbsp;30,36&nbsp;%) et secondairement, sur le bruit routier (`Lden`&nbsp;=&nbsp;11,64&nbsp;%).
- À la lecture des coordonnées factorielles, ces trois variables sont fortement corrélées positivement avec l'axe 2&nbsp;: `NO2` (0,93), `PM25` (0,92) et `Lden` (0,57). À l'autre extrémité de l'axe, la variable `Pct0_14` est négativement, mais pas fortement corrélée négativement (-0,53). La lecture de la matrice de corrélation au tableau \@ref(tab:dataacpMatriceCorr) confirment que ces trois variables environnementales sont fortement corrélées positivement entre elles (par exemple, un coefficient de corrélation de Pearson de 0,90 entre `NO2` et `PM25`).

**Analyse de la troisième composante principale (valeur propre de 1,042, 10,42&nbsp;%)**

- Le *pourcentage de personnes âgées* (`Pct_65`) contribue principalement à la formation de l'axe avec lequel est corrélée positivement (contribution de 49,26&nbsp;% et coordonnée factorielle de 0,72). S'en suit, la variable `Lden` qui joue un rôle beaucoup moins important (contribution de 24,80&nbsp;% et coordonnée factorielle de 0,51).

::: {.bloc_astuce data-latex=""}
**Lien entre la valeur propre d'un axe et le nombre de variables contribuant à sa formation**

Vous auvez compris que plus la valeur propre d'un axe est forte, plus il y a potentiellement de variables qui concourent à sa formation. Cela explique que pour la troisième composante qui a une faible valeur propre (1,042), seule une variable contribue significativement à sa formation. 
:::

**Analyse de la qualité de représentation des variables sur les premiers axes de l'ACP**

À titre de rappel, la qualité  est simplement la somme des cosinus carrés d’une variable sur les axes retenus. Si nous retenons trois axes, les six variables qui ont le plus d'influence sur les résultats de l'ACP sont `NO2` (0,92), `PM25` (0,87), `NivVieMed` (0,79), `Pct_Img` (0,78), `Pct_brevet` (0,77) et`Lden` (0,75).

**Qualification, dénomination de axes factoriels**

L'analyse des coordonnées, contributions et cosinus carrés doit vous permettre de formuler un intitulé pour chacun des axes retenus. Nous pourrions ainsi proposer les intitulés suivants&nbsp;: 

- *Niveau de défavorisation socioéconomique* (axe&nbsp;1). Plus la valeur de l'axe est élevé le niveau de défavorisation de l'entité spatiale (IRIS) est élevé.
- *Qualité environnementale* (axe&nbsp;2). Plus la valeur de l'axe est forte, plus les niveaux de pollution atmosphérique (dioxyde d'azote et particules fines) et de bruit (Lden) sont élevés.

**Le recours à des graphiques pour analyser les résultats de l'ACP pour des variables**

Plus le nombre de variables utilisées pour calculer l'ACP est important, plus l'analyse des coordonnées factorielles, des cosinus carrés et des contributions reportés dans un tableau devient fastidieuse. Puisque l’ACP a été calculée sur uniquement dix variables, l’analyse des valeurs du tableau \@ref(tab:dataacpCoordVars) a donc été assez facile et rapide. Imaginez maintenant que nous réalisons une ACP sur une centaine de variables, la taille du tableau des résultats pour les variables sera considérable... Par conséquent, il est recommandé de construire plusieurs graphiques qui facilitent l’analyse des résultats pour les variables. 

Par exemple, à la figure \@ref(fig:acpgraphvarscoords), nous avons construit des graphiques avec les coordonnées factorielles sur les trois premiers axes de l’ACP. En un coup d’œil, il est facile de repérer les variables plus corrélées positivement ou négatives avec chacun d’entre eux.
Aussi, il est fréquent de construire un nuage de points avec les coordonnées des variables sur les deux premiers axes factoriels, soit un graphique communément appelé **nuage de points des variables sur le premier plan factoriel** sur lequel est représenté le cercle des corrélations (figure \@ref(fig:acp1erplanfactVars)). Bien entendu, cet exercice peut être fait avec d’autres axes factoriels (les axes 3 et 4 par exemple).

```{r acpgraphvarscoords, echo=FALSE, fig.align='center', fig.cap="Coordonnées factorielles des variables", auto_pdf=TRUE, dev="png", dpi=300, out.width='100%'}
# Variance expliquée par les axes retenus
vppct <- round(dfACPvp[1:nComp,"VP_pct"],1)
# Dataframe des résultats pour les variables
CoordsVar <- res.acp$var$coord[, 1:nComp]
Cos2Var   <- res.acp$var$cos2[, 1:nComp]
CtrVar   <- res.acp$var$contrib[, 1:nComp]
dfACPVars <- data.frame(Variable =  row.names(res.acp$var$coord[, 1:nComp]),
                        Coord = CoordsVar,
                        Cos2 = Cos2Var,
                        Qualite = rowSums(Cos2Var),
                        Ctr = CtrVar)
row.names(dfACPVars) <- NULL
names(dfACPVars) <- str_replace(names(dfACPVars), ".Dim.", "Comp")

# Histogrammes pour les coordonnées
couleursCoords <- c("lightsalmon","steelblue")
plotCoordF1 <- ggplot(dfACPVars,
                      aes(y = reorder(Variable, CoordComp1),
                          x = CoordComp1, fill=CoordComp1<0))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=0, color = "black", size=1)+
  scale_fill_manual(name="Coordonnée",values=couleursCoords,labels = c("Positive","Négative"))+
  labs(x=paste0("Axe 1 (", tofr(vppct[1]),"%)"), y="Variable")+
  theme(legend.position="none")

plotCoordF2 <- ggplot(dfACPVars,
                      aes(y = reorder(Variable, CoordComp2),
                          x = CoordComp2, fill=CoordComp2<0))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=0, color = "black", size=1)+
  scale_fill_manual(name="Coordonnée",values=couleursCoords,labels = c("Positive","Négative"))+
  labs(x=paste0("Axe 2 (", tofr(vppct[2]),"%)"), y="Variable")+
  theme(legend.position="none")

plotCoordF3 <- ggplot(dfACPVars,
                      aes(y = reorder(Variable, CoordComp3),
                          x = CoordComp3, fill=CoordComp3<0))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=0, color = "black", size=1)+
  scale_fill_manual(name="Coordonnée", values=couleursCoords,labels = c("Positive","Négative"))+
  labs(x=paste0("Axe 3 (", tofr(vppct[3]),"%)"), y="Variable")

annotate_figure(ggarrange(plotCoordF1, plotCoordF2, plotCoordF3, nrow = nComp),
                text_grob("Coordonnées des variables sur les axes factoriels",
                          color = "black", face = "bold", size = 12))
```

```{r acp1erplanfactVars, echo=FALSE, fig.align='center', fig.cap="Premier plan factoriel pour les variables", message=FALSE, auto_pdf=TRUE, dev="png", dpi=300, out.width='75%', quietly=TRUE, warn.conflicts=TRUE}
library(factoextra)
library(ggplot2)
library(ggpubr)

fviz_pca_var(res.acp,
             geom = c("point", "text"),
             col.var="black",
             col.circle = "red",
             title = "Premier plan factoriel pour les variables")+
  theme_minimal()
```


#### Les résultats de l'ACP pour les individus {#sect12223}

Comme pour les variables, nous retrouvons les mêmes mesures que pour les individus&nbsp;: les coordonnées factorielles, les cosinus carrés et les contributions pour les individus. Les coordonnées factorielles des individus sont la projection des observations sur l'axe. Puisqu'en ACP normée, les variables utilisées pour l'ACP sont centrées réduites, la moyenne des coordonnées factorielles des individus pour un axe est toujours égale à zéro. Par contre, contrairement aux coordonnées factorielles pour les variables, les coordonnées pour les individus ne varient pas de -1 à 1!

Si le jeu de données comprend peu d'observations, il est toujours possible de créer un **nuage de points des individus sur le premier plan factoriel** sur lequel vous ajouterez les étiquettes permettant d'identifier les observations (figure \@ref(fig:acp1erplanfactIndiv)). Ce graphique est rapidement illisible lorsque le nombre d'observations est important.

```{r acp1erplanfactIndiv, echo=FALSE, fig.align='center', fig.cap="Premier plan factoriel pour les variables", message=FALSE, auto_pdf=TRUE, dev="png", dpi=300, out.width='75%', quietly=TRUE, warn.conflicts=TRUE}
library(factoextra)
library(ggplot2)
library(ggpubr)

fviz_pca_ind(res.acp,
             geom = c("point", "text"),
             col.var="black",
             col.circle = "red",
             title = "Premier plan factoriel pour les variables")+
  theme_minimal()
```

Lorsque les observations sont des unités spatiales, il est très intéressant de cartographier les coordonnées factorielles des individus (figure \@ref(fig:acp1erplanfactIndiv)). À la lecture de la carte choroplèthe de gauche (axe 1), nous pouvons constater que le niveau de défavorisation socioéconomique est élevé dans l'est (IRIS en vert), et inversement, très faible à l'ouest de l'agglomération (IRIS en rouge). À la lecture de la carte de droite (axe 2), sans surprise, la partie centrale de l'agglomération est caractérisée par des niveaux de pollution atmosphérique et de bruit routier bien plus élevés qu'en périphérie.

```{r acpcartoindiv, echo=FALSE, fig.align='center', fig.cap="Cartographie des coordonnées factorielles des individus", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
library("tmap")
library("RColorBrewer")

CoordsInd <- res.acp$ind$coord[, 1:nComp]
Cos2Ind   <- res.acp$ind$cos2[, 1:nComp]
CtrInd    <- res.acp$ind$contrib[, 1:nComp]
dfACPInd <- data.frame(Coord = CoordsInd, Cos2 = Cos2Ind, Ctr = CtrInd)
names(dfACPInd) <- str_replace(names(dfACPInd), ".Dim.", "Comp")
CartoACP <- cbind(LyonIris, dfACPInd)

Carte1 <- tm_shape(CartoACP) +
          tm_polygons(col = "CoordComp1", style = "cont",
                      midpoint = 0, title = 'Coordonnées')+
          tm_layout(title = "Axe 1 : Défavorisation socioéco.",
                    title.size = 3, attr.outside = TRUE, frame = FALSE)

Carte2 <- tm_shape(CartoACP) +
          tm_polygons(col = "CoordComp2", style = "cont",
                      midpoint = 0, title = 'Coordonnées')+
          tm_layout(title = "Axe 2 : Qualité environnementale",
                    title.size = 3, attr.outside = TRUE, frame = FALSE)

tmap_arrange(Carte1, Carte2)
```


::: {.bloc_aller_loin data-latex=""}

Nous n'avons pas abordé plusieurs autres éléments intéressants de l'ACP.

**Ajout de variables ou d'individus supplémentaires**. 

Premièrement, il est possible d'ajouter des variables continues ou des individus supplémentaires qui n'ont pas été pris en compte dans le calcul de l'ACP (figure \@ref(fig:acpvarindcorrsuppl)). Concernant les variables continues supplémentaires, il s'agit simplement de calculer leurs corrélations avec les axes retenus de l'ACP. Concernant les individus, il s'agit de les projeter sur les axes factoriels. Pour plus d'informations sur le sujet, consultez les excellents ouvrages de Ludovic Lebart, Alain Morineau et Marie Piron [-@lebart1995statistique, pp. 42-45] ou encore Jérôme Pagès [-@pages2013analyse, pp. 22-24].

```{r acpvarindcorrsuppl, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Variables et individus supplémentaires pour l'ACP",  out.width='28%', fig.pos = "H", out.extra = ""}
knitr::include_graphics('images/analysesfactorielles/AcpIndVarSuppl.png', dpi = NA)
```

**Pondération des individus et des variables**.

Deuxièmement, il est possible de pondérer à la fois les individus et plus rarement les variables lors du calcul du l'ACP.

**ACP non paramétrique**
Troisièmement, il est possible de calculer une ACP sur des variables préalablement transformées en rangs (section \@ref(sect02552)). Cela peut être justifié lorsque les variables sont très anormalement distribuées en raison de valeurs extrêmes. Les coordonnées factorielles pour les variables sont alors le coefficient de Spearman (section \@ref(sect0433)) et non de Pearson. Aussi, les variables sont centrées non pas sur leurs moyennes respectives, mais sur leurs médianes. Pour plus d'informations sur cette approche, consultez de nouveau Lebart et al. [-@lebart1995statistique, pp. 51-52].
:::


### Mise en œuvre dans R {#sect1223}

Plusieurs *packages* permettent de calculer une ACP dans R, notamment `psych` (fonction `principal`), `ade4` (fonction `dudi.pca`) et `FactoMineR` (fonction `PCA`). Ce dernier est certainement le plus abouti. De plus, il permet également de calculer une analyse des correspondances (AFC), une analyse des correspondances multiples (ACM) et une analyse factorielle de données mixtes (AFDM). Nous utilisons donc `FactoMineR` pour mettre en œuvre les trois types de méthodes factorielles abordées dans ce chapitre (ACP, AFC et ACM). Pour l'ACP, nous exploitons un jeu de données issu du *package* `geocmeans` qu'il faut préalablement charger à l'aide des lignes de code suivantes.

```{r echo=TRUE}
library(geocmeans)
data(LyonIris)
Data <- LyonIris@data[c("CODE_IRIS","Lden","NO2","PM25","VegHautPrt",
                        "Pct0_14","Pct_65","Pct_Img",
                        "TxChom1564","Pct_brevet","NivVieMed")]
```

#### Calcul et exploration d'une ACP avec `FactoMineR` {#sect12231}

Pour calculer l'ACP, il suffit d'utiliser la fonction `PCA` de `FactoMineR`, puis la fonction `summary(MonACP)` qui renvoie les résultats de l'ACP pour&nbsp;: 

- Les valeurs propres (section `Eigenvalues`) pour les composantes principales (`Dim.1` à `Dim.n`) avec leur variance expliquée brute (`Variance`), en pourcentage (`% of var.`) et en pourcentage cumulé (`Cumulative % of var.`).
- Les dix premières observations (section `Individuals`) avec les coordonnées factorielles (`Dim.1` à `Dim.n`), les contributions (`ctr`) et les cosinus carrés (`cos2`). Pour accéder aux résultats pour toutes les observations, utilisez les fonctions `res.acp$ind` ou encore `res.acp$ind$coord` (uniquement les coordonnées factorielles), `res.acp$ind$contrib` (uniquement les contributions) et `res.acp$ind$cos2` (uniquement les cosinus carrés).
- Les variables (section `Variables`) avec les coordonnées factorielles (D`im.1` à `Dim.n`), les contributions (`ctr`) et les cosinus carrés (`cos2`).

```{r echo=TRUE}
library(FactoMineR)
# Version classique avec FactoMineR
# Construction d'une ACP sur les colonnes 2 à 11 du dataframe Data
res.acp <- PCA(Data[,2:11], scale.unit=TRUE, graph=F)
# Affichage des résultats de la fonction PCA
print(res.acp)
# Résumé des résultats (valeurs propres, individus, variables)
summary(res.acp)
```

Avec les fonctions de base avec `barplot` et `plot`, il est possible de construire rapidement des graphiques pour explorer les résultats de l'ACP pour les valeurs propres, les variables et les individus.

```{r echo=TRUE}
# Graphiques pour les valeurs propres
barplot(res.acp$eig[,1], main="Valeurs propres", names.arg=1:nrow(res.acp$eig))
barplot(res.acp$eig[,2], main="Variance expliquée (%)", names.arg=1:nrow(res.acp$eig))
barplot(res.acp$eig[,3], main="Variance expliquée cumulée (%)", names.arg=1:nrow(res.acp$eig))
# Nuage du points du premier plan factoriel pour les variables et les individus
plot(res.acp, graph.type = "classic", choix="var", axes = 1:2, 
     title = "Premier plan factoriel (variables)")
plot(res.acp, graph.type = "classic", choix="ind", axes = 1:2, 
     title = "Premier plan factoriel (individus)")
```



::: {.bloc_aller_loin data-latex=""}
Nous avons vu dans un encadré ci-dessus qu'il est possible d'ajouter des variables et des individus supplémentaires dans une ACP, ce que permet la fonction `PCA` de `FactoMineR` avec les paramètres `ind.sup` et `quanti.sup`. Aussi, pour ajouter des pondérations aux individus ou aux variables, utilisez les paramètres `row.w` et `col.w`. Pour plus d'informations sur ces paramètres, consulter l'aide de la fonction en tapant `?PCA` dans la console de Rstudio.
:::

#### Exploration graphique des résultats de l'ACP avec `factoextra` {#sect12232}

Visuellement, vous avez pu constater que les graphiques ci-dessus (pour les valeurs propres et pour le premier plan factoriel pour les variables et les individus) réalisés avec les fonctions de base `barplot` et `plot` sont peu attrayants. Avec le *package* `factoextra`, quelques lignes de code suffissent pour construire des graphiques bien plus jolis. 

Premièrement, la syntaxe ci-dessous renvoie deux graphiques pour analyser les résultats des valeurs propres (figure \@ref(fig:factoextra1)).

```{r factoextra1, echo=TRUE, fig.align='center', fig.cap="Graphiques pour les valeurs propres avec factoextra", quietly = TRUE, warn.conflicts = TRUE,   message=FALSE, auto_pdf=TRUE, out.width='75%', dev="png", dpi=300}
library(factoextra)
library(ggplot2)
library(ggpubr)

# Graphiques avec factoextra
# Graphiques des variables propres
G1 <- fviz_screeplot(res.acp, choice ="eigenvalue", addlabels = TRUE,
                     x="Composantes",
                     y="Valeur propre",
                    title="")
G2 <- fviz_screeplot(res.acp, choice ="variance", addlabels = TRUE,
                     x="Composantes",
                     y="Pourcentage de la variance expliquée",
                     title="")
ggarrange(G1, G2)
```

Deuxièmement, la syntaxe ci-dessous renvoie trois graphiques pour analyser les contributions de chaque variable aux deux premiers axes de l'ACP (figures \@ref(fig:factoextra2) et \@ref(fig:factoextra3)) et la qualité de représentation des variables sur les trois premiers axes (figure \@ref(fig:factoextra4)), c'est-à-dire la somme des cosinus carrés sur les trois axes retenus.

```{r eval=FALSE, include=TRUE}
# Contributions des variables aux deux premières composantes avec factoextra
fviz_contrib(res.acp, choice = "var", axes = 1, top = 10,
             title = "Contributions des variables à la première composante")
fviz_contrib(res.acp, choice = "var", axes = 2, top = 10,
             title = "Contributions des variables à la première composante")
fviz_cos2(res.acp, choice = "var", axes = 1:3)+
  labs(x="", y="Somme des cosinus carrés sur les 3 axes retenus",
       title ="Qualité de représentation des variables sur les axes retenus de l'ACP")
```


```{r factoextra2, echo=FALSE, fig.align='center', fig.cap="Contributions des variables à la première composante avec factoextra", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
# Contributions des variables aux deux premières composantes
fviz_contrib(res.acp, choice = "var", axes = 1, top = 10,
             title = "Contributions des variables à la première composante")
```

```{r factoextra3, echo=FALSE, fig.align='center', fig.cap="Contributions des variables à la deuxième composante avec factoextra", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
# Contributions des variables aux deux premières composantes
fviz_contrib(res.acp, choice = "var", axes = 2, top = 10,
             title = "Contributions des variables à la deuxième composante")
```

```{r factoextra4, echo=FALSE, fig.align='center', fig.cap="Qualité des variables sur les trois premières composantes avec factoextra", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
fviz_cos2(res.acp, choice = "var", axes = 1:3)+
  labs(x="", y="Somme des cosinus carrés sur les 3 axes retenus",
       title ="Qualité de représentation des variables sur les axes retenus de l'ACP")
```


Troisièmement, le code ci-dessus renvoie un nuage de points pour le premier plan factoriel de l'ACP (axes 1 et 2) pour les variables (figure \@ref(fig:factoextra5)) et les individus (figure \@ref(fig:factoextra6)).

```{r eval=FALSE, include=TRUE}
# Premier plan factoriel pour les variables avec factoextra
fviz_pca_var(res.acp, col.var="steelblue",
             title = "Premier plan factoriel pour les variables")+
  theme_minimal()

fviz_pca_var(res.acp, col.var="contrib",
             title = "Premier plan factoriel pour les variables")+
  scale_color_gradient2(low="#313695", mid="#ffffbf", high="#a50026",
                        midpoint=mean(res.acp$var$contrib[,1]))

# Premier plan factoriel pour les individus avec factoextra
fviz_pca_ind(res.acp, label="none")
fviz_pca_ind(res.acp, col.ind="cos2") +
  scale_color_gradient2(low="blue", mid="white", high="red", midpoint=0.50)
```

```{r factoextra5, echo=FALSE, fig.align='center', fig.cap="Premier plan factoriel pour les variables avec factoextra", auto_pdf=TRUE, dev="png", dpi=300, out.width='100%'}
G1 <- fviz_pca_var(res.acp, col.var="steelblue",
             title = "Premier plan factoriel pour les variables")+
  theme_minimal()

G2 <- fviz_pca_var(res.acp, col.var="contrib",
             title = "Premier plan factoriel pour les variables")+
  scale_color_gradient2(low="#313695", mid="#ffffbf", high="#a50026",
                        midpoint=mean(res.acp$var$contrib[,1]))
ggarrange(G1, G2, ncol=1)
```

```{r factoextra6, echo=FALSE, fig.align='center', fig.cap="Premier plan factoriel pour les individus avec factoextra", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
G1 <- fviz_pca_ind(res.acp, label="none")
G2 <- fviz_pca_ind(res.acp, col.ind="cos2") +
  scale_color_gradient2(low="blue", mid="white", high="red", midpoint=0.50)
ggarrange(G1, G2, ncol=1)
```

#### Personnalisation des graphiques avec les résultats de l'ACP {#sect12233}

Avec un peu plus de code et l'utilisation d'autres *packages* (`ggplot2`, `ggpubr`, `stringr`, `corrplot`), vous pouvez aussi construire des graphiques personnalisés.

Premièrement, la syntaxe ci-dessous permet de réaliser trois graphiques pour analyser les valeurs propres (figure \@ref(fig:acpmesgraphs1)). Notez que, d'un coup d'œil, nous pouvons identifier les composantes principales avec une valeur propre égale ou supérieure à 1. 

```{r eval=FALSE, include=TRUE}
library(ggplot2)
library(ggpubr)
library(stringr)
library(corrplot)

# Calcul de l'ACP
res.acp <- PCA(Data[,2:11], ncp=5, scale.unit=TRUE, graph=F)
print(res.acp)

# Construction d'un dataframe pour les valeurs propres
dfACPvp <- data.frame(res.acp$eig)
names(dfACPvp) <- c("VP","VP_pct","VP_cumupct")
dfACPvp$Composante <- factor(1:nrow(dfACPvp), levels=rev(1:nrow(dfACPvp)))

couleursAxes <- c("steelblue","skyblue2")
vpsup1 <- round(sum(subset(dfACPvp, VP >= 1)$VP),2)
vpsup1cumul <- round(sum(subset(dfACPvp, VP >= 1)$VP_pct),2)

plotVP1 <- ggplot(dfACPvp,aes(x=VP, y=Composante,fill=VP<1))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=1, linetype="dashed", color = "azure4", size=1)+
  scale_fill_manual(name="Valeur\npropre",values=couleursAxes,labels = c(">= 1","< 1"))+
  labs(x="Valeur propre", y="Composante principale")

plotVP2 <- ggplot(dfACPvp, aes(x=VP_pct, y=Composante,fill=VP<1))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  scale_fill_manual(name="Valeur\npropre",values=couleursAxes,labels = c(">= 1","< 1"))+
  theme(legend.position="none")+
  labs(x="Pourcentage de la variance expliquée", y="")

plotVP3 <- ggplot(dfACPvp, aes(x=VP_cumupct, y=Composante,fill=VP<1, group=1))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  scale_fill_manual(name="Valeur\npropre",values=couleursAxes,labels = c(">= 1","< 1"))+
  geom_line(colour="brown", linetype="solid", size=.8) +
  geom_point(size=3, shape=21, color="brown", fill="brown")+
  theme(legend.position="none")+
  labs(x="Pourcentage cumulé de la variance expliquée", y="")

annotate_figure(ggarrange(plotVP1, plotVP2, plotVP3, ncol=2),
                text_grob("Analyse des valeurs propres", 
                         color = "black", face = "bold", size = 12),
                bottom = text_grob(
                         paste0("Somme des valeurs propres supérieures à 1 : ", 
                               vpsup1,
                               ".\nPourcentage cumulé des valeurs propres supérieures à 1 : ", 
                                vpsup1cumul, "%."),
                           color = "black", hjust = 1, x = 1, size = 10))
```

```{r acpmesgraphs1, echo=FALSE, fig.align='center', fig.cap="Graphiques personnalisés pour les valeurs propres", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
library(ggplot2)
library(ggpubr)
library(stringr)

# Calcul de l'ACP
res.acp <- PCA(Data[,2:11], ncp=5, scale.unit=TRUE, graph=F)

# Construction d'un dataframe pour les valeurs propres
dfACPvp <- data.frame(res.acp$eig)
names(dfACPvp) <- c("VP","VP_pct","VP_cumupct")
dfACPvp$Composante <- factor(1:nrow(dfACPvp), levels=rev(1:nrow(dfACPvp)))

couleursAxes <- c("steelblue","skyblue2")
vpsup1 <- round(sum(subset(dfACPvp, VP >= 1)$VP),2)
vpsup1cumul <- round(sum(subset(dfACPvp, VP >= 1)$VP_pct),2)

plotVP1 <- ggplot(dfACPvp,aes(x=VP, y=Composante,fill=VP<1))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=1, linetype="dashed", color = "azure4", size=1)+
  scale_fill_manual(name="Valeur\npropre",values=couleursAxes,labels = c(">= 1","< 1"))+
  labs(x="Valeur propre", y="Composante principale")

plotVP2 <- ggplot(dfACPvp, aes(x=VP_pct, y=Composante,fill=VP<1))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  scale_fill_manual(name="Valeur\npropre",values=couleursAxes,labels = c(">= 1","< 1"))+
  theme(legend.position="none")+
  labs(x="Variance expliquée (%)", y="")

plotVP3 <- ggplot(dfACPvp, aes(x=VP_cumupct, y=Composante,fill=VP<1, group=1))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  scale_fill_manual(name="Valeur\npropre",values=couleursAxes,labels = c(">= 1","< 1"))+
  geom_line(colour="brown", linetype="solid", size=.8) +
  geom_point(size=3, shape=21, color="brown", fill="brown")+
  theme(legend.position="none")+
  labs(x="Variance expliquée (% cumulé)", y="")

  annotate_figure(ggarrange(plotVP1, plotVP2, plotVP3, ncol=2, nrow=2),
                  text_grob("Analyse des valeurs propres", color = "black", face = "bold", size = 12),
                  bottom = text_grob(
                          paste0("Somme des valeurs propres supérieures à 1 : ", vpsup1,
                                 ".\nPourcentage cumulé des valeurs propres supérieures à 1 : ", vpsup1cumul, "%."),
                           color = "black", hjust = 1, x = 1, size = 10))
```

Deuxièmement, la syntaxe ci-dessous permet de&nbsp;: 

- Construire un *dataframe* avec les résultats des variables.
- Construire des histogrammes avec les coordonnées des variables sur les axes factoriels (figure \@ref(fig:acpmesgraphs2)). Notez que les coordonnées négatives sont indiquées avec des barres bleues et celles négatives avec des barres de couleur saumon.
- Un graphique avec les contributions des variables sur les axes retenus (figure \@ref(fig:acpmesgraphs3)).
- Un graphique avec les cosinus carrés des variables sur les axes retenus (figure \@ref(fig:acpmesgraphs4)). 
- Un histogramme avec la qualité des variables sur les axes retenus (figure \@ref(fig:acpmesgraphs5)), soit la sommation de leurs cosinus carrés sur les axes retenus.


```{r eval=FALSE, include=TRUE}
# Analyse des résultats de L'ACP pour les variables
# Indiquer le nombre d'axes à conserver suite à l'analyse des valeurs propres
library(corrplot)
library(stringr)
library(ggplot2)
library(ggpubr)
nComp <- 3
# Variance expliquée par les axes retenus
vppct <- round(dfACPvp[1:nComp,"VP_pct"],1)
# Dataframe des résultats pour les variables
CoordsVar <- res.acp$var$coord[, 1:nComp]
Cos2Var   <- res.acp$var$cos2[, 1:nComp]
CtrVar   <- res.acp$var$contrib[, 1:nComp]
dfACPVars <- data.frame(Variable =  row.names(res.acp$var$coord[, 1:nComp]),
                        Coord = CoordsVar,
                        Cos2 = Cos2Var,
                        Qualite = rowSums(Cos2Var),
                        Ctr = CtrVar)
row.names(dfACPVars) <- NULL
names(dfACPVars) <- str_replace(names(dfACPVars), ".Dim.", "Comp")
dfACPVars

# Histogrammes pour les coordonnées
couleursCoords <- c("lightsalmon","steelblue")
plotCoordF1 <- ggplot(dfACPVars,
                      aes(y = reorder(Variable, CoordComp1),
                          x = CoordComp1, fill=CoordComp1<0))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=0, color = "black", size=1)+
  scale_fill_manual(name="Coordonnée",values=couleursCoords,labels = c("Positive","Négative"))+
  labs(x=paste0("Axe 1 (", vppct[1],"%)"), y="Variable")+
  theme(legend.position="none")

plotCoordF2 <- ggplot(dfACPVars,
                      aes(y = reorder(Variable, CoordComp2),
                          x = CoordComp2, fill=CoordComp2<0))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=0, color = "black", size=1)+
  scale_fill_manual(name="Coordonnée",values=couleursCoords,labels = c("Positive","Négative"))+
  labs(x=paste0("Axe 2 (", vppct[2],"%)"), y="Variable")+
  theme(legend.position="none")

plotCoordF3 <- ggplot(dfACPVars,
                      aes(y = reorder(Variable, CoordComp3),
                          x = CoordComp3, fill=CoordComp3<0))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=0, color = "black", size=1)+
  scale_fill_manual(name="Coordonnée", values=couleursCoords,labels = c("Positive","Négative"))+
  labs(x=paste0("Axe 3 (", vppct[3],"%)"), y="Variable")

annotate_figure(ggarrange(plotCoordF1, plotCoordF2, plotCoordF3, nrow=nComp),
                text_grob("Coordonnées des variables sur les axes factoriels",
                          color = "black", face = "bold", size = 12))

# Contributions des variables à la formation des axes
couleurs <- colorRampPalette(c("#ffffd4","#993404"))
corrplot(CtrVar, is.corr=FALSE, method ="square", col = couleurs(20), addCoef.col = 1)

# La qualité des variables sur les composantes retenues : cosinus carrés
corrplot(Cos2Var, is.corr=FALSE, method ="square", col = couleurs(20), addCoef.col = 1)

ggplot(dfACPVars)+
  geom_bar(aes(y=reorder(Variable, Qualite), x=Qualite),
            stat="identity", width = .6, alpha=.8, fill="steelblue")+
  labs(x="", y="Somme des cosinus carrés sur les axes retenus",
       title ="Qualité de représentation des variables sur les axes retenus de l'ACP",
       subtitle = paste0("Variance expliquée par les ", nComp, 
                         " composantes : ", sum(vppct), "%"))
```


```{r acpmesgraphs2, echo=FALSE, fig.align='center', fig.cap="Histogrammes personnalisés avec les coordonnées factorielles pour les variables", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
# Indiquer le nombre d'axes à conserver suite à l'analyse des valeurs propres
nComp <- 3
# Variance expliquée par les axes retenus
vppct <- round(dfACPvp[1:nComp,"VP_pct"],1)
# Dataframe des résultats pour les variables
CoordsVar <- res.acp$var$coord[, 1:nComp]
Cos2Var   <- res.acp$var$cos2[, 1:nComp]
CtrVar   <- res.acp$var$contrib[, 1:nComp]
dfACPVars <- data.frame(Variable =  row.names(res.acp$var$coord[, 1:nComp]),
                        Coord = CoordsVar,
                        Cos2 = Cos2Var,
                        Qualite = rowSums(Cos2Var),
                        Ctr = CtrVar)
row.names(dfACPVars) <- NULL
names(dfACPVars) <- str_replace(names(dfACPVars), ".Dim.", "Comp")

# Histogrammes pour les coordonnées
couleursCoords <- c("lightsalmon","steelblue")
plotCoordF1 <- ggplot(dfACPVars,
                      aes(y = reorder(Variable, CoordComp1),
                          x = CoordComp1, fill=CoordComp1<0))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=0, color = "black", size=1)+
  scale_fill_manual(name="Coordonnée",values=couleursCoords,labels = c("Positive","Négative"))+
  labs(x=paste0("Axe 1 (", vppct[1],"%)"), y="Variable")+
  theme(legend.position="none")

plotCoordF2 <- ggplot(dfACPVars,
                      aes(y = reorder(Variable, CoordComp2),
                          x = CoordComp2, fill=CoordComp2<0))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=0, color = "black", size=1)+
  scale_fill_manual(name="Coordonnée",values=couleursCoords,labels = c("Positive","Négative"))+
  labs(x=paste0("Axe 2 (", vppct[2],"%)"), y="Variable")+
  theme(legend.position="none")

plotCoordF3 <- ggplot(dfACPVars,
                      aes(y = reorder(Variable, CoordComp3),
                          x = CoordComp3, fill=CoordComp3<0))+
  geom_bar(stat="identity", width = .6, alpha=.8, color="black")+
  geom_vline(xintercept=0, color = "black", size=1)+
  scale_fill_manual(name="Coordonnée", values=couleursCoords,labels = c("Positive","Négative"))+
  labs(x=paste0("Axe 3 (", vppct[3],"%)"), y="Variable")

annotate_figure(ggarrange(plotCoordF1, plotCoordF2, plotCoordF3, nrow = nComp),
                text_grob("Coordonnées des variables sur les axes factoriels",
                          color = "black", face = "bold", size = 12))
```

```{r acpmesgraphs3, echo=FALSE, message=FALSE, fig.align='center', fig.cap="Graphiques personnalisés avec les contributions des variables", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
library(corrplot)
couleurs <- colorRampPalette(c("#ffffd4","#993404"))
corrplot(CtrVar, is.corr=FALSE, method ="square", col = couleurs(100), addCoef.col = 1)
```

```{r acpmesgraphs4, echo=FALSE, fig.align='center', fig.cap="Graphiques personnalisés avec les cosinus carrés des variables", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
corrplot(Cos2Var, is.corr=FALSE, method ="square", col = couleurs(100), addCoef.col = 1)
```

```{r acpmesgraphs5, echo=FALSE, fig.align='center', fig.cap="Graphique personnalisé avec la qualité des variables sur les axes retenus de l'ACP", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
ggplot(dfACPVars)+
  geom_bar(aes(y=reorder(Variable, Qualite), x=Qualite),
            stat="identity", width = .6, alpha=.8, fill="steelblue")+
  labs(x="", y="Somme des cosinus carrés sur les axes retenus",
       title ="Qualité de représentation des variables sur les axes",
       subtitle = paste0("Variance expliquée par les ", nComp, 
                         " composantes : ", sum(vppct), "%"))
```

Troisièmement, lorsque les observations sont des unités spatiales, il convient de cartographier les coordonnées factorielles des individus. Dans le jeu de données utilisé, les observations sont des polygones délimitant les îlots regroupés pour l'information statistique (IRIS) pour l'agglomération de Lyon (France). Nous utilisons les *packages* `tmap` et `RColorBrewer` pour réaliser des cartes choroplèthes avec les coordonnées deux premières composantes (figure \@ref(fig:acpmesgraphs6)).


```{r acpmesgraphs6, echo=TRUE, fig.align='center', fig.cap="Cartographie des coordonnées factorielles des individus", auto_pdf=TRUE, dev="png", dpi=300, out.width='75%'}
library("tmap")
library("RColorBrewer")

# Analyse des résultats de l'ACP pour les individus
# Dataframe des résultats pour les individus
CoordsInd <- res.acp$ind$coord[, 1:nComp]
Cos2Ind   <- res.acp$ind$cos2[, 1:nComp]
CtrInd    <- res.acp$ind$contrib[, 1:nComp]
dfACPInd <- data.frame(Coord = CoordsInd, Cos2 = Cos2Ind, Ctr = CtrInd)
names(dfACPInd) <- str_replace(names(dfACPInd), ".Dim.", "Comp")

# Fusion du tableau original avec les résultats de l'ACP pour les individus
CartoACP <- cbind(LyonIris, dfACPInd)

# Cartographie des coordonnées factorielles pour les individus pour les
# deux premières composantes
Carte1 <- tm_shape(CartoACP) +
          tm_polygons(col = "CoordComp1", style = "cont",
                      midpoint = 0, title = 'Coordonnées')+
          tm_layout(title = paste0("Axe 1 (", vppct[1],"%)"),
             attr.outside = TRUE, frame = FALSE)

Carte2 <- tm_shape(CartoACP) +
          tm_polygons(col = "CoordComp2", style = "cont",
                      midpoint = 0, title = 'Coordonnées')+
  tm_layout(title = paste0("Axe 2 (", vppct[2],"%)"),
             attr.outside = TRUE, frame = FALSE)

tmap_arrange(Carte1, Carte2)
```


::: {.bloc_aller_loin data-latex=""}
**Exploration interactive des résultats d'une ACP avec le _package_ `explor`**. 

Vous avez compris qu'il ne suffit pas de calculer une ACP, il faut retenir les *n* premiers axes de l'ACP qui nous semblent les plus pertinents, puis les interpréter à la lecture des coordonnées factorielles, les cosinus carrés et les contributions des variables et des individus sur les axes. Il faut donc bien explorer les résultats à l’aide de tableaux et de graphiques! Cela explique que nous vous avons proposé plusieurs graphiques dans les deux sections précédentes (\@ref(sect12232) et \@ref(sect12233)). 
L’exploration des données d’une ACP peut aussi être réalisée avec des graphiques interactifs. Or, un superbe *package* dénommé `explor` (https://juba.github.io/explor/), reposant sur `Shiny` (https://shiny.rstudio.com/), permet d’explorer de manière interactive les résultats de plusieurs méthodes factorielles calculés avec `FactorMinerR`. Pour cela, il vous suffit de lancer les deux lignes de code suivantes&nbsp;:

`library(explor)`

`explor(res.acp)`
:::

## Analyses factorielles de correspondances (AFC)  {#sect123}

## Analyses factorielles de correspondances multiples (AFM)  {#sect124}

## Complémentarité entre analyses factorielles et méthodes de classification  {#sect125}

