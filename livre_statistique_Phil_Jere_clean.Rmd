--- 
title: "Introduction aux méthodes quantitatives en sciences sociales avec R"
author: "Philippe Apparicio et Jérémy Gelb"
date: "`r Sys.Date()`"
colorlinks: yes
cover-image: images/cover.png
bibliography: book.bib
description: Ce livre propose une introduction aux méthodes quantitatives en sciences
  sociales basée sur le logiciel ouvert R. Le contenu est pensé pour être accessible
  même à ceux n'ayant presque aucune base en statistique ou en programmation. Les
  personnes plus expérimentées y découvriront également des sections sur des méthodes
  poussées comme les modèles généralisés additifs à effets mixtes ou les méthodes
  factorielles mixtes. Ceux cherchant à passer à R et abandonner SPSS, SAS ou STATA
  trouveront dans cet ouvrage les éléments pour une transition en douceur. La philosophie
  de se livre est de donner toutes les clefs de compréhension et de mise en œuvre
  des méthodes abordées afin de faciliter l'assimilation par le lecteur. La présentation
  des méthodes est basée sur une approche compréhensive et intuitive plutôt que mathématique
  sans pour autant que la rigueur statistique ne soit négligée. Servez-vous votre
  boisson chaude ou froide favorite, installez-vous dans votre meilleur fauteuil et
  bonne lecture !
documentclass: book
fontsize: 11pt
github-repo: LAEQ/livre_statistique_Phil_Jere
graphics: yes
link-citations: yes
lof: yes
lot: yes
mainfont: Palatino Linotype
monofont: "Source Code Pro"
monofontoptions: "Scale=0.8"
mathfontoptions: "Scale=0.1"
site: bookdown::bookdown_site
biblio-style: apalike
url: https\://bookdown.org/yihui/rmarkdown/
geometry: "left=2cm,right=2cm,top=2cm,bottom=2cm"
toc-title: "Table des matières"
lang: fr
---


# Préface {-}

```{asis, echo=identical(knitr:::pandoc_to(), 'html')}

Ce livre propose une introduction aux méthodes quantitatives en sciences sociales basée sur le logiciel ouvert R. Il a d'ailleurs été écrit intégralement dans R avec **rmarkdown**. Le contenu est pensé pour être accessible même à ceux n'ayant presque aucune base en statistique ou en programmation. Les personnes plus expérimentées y découvriront également des sections sur des méthodes plus avancées comme les modèles généralisés additifs à effets mixtes ou les méthodes factorielles mixtes. Ceux cherchant à passer à R pour enfin abandonner SPSS, SAS ou STATA trouveront dans cet ouvrage les éléments pour une transition en douceur. La philosophie de se livre est de donner toutes les clefs de compréhension et de mise en œuvre des méthodes abordées afin de facilité l'assimilation par le lecteur. La présentation des méthodes est basée sur une approche compréhensive et intuitive plutôt que mathématique sans pour autant que la rigueur statistique ne soit négligée. Servez-vous votre boisson chaude ou froide favorite, installez-vous dans votre meilleur fauteuil et bonne lecture !


::: {.bloc_notes data-latex=""}

**Ce livre est un projet en cours d'écriture !**

Il sert de matériel pour le cours **Méthodes quantitatives appliquées aux études urbaines** (EUR8219), dispensé au Centre INRS Urbanisation Culture Société de l'INRS. Son contenu est amené à changer et des erreurs peuvent encore être présentes. À terme, nous espérons le publier. Par conséquent, **son contenu ne peut donc en aucun cas être partagé en dehors du cours**. Vos commentaires et suggestions sur le contenu et la forme sont bienvenus ! Si certains passages vous semblent peu clairs, n'hésitez à nous en faire part.

:::

```

## Comment lire ce livre {-}

Si vous googlez l'expression « comment lire un livre ? », vous trouverez une multitude de conseils et astuces. Pour ce livre, nous conseillons de le lire de gauche à droite et page par page. Plus sérieusement, il comprend plusieurs types de blocs de texte qui, on l'espère, faciliteront la lecture.


::: {.bloc_package data-latex=""}
**Bloc packages**: habituellement localisé en début du chapitre, il comprend la liste des packages R utilisés pour un chapitre.
:::

::: {.bloc_objectif data-latex=""}
**Bloc objectif**: comprend une description des objectifs d'une section.
:::

::: {.bloc_notes  data-latex=""}
**Bloc notes**: comprend une information secondaire sur une notion, un élément, une idée abordée dans une section.
:::

::: {.bloc_aller_loin data-latex=""}
**Bloc pour aller plus loin** : peut comprendre des références ou des extensions d'une méthode statistique abordée dans une section.
:::

::: {.bloc_astuce data-latex=""}
**Bloc astuce**: décrit un élément qui vous facilera le vie : une propriété statistique, un *package*, une fonction, une syntaxe R.
:::

::: {.bloc_attention data-latex=""}
**Bloc attention**:  comprend une notion ou un élément important à bien maîtriser.
:::



## Structure du livre {-}

À écrire plus tard.

## Remerciements {-}

Note au beau Cargo (chien Mira) qui nous supporte dans l'écriture du livre !
```{r cargo, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Cargo, le plus beau",  out.width='50%'}
knitr::include_graphics('images/Cargo.jpg', dpi = NA)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
## quelques parametres generaux en fonction du type de document
library(tidyverse)

if (knitr::is_latex_output()){
  font_size_table <- 8
}else{
  font_size_table <- 14
}

clean_df_latex <- function(df){
  if(inherits(df, "list") == F){
    df <- list(df)
  }
  df2 <- lapply(df, function(i){
    i %>% 
      mutate_if(~is.character(.), funs(str_replace_all(., "`", ""))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "\\*{2}([^\"]*)\\*{2}", replacement = "\\\\textbf{\\1}"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "\\*{2}([^\"]*)\\*{2}", replacement = "\\\\textbf{\\1}"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "%", "\\%"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "&", "\\\\&"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "\\^(?=(?:[^$]*$[^$]*$)*[^$]*$)", "\\\\textasciicircum{}"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "\\*{2}", "doublestars"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "\\*([^\"]*?)\\*", replacement = "\\\\textit{\\1}"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "doublestars", "**"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "\\\\@ref\\(([^\"]*?)\\)", "\\\\ref{\\1}"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "\\(\\%\\)", "(\\\\%)"))) %>%
      mutate_if(~is.character(.), funs(str_replace_all(., "_", "\\\\_")))
  })
  return(df2)
}

show_table <- function(df, col.names = NA, caption = NULL, col.to.resize = NULL, 
                       col.width = NULL, digits = getOption("digits"),
                       align = NULL){
  ## dans le cas d'un tableau LATEX
  if (knitr::is_latex_output()){
    df <- data.frame(df)
    #etape1 : supprimer les caracteres speciaux, echapper les gras et italic
    df2 <- clean_df_latex(df)
    
    if(is.na(col.names)==F){
      col.names <- gsub("\\(\\%\\)","(\\\\%)",col.names)
      col.names <- gsub("\\%","\\\\%",col.names)
    }
    if(is.null(caption)==F){
      caption <- gsub("\\*{2}([^\"]*)\\*{2}","\\\\textbf{\\1}", caption)
      caption <- gsub("\\*([^\"]*?)\\*","\\\\textit{\\1}", caption)
    }
    table1 <- knitr::kable(
      df2, booktabs = TRUE,
      format = "latex",
      digits = digits,
      format.args = list(decimal.mark = ",", big.mark = " "),
      valign = 't', row.names = FALSE,
      align = align,
      col.names = col.names,
      caption = caption,
      escape = FALSE) %>% 
      kableExtra::kable_styling(font_size = 8,protect_latex = T)
    if (is.null(col.to.resize) == FALSE){
      table1 <- table1 %>% 
        kableExtra::column_spec(col.to.resize, width=col.width)
    }
  ## dans le cas d'un tableau HTLM
  }else{
    table1 <- knitr::kable(
      df, booktabs = TRUE,
      format.args = list(decimal.mark = ",", big.mark = " "),
      col.names = col.names,
      align = align,
      valign = 't', row.names = FALSE,
      caption = caption,
      escape = FALSE)
  }
  return(table1)
}

```

<!--chapter:end:index.Rmd-->

# À propos des auteurs {#auteurs .unnumbered}

**Philippe Apparicio** (<http://www.ucs.inrs.ca/philippe-apparicio>) est professeur titulaire au Centre Urbanisation Culture Société de l'INRS (<http://www.ucs.inrs.ca/>). Il enseigne au programme de maîtrise en études urbaines (<http://www.ucs.inrs.ca/ucs/etudier/programmes/etudes-urbaines>) les cours *méthodes quantitatives appliquées aux études urbaines* et _analyses spatiales appliquées aux études urbaines_. Il a aussi créé et enseigné, il y a plusieurs années, le cours _systèmes d'information géographique appliqués aux études urbaines_. Durant les dernières années, il a offert plusieurs formations aux Écoles d'été du Centre interuniversitaire québécois de statistiques sociales (CIQSS, <https://www.ciqss.org/>). Titulaire de la Chaire de recherche du Canada (niveau 2) sur l'équité environnementale et la ville, il est le directeur du **laboratoire d'équité environnementale** (<http://laeq.ucs.inrs.ca>). Géographe de formation, ses intérêts de recherche actuels incluent la justice et l'équité environnementale, la pollution atmosphérique et le bruit et le vélo en ville. Il a publié une centaine d'articles dans différents domaines des études urbaines et de la géographie.

**Jérémy Gelb** est candidat au doctorat en études urbaines à l’INRS (sous la supervision de Philippe Apparicio) et membre du **laboratoire d'équité environnementale** (<http://laeq.ucs.inrs.ca>). Son sujet de thèse porte sur l’exposition des cyclistes aux pollutions atmosphériques et sonores en milieu urbain. Il utilise quotidiennement des systèmes d’information géographique (SIG) et est tombé dans la marmite de l'*open source* avec le triptyque QGIS, R et Python au début de sa maîtrise. Il a récemment développé deux packages R : **geocmeans** et **spNetwork**, permettant respectivement d’effectuer des analyses de classification floue non-supervisée pondérée spatialement et des estimations de densité par kernel sur réseau.

Philippe et Jérémy travaillent étroitement ensemble depuis déjà plusieurs années. Avec d’autres collègues, ils ont copubliés plusieurs articles [@2020_1; @2020_2; @2020_3; @2019_1; @2019_2; @2019_3; @2020_1; @2020_2; @2018_1; @2017_1; @2016_1]. Tous deux s’intéressent à l’exposition des cyclistes à la pollution atmosphérique et sonore dans plusieurs villes à travers le monde : Philippe ayant une préférence pour les collectes dans les villes du Sud Global (notamment indiennes, africaines et latino-américaines) et Jérémy dans les villes du Nord (européennes et nord-américaines).


```{r JerPhil, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Philippe Apparicio et Jérémy Gelb lors d’une collecte à New Delhi",  out.width='33%'}
knitr::include_graphics('images/01_about_figure01.jpg', dpi = NA)
```

<!--chapter:end:00-auteurs.Rmd-->

# (PART) Découverte de R {-} 

# Prise en main de R {#chap01}

Dans ce chapitre, nous reviendrons brièvement sur l’histoire de R et la philosophie qui entoure le logiciel. Nous donnerons quelques conseils pour son installation et la mise en place d’un environnement de développement. Nous présenterons les principaux objets qui sous-tendent le travail effectué avec R (dataframe, vecteur, matrice, etc.) et comment les manipuler avec des exemples appliqués. Enfin, nous terminerons cette section avec un tour d’horizon des capacités graphiques de R. Si vous maîtrisez déjà R, nullement besoin de lire ce chapitre !

::: {.bloc_package data-latex=""}
Dans cette section, nous utiliserons principalement les *packages* suivants : 

* Pour importer des fichiers externes :
  - **foreign** pour entre autres les fichiers *dbase* et ceux des logiciels SPSS et Stata
  - **sas7bdat** pour les fichiers du logiciel SAS
  - **xlsx** pour les fichiers Excel
* Pour manipuler des chaînes de caractères et des dates : 
  - **stringr** pour les chaînes de caractères
  - **lubridate** pour les dates
* Pour manipuler des données :
  - **dplyr ** du **tidyverse** propose une grammaire pour manipuler et structurer des données.
:::

## Histoire et philosophie de R{#sect011}

R est à la fois un langage de programmation et un logiciel libre (sous la licence publique générale GNU) dédié à l'analyse statistique et supporté par une fondation : _R foundation for Statistical computing_. Il est principalement écrit en C et Fortran.


R a été créé par Ross Ihaka et Robert Gentleman à l'Université d'Auckland en Nouvelle-Zélande. Si vous avez un jour l'occasion de passer dans le coin, une plaque est affichée dans le département de statistique de l'université, ça mérite le détour (figure \@ref(fig:fig01)). Une première version a été publiée en 1996, mais la première version stable ne date que de 2000, il s'agit donc d'un logiciel relativement récent si on le compare à ses concurrents SAS (1976), SPSS (1968) et Stata (1984).

```{r fig01, echo=FALSE, fig.align='center', fig.cap="Lieu de pélerinage de R", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='40%'}
library(dplyr)
knitr::include_graphics('images/introduction/plaque.jpg', dpi = NA)
```

R a cependant réussi à s'imposer tant dans la milieu de la recherche que dans le secteur privé. Pour s'en convaincre, il suffit de lire l'excellent article concernant la popularité des logiciels d'analyse de données tiré du site [r4stats.com](http://r4stats.com/articles/popularity){target="_blank"} (figure \@ref(fig:fig02)). 

```{r fig02, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Nombre d'articles trouvés sur Google Scholar (Source : Robert A. Muenchen)",  out.width='50%'}
knitr::include_graphics('images/introduction/r_citations.jpg', dpi = NA)
```

Les nombreux atouts de R justifient largement sa popularité sans cesse croissante : 

* R est un logiciel à code source ouvert (*open source*) et ainsi accessible à tous gratuitement.
* Le développement du langage R est centralisé, mais la communauté peut créer et partager facilement des *packages*. Les nouvelles méthodes sont ainsi rapidement implémentées comparativement aux logiciels propriétaires.
* R est un logiciel multi-plateforme, fonctionnant sur Linux, Unix, Windows et Mac.
* Comparativement à ses concurrents, R dispose d'excellentes solutions pour manipuler des données et réaliser des graphiques.
* R dispose de nombreuses interfaces lui permettant de communiquer, notamment avec des systèmes de bases de données SQL et non SQL (MySQL, PostgresSQL, MongoDB, etc.), à des systèmes de *big data* (Spark, Hadoop), à des systèmes d'information géographique (QGIS, ArcGIS) et même à des services en ligne comme Microsoft Azure ou Amazon AWS.
* R est un langage de programmation à part entière, ce qui lui donne plus de flexibilité que ses concurrents dans le domaine privé (SPSS, SAS, STATA). Avec R, vous pouvez accomplir des tâches aussi variées que : monter un site web, créer un robot collectant des données en ligne, effectuer des analyses qualitatives, combiner des fichiers PDF, composer des diapositives pour une présentation ou même éditer un livre (comme celui-ci), mais aussi, réaliser des analyses statistiques.

Un des principaux attrait de R est la quantité astronomique de *packages* actuellement disponibles. Un *package* est un ensemble de nouvelles fonctionnalités développées par un ou plusieurs utilisateurs de R et mises à disposition de l'ensemble de la communauté. Par exemple, le *package* **ggplot2** est dédié à la réalisation de graphiques; les *packages* **data.table** et **plyr** permettent de manipuler des tableaux de données; le *package* **car** apporte de nombreux outils pour faciliter l'analyse de modèles de régressions, etc. Ce partage des *packages* rend accessible à tous des méthodes d'analyses complexes et récentes et favorise grandement la reproductibilité de la recherche. Cependant, ce fonctionnement implique quelques désavantages : 

* il existe généralement plusieurs *packages* pour effectuer le même type d'analyse, ce qui peut devenir une source de confusion;
* certains *packages* cessent d'être mis à jour au fil des années, ce qui nécessite de leur trouver d'autres alternatives (et ainsi apprendre la syntaxe des nouveaux *packages*);
* il est impératif de s'assurer de la fiabilité des *packages* que vous souhaitez utiliser, car n'importe qui peut proposer un *package*.

Il nous semble important de relativiser d'emblée la portée du dernier point. Il est rarement nécessaire de lire et analyser le code source d'un *package* pour s'assurer de sa fiabilité. Nous ne sommes pas des spécialistes de tous les sujets et il peut être extrêmement ardu de comprendre la logique d'un code écrit par quelqu'un d'autre. Nous vous recommandons donc de privilégier l'utilisation de *packages* qui :

* ont fait l'objet d'une publication dans une revue à comité de lecture ou qui ont déjà été cités dans des études ayant fait l'objet d'une publication revue par les pairs;
* font partie de projets comme [ROpensci](https://ropensci.github.io/reproducibility-guide/sections/introduction/){target="_blank"} prônant la vérification  par les pairs ou subventionnés par des organisations comme [R Consortium](https://www.r-consortium.org/){target="_blank"}.
* sont disponibles sur l'un des deux principaux répertoires de *packages* R [CRAN](https://cran.r-project.org/){target="_blank"} ou [Bioconductor](https://www.bioconductor.org/){target="_blank"}.

Toujours pour nuancer notre propos, il convient de distinguer *package* de *package*! Certains d'entre eux sont des ensembles très complexes de fonctions permettant de réaliser des analyses poussées alors que d'autres sont des projets plus modestes dont l'objectif principal est de simplifier le travail des utilisateurs. Ces derniers ressemblent à des petites boites à outils et font généralement moins l'objet d'une vérification intensive.

Pour conclure cette section, l'illustration partagée sur Twitter par Darren L Dahly résume avec humour la force du logiciel R et de sa communauté (\@ref(fig:fig03)) : R apparait clairement comme une communauté hétéroclyte, mais diversifiée et adaptable.

```{r fig03, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Métaphore sur les langages et programmes d'analyse statistique",  out.width='60%'}
knitr::include_graphics('images/introduction/softwares_and_cars.jpeg', dpi = NA)
```

Dans ce livre, nous détaillerons les **packages** utilisés dans chaque section avec un encadré spécifique, accompagné de l'icône suivant : 

```{r fig04, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Icône des encadrés dédiés aux packages",  out.width='20%'}
knitr::include_graphics('css/images/package.png', dpi = NA)
```

## Environnement de travail{#sect012}

Dans cette section, nous vous proposons une visite de l'environnement de travail classique  R.

### Installer R {#sect0121}

La première étape pour travailler avec R est bien sûr de l'installer. Pour ce faire, il suffit de visiter le site web de [CRAN](https://cran.r-project.org/){target="_blank"} et de télécharger la dernière version de R en fonction de votre système d'exploitation : Windows, Linux ou Mac. Une fois installé, si vous démarrez R immédiatement, vous aurez alors accès à une console, plutôt rudimentaire, attendant sagement vos instructions (figure \@ref(fig:fig05)).

```{r fig05, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="La console de base de R",  out.width='85%'}
knitr::include_graphics('images/introduction/r_console.jpeg', dpi = NA)
```

Notez que vous pouvez aussi télécharger des version plus anciennes de R en allant sur ce [lien](https://cran.r-project.org/bin/windows/base/old/){target="_blank"}. Ceci peut être intéressant lorsque vous voulez reproduire des résultats d'une autre étude ou que certains *packages* ne sont plus disponibles dans les nouvelles versions.

### L'environnement RStudio{#sect0122}

Rares sont les utilisateurs de R qui préfèrent travailler directement avec la console classique. Nous vous recommandons vivement d'utiliser RStudio, soit un environnement de développement dédié à R, offrant une intégration très intéressante d'une console, d'un éditeur de texte, d'une fenêtre de visualisation des données, d'une autre pour les graphiques, d'un accès à la documentation, etc. En d'autres termes, si R est un vélo minimaliste, RStudio permet d'y rajouter des freins, des vitesses, un porte-bagage, des gardes-boues et une selle confortable. Vous pouvez [télécharger](https://rstudio.com/products/rstudio/download){target="_blank"} et installer RStudio sur Windows, Linux et Mac. La version de base est gratuite, mais l'entreprise qui développe ce logiciel propose aussi des versions commerciales du logiciel qui assurent essentiellement un support technique. Il existe d'autres environnements de développement pour travailler avec R (VisualStudio, Jupyter, Tinn-R, Radiant, RIDE, etc.), mais RStudio offre à ce jour la meilleure option en terme de facilité d'installation, de prise en main et de fonctionnalités proposées (voir l'interface de RStudio à la figure \@ref(fig:fig06)).

```{r fig06, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Environnement de base de RStudio",  out.width='85%'}
knitr::include_graphics('images/introduction/r_studio_01.jpeg', dpi = NA)
```

Avant d'aller plus loin, notez que : 

* La console actuellement ouverte dans RStudio vous informe de la version de R que vous utilisez. Vous pouvez en effet avoir plusieurs versions de R installées sur votre ordinateur et passer de l'une à l'autre avec RStudio. Pour cela, naviguez dans l'onglet *Tools / Global Options* et dans le volet  *General*, vous pouvez sélectionner la version de R que vous souhaitez utiliser.
* L'aspect de RStudio peut être modifié en navigant dans l'onglet *Tools / Global Options* et dans le volet *Appearance*. Nous avons une préférence pour le mode sombre avec le style *pastel on dark*, mais libre à chacun de choisir le style qui lui convient.

```{r fig07, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="RStudio avec le style pastel on dark",  out.width='85%'}
knitr::include_graphics('images/introduction/r_studio_02.jpeg', dpi = NA)
```

Une fois ces détails réglés, vous pouvez ouvrir votre première feuille de code en allant dans l'onglet *File / New File/ R Script*, votre environnement est maintenant découpé en quatre fenêtres (figure \@ref(fig:fig08)) : 

1. L'éditeur de code, vous permettant d'écrire le script que vous voulez exécuter et permettant de garder une trace de votre travail. Ce script peut être enregistré sur votre ordinateur avec l'extension **.R**, mais ce n'est qu'un simple fichier texte.
2. La console vous permettant d'exécuter votre code R et de voir les résultats s'afficher au fur et à mesure.
3. La fenêtre d'environnement vous montrant les objets, fonctions et jeux de données actuellement disponibles dans votre session (chargés dans la mémoire vive).
4. La fenêtre de l'aide, des graphiques et de l'explorateur de fichiers. Vous pouvez accéder ici à la documentation de R et des *packages* que vous utilisez, aux sorties graphiques que vous produisez et aux dossier de votre environnement de travail.

```{r fig08, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Les quatre fenêtres de RStudio",  out.width='85%'}
knitr::include_graphics('images/introduction/r_studio_03.jpeg', dpi = NA)
```

Prenons un bref exemple, tapez la syntaxe suivante dans l'éditeur de code (fenêtre 1 à la figure \@ref(fig:fig08)) : 

```{r ma_somme}
ma_somme <- 4+4
```

Sélectionnez ensuite cette syntaxe (mettre en surbrillance avec votre souris), quand vous utilisez le raccourci *Ctrl+Enter* ou cliquez sur le bouton *Run* (avec la flèche verte), cette syntaxe est envoyée à la console qui l'exécute immédiatement. Notez que rien ne se passe tant que le code n'est pas envoyé à la console. Il s'agit donc de deux étapes distinctes : écrire son code, puis l'envoyer à la console. Vous constaterez également qu'un objet *ma_somme* est apparu dans votre environnement et que sa valeur est bien 8. Votre console se "souvient" de cette valeur, elle est actuellement stockée dans votre mémoire vive sous le nom de *ma_somme* (figure \@ref(fig:fig09)).

```{r fig09, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Les quatre fenêtres de RStudio",  out.width='90%'}
knitr::include_graphics('images/introduction/r_studio_04.jpeg', dpi = NA)
```

Pour conclure cette section, nous vous invitons à enregistrer votre première syntaxe R (*File / Save As*) dans un fichier **.R** que vous pouvez appeler par exemple "mon_premier_script.R". Fermez ensuite RStudio, redémarrez le et ouvrez (*File / Open File*) votre fichier "mon_premier_script.R". Vous pouvez constater que votre code est toujours présent, mais que votre environnement est vide tant que vous n'exécutez pas votre syntaxe. En effet, lorsque vous fermez RStudio, l'environnement est vidé pour libérer de la mémoire vive. Ceci peut poser problème lorsque certains codes sont très longs à exécuter, nous verrons donc plus tard comment enregistrer l'environnement en cours pour le recharger par la suite.


### Installer et charger un *package*{#sect0123}

Dans la section sur la Philosophie de R, nous avons souligné la place centrale jouée par les *packages*. Voyons ensemble comment installer un *package* intitulé **lubridate**, qui nous permettra plus tard de manipuler des données temporelles.

#### Installer un *package* depuis CRAN{#sect01231}

Pour installer un *package*, vous devez être connecté à internet; en effet, R va accéder au répertoire de *packages* *CRAN* pour télécharger le *package* et l'installer sur votre machine. Cette opération est réalisée avec la fonction `install.packages`.

```{r message=FALSE, warning=FALSE, eval=FALSE}
install.packages("lubridate")
```

Notez qu'une fois que le *package* est installé, vous n'aurez plus besoin de le refaire. Le *package* est disponible localement sur votre ordinateur, à moins de le désinstaller explicitement avec la fonction `remove.packages`.

#### Installer un *package* depuis GitHub{#sect01232}

*CRAN* est le répertoire officiel des *packages* de R. Vous pouvez cependant télécharger des *packages* provenant d'autres sources. Très souvent, les *packages* sont disponibles sur le site web [GitHub](https://github.com/){target="_blank"} et l'on peut même y trouver des versions en développement avec des fonctionnalités encore non intégrées dans la version sur *CRAN*. Reprenons le cas de **lubridate**, sur GitHub, il est disponible à la page [suivante](https://github.com/tidyverse/lubridate){target="_blank"}. Pour l'installer nous devons d'abord installer un autre *package* appelé **devtools** (depuis *CRAN*).

```{r message=FALSE, warning=FALSE, eval=FALSE}
install.packages("devtools")
```

Maintenant que nous disposons de **devtools**, nous pouvons utiliser la fonction d'installation `devtools::install_github` pour directement télécharger **lubridate** depuis GitHub.

```{r message=FALSE, warning=FALSE, eval=FALSE}
devtools::install_github("tidyverse/lubridate")
```

#### Charger un *package* {#sect01233}

Maintenant que **lubridate** est installé, nous pouvons le charger dans notre session actuelle de R et accéder aux fonctions qu'il propose. Pour cela, suffit d'utiliser la fonction `library`. Notez que conventionnellement, l'appel des *packages* se fait au tout début du script que vous rédigez. Rien ne vous empêche de le faire au fur et à mesure de votre code mais vous perdez alors en lisibilité.  

```{r message=FALSE, warning=FALSE}
library(lubridate)
```

Si vous obtenez un message d'erreur du type : 

<span class="error_message">Error in library(mon_package) : aucun *package* nommé ‘mon_package’ n'est trouvé</span>

C'est que le *package* que vous tentez de charger n'est pas encore installé sur votre ordinateur. Dans ce cas, réessayer de l'installer avec la fonction `install.packages`. Si le problème persiste, vérifiez que vous n'avez pas fait de faute de frappe dans le nom du *package.* Vous pouvez également redémarrer RStudio et réessayer d'installer le *package*.

### Obtenir de l'aide

Lorsque vous installez des *packages* dans R, vous téléchargez aussi leur documentation. Tous les *packages* de *CRAN* disposent d'une documentation, mais ceci n'est pas forcément vrai pour *GitHub*. Dans RStudio, vous pouvez accéder à la documentation des *packages* dans l'onglet **Packages** (figure \@ref(fig:fig010)). Vous pouvez utiliser la barre de recherche pour retrouver rapidement un *package* installé. Si vous cliquez sur le nom du *package*, vous accédez directement à sa documentation dans cette fenêtre.

```{r fig010, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Description des packages",  out.width='45%'}
knitr::include_graphics('images/introduction/rstudio_packages.jpeg', dpi = NA)
```

Vous pouvez également accéder à ces informations en utilisant la syntaxe suivante dans votre console : 

```{r eval = FALSE}
help(package = 'lubridate')
```

Souvent, vous aurez besoin d'accéder à la documentation d'une fonction spécifique d'un *package*. Affichons la documentation de la fonction `now` de **lubridate** : 

```{r eval = FALSE}
help(now, package = 'lubridate')
```

ou plus simplement :
```{r eval = FALSE}
?lubridate::now
```


Vous pouvez aussi utiliser le raccourci suivant : 

```{r eval = FALSE}
?now
```

Si vous connaissez le nom d'une fonction, mais vous ne vous souvenez plus à quel *package* elle appartient, lancez une recherche en utilisant un double point d'interrogation : 

```{r eval = FALSE}
??now
```

Vous allez ainsi découvrir que la fonction `now` n'existe pas que dans **lubridate**, ce qui souligne l'importance de bien connaître les *packages* que l'on installe et que l'on charge dans notre session ! 

Maintenant que nous avons fait le tour de l'environnement de travail, nous allons pouvoir entamer les choses sérieuses avec les bases du langage R.

## Les bases du langage R {#sect013}

R est un langage de programmation. Il vous permet de communiquer avec votre ordinateur pour lui donner des tâches à accomplir. Dans cette section, nous aborderons les bases du langage. Ce type de section introductive à R est présente dans tous les manuels sur R ; elle est donc incontournable. À la première lecture, elle vous semblera probablement aride, et ce, d'autant plus que nous ne réalisons pas d'analyse à proprement parler. Gardez en tête que l'analyse de données requiert au préalable une phase de structuration de ces dernières, opération qui nécessite la maîtrise des notions abordées dans cette section. Nous vous recommandons une première lecture de ce chapitre pour comprendre quelles manipulations vous pouvez effectuer avec R, la lecture des chapitres suivants dédiés aux statistiques, puis de consulter à nouveau cette section au besoin. Notez aussi que la maîtrise des différents objets et opérations de base de R ne s’acquiert qu'en pratiquant. Vous gagnerez cette expertise au fil de vos prochains codes R, période durant laquelle vous pourrez consulter ce chapitre tel un guide de références des différents objets et notions fondamentales de R.


### Hello World ! {#sect0131}

Une introduction à un langage de programmation se doit de commencer par le rite de passage **Hello World**. Il s'agit d'une forme de tradition consistant à montrer aux nouveaux utilisateurs comment afficher le message "Hello World" à l'écran avec le langage en question.

En C, cela donne : 
```
#include <stdio.h>

main()
{
    printf("hello, world\n");
}
```
En COBOL : 
```
IDENTIFICATION DIVISION.
PROGRAM-ID. HELLO-WORLD.

ENVIRONMENT DIVISION.

DATA DIVISION.

PROCEDURE DIVISION.
    DISPLAY "Hello, world!".
    STOP RUN.
```

et plus simplement en R :
```{r}
print("Hello World")
```

Bravo ! Vous venez officiellement de faire votre premier pas dans R !

### Objets et expressions {#sect0132}

Dans R, nous passons notre temps à manipuler des **objets** à l'aide d'**expressions**. Prenons un exemple concret, si vous tapez la syntaxe `4 + 3`, vous manipulez deux objets (4 et 3) au travers d'une expression indiquant que vous souhaitez obtenir la somme des deux objets.

```{r}
4 + 3
```

Cette expression est correcte, R comprends vos indications et effectue le calcul.

Il est possible d'enregistrer le résultat d'une expression et de la conserver dans un nouvel objet. On appelle cette opération déclarer une variable.

```{r}
ma_somme <- 4 + 3
```

Concrêtement, nous venons de demander à R d'enregistrer le résultat de `4 + 3` dans un espace spécifique de notre mémoire vive. Si vous regardez dans votre fenêtre **Environment**, vous verrez en effet qu'un objet appelé ma_somme est actuellement en mémoire et a pour valeur 7.

Notez ici que le nom des variables ne peut être composé que de lettres, de chiffres, de points (.) et de tiret bas (_) et doit commencer par une lettre. R est sensible à la case, en d'autre termes, les variables `Ma_somme`, `ma_sommE`, `ma_SOMME`, et `MA_SOMME` renvoient toutes à un objet différent. Attention donc aux fautes de frappes. Si vous déclarez une variable en utilisant le nom d'une variable existante, la première est écrasée par la seconde : 

```{r}
age <- 35
age

age <- 45
age
```
Attention donc aux noms de variables que vous utilisez et réutilisez.


Réutilisons notre objet `ma_somme` dans une nouvelle expression : 

```{r}
ma_somme2 <- ma_somme + ma_somme
```

Avec cette nouvelle expression, nous indiquons à R que nous souhaitons déclarer une nouvelle variable appelée `ma_somme2`, et que cette variable aura pour valeur `ma_somme + ma_somme`, soit `7 + 7`. Sans surprise, `ma_somme2` a pour valeur 14.

Notez que la mémoire vive (l'environnement) est vidée lorsque vous fermez R. En d'autres termes, R perd complètement la mémoire lorsque vous le fermez. Vous pouvez bien sûr recréer vos objets en relançant les mêmes syntaxes. C'est pourquoi vous devez conserver vos feuilles de codes et ne pas seulement travailler dans la console. La console ne garde aucune trace de votre travail. Pensez donc à bien enregistrer votre code !

Nous verrons dans un autre chapitre comment sauvegarder des objets et les recharger dans une session ultérieure de R (LIEN SECTION). Ce type d'opération est pertinent quand le temps de calcul nécessaire à la production de certains objets est très long.

### Fonctions et arguments {#sect0_133}

Dans R, nous manipulons le plus souvent nos objets avec des **fonctions**. Une fonction est elle-même un objet, mais qui a la particularité de pouvoir effectuer des opérations sur d'autres objets. Par exemple, déclarons l'objet `taille` avec une valeur de 175.897 : 

```{r}
taille <- 175.897
```

Nous allons utiliser la fonction `round` dont l'objectif est d'arrondir un nombre à virgule pour obtenir un nombre entier.

```{r}
round(taille)
```

Pour effectuer leurs opérations, les fonctions ont généralement besoin d'**arguments**. Ici, `taille` est un argument passé à la fonction `round`. Si nous regardons la documentation de `round` avec `help(round)`, nous constatons que cette fonction prend en réalité deux argments : *x* et *digits*. Le premier est le nombre que nous souhaitons arrondir et le second le nombre de décimales à conserver. On peut lire dans la documentation que la valeur par défaut de *digits* est 0, ce qui explique que `round(taille)` a produit le résultat de 176.

```{r fig011, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Arguments de la fonction round",  out.width='45%'}
knitr::include_graphics('images/introduction/help_round.jpeg', dpi = NA)
```

Réutilisons maintenant la fonction `round` mais en gardant une décimale : 

```{r}
round(taille, digits = 1)
```
Il est aussi possible que certaines fonctions ne requièrent pas d'arguments. Par exemple, la fonction `now` va indiquer la date précise (avec l'heure) et n'a besoin d'aucun argument pour le faire : 

```{r}
now()
```

Par contre, si nous essayons de lancer la fonction `round` sans argument, nous obtiendrons une erreur : 

```{r eval = FALSE}
round()
```
<span class = "error_message">Erreur : 0 arguments passed to 'round' which requires 1 or 2 arguments</span>

Le message est très clair, `round` a besoin d'au moins un argument pour fonctionner. Si au lieu d'un nombre, nous avions donné du texte à la fonction `round`, nous aurions aussi obtenu une erreur : 

```{r eval = FALSE}
round("Hello World")
```
<span class = "error_message">Error in round("Hello World") : 
  non-numeric argument to mathematical function</span>

À nouveau le message est très explicite : nous avons passé un argument non-numérique à une fonction mathématique. Lisez toujours vos messages d'erreurs qui vous permettront d'identifier des coquilles et de corriger votre code !

Une fonction essentielle est la fonction `print` qui permet d'afficher la valeur d'une variable.

```{r}
print(ma_somme)
```


### Principaux types de données {#sect0134}

Depuis le début de ce chapitre, nous avons déclaré plusieurs variables et essentiellement des données numériques. Dans R, il existe trois principaux types de données de base : 

* Les données numériques, qui peuvent être des nombres entiers (appelés *integers*), ou des nombres décimaux (appelés *floats*), `15` et `15.3`.
* Les données textuelles, qui sont des chaînes de caratères (appelées *strings*) et déclarées entre guillemets `"abcdefg"`
* Les données booléennes (*booleans*) qui représentent les concepts de vrai (`TRUE`) ou de faux (`FALSE`).

Déclarons une variable pour chacun de ces types : 

```{r}
age <- 35
taille <- 175.5
adresse <- '4225 rue de la gauchetiere'
proprietaire <- TRUE
```

Notez également qu'il existe des types pour représenter l'absence de données : 

* pour représenter un objet vide, on utilisera l'objet `NULL`,
* pour représenter une données manquante, on utilisera l'objet `NA`,
* pour représenter un texte vide, on utilisera une chaîne de caractère de longueur 0 `""`.

```{r}
age2 <- NULL
taille2 <- NA
adresse2 <- ''
```

### Opérateurs {#sect0135}

Nous avons vu que les fonctions nous permettent de manipuler des objets. Nous pouvons également effectuer un grand nombre d'opérations avec des opérateurs.

#### Opérateurs mathématiques {#sect01351}

Les opérateurs mathématiques permettent d'effectuer du calcul avec des données de type numérique.

```{r tableOperateurMath, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(
        Operateur = c("`+`","`-`","`*`","`/`", "`^`", "`**`",
                      "`%%`", "`%/%`"),
        Description = c("Addition", 'Soustraction', 'Multiplication',
                        'Division', 'Exponentiel', 'Exponentiel',
                        'Reste de division', 'Division entière'),
        Syntaxe = c("`4 + 4`", "`4 - 3`", "`4 * 3`", "`12 / 4`",
                    "`4 ^ 3`", '`4 ** 3`', "`15.5 %% 2`",
                    "`15.5 %/% 2`"), 
        Resultat = c(8,1,12,3,64,64,1.5,7))

show_table(df, 
           col.names = c("Opérateur","Description","Syntaxe","Résultat"),
           caption = 'Opérateurs mathématiques'
           )

```

#### Opérateurs relationnels  {#sect01352}

Les opérateurs relationnels permettent de vérifier des conditions dans R. Ils renvoient un booléen, `TRUE` si la condition est vérifiée et `FALSE` si ce n'est pas le cas.

```{r tableOperateurRela, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(
        Operateur = c("`==`","`!=`","`>`","`<`", "`>=`", "`<=`"),
        Description = c("Égalité", 'Différence', 'Est supérieur ', 'Est inférieur', 'Est supérieur ou égal', 'Est inférieur ou égal'),
        Syntaxe = c("`4 == 4`", "`4 != 4`", "`5 > 4`", "`5 < 4`", "`5 >= 4`", '`5 <= 4`'), 
        Resultat = c(TRUE, FALSE, TRUE, FALSE, TRUE, FALSE))

show_table(df, 
           col.names = c("Opérateur","Description","Syntaxe","Résultat"),
           caption = 'Opérateurs relationnels'
           )
```

#### Opérateurs logiques {#sect01353}

Les opérateurs logiques permettent de combiner plusieurs conditions :

* L'opérateur **ET** permet de vérifier que deux conditions (l'une ET l'autre) sont TRUE. Si l'une des deux est FALSE, il renvoie FALSE.

* L'opérateur **OU** permet de vérifier que l'une des deux conditions est TRUE (l'une OU l'autre). Si les deux sont FALSE, alors il renvoit FALSE.

* L'opérateur **NOT** permet d'inverser une condition. Ainsi NOT TRUE est FALSE et NOT FALSE est TRUE.


```{r tableOperateurLogi, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(
        Operateur = c("`&`","`|`","`!`"),
        Description = c("ET", "OU", "NOT"),
        Syntaxe = c("`TRUE & FALSE`", "`TRUE | FALSE`", "`! TRUE`"), 
        Resultat = c(FALSE, TRUE, FALSE))

show_table(df, 
           col.names = c("Opérateur","Description","Syntaxe","Résultat"),
           caption = 'Opérateurs logiques'
           )
```

Prenons le temps pour un rapide exemple : 

```{r}

a <- 4 
b <- 10
c <- -5

# produit TRUE car a est bien plus petit que b et c est bien plus petit que a
a < b & c < a

# produit FALSE car si a est bien plus petit que b, 
# b est en revanche plus grand que c
a < b & b < c

# produit TRUE car la seconde condition est inversée
a < b &  ! b < c

# produit TRUE car au moins une des deux conditions est juste
a < b |  b < c

```
Notez que l'opérateur **ET** est prioritaire sur l'opérateur **OU** et que les parenthèses sont prioritaires sur tous les opérateurs : 

```{r}
# produit TRUE car on va commencer par tester a < b ET b < c ce qui donne FALSE
# on obtient ensuite
# FALSE |  a > c
# enfin, a est bien supérieur à c, donc l'une des deux conditions est vraie
a < b & b < c |  a > c

```
Notez qu'en arrière-plan, les opérateurs sont en réalité des fonctions déguisées. Il est donc possible de définir de nouveau comportements pour les opérateurs. Il est par exemple possible d'additionner ou comparer des objets spéciaux comme des dates, des géométries, des graphes, etc.

### Structures de données {#sect0136}

Jusqu'ici, nous avons travaillé avec des objets ne comprenant qu'une seule valeur. Lors d'une analyse statistique, nous allons travailler avec des volumes de données bien plus conséquents. Pour stocker plusieurs valeurs, nous allons travailler avec les structures de données que sont les vecteurs, les matrices, les *dataframes* et les listes.

#### Vecteurs {#sect01361}

Les vecteurs sont la brique élémentaire de R. Ils permettent de stocker une série de valeur du même type dans une seule variable. Pour déclarer un vecteur, on utilise la fonction *c()* : 

```{r}
ages <- c(35,45,72,56,62)
tailles <- c(175.5,180.3,168.2,172.8,167.6)
adresses <- c('4225 rue de la gauchetiere',
              '4223 rue de la gauchetiere',
              '4221 rue de la gauchetiere',
              '4219 rue de la gauchetiere',
              '4217 rue de la gauchetiere')
proprietaires <- c(TRUE,TRUE,FALSE,TRUE,TRUE)
```

Nous venons ainsi de déclarer quatre nouvelles variables étant chacune un vecteur de longueur cinq (comprenant chacun cinq valeurs). Ces vecteurs représentent, par exemple, les réponses de plusieurs répondants à un questionnaire.

::: {.bloc_attention data-latex=""}
Il existe dans R une subtilité à l'origine de nombreux malentendus : la distinctions entre un vecteur de type texte et un vecteur de type facteur. Dans l'exemple précédent, le vecteur *adresses* est un vecteur de type texte. Chaque nouvelle valeur ajoutée dans le vecteur peut être n'importe quelle nouvelle adresse. Déclarons un nouveau vecteur qui contiendrait cette fois-ci la couleur des yeux de personnes ayant répondu au questionnaire.

```{r}
couleurs_yeux <- c('marron','marron','bleu','bleu','marron','vert')
```

Contrairement aux adresses, il y a un nombre limité de couleurs que nous pouvons mettre dans ce vecteur. Il serait intéressant de fixer les valeurs possibles du vecteur pour s'assurer que de nouvelles ne soient pas ajoutées par erreur. Pour cela, nous pouvons convertir ce vecteur texte en vecteur de type facteur avec la fonction `as.factor`.

```{r}
couleurs_yeux_facteur <- as.factor(couleurs_yeux)
```

Notez que à présent, nous pouvons ajouter une nouvelle couleur dans le 1er vecteur, mais pas dans le second.

```{r}
couleurs_yeux[7] <- "rouge"
couleurs_yeux_facteur[7] <- "rouge"
```
Le message d'erreur nous informe que nous avons tenté d'introduire une valeur invalide dans le facteur.

Les facteurs peuvent sembler restrictifs et très régulièrement, on préfère travailler avec de simples vecteurs de type texte plutôt que des facteurs. Cependant, de nombreuses fonctions d'analyse nécessitent d'utiliser des facteurs car ils assurent une certaine cohérence dans les données. Il est donc essentiel de savoir passer du texte au facteur avec la fonction `as.factor`. À l'inverse, il est parfois nécessaire de revenir à une variable de type texte avec la fonction `as.character`.

Notez que des vecteurs numériques peuvent aussi être convertis en facteurs : 

```{r}
tailles_facteur <- as.factor(tailles)
```

Cependant, si vous souhaitez reconvertir ce facteur en format numérique, il faudra passer dans un premier temps par le format texte : 

```{r}
as.numeric(tailles_facteur)
```

Comme vous pouvez le voir, convertir un facteur en valeur numérique renvoie des nombres entiers. Ceci est dû au fait que les valeurs dans un facteur sont recodées sous forme de nombres entiers, chaque nombre correspondant à une des valeurs originales (appelées niveaux). Si on convertit un facteur en valeurs numériques, on obtient donc ces nombres entiers.

```{r}
as.numeric(as.character(tailles_facteur))
```
Moralité de l'histoire, ne confondez pas les données de type texte et de type facteur. Dans le doute, vous pouvez demander à R quel est le type d'un vecteur avec la fonction `class`.

```{r}
class(tailles)
class(tailles_facteur)
class(couleurs_yeux)
class(couleurs_yeux_facteur)
```
:::

Quasiment toutes les fonctions utilisent des vecteurs. Par exemple, on pourrait calculer la moyenne du vecteur *ages* en utilisant la fonction *mean* présente de base dans R.

```{r}
mean(ages)
```

Quand nous disons que le vecteur est la brique élémentaire de R, ce n'est pas juste une façon de parler. Toutes les variables que nous avons déclarés dans les sections précédentes sont aussi des vecteurs, mais de longueur 1 !

#### Matrices {#sect01362}

Il est possible de combiner des vecteurs pour former des matrices. Une matrice est un tableau en deux dimensions (colonnes et lignes) généralement utilisé pour représenter certaines structures de données comme des images (pixels), effectuer du calcul matriciel ou plus simplement présenter des matrices de corrélations. Vous aurez rarement à travailler directement avec des matrices, mais il est bon de savoir ce qu'elles sont. Créons deux matrices à partir de nos précédents vecteurs.

```{r}
matrice1 <- cbind(ages,tailles)
# afficher la matrice 1
print(matrice1)
# afficher les dimensions de la matrice 1
print(dim(matrice1))

matrice2 <- rbind(ages, tailles)
# afficher la matrice 2
print(matrice2)
# afficher les dimensions de la matrice 2
print(dim(matrice2))
```
Comme vous pouvez le constater, la fonction `cbind` permet de concaténer des vecteurs comme s'ils étaient les colonnes d'une matrice, alors que `rbind` les combine comme s'ils étaient des lignes d'une matrice. La figure \@ref(fig:fig012) présente graphiquement le passage du vecteur à la matrice.

```{r fig012, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Du vecteur à la matrice",  out.width='30%'}
knitr::include_graphics('images/introduction/vecteur_to_matrix.png', dpi = NA)
```

Notez que vous pouvez transposer une matrice avec la fonction `t`. Si nous essayons maintenant de comparer la matrice 1 et la matrice 2 nous allons avoir une erreur car elles n'ont pas les mêmes dimensions.

```{r eval = FALSE}
matrice1 == matrice2
```
<span class="error_message">Error in matrice1 == matrice2 : non-conformable arrays</span>

En revanche, on pourrait transposer la matrice 1 et refaire cette comparaison : 

```{r}
t(matrice1) == matrice2
```

Le résultat souligne bien que l'on a les mêmes valeurs dans les deux matrices. Il est aussi possible de construire des matrices directement avec la fonction `matrix`, ce que nous montrons dans la prochaine section.

#### *Arrays*  {#sect01363}

S'il est rare de travailler directement avec des matrices, il est encore plus rare de travailler avec des *arrays*. Un *array* est une matrice spéciale qui peut avoir plus que deux dimensions. Un cas simple serait un *array* en trois dimensions : lignes, colonnes, profondeur, que l'on pourrait se représenter comme un cube divisé en sous cubes. Au delà de trois dimensions, il devient difficile de se les représenter. Cette structure de données peut être utilisée pour représenter les différentes bandes spectrales d'une image satellitaire. Les lignes et les colonnes délimiteraient les pixels de l'image, la profondeur quant à elle délimiterait les différents bandes composant l'image (figure \@ref(fig:fig012)).

```{r fig013, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Un array avec trois dimension",  out.width='15%'}
knitr::include_graphics('images/introduction/array.png', dpi = NA)
```

Créons un array en combinant trois matrices avec la fonction `array`. Chacune de ces matrices sera composée respectivement de 1, de 2 et de 3 et aura une dimension de 5 x 5. L'array final aura donc des dimensions de 5 x 5 x 3.

```{r}
mat1 <- matrix(1, nrow = 5, ncol = 5)
mat2 <- matrix(2, nrow = 5, ncol = 5)
mat3 <- matrix(3, nrow = 5, ncol = 5)

mon_array <- array(c(mat1, mat2, mat3), dim = c(5,5,3))

print(mon_array)
```


#### *DataFrames* {#sect01364}

S'il est rare de manipuler des matrices et des *arrays*, le *DataFrame* (tableau de données en français) est la structure de données avec laquelle vous travaillerez le plus souvent.
Dans cette structure, chaque ligne du tableau représente un individu et chaque colonne représente une caractéristique de ces individus. Ces colonnes ont des noms, ce qui permet facilement d'accéder à leurs valeurs. Créons ensemble un *DataFrame* à partir de nos quatres vecteurs et de la fonction `data.frame`.

```{r}
df <- data.frame(
  "age" = ages,
  "taille" = tailles,
  "adresse" = adresses,
  "proprietaire" = proprietaires
)
```

```{r tabfirsttable, echo=FALSE, message=FALSE, warning=FALSE}
show_table(df, 
           caption = 'Un premier DataFrame')
```

Dans Rstudio, vous pouvez visualiser votre tableau de données avec la fonction `View(df)`. Comme vous pouvez le constater, chaque vecteur est devenu une colonne de votre tableau de données *df*. La figure \@ref(fig:fig014) résume ce passage d'une simple donnée à un DataFrame en passant par un vecteur.

```{r fig014, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="De la donnée au DataFrame",  out.width='25%'}
knitr::include_graphics('images/introduction/vecteur_to_dataframe.png', dpi = NA)
```

Plusieurs fonctions de base de R fournissent des informations importantes sur un *DataFrame* : 

* `names` renvoie les noms des colonnes du DataFrame;
* `nrow` renvoie le nombre de lignes;
* `ncol` renvoie le nombre de colonnes.

```{r}
names(df)
nrow(df)
ncol(df)
```

Vous pouvez accéder à chaque colonne de *df* en utilisant le symbole `$` ou `[["nom_de_la_colonne"]]`. Recalculons ainsi la moyenne des âges :

```{r}
mean(df$age)
mean(df[["age"]])
```

#### Listes {#sect01365}

La dernière structure de données à connaître est la liste. Elle ressemble à un vecteur, au sens où elle permet de stocker un ensemble d'objets les uns à la suite des autres. Cependant, une liste peut contenir n'importe quel type d'objets. Vous pouvez ainsi construire des listes de matrices, des listes d'*arrays*, des listes mixant des vecteurs, des graphiques, des *DataFrames*, des listes de listes...

Créons ensemble une liste qui va contenir des vecteurs et des matrices à l'aide de la fonction `list.`

```{r}
ma_liste <- list(c(1,2,3,4),
                 matrix(1, ncol = 3, nrow = 5),
                 matrix(5, ncol = 3, nrow = 7),
                 'A'
                 )
```

Il est possible d'accéder aux éléments de la liste par leur position dans cette dernière en utilisant les doubles crochets `[[ ]]`: 

```{r}
print(ma_liste[[1]])
print(ma_liste[[4]])
```

Il est aussi possible de donner des noms aux éléments de la liste et d'utiliser le symbole `$` pour y accéder. Créons une nouvelle liste de vecteurs et donnons leurs des noms avec la fonction `names`.

```{r}
liste2 <- list(c(35,45,72,56,62), 
               c(175.5,180.3,168.2,172.8,167.6),
               c(TRUE,TRUE,FALSE,TRUE,TRUE)
)
names(liste2) <- c("age",'taille','proprietaire')

print(liste2$age)
```

Si vous avez bien suivi, vous devez avoir compris qu'un *DataFrame* n'est en fait rien d'autre qu'une liste de vecteurs avec des noms !

Bravo ! Vous venez de faire le tour des bases du langage R. Nous allons pouvoir passer à la suite et apprendre à manipuler des données dans des *DataFrames* !

## Manipuler des données {#sect014}

Dans cette section, vous apprendrez à charger et manipuler des *DataFrames* en vue d'effectuer des opérations classiques de gestion de données.

### Charger un *DataFrame* depuis un fichier {#sect0141}

Il sera rarement nécessaire de créer vos *DataFrames* manuellement comme réalisé dans la section précédente. Le plus souvent, vous disposerez de fichiers contenant vos données et utiliserez des fonctions pour les importer dans R sous forme d'un *DataFrame*. Les formats à importer les plus répandus sont : 

* *.csv*, soit un fichier texte dont chaque ligne représente une ligne du tableau de données dont les colonnes sont séparées par un délimiteur (généralement une virgule ou un point-virgule).
* *.dbf*, ou fichier *dBase*, souvent associés à des fichiers d'information géographique au format *ShapeFile*.
* *.xls* et *.xlsx*, soit des fichiers générés par Excel.
* *.json*, soit un fichier texte utilisant la norme d'écriture propre au langage JavaScript.

Plus rarement, il se peut que vous aillez à charger des fichiers provenant de logiciels propriétaires :

* *.sas7bdat* (SAS),
* *.sav* (SPSS) et
* *.dta* (STATA).

Pour lire la plupart de ces fichiers, nous allons utiliser le *package* **foreign** dédié à l'importation d'une multitude de formats. Commencez donc par l'installer (`install.packages("foreign")`). Nous allons charger cinq fois le même jeu de données enregistré dans des formats différents (*csv*, *dbf*, *dta*, *sas7bdat* et *xlsx*). Aussi, nous mesurerons le temps nécessaire pour importer chacun de ces fichiers avec la fonction `Sys.time`.

#### Lire un fichier *csv* {#sect01411}

Pour le format *csv*, il n'y a pas besoin d'utiliser un *package* puisque R dispose d'une fonction de base pour lire ce format.

```{r message=FALSE, warning=FALSE}
t1 <- Sys.time()
df1 <- read.csv("data/priseenmain/SR_MTL_2016.csv", 
         header = TRUE, sep = ",", dec = ".",
         stringsAsFactors = FALSE)
t2 <- Sys.time()
d1 <- as.numeric(difftime(t2,t1,units="secs"))

cat('le dataframe df1 a ',nrow(df1),' observations',
    'et ',ncol(df1),"colonnes\n")
```

Rien de bien compliqué ! Notez tout de même que : 

* Lorsque vous chargez un fichier *csv*, vous devez connaître le **séparateur**, soit le caractère utilisé pour délimiter les colonnes. Dans le cas présent, il s'agit d'une virgule (spécifiez avec l'argument `sep = ","`), mais il pourrait tout aussi bien être un point virgule (`sep = ";"`) une tabulation (`sep = "    "`), etc.
* Vous devez également spécifier le caractère utilisé comme séparateur de décimales. Le plus souvent, ce sera le point (`dec = "."`), mais certains logiciels avec des paramètres régionaux de langue française (notamment Excel) exportent des fichiers *csv* avec des virgules comme séparateur de décimales (utilisez alors `dec = ","`).
* L'argument `header` indique si la première ligne (l'entête) du fichier comprend ou non les noms des colonnes du jeu de données (avec les valeurs `TRUE` ou `FALSE`). Il arrive que certains fichiers *csv* soient fournis sans entête et que les noms et descriptions des colonnes soient fournis dans un autre fichier.
* L'argument *stringsAsFactors* permet d'indiquer à R que les colonnes comportant du texte doivent être chargées comme des vecteurs de type texte et nom de type facteur.

#### Lire un fichier *dbase* {#sect01412}

Pour lire un fichier *dbase* (.dbf), nous utilisons la fonction `read.dbf` du *package* **foreign** installé précédemment : 

```{r message=FALSE, warning=FALSE}
library(foreign)

t1 <- Sys.time()
df2 <- read.dbf("data/priseenmain/SR_MTL_2016.dbf")
t2 <- Sys.time()
d2 <- as.numeric(difftime(t2,t1,units="secs"))

cat('le dataframe df2 a ',nrow(df2),' observations',
    'et ',ncol(df2),"colonnes\n")
```
Comme vous pouvez le constater, nous obtenons les mêmes résultats qu'avec le fichier *csv*.

#### Lire un fichier *dta* (Stata) {#sect01413}

Si vous travaillez avec des collègues utilisant le logiciel Stata, il se peut que ces derniers vous partagent des fichiers *dta*. Toujours en utilisant le *package* **foreign**, vous serez en mesure de les charger directement dans R.

```{r message=FALSE, warning=FALSE}
t1 <- Sys.time()
df3 <- read.dta("data/priseenmain/SR_MTL_2016.dta")
t2 <- Sys.time()
d3 <- as.numeric(difftime(t2,t1,units="secs"))

cat('le dataframe df3 a ',nrow(df3),' observations',
    'et ',ncol(df3),"colonnes\n", sep = "")
```


#### Lire un fichier *sav* (SPSS) {#sect01414}

SPSS est encore utilisé dans le milieu académique, surtout au premier cycle, bien que de moins en moins. Pour importer un fichier *sav*, vous pourrez utiliser la fonction `read.spss` *package* **foreign**.

```{r message=FALSE, warning=FALSE}
t1 <- Sys.time()
df4 <- as.data.frame(read.spss("data/priseenmain/SR_MTL_2016.sav"))
t2 <- Sys.time()
d4 <- as.numeric(difftime(t2,t1,units="secs"))

cat('le dataframe df4 a ',nrow(df4),' observations',
    'et ',ncol(df4),"colonnes\n", sep = "")
```

#### Lire un fichier *sas7bdat* (SAS) {#sect01415}

SAS est encore utilisé dans les milieux académiques, gouvernementaux et privés. Pour importer un fichier *sas7bdat*, vous pourrez utiliser le *package* **sas7bdat** que vous devrez préalablement installer (`install.packages("sas7bdat")`) et charger (`library(sas7bdat)`).

```{r message=FALSE, warning=FALSE}
library(sas7bdat)

t1 <- Sys.time()
df5 <- read.sas7bdat("data/priseenmain/SR_MTL_2016.sas7bdat")
t2 <- Sys.time()
d5 <- as.numeric(difftime(t2,t1,units="secs"))

cat('le dataframe df5 a ',nrow(df5),' observations',
    'et ',ncol(df5),"colonnes\n", sep ="")
```


#### Lire un fichier *xlsx* (Excel) {#sect01416}

Lire un fichier Excel dans R n'est pas toujours une tâche facile. Généralement, nous recommandons d'exporter les fichiers en question au format *csv* dans un premier temps, puis de le lire avec la fonction `read.csv` dans un second temps (LIEN SECTION). 
Il est néanmoins possible de lire directement un fichier *xlsx* avec le *package* **xlsx**. Ce dernier requiert que le logiciel JAVA soit installé sur votre ordinateur (Windows, Mac ou Linux). Si vous utilisez la version 64 bit de R, vous devrez télécharger et installer la version 64 bit de JAVA. Une fois que ce logiciel tiers est installé, il ne vous restera plus qu'à installer (`install.packages("xlsx")`) et charger (`library(xlsx)`) le *package* **xlsx**.

```{r message=FALSE, warning=FALSE}
library(xlsx)

t1 <- Sys.time()
df6 <- read.xlsx(file="data/priseenmain/SR_MTL_2016.xlsx",
                 sheetIndex = 1,
                 as.data.frame = TRUE)
t2 <- Sys.time()
d6 <- as.numeric(difftime(t2,t1,units="secs"))

cat('le dataframe df6 a ',nrow(df6),' observations',
    'et ',ncol(df6),"colonnes\n", sep = "")
```

Il est possible d'accélérer significativement la vitesse de lecture d'un fichier *xlsx* en utilisant la fonction `read.xlsx2`. Il faut cependant indiquer à cette dernière le type de données de chaque colonne. Dans le cas présent, les cinq premières colonnes contiennent des données au format texte (`character`), alors que les 43 autres sont des données numériques (`numeric`). Nous utilisons la fonction `rep` afin de ne pas avoir à écrire plusieurs fois *character* et *numeric*.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(xlsx)

t1 <- Sys.time()
df7 <- read.xlsx2(file="data/priseenmain/SR_MTL_2016.xlsx",
                  sheetIndex = 1, 
                  as.data.frame = TRUE,
                  colClasses = c(rep("character",5),rep("numeric",43))
                  )
t2 <- Sys.time()
d7 <- as.numeric(difftime(t2,t1,units="secs"))

cat('le dataframe df6 a ',nrow(df7),' observations',
    'et ',ncol(df7),"colonnes\n", sep = "")
```

Si l'on compare les temps d'exécution (tableau \@ref(tab:tableduration)), on constate que la lecture des fichiers *xlsx* peut être extrêmement longue si l'on ne spécifie pas le type des colonnes. Ceci peut devenir problématique pour des fichiers volumineux. Notez également que la lecture des fichiers *csv* devient de plus en plus laborieuse à mesure que la taille du fichier *csv* augmente. Si vous devez un jour charger des fichiers *csv* de plusieurs gigaoctets, nous vous recommandons vivement d'utiliser la fonction `fread` du *package* **data.table** qui est beaucoup plus rapide.

```{r tableduration, echo=FALSE, message=FALSE, warning=FALSE}

DureeImportation <- data.frame(
  "duree" = c(d1,d2,d3,d4,d5,d6,d7),
  "fonction" = c("read.csv","read.dbf","read.spss","read.dta",
                 'read.sas7bdat',"read.xlsx","read.xlsx2")
)

show_table(DureeImportation, 
           digits = 2, 
           col.names = c("Durée (s)","fonction"),
           caption = 'Temps nécessaire pour lire les données en fonction du type de fichiers')
```

### Manipuler un *DataFrame* {#sect0142}

Une fois le *DataFrame* chargé, voyons comment il est possible de le manipuler.

#### Un petit mot sur le **tidyverse** {#sect01421}

**Tidyverse** est un ensemble de *packages* conçus pour faciliter la structuration et la manipulation des données dans R. Avant d'aller plus loin, il est important d'aborder brièvement un débat actuel dans la Communauté R. Entre 2010 et 2020, l'utilisation du **tidyverse** s'est peu à peu répandue. Développé et maintenu par Hadley Wickham, **tidyverse** introduit une philosophie et une grammaire spécifiques qui diffèrent du langage R traditionnel. Une partie de la communauté a pour ainsi dire complètement embrassé le **tidyverse** et de nombreux *packages* en dehors du **tidyverse** ont adopté sa grammaire et sa philosophie. À l'inverse, une autre partie de la communauté est contre cette évolution ([voir l'article du blogue suivant](https://blog.ephorie.de/why-i-dont-use-the-tidyverse){target="_blank"}). Les arguments pour et contre **tidyverse** sont résumés dans le tableau suivant. 



```{r tableTidyverse, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(
          Pour = c("Simplicité d'écriture et d'apprentissage",
  "Ajout de l'opérateur `%>%` permettant d'enchaîner les traitements",
  "La meilleure librairie pour réaliser des graphiques : **ggplot2**",
  "Crée un écosystème cohérent",
  "Package en développement et de plus en plus utilisé"),
          Contre = c("Nouvelle syntaxe à apprendre",
  "Perte de lisibilité avec l'opérateur `->`",
  "Certaines fonctions de base sont remplacées par **tidyverse** lors de son chargement, pouvant créer des erreurs.",
  "Ajoute une dépendance dans le code",
  "Philosophie d'évolution agressive, aucune assurance de rétro-compatibilité")
)

show_table(df, 
      col.names = c("Avantage du tidyverse",
                  "Problème posé par le tidyverse"),
      caption = 'Avantages et inconvénients du tidyverse', 
      col.to.resize = c(1,2), 
      col.width = "7cm")
```


Le dernier point est probablement le plus problématique. Dans sa volonté d'évoluer au mieux et sans restriction, le *package* **tidyverse** n'offre aucune garantie de rétro-comptatibilité. En d'autre termes, des changements importants peuvent être introduit d'une version à l'autre rendant potentiellement obsolète votre propre code. Nous n'avons pas d'opinion tranchée sur le sujet : **tidyverse** est un outil très intéressant dans de nombreux cas; nous évitons simplement de l'utiliser systématiquement et préférons charger directement des sous-packages (comme **dplyr** ou **ggplot2**) du **tidyverse**. Notez que le *package* **data.table** offre une alternative au **tidyverse** dans la manipulation de données. Au prix d'une syntaxe généralement un peu plus complexe, le package **data.table** offre une vitesse de calcul bien supérieure au **tidyverse** et assure une bonne rétro-compatibilité.


#### Gérer les colonnes d'un *DataFrame* {#sect01422}

Repartons du *DataFrame* que nous avions chargé précédemment grâce à un fichier *csv*.

```{r message=FALSE, warning=FALSE}
df <- read.csv(file="data/priseenmain/SR_MTL_2016.csv", 
               header = TRUE, sep = ",", dec = ".",
               stringsAsFactors = FALSE)
```

##### Sélectionner une colonne {#sect014221}

Pour rappel, il est possible d'accéder aux colonnes dans ce *DataFrame* en utilisant le symbole dollar `$ma_colonne` ou les doubles crochets `[["ma_colonne"]]`.

```{r message=FALSE, warning=FALSE}
# Calcul de la superficie totale de l'Île de Montréal
sum(df$KM2)
sum(df[["KM2"]])
```

##### Sélectionner plusieurs colonnes {#sect014222}

Il est possible de sélectionner plusieurs colonnes d'un *DataFrame* et filtrer ainsi les colonnes inutiles. Pour cela, on peut utiliser un vecteur contenant soit la position de la colonne (1 pour la première colonne, 2 pour la seconde et ainsi de suite), soit les noms des colonnes.

```{r message=FALSE, warning=FALSE}
# Conserver les 5 premières colonnes
df2 <- df[1:5]

# Conserver les colonnes 1,5,10 et 15
df3 <- df[c(1,5,10,15)]

# Cela peut aussi être utilisé pour changer l'ordre des champs
df3 <- df[c(10,15,1,5)]

# Conserver les colonnes 1 à 5, 7 à 12, 17 et 22
df4 <- df[c(1:5,7:12,17,22)]

# Conserver les colonnes avec leurs noms
df5 <- df[c("SRIDU","KM2","Pop2016","MaisonIndi","LoyerMed")]
```

##### Supprimer des colonnes {#sect014223}

Il est parfois plus intéressant et rapide de directement supprimer des colonnes plutôt que de recréer un nouveau *DataFrame*. Pour ce faire, on attribue la valeur `NULL` à ces colonnes.

```{r message=FALSE, warning=FALSE}
# Supprimer le colonnes 2, 3 et 5
df3[c(2,3,5)] <- list(NULL)

# Supprimer une colonne avec son nom
df4$OID <- NULL

# Supprimer plusieurs colonnes par leur nom
df5[c("SRIDU","LoyerMed")] <- list(NULL)
```
Notez que si vous supprimez une colonne, vous ne pouvez pas revenir en arrière. Il faudra recharger votre jeu de données ou éventuellement relancer les calculs qui avaient produit cette colonne.

##### Renommer des colonnes {#sect014224}

Il est possible de changer le nom d'un colonne. Cette opération est importante pour faciliter la lecture du *DataFrame* ou encore s'assurer que l'exportation du *DataFrame* dans un format ne posera pas de problème.

```{r message=FALSE, warning=FALSE}
# Voici les noms des colonnes
names(df5)

# Renommer toutes les colonnes
names(df5) <- c('superficie_km2','population_2016', 'maison_individuelle_prt')
names(df5)

# Renommer avec dplyr
library(dplyr)
df4 <- rename(df4, "population_2016" = "Pop2016",
              "prs_moins_14ans_prt" = "A014",
              "prs_15_64_ans_prt" = "A1564",
              "prs_65plus_ans_prt" = "A65plus"
              )
```


#### Calculer de nouvelles variables {#sect01423}

Il est possible d'utiliser les colonnes de type numérique pour calculer de nouvelles colonnes en utilisant les opérateurs mathématiques vus dans la section \@ref(sect01_35). Prenons un exemple concret : calculons la densité de population par secteur de recensement dans notre *DataFrame* et affichons un résumé de cette nouvelle variable.

```{r message=FALSE, warning=FALSE}
# Calcul de la densité
df$pop_density_2016 <- df$Pop2016 / df$KM2

# Statistiques descriptives
summary(df$pop_density_2016)
```

Nous pouvons aussi calculer le ratio entre le nombre de maisons et le nombre d'appartements.

```{r message=FALSE, warning=FALSE}

# Calcul du ratio
df$total_maison <- (df$MaisonIndi + df$MaisJumule + df$MaisRangee + df$AutreMais)
df$total_apt <- (df$AppDuplex + df$App5Moins + df$App5Plus)
df$ratio_maison_apt <- df$total_maison / df$total_apt
```

Retenez ici que R va appliquer le calcul à chaque ligne de votre jeu de données et stocker le résultat dans une nouvelle colonne. Cette opération est du calcul vectoriel : toute la colonne est calculée en une seule fois. R est d'ailleurs optimisé pour le calcul vectoriel.

#### Fonctions mathématiques {#sect01424}

R propose un ensemble de fonctions de base pour effectuer du calcul. Voici une liste non-exhaustive des principales fonctions : 

* `abs` calcule les valeurs absolues des valeurs d'un vecteur
* `sqrt` calcule les racines carrées des valeurs d'un vecteur
* `log` calcule les logarithmes des valeurs d'un vecteur
* `exp` calcule les exponentiels des valeurs d'un vecteur
* `factorial` calcule la factorielle des valeurs d'un vecteur
* `round` arrondit les valeurs d'un vecteur
* `ceiling`, `floor` arrondit à l'unité supérieure ou inférieure les valeurs d'un vecteur
* `sin`,`asin`,`cos`,`acos`,`tan`,`atan` sont des fonctions de trigonométrie classiques
* `cumsum` calcule la somme cumulative des valeurs d'un vecteur.

Ces fonctions sont des fonctions vectorielles puisqu'elles s'appliquent à tous les éléments d'un vecteur. Si votre vecteur en entrée comprend cinq valeurs, le vecteur en sortie comprendra aussi cinq valeurs.

À l'inverse, les fonctions suivantes s'appliquent directement à l'ensemble d'un vecteur et ne vont renvoyer qu'une seule valeur :

* `sum` calcule la somme des valeurs d'un vecteur
* `prod` calcule le produit des valeurs d'un vecteur
* `min`, `max` renvoient les valeurs maximale et minimale d'un vecteur
* `mean`, `median` renvoient la moyenne et la médiane d'un vecteur
* `quantile` renvoit les percentiles d'un vecteur.

#### Fonctions pour manipuler du texte {#sect01425}

En plus des données numériques, vous aurez à travailler avec des données textuelles. Le **tidyverse** avec le *package* **stringr** offre des fonctions très intéressantes pour manipuler ce type de données. Pour un aperçu de toutes les fonctions offertes par **stringr**, référer-vous à sa [Cheat Sheet](https://github.com/rstudio/cheatsheets/blob/master/strings.pdf){target="_blank"}. Commençons avec un *DataFrame* assez simple comprenant des adresses et des noms de personnes.

```{r message=FALSE, warning=FALSE}
library(stringr)

df <- data.frame(
  noms = c("Jérémy Toutanplace","constant Tinople","dino Resto","Luce tancil"),
  adresses = c('15 rue Levy', '413 Blvd Saint-Laurent', '3606 rue Duké', '2457 route St Marys')
)
```

##### Majuscules et minuscules {#sect014251}

Pour harmoniser ce *dataframe*, nous allons dans un premier temps mettre des majuscules au premier caractère des prénoms et noms des individus avec la fonction `str_to_title`.

```{r message=FALSE, warning=FALSE}
df$noms_corr <- str_to_title(df$noms)
print(df$noms_corr)
```

On pourrait également tout mettre en minuscule ou tout en majuscule.
```{r message=FALSE, warning=FALSE}
df$noms_min <- tolower(df$noms)
df$noms_maj <- toupper(df$noms)
print(df$noms_min)
print(df$noms_maj)
```

##### Remplacer du texte {#sect014252}

Dans les adresses, nous avons des caractères accentués. Ce type de caractères pose régulièrement des problèmes d'encodage et nous pourrions décider de les remplacer par des caractères simples avec la fonction `str_replace_all`.

```{r message=FALSE, warning=FALSE}
df$adresses_1 <- str_replace_all(df$adresses,'é','e')
print(df$adresses_1)
```

Nous pouvons utiliser la même fonction pour remplacer les *St* par Saint et les *Blvd* par Boulevard.

```{r message=FALSE, warning=FALSE}
df$adresses_2 <- str_replace_all(df$adresses_1,' St ',' Saint ')
df$adresses_3 <- str_replace_all(df$adresses_2,' Blvd ',' Boulevard ')
print(df$adresses_3)
```

##### Découper du texte {#sect014253}

Il est parfois nécessaire de découper du texte pour en extraire des éléments. On doit alors choisir un caractère de découpage. Dans notre exemple, on pourrait vouloir extraire les numéros civiques des adresses, en utilisant le premier espace comme caractère de découpage, en utilisant la fonction `str_split_fixed`.

```{r message=FALSE, warning=FALSE}
df$num_civique <- str_split_fixed(df$adresses_3, ' ',n=2)[,1]
print(df$num_civique)
```

Pour être exact, sachez que pour notre exemple, la fonction `str_split_fixed` renvoie deux colonnes de texte : une avec le texte avant le premier espace (donc le numéro civique) et une avec le reste du texte. Le nombre de colonnes est contrôlé par l'argument `n`. Si `n = 1`, la fonction ne fait aucun découpage, avec `n = 2` la fonction va découper en deux parties le texte avec la première occurence du délimiteur, et ainsi de suite.  En ajoutant `[,1]` à la fin, nous indiquons que nous souhaitons seulement garder la première des deux colonnes.

##### Coller du texte {#sect014254}

À l'inverse du découpage, il est parfois nécessaire de concaténer des éléments de texte, ce qu'il est possible de faire avec la fonction `paste`.

```{r message=FALSE, warning=FALSE}
df$texte_complet <- paste(df$noms_corr, df$adresses_3, sep = " : ")
print(df$texte_complet)
```

Le paramètre `sep` permet de choisir le ou les caractères à intercaler entre les éléments à concaténer. Notez qu'il est possible de concaténer plus que deux éléments.

```{r message=FALSE, warning=FALSE}
df$ville <- c('Montreal','Montreal','Montreal','Montreal')
paste(df$noms_corr, df$adresses_3, df$ville, sep = ", ")
```

#### Manipuler des colonnes de type date {#sect01426}

Nous avons vu que les principaux types de données dans R sont le numérique, le texte, le booléen et le facteur. Il existe d'autres types, introduits par différent *packages*. Nous abordons ici les types date et temps (*date* and *time*). Pour les manipuler, nous privilégions l'utilisation du *package* **lubridate** du **tidyverse**. Pour illuster le tout, nous l'appliquerons avec un jeu de données ouvertes de la ville de Montréal représentant les accidents de la route incluant au moins un vélo après le premier janvier 2017.

```{r message=FALSE, warning=FALSE}
accidents_df <- read.csv(file="data/priseenmain/accidents.csv", sep = ",")
names(accidents_df)
```

Nous disposons de trois colonnes représentant respectivement l'heure, la date et le nombre de victimes impliquées dans l'accident.

##### Du texte à la date {#sect014261}

Actuellement, les colonnes *HEURE_ACCDN* et *DT_ACCDN* sont au format texte. Nous pouvons afficher quelques lignes du jeu de données avec la fonction `head` pour visualiser comment elles ont été saisies.

```{r message=FALSE, warning=FALSE}
head(accidents_df, n = 5)
```

Un peu de ménage s'impose : les heures sont indiquées comme des périodes d'une heure. Nous utilisons la fonction `str_split_fixed` du *package* **stringr** pour ne garder que la première partie de l'heure (avant le tiret). Nous allons ensuite concaténer l'heure et la date avec la fonction `paste`, puis nous convertirons ce résultat en un objet *date-time*.

```{r message=FALSE, warning=FALSE}
library(lubridate)

# Étape 1 : découper la colonne Heure_ACCDN
accidents_df$heure <- str_split_fixed(accidents_df$HEURE_ACCDN, "-", n=2)[,1]

# Étape 2 : concaténer l'heure et la date
accidents_df$date_heure <- paste(accidents_df$DT_ACCDN, 
                                 accidents_df$heure,
                                 sep = ' ')

# Étape 3 : convertir au format datetime
accidents_df$datetime <- as_datetime(accidents_df$date_heure,
                                     format = "%Y/%m/%d %H:%M:%S")
```

Pour effectuer la conversion, nous avons utilisé la fonction `as_datetime` du package **lubridate**. Elle prend comme paramètre un vecteur de texte et une indication du format de ce vecteur de texte. Il existe de nombreuses façons de spécifier une date et une heure et l'argument *format* permet de spécifier quelle nomenclature est utilisée. Dans cet exemple, la date est structurée comme suit : 
`année/mois/jour heure:minute:seconde`, ce qui se traduit par le format `%Y/%m/%d %H:%M:%S`.

* %Y signifie une année indiquée avec quatre caractères : 2017
* %m signifie un mois, indiqué avec deux caractères : 01, 02, 03, ... 12
* %d signifie un jour, indiqué avec deux caractères : 01, 02, 03, ... 31
* %H signifie une heure, au format 24 heures avec deux caractères : 00, 02, ... 23
* %M signifie des minutes indiquées avec deux caractères : 00, 02, ... 59
* %S signifie des secondes, indiquées avec deux caractères : 00, 02, ... 59

Notez que les caractères séparant les années, jours, heures, etc. sont aussi à indiquer dans le format. Dans notre exemple, nous utilisons des `/` pour séparer les éléments de la date, des `:` pour l'heure, et un espace pour séparer la date et l'heure.

Il existe d'autres nomenclatures pour spécifier un format *datetime* : par exemple, des mois renseignés par leur nom, l'indication AM-PM, etc. Vous pouvez vous référez à la documentation de la fonction `strptime` (`help(strptime)`) pour explorer les différentes nomenclatures et choisir celle qui vous convient. Bien évidemment, il est **nécessaire** que toutes les dates de votre colonne soient renseignées dans le même format. Sinon, la fonction renverra des valeurs `NA` aux endroits où elle a échoué à lire le format.

Après toutes ces opérations, rejettons un oeil à notre *DataFrame*.

```{r message=FALSE, warning=FALSE}
head(accidents_df, n = 5)
```
##### Extraire des informations d'une date {#sect014262}

À partir de la nouvelle colonne `datetime`, nous sommes en mesure d'extraire des informations intéressantes comme : 

* le nom du jour de la semaine avec la fonction `weekdays`
```{r message=FALSE, warning=FALSE}
accidents_df$jour <- weekdays(accidents_df$datetime)
```

* la période de la journée avec les fonctions `am` et `pm` 
```{r message=FALSE, warning=FALSE}
accidents_df$AM <- am(accidents_df$datetime)
accidents_df$PM <- pm(accidents_df$datetime)

head(accidents_df[c("jour", "AM", "PM")], n=5)
```

Il est aussi possible d'accéder aux sous-éléments d'un *datetime* comme l'année, le mois, le jour, l'heure, la minute, la seconde avec les fonctions `year()`, `month()`,`day()`, `hour()`,  `minute()` et `second()`.

##### Calculer une durée entre deux *datetime* {#sect014263}

Une autre utilisation intéressante du format *datetime* est de calculer des différences de temps. Par exemple, nous pourrions utiliser le nombre de minutes écoulées depuis 07:00 le matin comme une variable dans une analyse visant à déterminer le moment critique des accidents durant l'heure de pointe du matin. 
Pour cela, nous devons créer un *datetime* de référence en concaténant la date de chaque observation, et le temps 07:00:00 qui sera notre point de départ.

```{r message=FALSE, warning=FALSE}
accidents_df$date_heure_07 <- paste(accidents_df$DT_ACCDN, 
                                 '07:00:00',
                                 sep = ' ')
accidents_df$ref_datetime <- as_datetime(accidents_df$date_heure_07,
                                     format = "%Y/%m/%d %H:%M:%S")
```
Il ne nous reste plus qu'à calculer la différence de temps entre la colonne *datetime* et notre temps de référence *ref_datetime*.

```{r message=FALSE, warning=FALSE}
accidents_df$diff_time <- difftime(accidents_df$datetime,
                                   accidents_df$ref_datetime,
                                   units = 'min')
```

Notez qu'ici la colonne *diff_time* est d'un type spécial : une différence temporelle (*difftime*). Il faut encore la convertir au format numérique pour pourvoir l'utiliser avec la fonction `as.numeric`.

Par curiosité, réalisons rapidement un histogramme avec la fonction `hist` pour analyser rapidement cette variable d'écart de temps !

```{r fig015, fig.align='center', auto_pdf = TRUE, fig.cap="Répartition temporelle des accidents à vélo",  out.width='65%'}

accidents_df$diff_time_num <- as.numeric(accidents_df$diff_time)
hist(accidents_df$diff_time_num, breaks = 50)

```

On observe clairement deux pics, un premier entre 0 et 100 (donc entre 07:00 08:30 environ) et un second plus important entre 550 et 650 (entre 16:00 et 17:30 environ), ce qui correspond sans surprise aux heures de pointe. Il est intéressant de noter que plus d'accidents se produisent à l'heure de pointe du soir qu'à celle du matin.

#### Recoder des variables {#sect01427}

Recoder des variables signifie changer la valeur d'une variable selon une condition afin d'obtenir une nouvelle variable. Si nous reprenons notre exemple précédent avec les accidents à vélo, nous pourrions créer une nouvelle colonne nous indiquant si l'accident a eu lieu en heure de pointe ou hors heure de pointe. On obtiendrait ainsi une nouvelle variable avec seulement deux catégories plutôt que la variable numérique originale. Nous pourrions aussi définir trois catégories avec l'heure de pointe du matin, l'heure de pointe du soir, le reste de la journée et la nuit.

##### Le cas binaire avec ifelse {#sect014271}

Si l'on ne souhaite créer que deux catégories, le plus simple est d'utiliser la fonction `ifelse`. Cette fonction va évaluer une condition (section \@ref(sect01_35)) pour chaque ligne d'un *DataFrame* et produire un nouveau vecteur. Créons donc une variable binaire indiquant si un accident a eu lieu durant les heures de pointe ou hors heures de pointe. Nous devons alors évaluer les conditions suivantes : 

Est-ce que l'accident a eu lieu entre 07:00 (0) **ET** 09:00 (120), **OU** est ce que l'accident a eu lieu entre 16:30 (570) **ET** 18:30 (690)?

```{r message=FALSE, warning=FALSE}
Cond1 <- accidents_df$diff_time_num >= 0 & accidents_df$diff_time_num <= 120
Cond2 <- accidents_df$diff_time_num >= 570 & accidents_df$diff_time_num <= 690

accidents_df$moment_bin <- ifelse(Cond1 | Cond2,
                                  "en heures de pointe",
                                  "hors heures de pointe")
```

Comme vous pouvez le constater, la fonction `ifelse` nécessite trois arguments : 

* Une condition, pouvant être `TRUE` ou `FALSE`,
* La valeur à renvoyer si la condition est `TRUE`
* La valeur à renvoyer si la condition est `FALSE`

Avec la fonction `table`, nous pouvons rapidement compter les effectifs des deux  catégories ainsi créées : 

```{r message=FALSE, warning=FALSE}
table(accidents_df$moment_bin)
```
Les heures de pointes représentent quatre heures de la journée, ce qui nous laisse neuf heures hors heure de pointe entre 07:00 et 20:00.

```{r message=FALSE, warning=FALSE}
# Ratio d'accidents en heures de pointe
(841 / 2414) / (4 / 13)

# Ratio d'accidents hors heure de pointe
(1573 / 2414) / (9 / 13)
```
En rapportant les accidents aux durées des deux périodes, on observe une nette sur-représentation des accidents impliquant un vélo pendant les heures de pointe d'environ 13% comparativement à la période hors des heures de pointe.

##### Le cas multiple avec la fonction *case_when* {#sect014272}

Lorsque l'on souhaite créer plus que deux catégories, il est possible soit d'enchaîner plusieurs fonctions `ifelse` (ce qui produit un code plus long et moins lisible), soit d'utiliser la fonction `case_when` provenant du *package* **dplyr** du **tidyverse**. Reprenons notre exemple et créons quatre catégories : 

* En heures de pointe du matin
* En heures de pointe du soir
* Le reste de la journée (entre 07:00 et 20:00)
* La nuit (entre 21:00 et 07:00)

```{r message=FALSE, warning=FALSE}
library(dplyr)

accidents_df$moment_multi <- case_when(
  accidents_df$diff_time_num >= 0 & accidents_df$diff_time_num <= 120 ~ "pointe matin",
  accidents_df$diff_time_num >= 570 & accidents_df$diff_time_num <= 690 ~ "pointe soir",
  accidents_df$diff_time_num > 690 & accidents_df$diff_time_num < 780 ~ "journee",
  accidents_df$diff_time_num > 120 & accidents_df$diff_time_num < 570 ~ "journee",
  accidents_df$diff_time_num < 0 | accidents_df$diff_time_num >= 780 ~ "nuit"
)

table(accidents_df$moment_multi)
```
La syntaxe de cette fonction est un peu particulière. Elle accepte un nombre illimité d'arguments. Chaque argument est composé d'une condition et d'une valeur à renvoyer si la condition est vraie; ces deux éléments étant reliés par le symbole `~`. Notez que toutes les évaluations sont effectuées dans l'ordre des arguments. En d'autres termes, la fonction va d'abord tester la première condition et assigner ces valeurs, puis recommencer pour les prochaines conditions. Ainsi, si une observation (ligne du tableau de données) obtient `TRUE` à plusieurs conditions, elle obtiendra la valeur de la dernière condition qu'elle a validée.

#### Sous-sélection d'un *DataFrame* {#sect01428}

Dans cette section, nous verrons comment extraire des sous-parties d'un *DataFrame*. Il est possible de sous-sélectionner des lignes et des colonnes en se basant sur des conditions ou leurs index. Pour cela, nous allons utiliser un jeu de données fourni avec R : le jeu de données **iris** décrivant des fleurs du même nom.

```{r message=FALSE, warning=FALSE}
data("iris")
dim(iris)
```
##### Sous-sélection des lignes {#sect014281}

Sous-sélectionner des lignes par index est relativement simple. Admettons que nous souhaitons sélectionner les lignes 1 à 5, 10 à 25, 37 et 58.

```{r message=FALSE, warning=FALSE}
sub_iris <- iris[c(1:5, 10:25, 37, 58),]
nrow(sub_iris)
```
Sous-sélectionner des lignes avec une condition peut être effectué soit avec une syntaxe similaire, soit en utilisant la fonction `subset`. Sélectionnons toutes les fleurs de l'espèce Virginica.

```{r message=FALSE, warning=FALSE}
iris_virginica1 <- iris[iris$Species == "virginica",]
iris_virginica2 <- subset(iris, iris$Species == "virginica")

# Vérifions que les deux dataframes ont le même nombre de lignes
nrow(iris_virginica1) == nrow(iris_virginica2)
```

Vous pouvez utiliser dans les deux cas tous les opérateurs vus dans les sections \@ref(sect01_352) et \@ref(sect01_353). L'enjeu est d'arriver à un vecteur booléen final permettant d'identifier les observations à conserver.

##### Sous-sélectionner des colonnes {#sect014282}

Nous avons déjà vu comment sélectionner des colonnes en utilisant leur nom ou leur index dans la section \@ref(sect01_4221). Ajoutons ici un cas particulier où nous souhaiterions sélectionner des colonnes selon une condition. Par exemple, nous pourrions vouloir conserver que les colonnes comprenant le mot *Length*. Pour cela, nous utiliserons la fonction `grepl`, permettant de déterminer si des caractères sont présents dans une chaîne de caractères.

```{r message=FALSE, warning=FALSE}
nom_cols <- names(iris)
print(nom_cols)

test_nom <- grepl("Length",nom_cols, fixed = TRUE)
ok_nom <- nom_cols[test_nom]

iris_2 <- iris[ok_nom]
print(names(iris_2))
```
Il est possible d'obtenir ce résultat en une seule ligne de code, mais elle est un peu moins lisible.

```{r message=FALSE, warning=FALSE}
iris2 <- iris[names(iris)[grepl("Length",names(iris), fixed = TRUE)]]
```

##### Sélectionner des colonnes et des lignes {#sect014283}

Nous avons vu qu'avec les crochets, nous pouvons extraires les colonnes et les lignes d'un *DataFrame*. Il est possible de combiner les deux opérations en même temps. Pour cela, il faut indiquer en premier les indices ou la condition permettant de sélectionner une ligne, puis les indices ou la condition pour sélectionner les colonnes : `[index_lignes , index_colonnes]`. Sélectionnons les trois premières colonnes et les cinq premières lignes du jeu de données iris : 

```{r message=FALSE, warning=FALSE}
iris_5x3 <- iris[c(1,2,3,4,5),c(1,2,3)]
print(iris_5x3)
```

Combinons nos deux exemples précédents pour sélectionner uniquement les lignes avec des fleurs de l'espèce virginica, et les colonnes avec le mot Length.

```{r message=FALSE, warning=FALSE}
iris_virginica3 <- iris[iris$Species == "virginica",
                       names(iris)[grepl("Length",names(iris), fixed = TRUE)]]
head(iris_virginica3, n=5)
```

#### Fusionner des *DataFrames* {#sect01429}

Terminons cette section avec la fusion de *DataFrames* qu'il est possible de réaliser de deux façons : par ajout ou par jointure.

##### Fusionner des *DataFrame* par ajout {#sect014291}

Ajouter deux *DataFrames* peut se faire en fonction de leurs colonnes, ou en fonction de leurs lignes. Dans ces deux cas, on utilisera respectivement les fonction `cbind` et `rbind`. La figure \@ref(fig:fig016) résume graphiquement le fonctionnement des deux fonctions.

```{r fig016, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Fusion de DataFrames",  out.width='30%'}
knitr::include_graphics('images/introduction/rbind_cbind.png', dpi = NA)
```

Pour que `cbind` fonctionne, il faut que les deux *DataFrames* aient le même nombre de lignes. Pour `rbind`, les deux *DataFrames* doivent avoir le même nombre de colonnes.

Prenons à nouveau comme exemple le jeu de données iris. Nous allons commencer par le séparer en trois sous-jeux de données comprenant chacun une espèce d'iris. Puis, nous fusionnerons deux d'entre eux avec la fonction `rbind`.

```{r message=FALSE, warning=FALSE}
iris1 <- subset(iris, iris$Species == "virginica")
iris2 <- subset(iris, iris$Species == "versicolor")
iris3 <- subset(iris, iris$Species == "setosa")

iris_comb <- rbind(iris2,iris3)
```

Nous pourrions aussi extraire dans les deux *DataFrames* les colonnes comprenant le mot *Length* et le mot *Width*, puis les fusionner.

```{r message=FALSE, warning=FALSE}
iris_l <- iris[names(iris)[grepl("Length",names(iris), fixed = TRUE)]]
iris_w <- iris[names(iris)[grepl("Width",names(iris), fixed = TRUE)]]

iris_comb <- cbind(iris_l,iris_w)
names(iris_comb)
```
##### Joindre des *DataFrame* {#sect014292}

Une jointure est une opération un peu plus complexe qu'un simple ajout. L'idée est d'associer des informations de plusieurs *DataFrames* en utilisant une colonne (appelée une clef) présente dans les deux jeux de données. On distingue plusieurs types de jointure : 

* Les jointures internes permettant de combiner les éléments communs entre un DataFrame A et B
* La jointure complète permettant de combiner les éléments présents dans A ou B
* La jointure à gauche, permettant de ne conserver que les éléments présents dans A même s'ils ne trouvent pas leur correspondance dans B.

Ces trois jointures sont présentées à la figure \@ref(fig:fig016); dans ces trois cas, la colonne commune se nomme *id*.

```{r fig017, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Jointure de DataFrames",  out.width='30%'}
knitr::include_graphics('images/introduction/merging.png', dpi = NA)
```

Vous noterez que les deux dernières jointures peuvent produire des valeurs manquantes. Pour réaliser ces opérations, il est possible d'utiliser la fonction `merge`. Prenons un exemple simple à partir d'un petit jeu de données.

```{r message=FALSE, warning=FALSE}
auteurs <- data.frame(
    name = c("Tukey", "Venables", "Tierney", "Ripley", "McNeil", "Apparicio"),
    nationality = c("US", "Australia", "US", "UK", "Australia", "Canada"),
    retired = c("yes", rep("no", 5)))
livres <- data.frame(
    aut = c("Tukey", "Venables", "Tierney", "Ripley", "Ripley", "McNeil","Wickham"),
    title = c("Exploratory Data Analysis",
              "Modern Applied Statistics ...",
              "LISP-STAT",
              "Spatial Statistics", "Stochastic Simulation",
               "Interactive Data Analysis", "R for Data Science"))
```

Nous avons donc deux *DataFrames*, le premier décrivant des auteurs et le second des livres. Effectuons une première jointure interne afin de savoir pour chaque livre la nationnalité de son auteur et si ce dernier est à la retraite.

```{r message=FALSE, warning=FALSE}
df1 <- merge(livres, auteurs, #les deux DataFrames 
             by.x = "aut", by.y = 'name', #les noms des colonnes de jointures
             all.x = FALSE, all.y = FALSE)

print(df1)
```
Cette jointure est interne car les deux paramètres *all.x* et *all.y* ont pour valeur `FALSE`. Ainsi, nous indiquons à la fonction que nous ne souhaitons ni garder tous les éléments du premier *DataFrame* ni tous les éléments du second, mais uniquement les éléments présents dans les deux. Vous noterez ainsi que le livre "R for Data Science" n'est pas présent dans le jeu de données final car son auteur "Wickham" ne fait pas partie du *DataFrame* auteurs. De même, l'auteur "Apparicio" n'apparaît pas dans la jointure, car aucun livre dans le *DataFrame* books n'a été écrit par cet auteur.

Pour conserver tous les livres, nous pouvons effectuer une jointure à gauche en renseignant `all.x = TRUE`. Nous allons ainsi forcer la fonction à garder tous les livres et mettre des valeurs vides aux informations manquantes des auteurs.

```{r message=FALSE, warning=FALSE}
df2 <- merge(livres, auteurs, #les deux DataFrames 
             by.x = "aut", by.y = 'name', #les noms des colonnes de jointures
             all.x = TRUE, all.y = FALSE)

print(df2)
```

Et pour garder tous les livres et tous les auteurs, nous pouvons faire une jointure complète en indiquant `all.x = TRUE` et `all.y = TRUE`.

```{r message=FALSE, warning=FALSE}
df3 <- merge(livres, auteurs, #les deux DataFrames 
             by.x = "aut", by.y = 'name', #les noms des colonnes de jointures
             all.x = TRUE, all.y = TRUE)

print(df3)
```
## Conclusion et ressources pertinentes  {#sect015}
Voilà qui conclut ce chapitre sur les bases du langage R. Vous avez maintenant les connaissances nécessaires pour commencer à travailler. N'hésitez pas à revenir sur les différentes sous-sections au besoin ! Pour aller plus loin dans l'apprentissage du langage, vous pouvez également vous plonger dans le chapitre R AVANCÉ. Cependant, nous vous recommandons de faire vos premiers pas avec cette base avant de vous lancer dans cette partie davantage orientée programmation. Quelques ressources pertinentes qui pourraient vous être utiles sont aussi reportées au tableau ci-dessous. 


```{r tableRessources, echo=FALSE, message=FALSE, warning=FALSE}
if(knitr::is_latex_output()){
  df <- data.frame(
        Ressource = c("Rbloggers","CRAN packages by date", "Introduction à R et au TidyVerse", "Numyard", "Cheasheets"),
        Description = c("Un recueil de nombreux blogues sur R : parfait pour être tenu au courant des nouveautés et faire des découvertes", "Les derniers packages publiés sur CRAN : cela permet de garder un oeil sur les nouvelles fonctionnalités de ses packages préférés", "Une excellente ressource en français pour en apprendre plus sur le tidyverse", "Une chaîne YouTube pour revoir les bases de R en vidéo", "Des feuilles de triche résumant les fonctionnalités de nombreux packages"),
        Url = c("https://www.r-bloggers.com", "https://cran.r-project.org/web/packages","https://juba.github.io/tidyverse","https://www.youtube.com/user/TheLearnR","https://rstudio.com/resources/cheatsheets"))

  show_table(df,
             col.names = c("Ressource","Description","Url"),
             caption = 'Ressources pertinente pour en apprendre plus sur R',
             col.to.resize = c(2,3),
             col.width = "6cm")
}else{
  df <- data.frame(
        Ressource = c("[Rbloggers](https://www.r-bloggers.com){target='_blank'}","[CRAN packages by date](https://cran.r-project.org/web/packages/available_packages_by_date.html){target='_blank'}", "[Introduction à R et au TidyVerse](https://juba.github.io/tidyverse/index.html){target='_blank'}", "[Numyard](https://www.youtube.com/user/TheLearnR/featured){target='_blank'}", "[Cheasheets](https://rstudio.com/resources/cheatsheets){target='_blank'}"), 
        Description = c("Un recueil de nombreux blogues sur R : parfait pour être tenu au courant des nouveautés et faire des découvertes", "Les derniers *packages* publiés sur CRAN : cela permet de garder un oeil sur les nouvelles fonctionnalités de ses *packages* préférés", "Une excellente ressource en français pour en apprendre plus sur le tidyverse", "Une chaîne YouTube pour revoir les bases de R en vidéo", "Des feuilles de triche résumant les fonctionnalités de nombreux *packages*"))

  show_table(df,
             col.names = c("Ressource","Description"),
             caption = 'Ressources pertinente pour en apprendre plus sur R')
}
```


<!--chapter:end:01-priseenmainR.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# (PART) Analyses univariées {-} 

# Statistiques descriptives univariées {#chap02}

Dans ce chapitre, nous décrirons la notion de variable, permettant l’opérationnalisation d’un concept. Comprendre les différents types de variables est essentiel en statistiques. En effet, en fonction du type de variable à l'étude, les tests d’hypothèse et les méthodes de statistique inférentielle que l’on pourra appliquer seront différents. Nous distinguerons ainsi cinq types de variables : nominale, ordinale, discrète, continue et semi-quantitative. Ausi, nous abordons un concept central de la statistique : les distributions. Nous présenterons ensuite les différentes statistiques descriptives univariées qui peuvent s’appliquer à ces types de variables.

::: {.bloc_package data-latex=""}
Dans cette section, nous utiliserons principalement les packages suivants (À MODIFIER PLUS TARD) : 

* Pour créer des graphiques :
  - **ggplot2**, le seul, l'unique
  - **ggpubr** pour combiner des graphiques et réaliser des diagrammes
  
* Pour créer des distribution :
  - **fitdistrplus** pour générer différentes distributions
  - **actuar** pour la fonction de densité de Pareto
  - **gamlss.dist** pour des distributions de Poisson

 * Pour les statistiques descriptives :
  - **stats** pour les statistiques descriptives
  - **nortest** pour le test de Kolmogorov-Smirnov
  - **DescTools** pour les tests de Lilliefors, Shapiro-Wilk, Anderson-Darling et Jarque-Bera

 * Autres *packages* :
  - **Hmisc** et **Weighted.Desc.Stat** pour les statistiques descriptives pondérées
  - **foreign** pour importer des fichiers externes
:::

## Notion de variable {#sect021}

### La variable : l'opérationnalisation d'un concept {#sect0211}

Une variable permet d'opérationnaliser un concept, soit une « idée générale et abstraite que se fait l'esprit humain d'un objet de pensée concret ou abstrait, et qui lui permet de rattacher à ce même objet les diverses perceptions qu'il en a, et d'en organiser les connaissances » ([Larousse](https://www.larousse.fr/dictionnaires/francais/concept/17875?q=concept#17749){target="_blank"}). Pour valider un modèle théorique, il convient alors d'opérationnaliser ses différentes concepts et d'établir les relations qu'ils partagent. L'opérationnalisation d'un concept nécessite soit de mesurer (dans un intervalle de valeurs, c'est-à-dire de manière quantitative), soit de qualifier (avec plusieurs catégories, c'est-à-dire de manière qualitative) un phénomène. 

Selon [Statistique Canada](https://www.statcan.gc.ca/fra/concepts/variable){target="_blank"}, « une variable est une caractéristique d'une unité statistique que l'on observe et pour laquelle une valeur numérique ou une catégorie d'une classification peut être attribuée ». Il convient alors de bien saisir à quelle unité statistique (ou unité d'observation) s'applique les valeurs d'une variable : des personnes, des ménages, des municipalités, etc. 

Prenons deux exemples concrets tirées du Recensement de 2016 de Statistique Canada :

* Le concept **famille de recensement** est défini comme étant « un couple marié et les enfants, le cas échéant, du couple et/ou de l'un ou l'autre des conjoints; un couple en union libre et les enfants, le cas échéant, du couple et/ou de l'un ou l'autre des partenaires; ou un parent seul, peu importe son état matrimonial, habitant avec au moins un enfant dans le même logement et cet ou ces enfants. Tous les membres d'une famille de recensement particulière habitent le même logement. Un couple peut être de sexe opposé ou de même sexe. Les enfants peuvent être des enfants naturels, par le mariage, par l'union libre ou par adoption, peu importe leur âge ou leur état matrimonial, du moment qu'ils habitent dans le logement sans leur propre conjoint marié, partenaire en union libre ou enfant. Les petits-enfants habitant avec leurs grands-parents, alors qu'aucun des parents n'est présent, constituent également une famille de recensement » ([Statistique Canada](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/fam004-fra.cfm){target="_blank"}). À partir de cette définition, les familles de recensement peuvent être qualifiées selon plusieurs modalités : couples mariés sans enfant, couples mariés avec enfants, couples en union libre sans enfant, couples en union libre avec enfant, famille monoparentale (avec un parent de sexe féminin), famille monoparentale (avec un parent de sexe masculin).
* Le concept de **revenu d'emploi** est défini comme étant « tous les revenus reçus sous forme de traitements, salaires et commissions d'un travail rémunéré ou le revenu net d'un travail autonome dans une entreprise agricole ou non agricole non constituée en société et/ou dans l'exercice d'une profession au cours de la période de référence. Pour le Recensement de 2016, la période de référence est l'année civile 2015 pour toutes les variables de revenu » ([Statistique Canada](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop027-fra.cfm){target="_blank"}). Il est donc mesurée en dollars pour chaque individu de 15 ans et plus. Pour l'ensemble de la population de 15 ans et plus, il peut ensuite être classé en déciles de revenu d'emploi, soit en dix groupes ([Statistique Canada](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop204-fra.cfm"}){target="_blank"}).

::: {.bloc_attention data-latex=""}
**Maîtriser la définition des variables que vous utilisez : un enjeu crucial ! **

Nous avons vu qu'une variable est l'opérationnalisation d'un concept. Par conséquent, ne pas maîtriser la définition d'une variable revient à ne pas bien saisir le concept sous-jacent qu'elle tente de mesurer. Si vous exploitez des données secondaires – par exemple, issues d'un recensement de population ou d'une enquête longitudinale ou transversale –, il faut impérativement lire les définitions des variables que vous souhaiteriez utiliser. Ne pas le faire risque d'aboutir à :

* Une mauvaise opérationnalisation de votre modèle théorique, même si votre analyse est bien menée statistiquement parlant. Autrement dit, vous risquez de ne pas sélectionner les bonnes variables. Prenons un exemple concret. Vous avez construit un modèle théorique dans lequel vous souhaitez inclure un concept sur la langue des personnes. Dans le recensement canadien de 2016, plusieurs variables relatives à la langue sont disponibles : [connaissance des langues officielles](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop055-fra.cfm){target="_blank"},
[langue parlée à la maison](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop042-fra.cfm){target="_blank"}, [langue maternelle](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop095-fra.cfm){target="_blank"}, [première langue officielle parlée](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop034-fra.cfm){target="_blank"},  [connaissance des langues non officielles](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop054-fra.cfm){target="_blank"} et [langue de travail](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/pop059-fra.cfm){target="_blank"} ([Statistique Canada, 2019](https://www12.statcan.gc.ca/census-recensement/2016/ref/guides/003/98-500-x2016003-fra.cfm){target="_blank"}). La sélection de l'une de ces variables doit être faite de manière rigoureuse, c'est-à-dire en lien avec votre cadre théorique et suite à une bonne compréhension des définitions des variables. Dans une étude sur le marché du travail, on sélectionnerait probablement la variable *sur la connaissance des langues officielles du Canada*, afin d'évaluer son effet sur l'employabilité, toutes choses étant égales par ailleurs. Dans une autre étude portant sur la réussite ou la performance scolaire, il est probable qu'on utilise plutôt la *langue maternelle*.

* Une mauvaise interprétation et discussion de vos résultats en lien avec votre cadre théorique.
* Une mauvaise identification des pistes de recherche.

Finalement, la définition d'une variable peut évoluer à travers plusieurs recensements de population : la société évolue, les variables aussi ! Par conséquent, si vous comptez utiliser plusieurs années de recensement dans une même étude, assurez-vous que les définitions des variables soient similaires d'un jeu de données à l'autre et qu'elles mesurent ainsi la même chose. 

**Comprendre les variables utilisées dans un article scientifique : un exercice indispensable dans l'élaboration d'une revue de littérature**

Une lecture rigoureuse d'un article scientifique suppose, entre autres, de bien comprendre les concepts et variables mobilisés. Il convient alors de lire attentivement la section méthodologique (pas uniquement la section des résultats ou pire le résumé), sans quoi vous risquez d'aboutir à une revue de littérature approximative. 
Ayez aussi un **regard critique** sur les variables visant à opérationnaliser les  concepts clés de l'étude. Certains concepts sont très difficiles à traduire en variables; leurs opérationalisations (mesures) peuvent ainsi faire l'objet de vifs débats parmi les chercheurs. Très succinctement, c'est notamment le cas du concept de capital social. D'une part, les définitions et ancrages sont biens différents selon Bourdieu (sociologue, ancrage au niveau des individus) et Putman (politologue, ancrage au niveau des collectivités); d'autre part, aucun consensus ne semble clairement se dégager quant à la définition de variables permettant de le mesurer efficacement (de manière quantitative).   

**Variable de substitution (*proxy variable* en anglais)**

On fait la moins pire des recherches ! En effet, les données disponibles sont parfois imparfaites pour répondre avec précision à une question de recherche; on peut toujours les exploiter, tout en signalant honnêtemment leurs faiblesses et limites, et ce, tant pour les données que les variables utilisées.

* Des bases de données peuvent être en effet imparfaites. Par exemple, en criminologie, des chercheur.e.s exploitant des données policières signalent habituellement la limite du **chiffre noir** : les données policières comprennent uniquement les crimes et délits découverts par la police et occultent ainsi les crimes non-découverts; ils ne peuvent ainsi refléter la criminalité réelle sur un territoire donné.

* Des variables peuvent aussi être imparfaites. Dans un jeu de données, il est fréquent qu'une variable opérationnalisant un concept précis ne soit pas disponible ou qu'elle n'ait tout simplement pas été  mesurée. On cherchera alors une variable de substitution (*proxy*) pour la remplacer. Prenons un exemple concret portant sur l'exposition des cyclistes à la pollution atmosphérique ou au bruit environnemental. L'un des principaux facteurs d'exposition à ces pollutions est le trafic routier : plus ce dernier est élevé, plus le cycliste risque de rouler dans un environnement bruyant et pollué. Toutefois, il est rare de disposer de mesures du trafic en temps réel qui nécessitent des comptages de véhicules pendant le trajet des cyclistes (par exemple, à partir de vidéos captées par une caméra fixée sur le guidon). Pour pallier à l'absence de mesures directes, plusieurs auteurs utilisent des variables de substitution de la densité du trafic, comme la typologie des types d'axes (primaire, secondaire, tertiaire, rue locale, etc.), supposant ainsi qu'un axe primaire supporte un volume de véhicules supérieur à un axe secondaire.
:::

### Les types de variables {#sect0212}
On distingue habituellement les variables qualitatives (nominale ou ordinale) des variables quantitatives (discrète ou continue). Tel qu'illustré à la figure \@ref(fig:figunivarie1), l'opérationnalisation du concept en variable est réalisée par différents mécanismes visant à qualifier, classer, compter ou mesurer afin de caractériser les unités statistiques (observations) d'une population ou d'un échantillon.

```{r figunivarie1, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Les types de variables",  out.width='70%'}
knitr::include_graphics('images/univariee/figure1.jpg', dpi = NA)
```

#### Les variables qualitatives {#sect02121}

**Une variable nominale** permet de **qualifier** des observations (individus) à partir de plusieurs catégories dénommées modalités. Par exemple, la variable _couleur des yeux_ pourrait comprendre les modalités _bleu_, _marron_, _vert_ tandis que les *types de familles* compendrait les modalités _couple marié_, _couple en union libre_ et _famille monoparentale_.

**Une variable ordinale** permet de **classer** des observations à partir de plusieurs modalités hiérarchisées. L'exemple le plus connu est certainement l'échelle de Likert, très utilisée dans les sondages évaluant le degré d'accord d'une personne à une affirmation avec les modalités suivantes : _tout à fait d'accord_, _d'accord_, _ni en désaccord ni d'accord_, _pas d'accord_ et _pas du tout d'accord_. Une multitude de variantes sont toutefois possibles pour classer la fréquence d'un phénomène (_Très souvent_, _souvent_, _parfois_, _rarement_, _jamais_), l'importance accordée à un phénomène (_Pas du tout important_, _peu important_, _plus ou moins important_, _important_, _très important_) ou la proximité perçue d'un lieu (_très éloigné_, _loin_, _plus ou moins proche_, _proche_, _très proche_).

En fonction du nombre de modalités qu'elle comprend, une variable qualitative (nominale ou ordinale) est soit **dichtomique (binaire)** (deux modalités), soit **polytomique** (plus de deux modalités). Par exemple, dans le recensement canadien, le *sexe* est une variable binaire (avec les modalités *sexe masculin*, *sexe féminin*), tandis que le *genre* est une variable polytomique (avec les modalités *genre masculin*, *genre féminin* et *diverses identités de genre*).

::: {.bloc_attention data-latex=""}
Les variables nominales et ordinales sont habituellement encodées avec des valeurs numériques entières (par exemple, 1 pour _couple marié_, 2 pour _couple en union libre_ et 3 pour _famille monoparentale_). Toutefois, aucune opération arithmétique (moyenne ou écart-type par exemple) n'est possible sur ces valeurs. Dans R, on utilisera un facteur pour attribuer un intitulé à chacune des valeurs numériques de la variable qualitative :

`df$Famille <- factor(df$Famille, c(1,2,3), labels = c("couple marié","couple en union libre", "famille monoparentale"))`

On calculera toutefois les fréquences des différentes modalités pour une variable nominale ou ordinale. Il est aussi possible de calculer la médiane sur une variable ordinale.
:::

#### Les variables quantitatives {#sect02122}

**Une variable discrète** permet de **compter** un phénomène dans un ensemble fini de valeurs, comme le nombre d'accidents impliquant un cycliste à une intersection sur une période de cinq ans ou encore le nombre de vélos en libre service disponibles à une station. Il existe ainsi une variable binaire sous-jacente : la présence ou non d'un accident à l'intersection ou d'un vélo ou non à la station pour laquelle on opère un comptage. Habituellement, une variable discrète ne peut prendre que des valeurs entières (sans décimales), comme le nombre de personnes fréquentant un parc.

**Une variable continue** permet de **mesurer** un phénomène avec un nombre infini de valeurs réelles (avec décimales) dans un intervalle donné. Par exemple, une variable relative à la distance de dépassement d'un cycliste par un véhicule motorisé pourrait varier de 0 à 5 mètres ($X \in \left[0,5\right]$); toutefois cette distance peut être de 0,759421 ou de 4,785612 mètres. Le nombre de décimales de la valeur réelle dépendra de la précision et de la fiabilité de la mesure. Pour un capteur de distance de dépassement, le nombre de décimales dépendra de la précision du lidar ou du sonar de l'appareil; aussi, l'utilisation de trois décimales – soit une précision au millimètre – est largement suffisant pour mesurer la distance de dépassement. Une variable continue est soit une variable d'intervalle, soit une variable de rapport. Les **variables d'intervalle** ont une échelle relative, c'est-à-dire que les intervalles entre les valeurs de la variables ne sont pas constants; elles n'ont pas de vrai zéro. Ces valeurs peuvent être manipulées uniquement par addition et soustraction et non par multiplication et division. La variable d'intervalle la plus connue est certainement celle de la température. S'il fait 10 degrés Celsius à Montréal et 30°C à Mumbai (soit 50 et 86 degrés en Fahrenheit), on peut affirmer qu'il y a 20°C ou 36°F d'écart entre les deux villes, mais on ne peut pas affirmer qu'il fait trois fois plus chaud à Mumbai. Presque toutes les mesures statistiques sur une variable d'intervalle peuvent être calculées, exceptés le coefficient de variation et la moyenne géométrique puisqu'il n'y a pas de vrai zéro et d'intervalles constants entre les valeurs.  À l'inverse, les **variables de rapport** ont une échelle absolue, c'est-à-dire que les intervalles entre les valeurs sont constants et elles ont un vrai zéro. Elles peuvent ainsi être manipulées par addition, soustraction, multiplication et division. Par exemple, le prix d'un produit exprimé dans une unité monétaire ou la distance exprimée dans le système métrique sont des variables de rapport. Un vélo dont le prix affiché est de 1000$ est bien deux fois plus cher qu'un autre à 500$, une piste cyclable hors rue à 25 mètres du tronçon routier le plus proche est bien quatre fois plus proche qu'une autre à 100 mètres.

**Une variable semi-quantitative**, appelée aussi variable quantitative ordonnée, est une variable discrète ou continue dont les valeurs ont été regroupées en classes hiérarchisées. Par exemple, l'âge est une variable continue pouvant être transformée avec les groupes d'âge ordonnés suivants : *moins 25 ans*, *25 à 44 ans*, *45 à 64 ans* et *65 ans et plus*.


## Les types de données {#sect022}

Différents types de données sont utilisés en sciences sociales. L'objectif ici n'est pas de les décrire en détail, mais plutôt de donner quelques courtes définitions. En fonction de votre question de recherche et des bases des données disponibles ou non, il s'agira de sélectionner le ou les types de données les plus appropriés à votre sujet.

### Données secondaires *versus* données primaires {#sect0221}

Les **données secondaires** sont des données qui existent déjà au début de votre projet de recherche : pas besoin de les collecter, il suffit de les exploiter! Une multitude de données de recensements ou d'enquêtes de Statistique Canada sont disponibles et largement exploitées en sciences sociales (par exemple, l'enquête nationale auprès des ménages – ENM, l'enquête sur la dynamique du marché du travail et du revenu – EDTR, l'enquête longitudinale auprès des immigrants – ELIC, etc.). 
  
::: {.bloc_notes data-latex=""}
Au Canada, les chercheurs (étudiants et professeurs) ont accès aux microdonnées des enquêtes de Statistique Canada dans les Centres de données de recherche (CDR). Vous pouvez consulter le moteur de recherche du ([RCCDR](https://crdcn.org/fr/donn%C3%A9es){target="_blank"}) afin d'explorer les différentes enquêtes disponibles.

Au Québec, l'accès à ces enquêtes est possible dans les différentes antennes du Centre interuniversitaire québécois de statistiques sociales de Statistique Canada ([CIQSS](https://www.ciqss.org/){target="_blank"}).
:::

Par opposition, les **données primaires** n'existent pas quand vous démarrez votre projet : vous devez les collecter spécifiquement pour votre étude! Par exemple, une chercheure souhaitant analyser l'exposition des cyclistes au bruit et à la pollution dans une ville donnée devra réaliser une collecte de données avec idéalement plusieurs participants (équipés de différents capteurs), et ce, sur plusieurs jours. 
Une collecte de données primaires est peut aussi être réalisée avec une enquête par sondage. Brièvement, réaliser une collecte de données primaires nécessite différentes phases complexes comme la définition de la méthode de collecte, de la population à l'étude, l’estimation de la taille de l'échantillon, la validation des outils de collecte avec une phase de test, la réalisation de la collecte, la structuration, la gestion et l'exploitation de données collectées. Finalement, dans le milieu académique, une collecte de données primaires auprès d'individus doit être approuvée par le comité d'éthique de la recherche de l'université à laquelle est affilié le responsable du projet de recherche (qu'il soit professeur, chercheur ou étudiant).

###  Données transversales *versus* données longitudinales {#sect0222}
Les **données transversales** sont des mesures pour une période relativement courte. L’exemple classique est un jeu de données constitué des variables extraites d’un recensement de population pour une année donnée (comme celui 2016 de Statistique Canada). 

Les **données longitudinales**, appelées aussi données par panel, sont des mesures répétées pour plusieurs observations au cours du temps (*N* observations pour *T* dates). Par exemple, des observations pourraient être des pays, les dates pourraient être différentes années (de 1990 à 2019) pour lesquelles différentes variables seraient disponibles (population totale, taux d’urbanisation, produit intérieur brut par habitant, émissions de gaz à effet de serre par habitant, etc).

### Données spatiales versus données aspatiales {#sect0223}

Les observations des **données spatiales** sont des unités spatiales géoréférencées (points, lignes, polygones ou encore pixels d’une image). Elles peuvent être par exemple :

* des points *(x,y)* ou *(lat-long)* représentant des entreprises avec plusieurs variables (adresse, date de création, nombre d'employés, secteurs d'activité, etc.);
*  les lignes représentant des tronçons de rues pour lesquels plusieurs variables sont disponibles (types d’axe, longueur en mètres, nombre de voies, débit journalier moyen annuel, etc.);
 * des polygones délimitant des régions ou des arrondissements pour lesquels une multitude de variables sociodémographiques et socioéconomiques sont disponibles.

À l’inverse, aucune information spatiale n’est disponible pour des **données aspatiales**. 


### Données individuelles *versus* données agrégées {#sect0224}

Comme son nom l'indique, pour des **données individuelles**, chaque observation correspond à un individu. Les microdonnées de recensement ou d'enquêtes, par exemple, sont des données individuelles pour lesquelles toute une série de variables est disponible. Une étude analysant les caractéristiques de chaque arbre d'un quartier nécessite aussi des données individuelles : l'information doit être disponible pour chaque arbre. Pour les microdonnées des recensements canadiens, « chaque enregistrement au niveau de la personne comprend des identifiants (comme les identifiants du ménage et de la famille), des variables géographiques et des variables directes et dérivées tirées du questionnaire » ([Statistique Canada](https://www150.statcan.gc.ca/n1/pub/12-002-x/2012001/article/11642-fra.htm){target="_blank"}). Comme signalé plus haut, ces microdonnées de recensement ou d'enquêtes sont uniquement accessibles dans les Centres de données de recherche (CDR).

Les données individuelles peuvent être **agrégées** à un niveau supérieur. Prenons le cas de microdonnées d'un recensement. Les informations disponibles pour chaque individu sont agrégées par territoire géographique (province, région économique, division de recensement, subdivision de recensement, région et agglomération de recensement, secteurs de recensement, aires de diffusion, etc.) en fonction du lieu d'habitation des individus. Des sommaires statistiques – basés sur la moyenne, la médiane, la somme ou la proportion de chacune des variables mesurées au niveau individuel (âge, sexe, situation familiale, revenu, etc.) – sont alors construits pour ces différents découpages géographiques ([Statistique Canada](https://www.statcan.gc.ca/fra/idd/trousse/section5#a4){target="_blank"}).

L'agrégation n'est pas nécessairement géographique. En éducation, il est fréquent de travailler avec des données concernant les élèves, mais agrégées au niveau des écoles. La figure \@ref(fig:figunivarie1b) donne un exemple simple d'agrégation de données individuelles.

```{r figunivarie1b, echo=FALSE, fig.align='center', fig.cap="Exemple d'agrégation de données individuelles", auto_pdf=TRUE, out.width='65%'}
  knitr::include_graphics('images/univariee/aggregation.png', dpi = NA)
```

Pour le cas de l'agrégation géographique, il convient alors de bien comprendre la hiérarchie des régions géographiques délimitées par l’organisme ou l’agence ayant la responsabilité de produire, gérer et diffuser les données des recensements et des enquêtes, puis de sélectionner le découpage géographique qui répond le mieux à votre question de recherche.

::: {.bloc_astuce data-latex=""}
Pour le recensement de 2016 de Statistique Canada vous pourrez consulter :

* la [hiérarchie des régions géographiques normalisées pour la diffusion](https://www12.statcan.gc.ca/census-recensement/2016/ref/dict/figures/f1_1-fra.cfm){target="_blank"}
* le [glossaire illustré](https://www150.statcan.gc.ca/n1/pub/92-195-x/92-195-x2016001-fra.htm){target="_blank"} des régions géographiques

* les différents [profils du recensement de 2016](https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/prof/details/download-telecharger/comp/page_dl-tc.cfm?Lang=F){target="_blank"} à télécharger pour les différentes régions géographiques.
:::

::: {.bloc_notes data-latex=""}
Bien entendu, les différents types de données abordés ci-dessus ne sont pas exclusifs. Par exemple, des données pour des régions administratives extraites de plusieurs recensements sont en fait des données secondaires, spatiales, agrégées et longitudinales. 

Une collecte de données sur la pollution atmosphérique et sonore réalisée à vélo (avec différents capteurs et un GPS) sont des données spatiales primaires.
:::

## Statistique descriptive et statistique inférentielle {#sect023}

### Population, échantillon et inférence {#sect0231}

Les notions de **population** et d'**échantillon** sont essentielles en statistique puisqu'elles sont le socle de l'inférence statistique.
Un échantillon est un **sous-ensemble représentatif** d'une population donnée. Prenons un exemple concret. Une chercheure veut comprendre la mobilité des étudiants d'une université. Bien entendu, elle ne pourra interroger l’ensemble des étudiants de son université.  Elle devra alors s’assurer d'obtenir un échantillon de taille suffisante et représentatif de la population étudiante. Une fois les données collectées (avec un sondage par exemple), elle pourra utiliser des techniques inférentielles pour analyser la mobilité des étudiants interrogés. Si son échantillon est représentatif, les résultats obtenus pourront être inférés – c'est-à-dire généralisés, extrapolés – à l’ensemble de la population.

::: {.bloc_aller_loin data-latex=""}
**Les méthodes d’échantillonnage**

Nous n’abordons pas ici les méthodes d’échantillonnage. Sachez toutefois qu’il existe plusieurs méthodes probabilistes pour constituer un échantillon, notamment de manière aléatoire, systématique, stratifiée, par grappes ([voir par exemple cette publique de Statistique Canada](https://www150.statcan.gc.ca/n1/edu/power-pouvoir/ch13/prob/5214899-fra.htm){target="_blank"}).
:::

Autre exemple, une autre chercheure souhaite comprendre les facteurs influençant le sentiment de sécurité des cyclistes dans un quartier. De nouveau, elle ne pourra pas enquêter tous les cyclistes du quartier et devra constituer un échantillon représentatif. Par la suite, la mise en œuvre de techniques inférentielles lui permettra d'identifier les caractéristiques individuelles (âge, sexe, habiletés à vélo, etc.) et de l'environnement urbain (types de voies empruntés, niveaux de trafic, de pollution, de bruit, etc.) ayant des effets significatifs sur le sentiment de sécurité. Si l'échantillon est représentatif, les résultats pourront être généralisés à l'ensemble des cyclistes du quartier.


### Deux grandes familles de méthodes statistiques {#sect0232}

On distingue deux grandes familles de méthodes statistiques :

* « **La statistique descriptive et exploratoire** : elle permet, par des résumés et des graphiques plus ou moins élaborés, de décrire des ensembles de données statistiques, d’établir des relations entre les variables sans faire jouer de rôle privilégié à une variable particulière. Les conclusions ne portent dans cette phase de travail que sur les données étudiées, sans être inférées à une population plus large. L’analyse exploratoire s’appuie essentiellement sur des notions élémentaires telles que des indicateurs de moyenne et de dispersion, sur des représentations graphiques. [...]
* **La statistique inférentielle et confirmatoire** : elle permet de valider ou d’infirmer, à partir de tests statistiques ou de modèles probabilistes, des hypothèses formulées a priori (ou après une phase exploratoire), et d’extrapoler, c’est-à-dire d’étendre certaines propriétés d’un échantillon à une population plus large. Les conclusions obtenues à partir des données vont au-delà de ces données. La statistique confirmatoire fait surtout appel aux méthodes dites explicatives et prévisionnelles, destinées comme leurs noms l’indiquent, à expliquer puis à prévoir, suivant des règles de décision, une variable privilégiée à l’aide d’une ou plusieurs variables explicatives (régressions multiples et logistiques, analyse de variance, analyse discriminante, segmentation, etc.) » [@lebart1995statistique, p. 209].


## La notion de distribution {#sect024}

::: {.bloc_objectif data-latex=""}
Dans cette section, nous abordons un concept central de la statistique : les distributions. Prenez le temps de lire cette section à tête reposée et assurez-vous de bien comprendre chaque idée avant de passer à la suivante. N’hésitez pas à y revenir plusieurs fois si nécessaire, car la compréhension de ces concepts est essentielle pour utiliser adéquatement les méthodes que nous abordons dans ce livre.
:::

### Définitions générales

En statistique, on s’intéresse aux résultats d’expériences. Lancer un dé, mesurer la pollution atmosphérique, compter le nombre de collisions à une intersection, demander à une personne d’évaluer son sentiment de sécurité sur une échelle de 1 à 10 sont autant d’expériences pouvant produire des résultats.

**Une distribution est une fonction permettant d’associer pour chaque résultat possible d’une expérience la probabilité d’obtenir ce résultat**. En d’autres termes, il s’agit d’une fonction indiquant par exemple que pour l’expérience : « mesurer la concentration d’ozone à Montréal à 13h en été », la probabilité de mesurer une valeur inférieure à 15 μg/m<sup>3</sup> est de seulement 2%.

Les distributions sont toujours définies dans un intervalle en dehors duquel elles sont indéfinies; les valeurs dans cet intervalle sont appelées **l’espace d’échantillonnage**. Il s’agit donc des valeurs possibles que peut produire l’expérience. La somme des probabilités de l’ensemble des valeurs de l’espace d’échantillonnage est 1 (100%). Intuitivement, cela signifie que si l’on réalise l’expérience, on est obligé d’obtenir un résultat, et que cette probabilité totale est répartie entre tous les résultats possibles de l’expérience. En langage mathématique, on dit que l’intégrale des fonctions de distribution est 1 dans leur intervalle de définition.

Prenons un exemple concret avec l’expérience suivante : tirer à pile ou face avec une pièce de monnaie non truquée. Si l’on souhaite décrire la probabilité d’obtenir pile ou face, on peut utiliser une distribution qui aura comme espace d’échantillonnage [pile ; face] et ces deux valeurs auront chacune comme probabilité 0,5. Il est facile d’étendre cet exemple au cas d’un dé à six faces. La distribution de probabilité décrivant l’expérience « lancer le dé » a pour espace d’échantillonnage [1,2,3,4,5,6], chacune de ces valeurs étant associée à la probabilité 1/6.

Les deux distributions précédentes appartiennent à la famille des distributions **discrètes**. Elles servent à décrire des expériences dont le nombre de valeurs possibles est fini. Par opposition, la seconde famille de distributions regroupe les distributions **continues**, décrivant des expériences dont le nombre de résultats possibles est infini. Par exemple, mesurer la taille d’une personne adulte sélectionnée au hasard peut produire un nombre infini de valeurs comprises entre 50 cm et 280 cm. Les distributions sont utiles pour décrire les résultats attendus d’une expérience. Reprenons notre exemple du dé. Nous savons que chaque face a une chance sur six d’être tirée au hasard. Nous pouvons représenter cette distribution avec un graphique (figure \@ref(fig:fig251)). 

```{r fig251, echo=FALSE, fig.align='center', fig.cap="Distribution théorique d'un lancé de dé", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='50%'}
library(ggplot2)
df <- data.frame(
  face = 1:6,
  prob_theorique = 1/6
)
ggplot(mapping = aes(x = face, weight = prob_theorique), data = df) + 
  geom_bar()+
  geom_bar()+
  labs(x = "face du dé",
       y = "probabilité")+
  scale_x_continuous(breaks = c(1,2,3,4,5,6))+
  ylim(c(0,0.5))
```
Nous avons donc sous les yeux un modèle statistique décrivant le comportement attendu d’un dé, nous l’appelons la distribution **théorique**. Cependant, si nous effectuons l’expérience 10 fois (nous collectons donc un échantillon), nous obtiendrons une distribution différente de cette distribution théorique (figure \@ref(fig:fig252)).

```{r fig252, echo=FALSE, fig.align='center', fig.cap="Distribution empirique d'un lancé de dé (n=10)", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='50%'}
n <- 10
results <- sample(c(1,2,3,4,5,6),size = n,replace = T)
counts <- table(results) / n
df2 <- data.frame(face = as.numeric(names(counts)),
                prob_exp10 = as.vector(counts))
df <- merge(df, df2, by = "face", all.x=T)
ggplot(mapping = aes(x = face, weight = prob_exp10), data = df) + 
  geom_bar()+
  geom_bar()+
  labs(x = "face du dé",
       y = "probabilité")+
  scale_x_continuous(breaks = c(1,2,3,4,5,6))+
  ylim(c(0,0.5))
```
Nous appelons cette distribution la distribution **empirique**. Chaque échantillon aura sa propre distribution empirique. Cependant, comme le prédit la loi des grands nombres (ou théorème de Bernoulli) : si une expérience est répétée un grand nombre de fois, la probabilité empirique d’un résultat se rapproche de la probabilité théorique à mesure que le nombre de répétitions augmente. Pour nous en convaincre, collectons trois échantillons de lancer de dé de respectivement 30, 100 et 1000 observations (figure \@ref(fig:fig253)).

```{r fig253, echo=FALSE, fig.align='center', fig.cap="Distribution empirique d'un lancé de dé (n=10)", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
library(reshape2)
library(dplyr)
#### empirical, 30 replications
n <- 30
results <- sample(c(1,2,3,4,5,6),size = n,replace = T)
counts <- table(results) / n
df3 <- data.frame(face = as.numeric(names(counts)),
                  prob_exp30 = as.vector(counts))
df <- merge(df, df3, by = "face", all.x=T)
#### empirical, 100 replications
n <- 100
results <- sample(c(1,2,3,4,5,6),size = n,replace = T)
counts <- table(results) / n
df4 <- data.frame(face = as.numeric(names(counts)),
                  prob_exp100 = as.vector(counts))
df <- merge(df, df4, by = "face", all.x=T)
#### empirical, 10000 replications
n <- 10000
results <- sample(c(1,2,3,4,5,6),size = n,replace = T)
counts <- table(results) / n
df5 <- data.frame(face = as.numeric(names(counts)),
                  prob_exp10000 = as.vector(counts))
df <- merge(df, df5, by = "face", all.x=T)
df$prob_theorique <- NULL
# ploting the resul
df_melt <- melt(df,id.vars = c('face'))
df_melt$variable <- case_when(
  df_melt$variable == 'prob_theorique' ~ "distribution théorique",
  df_melt$variable == 'prob_exp10' ~ "distribution empirique (n=10)",
  df_melt$variable == 'prob_exp30' ~ "distribution empirique (n=30)",
  df_melt$variable == 'prob_exp100' ~ "distribution empirique (n=100)",
  df_melt$variable == 'prob_exp10000' ~ "distribution empirique (n=10000)"
)
df_melt$f_exp <- factor(df_melt$variable,
                           levels = c("distribution théorique",
                                      "distribution empirique (n=10)",
                                      "distribution empirique (n=30)",
                                      "distribution empirique (n=100)",
                                      "distribution empirique (n=10000)"))
ggplot(mapping = aes(x = face, weight = value), data = df_melt)+
  geom_bar()+
  labs(x = "face du dé",
       y = "probabilité")+
  scale_x_continuous(breaks = c(1,2,3,4,5,6))+
  facet_wrap(vars(f_exp), ncol=2)
```
On constate bien qu’au fur et à mesure que la taille de l’échantillon augmente, on tend vers la distribution théorique. Ces dernières sont donc utilisées pour modéliser des phénomènes réels et sont à la base de presque tous les tests statistiques d'inférence fréquentiste ou bayésienne.

En pratique, la question que l’on se pose le plus souvent est : quelle distribution théorique peut le mieux décrire le phénomène empirique à l’étude ? Pour répondre à cette question, deux approches sont possibles :

* Considérant la littérature existante sur le sujet, les connaissances accumulées et la nature de la variable étudiée, il est possible de sélectionner des distributions théoriques pouvant vraisemblablement correspondre à la variable.
* Comparer visuellement ou à l’aide de tests statistiques la distribution empirique de la variable et diverses distributions théoriques pour trouver la plus adaptée.

Idéalement, le choix d’une distribution théorique devrait reposer sur ces deux méthodes combinées.

### Anatomie d'une distribution

Puisqu’une distribution est une fonction, il est possible de la représenter à l’aide d’une formule mathématique (appelée **fonction de masse** pour les distributions discrètes et **fonction de densité** pour les distributions continues). Prenons un premier exemple concret avec la distribution théorique associée au lancer de pièce de monnaie : la distribution de **Bernoulli**. Sa formule est la suivante :

\begin{equation}\footnotesize f(x ; p)=\left\{\begin{array}{ll}
q=1-p & \text { si } x=0 \\
p & \text { si } x=1
\end{array}\right.
(\#eq:Bernoulli)
\end{equation}

avec *p* la probabilité d’obtenir $x = 1$ (pile), et $1 – p$ la probabilité d’avoir $x = 0$ (face). La distribution de Bernoulli ne dépend que d’un paramètre : *p*. Avec différentes valeurs de *p*, on peut obtenir différentes formes pour la distribution de Bernoulli. Si *p* = 1/2, la distribution de Bernoulli décrit parfaitement l’expérience : obtenir pile à un lancer de pièce de monnaie. Si *p* = 1/6, elle décrit alors l’expérience : obtenir 4 (tout comme n’importe quelle valeur de 1 à 6) à un lancer de dé. Pour un exemple plus appliqué, la distribution de Bernoulli est utilisée en analyse spatiale pour étudier la concentration d’accidents de la route ou de crimes en milieu urbain. En chaque endroit du territoire, il est possible de calculer la probabilité qu’un tel évènement ait lieu ou non en se basant sur les données observées et cette distribution.
La distribution continue la plus simple à décrire est certainement la distribution **uniforme**. Il s’agit d’une distribution un peu spéciale puisqu’elle attribue la même probabilité à toutes ses valeurs dans son espace d’échantillonnage. Elle est définie sur l’intervalle [-Inf ; +Inf] et a la fonction de densité suivante : 

\begin{equation}\footnotesize f(x ; \mathrm{a} ; \mathrm{b})=\left\{\begin{array}{cc}
\frac{1}{a-b} & \text { si } a \geq x \geq b \\
0 & \text { sinon }
\end{array}\right.
(\#eq:Uniforme)
\end{equation}

La fonction uniforme a donc deux paramètres, *a* et *b*, représentant respectivement les valeurs maximale et minimale au-delà desquelles les valeurs ont une probabilité 0 d’être obtenues. Pour avoir une meilleure intuition de ce que décrit une fonction de densité, il est intéressant de la représenter avec un graphique (figure \@ref(fig:fig254)). Notez que sur ce graphique, l'axe des ordonnées n'indique pas précisément la probabilité associée à chaque valeur car celle-ci serait infinidécimale. Il sert uniquement à représenter la densité de la fonction de distribution.

```{r fig254, echo=FALSE, fig.align='center', fig.cap="Distributions uniformes continues", auto_pdf=TRUE, out.width='60%'}
ggplot()+
  xlim(-18,18)+
  stat_function(aes(color = '#d90429'),fun = dunif, 
                args = list(min = -15, max = 15), size = 1)+
  stat_function(aes(color = '#8d99ae'),fun = dunif, 
                args = list(min = -10, max = 10), size = 1)+
  stat_function(aes(color = '#2b2d42'), fun = dunif, 
                args = list(min = 1, max = 11), size = 1)+
  labs(y = 'densité',
       x = "x",
       title = 'distribution uniforme')+
  scale_color_identity(name = "Paramètres",
                      breaks = c('#d90429', '#8d99ae', '#2b2d42'),
                      labels = c("a = 15 ; b = -15", "a = 10 ; b = -10",
                                 "a = 1 ; b = 11"),
                      guide = "legend")
```

On observe clairement que toutes les valeurs de *x* entre *a* et *b* ont la même probabilité pour chacune de trois distributions uniformes présentées dans le graphique. Plus l’étendue est grande ($a-b$), plus l’espace d’échantillonnage est grand et plus la probabilité totale est répartie dans cet espace. Cette distribution serait donc idéale pour décrire un phénomène pour lequel chaque valeur a autant de chance de se produire qu’une autre.  Prenons pour exemple un cas fictif avec un jeu de hasard qui vous proposerait la situation suivante : en tirant sur la manette d’une machine à sous, un nombre est tiré aléatoirement entre -60 et +50. Si le nombre est négatif, vous perdez de l’argent et inversement si le nombre est positif. Nous pouvons représenter cette situation avec une distribution uniforme continue et l’utiliser pour calculer quelques informations essentielles : 

1. Selon cette distribution, quelle est la probabilité de gagner de l’argent lors d’un tirage (x > 0)? 
2. Quelle est la probabilité de perdre de l’argent ? (x < 0)?
3. Si je perds moins de 30$ au premier tirage, quelle est la probabilité que ai-je d’au moins récupérer ma mise au second tirage (x > 30)?

Il est assez facile de calculer ces probabilités en utilisant la fonction `punif` dans R. Concrètement, cela permet de calculer l’intégrale de la fonction de masse sur un intervalle donné.

```{r}
# Probabilité d'obtenir une valeur supérieure ou égale à 0
punif(0,min = -60, max = 50)
# Probabilité d'obtenir une valeur inférieure à 0
punif(0,min = -60, max = 50, lower.tail = F)
# Probabilité d'obtenir une valeur supérieure à 30
punif(30, min = -60, max = 50,lower.tail = F)
```

Les paramètres permettent donc d’ajuster la fonction de masse ou de densité d’une distribution afin de lui permettre de prendre des formes différentes. Certains paramètres vont changer la localisation de la distribution (la déplacer vers la droite ou la gauche de l’axe des X), d’autres son degré de dispersion (distribution pointue ou aplatie) ou encore sa forme (symétrie). Les différents paramètres d’une distribution correspondent donc à sa carte d’identité et donnent une idée précise sur sa nature.

### Principales distributions

Il existe un très grand nombre de distributions théoriques et parmi elles, de nombreuses sont en fait des cas spéciaux d’autres distributions. Pour un petit aperçu du bestiaire, vous pouvez faire un saut à la page [Univariate Distribution Relationships](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html){target='_blank'}, qui liste près de 80 distributions. 

Nous nous concentrons ici sur une sélection de 18 distributions très répandues en sciences sociales. La figure \@ref(fig:fig255) présente graphiquement leurs fonctions de masse et de densité présentées dans cette section. Notez que ces graphiques correspondent tous à une forme possible de chaque distribution. En modifiant leurs paramètres, il serait possible de produire une figure très différente. Les distributions discrètes sont représentées avec des graphiques en barres, et les distributions continues avec des graphiques de densité.

(ref:figdistribs) 18 distributions essentielles, design inspiré de @SeanOwendist

```{r figdistribs, echo=FALSE, fig.align='center', fig.cap='(ref:figdistribs)', auto_pdf=TRUE, out.width='95%'}
knitr::include_graphics('images/distributions/all_distributions.png', dpi = NA)
```

#### La distribution uniforme discrète
Nous avons déjà abordé cette distribution dans les exemples précédents. Elle permet de décrire un phénomène dont tous les résultats possibles ont exactement la même probabilité de se produire. L’exemple classique est bien sûr un lancer de dé.

#### La distribution de Bernoulli
La distribution de Bernoulli permet de décrire une expérience pour laquelle deux résultats sont possibles. Son espace d’échantillonnage est donc $[0 ; 1]$. Sa fonction de masse est la suivante : 

\begin{equation}\footnotesize f(x ; p)=\left\{\begin{array}{ll}
q=1-p & \text { si } x=0 \\
p & \text { si } x=1
\end{array}\right.
(\#eq:BernoulliB)
\end{equation}

avec *p*, la probabilité d’obtenir $x = 1$ (réussite) et donc $1 – p$, la probabilité d’avoir $x = 0$ (échec). La distribution de Bernoulli ne dépend que d’un paramètre : *p* contrôlant la probabilité de réussite de l’expérience. Notez que si $p = 1/2$, alors la distribution de Bernoulli est également une distribution uniforme.  Un exemple d’application de la distribution de Bernoulli en études urbaines serait la modélisation de la survie d’un cycliste (1 pour survie, 0 pour décès) lors d’une collision avec une voiture selon une vitesse donnée.

#### La distribution binomiale
La distribution binomiale est utilisée pour caractériser une somme de distributions de Bernoulli. Un exemple simple serait l’accumulation des lancers d’une pièce de monnaie. Si l’on compte le nombre de fois où l’on fait pile, cette expérience est décrite par une distribution binomiale. Son espace d’échantillonnage est donc $[0 ; +\infty[$ (limité aux nombres entiers). Sa fonction de masse est la suivante : 

\begin{equation}\footnotesize 
    f(x ; n )=\binom{n}{x}p^x(1-p)^{n-x}
(\#eq:Binomial)
\end{equation}

avec *x* le nombre de tirages réussis sur *n* essais avec une probabilité *p* de réussite à chaque tirage. Pour reprendre l’exemple précédent concernant les accidents de la route, une distribution binomiale permettrait de représenter la distribution du nombre de cyclistes survivants sur dix accidents impliquant une voiture à une intersection.  

```{r fig256, echo=FALSE, fig.align='center', fig.cap="La distribution binomiale", auto_pdf=TRUE, out.width='95%'}
data <- data.frame(y1 = dbinom(1:15, size=15, prob=.10),
                   y2 = dbinom(1:15, size=15, prob=.25),
                   y3 = dbinom(1:15, size=15, prob=.5),
                   y4 = dbinom(1:15, size=15, prob=.75),
                   x = 1:15)
df <- melt(data, id.vars = "x")
df$f_prob <- case_when(
  df$variable == "y1" ~ "p = 0.1",
  df$variable == "y2" ~ "p = 0.25",
  df$variable == "y3" ~ "p = 0.5",
  df$variable == "y4" ~ "p = 0.75",
)
df$f_prob <- factor(as.character(df$f_prob),
                           levels = c("p = 0.1",
                                      "p = 0.25",
                                      "p = 0.5",
                                      "p = 0.75"))
 
ggplot(df) +
  geom_bar(aes(x=x, weight = value), width = 0.2, fill = '#99ccff')+
  ylim(0,0.4)+
  labs(x = "Nombre de tirages réussis pour 15 tirages",
       y = "Probabilité")+
   facet_wrap(vars(f_prob), ncol=2)
```

#### La distribution géométrique
La distribution géométrique permet de représenter le nombre de tirages nécessaires avec une distribution de Bernoulli avant d’obtenir une réussite. Par exemple, avec un lancer de dé, l’idée serait de compter le nombre de lancers nécessaires avant de tomber sur un 6. Son espace d’échantillonnage est donc $[1 ; +\infty[$ (limité aux nombres entiers). Sa distribution de masse est la suivante : 

\begin{equation}\footnotesize f(x ; p)= (1-p)^xp
(\#eq:geometrique)
\end{equation}

avec *x* le nombre de tentatives avant d’obtenir une réussite, $f(x)$ la probabilité que le premier succès n’arrive qu’après *x* tentatives et *p* la probabilité de réussite à chaque tentative. Cette distribution est notamment utilisée en marketing pour modéliser le nombre d’appels nécessaires avant de réussir une vente.

```{r fig257, echo=FALSE, fig.align='center', fig.cap="La distribution géométrique", auto_pdf=TRUE, out.width='95%'}
data <- data.frame(y1 = dgeom(1:15, prob=.10),
                   y2 = dgeom(1:15, prob=.25),
                   y3 = dgeom(1:15, prob=.5),
                   y4 = dgeom(1:15, prob=.75),
                   x = 1:15)
df <- melt(data, id.vars = "x")
df$f_prob <- case_when(
  df$variable == "y1" ~ "p = 0.1",
  df$variable == "y2" ~ "p = 0.25",
  df$variable == "y3" ~ "p = 0.5",
  df$variable == "y4" ~ "p = 0.75",
)
df$f_prob <- factor(as.character(df$f_prob),
                           levels = c("p = 0.1",
                                      "p = 0.25",
                                      "p = 0.5",
                                      "p = 0.75"))
 
ggplot(df) +
  geom_bar(aes(x=x, weight = value), width = 0.2, fill = '#99ccff')+
  ylim(0,0.3)+
  scale_x_continuous(breaks = seq(1,15,by = 2))+
  labs(x = "Nombre de tirages avant d'obtenir une réussite",
       y = "Probabilité")+
   facet_wrap(vars(f_prob), ncol=2)
```

#### La distribution binomiale négative
La distribution binomiale négative est proche de la distribution géométrique. Elle permet de représenter le nombre de tentatives nécessaires afin d’obtenir un nombre *n* de réussites $[1 ; +\infty[$ (limité aux nombres entiers positifs). Sa formule est la suivante : 

\begin{equation}\footnotesize f(x ; n ; p)=\left(\begin{array}{c}
x+n-1 \\
n
\end{array}\right) p^{n}(1-p)^{x}
(\#eq:binomialnegative)
\end{equation}

avec *x* le nombre de tentatives avant d’obtenir *n* réussites et *p* la probabilité d’obtenir une réussite à chaque tentative. Cette distribution pourrait être utilisée pour modéliser le nombre de questionnaires *x* à envoyer pour une enquête si l’on espère au moins *n* réponses, sachant que la probabilité d’une réponse est *p*.

```{r fig258, echo=FALSE, fig.align='center', fig.cap="La distribution binomiale négative", auto_pdf=TRUE, out.width='95%'}
data <- data.frame(y1 = dnbinom(1:15, size = 5, prob=.10),
                   y2 = dnbinom(1:15, size = 5, prob=.25),
                   y3 = dnbinom(1:15, size = 5, prob=.5),
                   y4 = dnbinom(1:15, size = 5, prob=.75),
                   x = 1:15)
df <- melt(data, id.vars = "x")
df$f_prob <- case_when(
  df$variable == "y1" ~ "p = 0.1",
  df$variable == "y2" ~ "p = 0.25",
  df$variable == "y3" ~ "p = 0.5",
  df$variable == "y4" ~ "p = 0.75",
)
df$f_prob <- factor(as.character(df$f_prob),
                           levels = c("p = 0.1",
                                      "p = 0.25",
                                      "p = 0.5",
                                      "p = 0.75"))
 
ggplot(df) +
  geom_bar(aes(x=x, weight = value), width = 0.2, fill = '#99ccff')+
  ylim(0,0.3)+
  scale_x_continuous(breaks = seq(1,15,by = 2))+
  labs(x = "Nombre de tirages avant d'obtenir cinq réussites",
       y = "Probabilité")+
   facet_wrap(vars(f_prob), ncol=2)
```

#### La distribution de poisson
La distribution de poisson est utilisée pour modéliser des comptages. Son espace d’échantillonnage est donc $[0 ; +\infty[$ (limité aux nombres entiers positifs). Par exemple, il est possible de compter à une intersection le nombre de collisions entre des automobilistes et des cyclistes sur une période donnée. Cet exemple devrait vous faire penser à la distribution binomiale vue plus haut. En effet, il serait possible de noter chaque rencontre entre une voiture et un cycliste et de considérer que leur collision est une « réussite » (0 : pas d’accidents, 1 : accident). Cependant, ce type de données serait fastidieux à collecter comparativement au simple comptage des accidents. La distribution de poisson à une fonction de densité avec un seul paramètre $\lambda$ (lambda) et est décrite par la formule suivante : 

\begin{equation}\footnotesize f(x ; \lambda)=\frac{\lambda^{x}}{x !} e^{-\lambda}
(\#eq:poisson)
\end{equation}

avec *x* le nombre de cas, *f(x)* la probabilité d’obtenir *x* sachant $\lambda$. $\lambda$ peut être vue comme le taux moyen d’occurrences (nombre d’évènements divisé par la durée totale de l’expérience). Il permet à la fois de caractériser le centre et la dispersion de la distribution. Notez également que plus le paramètre \lambda augmente, plus la distribution de poisson tend vers une distribution normale.

```{r fig259, echo=FALSE, fig.align='center', fig.cap="La distribution de poisson", auto_pdf=TRUE, out.width='95%'}
data <- data.frame(y1 = dpois(1:20, lambda = 1),
                   y2 = dpois(1:20, lambda = 3),
                   y3 = dpois(1:20, lambda = 5),
                   y4 = dpois(1:20, lambda = 10),
                   x = 1:20)
df <- melt(data, id.vars = "x")
df$f_prob <- case_when(
  df$variable == "y1" ~ "lambda = 1",
  df$variable == "y2" ~ "lambda = 3",
  df$variable == "y3" ~ "lambda = 5",
  df$variable == "y4" ~ "lambda = 10",
)
df$f_prob <- factor(as.character(df$f_prob),
                           levels = c("lambda = 1",
                                      "lambda = 3",
                                      "lambda = 5",
                                      "lambda = 10"))
 
ggplot(df) +
  geom_bar(aes(x=x, weight = value), width = 0.2, fill = '#99ccff')+
  scale_x_continuous(breaks = seq(1,20,by = 2))+
  labs(x = "Nombre de cas",
       y = "Probabilité")+
   facet_wrap(vars(f_prob), ncol=2)
```

#### La distribution de poisson avec excès de zéros
Il arrive régulièrement qu’une variable de comptage mesurée produise un très grand nombre de zéros. Prenons pour exemple le nombre de seringues de drogue injectable par tronçon de rue ramassées sur une période d’un mois. À l’échelle de toute une ville, un très grand nombre de tronçons n’auront tout simplement aucune seringue et dans ce contexte, la distribution classique de poisson n’est pas adaptée. On lui préfère alors sa version avec une inflation de zéros qui inclut un paramètre contrôlant la forte présence de zéros. Sa fonction de densité est la suivante : 

\begin{equation}\footnotesize f(x ; \lambda; p)=(1-p)\frac{\lambda^{x}}{x !} e^{-\lambda}
(\#eq:poissonzi)
\end{equation}

Plus exactement, la distribution de poisson avec excès de zéro (zero-inflated en anglais) est une combinaison de deux processus générant des zéros. En effet, un zéro peut être produit par la distribution de poisson originale (aussi appelé vrai zéro) ou alors par le processus menant à la surreprésentation des 0 dans le jeu de données, capturée par la probabilité *p* (faux zéro). *p* est donc le paramètre contrôlant la probabilité d’obtenir un zéro, indépendamment du phénomène étudié.

```{r fig259b, echo=FALSE, fig.align='center', fig.cap="La distribution de poisson avec excès de zéros", auto_pdf=TRUE, out.width='95%'}
library(VGAM)
data <- data.frame(y1 = dzipois(0:20, lambda = 1, pstr0 = 0.2),
                   y2 = dzipois(0:20, lambda = 3, pstr0 = 0.4),
                   y3 = dzipois(0:20, lambda = 5, pstr0 = 0.1),
                   y4 = dzipois(0:20, lambda = 10, pstr0 = 0.5),
                   x = 0:20)

df <- melt(data, id.vars = "x")
df$f_prob <- case_when(
  df$variable == "y1" ~ "lambda = 1 & p = 0.2",
  df$variable == "y2" ~ "lambda = 3 & p = 0.4",
  df$variable == "y3" ~ "lambda = 5 & p = 0.1",
  df$variable == "y4" ~ "lambda = 10 & p = 0.5",
)


df$f_prob <- factor(as.character(df$f_prob),
                           levels = c("lambda = 1 & p = 0.2",
                                      "lambda = 3 & p = 0.4",
                                      "lambda = 5 & p = 0.1",
                                      "lambda = 10 & p = 0.5"))
 

ggplot(df) +
  geom_bar(aes(x=x, weight = value), width = 0.2, fill = '#99ccff')+
  scale_x_continuous(breaks = seq(0,20,by = 2))+
  labs(x = "Nombre de cas",
       y = "Probabilité")+
   facet_wrap(vars(f_prob), ncol=2)

```

#### La distribution gaussienne
Plus communément appelée la distribution normale, la distribution gaussienne est utilisée pour représenter des variables continues centrées sur leur moyenne. Son espace d’échantillonnage est ]-$\infty$ ; +$\infty$[. Cette distribution joue un rôle central en statistique. Le théorème central limite stipule que la somme d’un grand nombre de distributions tend généralement vers une distribution normale. Autrement dit, lorsque nous répétons une même expérience et que nous conservons les résultats de ces expériences, la distribution du résultat de ces expériences tend vers la normalité. Ceci s’explique par le fait qu’en moyenne, chaque répétition de l’expérience produit le même résultat, mais qu’un ensemble de petits facteurs aléatoires viennent rajouter de la variabilité dans les données collectées. Prenons un exemple concret, si l’on plante une centaine d’arbres simultanément dans un parc avec un degré d’ensoleillement identique et qu’on leur apporte les mêmes soins pendant dix ans, la distribution de leurs tailles suivra une distribution normale. Un ensemble de facteurs aléatoires (composition du sol, exposition au vent, aléas génétiques, passage de nuages, etc.) auront affecté différemment chaque arbre, ajoutant ainsi un peu de hasard dans leurs tailles finales. Ces dernières seront cependant davantage affectées par des paramètres centraux (espèces, ensoleillement, arrosage, etc.), et seront donc centrées autour d’une moyenne.
La fonction de densité de la distribution normale est la suivante :

\begin{equation}\footnotesize f(x ; \mu ; \sigma)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}
(\#eq:gaussien)
\end{equation}

avec *x* une valeur dont on souhaite connaître la probabilité, *f(x)* sa probabilité, $\mu$ (mu) la moyenne de la distribution normale (paramètre de localisation) et $\sigma$ (sigma) son écart-type (paramètre de dispersion). La courbe normale suit une forme de cloche. Notez que :

* 68,2% de la masse de la distribution normale est comprise dans l’intervalle $[\mu- \sigma≤x≤ \mu+ \sigma]$ 
* 95,4% dans l’intervalle $[\mu- 2\sigma≤x≤ \mu+ 2\sigma]$
* 99,7% dans l’intervalle $[\mu- 3\sigma≤x≤ \mu+ 3\sigma]$

Autrement dit, dans le cas d’une distribution normale, il est très invraisemblable d’observer des données situées à plus de trois écarts types de la moyenne.
Notez ici que lorsque $\mu = 0$ et $\sigma = 0$, on obtient la loi normale générale (ou centrée-réduite) (section \@ref(sect02552)).

```{r fig260, echo=FALSE, fig.align='center', fig.cap="La distribution Gaussienne", auto_pdf=TRUE, out.width='70%'}
library(VGAM)
generate_plot <- function(fun,params, real_names, xlim, colors){
  params_names <- names(params)
  ## creating vectors with the parameters
  layers_labs <- lapply(1:length(params[[1]]), function(i){
    val <- (lapply(params_names, function(n){
      return(params[[n]][[i]])
    }))
    names(val) <- params_names
    label_params <- paste(paste(real_names,val, sep =" = "),collapse = " & ")
    layer <- stat_function(aes(color = colors[[i]]), size = 1,
                           fun = fun, args = val)
    return(list(layer,label_params))
  })
  
  final_plot <- ggplot()
  all_labels <- sapply(layers_labs, function(i){i[[2]]})
  all_layers <- lapply(layers_labs, function(i){i[[1]]})
  for(layer in all_layers){
    final_plot <- final_plot + layer
  }
  final_plot <- final_plot + scale_color_identity(name = "Paramètres",
                      breaks = colors,
                      labels = all_labels,
                      guide = "legend") + theme(
                        axis.title.y = element_blank(),
                        axis.ticks.y = element_blank(),
                        axis.text.y = element_blank(),
                        panel.background = element_blank(),
                        panel.grid = element_blank()
                      ) + xlim(xlim)
  return(final_plot)
  
}
parametres <- list(mean = c(-5,0,5),
                   sd = c(1.5,1,3))
real_names <- c("mu","sigma")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(-15,15)
generate_plot(dnorm,parametres,real_names,xlim,colors)
```

#### La distribution gaussienne asymétrique
La distribution normale asymétrique (skew-normal) est une extension de la distribution gaussienne permettant de modifier la forme de la distribution normale pour qu’elle ne soit plus symétrique. Son espace d’échantillonnage est donc ]-$\infty$ ; +$\infty$[. Sa fonction de densité est la suivante :

\begin{equation}\footnotesize f(x;\xi;\omega;\alpha) = \frac{2}{\omega \sqrt{2 \pi}} e^{-\frac{(x-\xi)^{2}}{2 \omega^{2}}} \int_{-\infty}^{\alpha\left(\frac{x-\xi}{\omega}\right)} \frac{1}{\sqrt{2 \pi}} e^{-\frac{t^{2}}{2}} d t
(\#eq:skewgaussien)
\end{equation}

avec $\xi$ (xi) le paramètre de localisation, $\omega$ (omega) le paramètre de dispersion (ou d’échelle) et $\alpha$ (alpha) le paramètre de forme (contrôlant le degré de symétrie). Si  $\alpha = 0$, alors la distribution skew-normal est une simple distribution normale. Ce type de distribution est très utile lorsque que l’on souhaite modéliser une variable pour laquelle on sait que des valeurs plus extrêmes s’observeront d’un côté ou de l’autre de la distribution. Les revenus totaux annuels des personnes ou des ménages sont de très bons exemples puisqu’ils sont distribués généralement avec une asymétrie positive : bien qu’une moyenne existe, il y a généralement plus de personnes ou de ménages avec des revenus très faibles, que de personnes ou de ménages avec des revenus très élevés.

```{r fig261, echo=FALSE, fig.align='center', fig.cap="La distribution skew-Gaussienne", auto_pdf=TRUE, out.width='70%'}
parametres <- list(location = c(-10,-5,10),
                   scale = c(2,2,5),
                   shape = c(0,4,-4))
real_names <- c("xi","omega","alpha")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(-20,20)
generate_plot(dskewnorm,parametres,real_names,xlim,colors)
```

#### La distribution log-normale
Au même titre que la distribution skew-normal, la distribution log-normal est une version asymétrique de la distribution normale. Son espace d’échantillonnage est ]0 ; +$\infty$[. Cela signifie que cette distribution ne peut décrire que des données continues et positives. Sa fonction de densité est la suivante : 
\begin{equation}\footnotesize f(x ; \mu ; \sigma)=\frac{1}{x \sigma \sqrt{2 \pi}} e^{-\left(\frac{(\ln x-\mu)^{2}}{2 \sigma^{2}}\right)}
(\#eq:loggaussien)
\end{equation}

À la différence la distribution skew-normal, la distribution log-normal ne peut avoir qu’une asymétrie positive (étirée vers la droite). Elle est cependant intéressante puisqu’elle ne compte que deux paramètres ($\mu$ et $\sigma$) ce qui la rend plus facile à ajuster. À nouveau, une distribution log-normal pourrait être utilisée pour décrire les revenus totaux annuels des individus ou des ménages ou les revenus d’emploi. Elle est aussi utilisée en économie sur les marchés financiers pour représenter les cours des actions et des biens (ces derniers ne pouvant pas être inférieurs à 0).

```{r fig262, echo=FALSE, fig.align='center', fig.cap="La distribution log-gaussienne", auto_pdf=TRUE, out.width='70%'}
parametres <- list(meanlog = c(1,2,3),
                   sdlog = c(1,1.5,1))
real_names <- c("mu","sigma")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(0,30)
generate_plot(dlnorm,parametres,real_names,xlim,colors)
```

#### La distribution de Student
La distribution de Student joue un rôle important en statistique, elle est par exemple utilisée lors du test *t* pour calculer le degré de significativité du test. Comme la distribution gaussienne, la distribution de Student a une forme de cloche, est centrée sur sa moyenne et définie sur ]-$\infty$ ; +$\infty$[. Elle a cependant des « queues plus lourdes » (*heavy tails* en anglais). Entendez par-là que les valeurs extrêmes ont une plus grande probabilité d’occurrence dans une distribution de Student que dans une distribution gaussienne. Sa fonction de densité est la suivante : 

\begin{equation}\footnotesize p(x ; \nu ; \hat{\mu} ; \hat{\sigma})=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right) \sqrt{\pi \nu} \hat{\sigma}}\left(1+\frac{1}{\nu}\left(\frac{x-\hat{\mu}}{\hat{\sigma}}\right)^{2}\right)^{-\frac{\nu+1}{2}}
(\#eq:student)
\end{equation}

avec $\mu$ le paramètre de localisation, $\sigma$ le paramètre de dispersion (qui n’est cependant pas un écart-type comme pour la distribution normale) et $\nu$ le nombre de degré de liberté. Plus $\nu$ est grand, plus la distribution de Student tend vers une distribution normale. $\Gamma$ représente la fonction mathématique gamma (à ne pas confondre avec la distribution de Gamma). Un exemple d’application en études urbaines serait l’exposition au bruit environnemental de cyclistes. Cette distribution s’approcherait certainement d’une distribution normale, mais les cyclistes croisent régulièrement des secteurs peu bruyants (parcs, rues résidentielles, etc.) et des secteurs très bruyants (artères majeures, zones industrielles, etc.), ce qui conduit vers une distribution de Student.

```{r fig263, echo=FALSE, fig.align='center', fig.cap="La distribution de Student", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
library(LaplacesDemon)
parametres <- list(mu = c(-10,0,10),
                   sigma = c(1,3,6),
                   nu = c(2,10,30)
                   )
real_names <- c("mu","sigma", "nu")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(-25,25)
generate_plot(dst,parametres,real_names,xlim,colors)
```

#### La distribution de Cauchy
La distribution de Cauchy est également une distribution symétrique définie sur l’intervalle ]-$\infty$ ; +$\infty$[. Elle a comme particularité d’avoir des queues potentiellement plus lourdes que la distribution de Student. Elle est notamment utilisée pour modéliser des phénomènes extrêmes comme les précipitations maximales annuelles, les niveaux d’inondations maximaux annuels ou les *values at risk* pour les portefeuilles financiers. Il est également intéressant de noter que le quotient de deux variables indépendantes normalement distribuées suit une distribution de Cauchy. Sa fonction de densité est la suivante : 

\begin{equation}\footnotesize \frac{1}{\pi \gamma}\left[\frac{\gamma^{2}}{\left(x-x_{0}\right)^{2}+\gamma^{2}}\right]
(\#eq:cauchy)
\end{equation}

Elle dépend donc de deux paramètres : $x_0$, le paramètre de localisation indiquant le pic de la distribution et $\gamma$, un paramètre de dispersion.

```{r fig264, echo=FALSE, fig.align='center', fig.cap="La distribution de Cauchy", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
parametres <- list(location = c(-10,0,10),
                   scale = c(1,3,6)
                   )
real_names <- c("x0","gamma")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(-25,25)
generate_plot(dcauchy,parametres,real_names,xlim,colors)
```

#### La distribution du Chi-carré

La distribution du Chi<sup>2</sup> est utilisée dans de nombreux tests statistiques. Spécifiquement, le test du Chi<sup>2</sup> de Pearson est utilisé pour comparer les écarts au carré entre des fréquences attendues et observées de deux variables qualitatives. La distribution du Chi<sup>2</sup> décrit donc les sommes des carrés d’un nombre *k* de variables indépendantes normalement distribuées. Il est assez rare de modéliser un phénomène à l’aide d’une distribution du Chi<sup>2</sup>, mais son omniprésence dans les tests statistiques justifie qu’elle soit mentionnée ici. Cette distribution est définie sur l’intervalle [0 ; +$\infty$[ et a pour fonction de densité : 

\begin{equation}\footnotesize f(x;k) = \frac{1}{2^{k / 2} \Gamma(k / 2)} x^{k / 2-1} e^{-x / 2}
(\#eq:chi2)
\end{equation}

Cette fonction n’a qu’un paramètre *k*, représentant donc le nombre de variables au carré sommées pour obtenir la distribution du Chi<sup>2</sup>

```{r fig265, echo=FALSE, fig.align='center', fig.cap="La distribution du Chi<sup>2</sup>", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
parametres <- list(df = c(2,5,10))
real_names <- c("k")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(0,25)
generate_plot(dchisq,parametres,real_names,xlim,colors)
```

#### La distribution exponentielle
La distribution exponentielle est une version continue de la distribution géométrique. Pour cette dernière, on s’intéresserait au nombre de tentatives nécessaires pour obtenir un résultat positif, soit une dimension discrète. Pour la distribution exponentielle, cette dimension discrète est remplacée par une dimension continue. L’exemple le plus intuitif est sûrement le cas du temps. Dans ce cas, la distribution exponentielle servirait à décrire le temps d’attente nécessaire pour qu’un évènement se produise. Il pourrait aussi s’agir d’une force que l’on applique jusqu’à ce qu’un matériau cède. Cette distribution est donc définie sur l’intervalle [0 ; +$\infty$[ et a pour fonction de densité :

\begin{equation}\footnotesize f(x;\lambda) = \lambda e^{-\lambda x}
(\#eq:exponentiel)
\end{equation}


```{r fig266, echo=FALSE, fig.align='center', fig.cap="La distribution exponentielle", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
parametres <- list(rate = c(1/2,1/5,1/10))
real_names <- c("lambda")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(0,25)
generate_plot(dexp,parametres,real_names,xlim,colors)
```

#### La distribution de Gamma
La distribution de Gamma est une généralisation d’un grand nombre de distributions. Elle regroupe ainsi la distribution exponentielle et du Chi<sup>2</sup>. En d’autres termes, les distributions du chi<sup>2</sup> et exponentielles sont des cas particuliers de la distribution de Gamma. Cette distribution est définie sur l’intervalle ]0 ; +$\infty$[ (notez que le 0 est exclu) et sa fonction de densité est la suivante : 

\begin{equation}\footnotesize f(x ; \alpha; \beta)=\frac{\beta^{\alpha} x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)}
(\#eq:gamma)
\end{equation}

Elle comprend donc deux paramètres : $\alpha$ et $\beta$. Le premier est le paramètre de forme et le second un paramètre d’échelle (à l’inverse d’un paramètre de dispersion, plus sa valeur est petite, plus la distribution sera dispersée). Notez que cette distribution ne dispose pas d’un paramètre de localisation. Du fait de sa flexibilité, cette distribution est largement utilisée, que ce soit dans la modélisation des temps d’attente avant un évènement, la taille des réclamations d’assurance, les quantités de précipitations, etc.

```{r fig267, echo=FALSE, fig.align='center', fig.cap="La distribution de Gamma", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
parametres <- list(shape = c(1,2,6),
                  rate = c(1,0.4,0.8))
real_names <- c("alpha","beta")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(0,25)
generate_plot(dgamma,parametres,real_names,xlim,colors)
```

#### La distribution de Beta
La distribution de Beta est définie sur l’intervalle [0 ; 1], elle est donc énormément utilisée pour représenter des variables étant des proportions ou des probabilités. Elle a aussi une utilité pratique en statistique, car en combinaison avec d’autres distributions, elle permet de modéliser leurs paramètres de probabilité (distribution beta-binomial, beta-negative-binomial, etc.). Un autre usage plus rare, mais intéressant est la modélisation de la fraction du temps représentée par une tâche dans le temps nécessaire à la réalisation de deux tâches de façon séquentielle. Ceci est dû au fait que la distribution d’une distribution gamma *g1* divisée par la somme de *g1* et d’une autre distribution gamma *g2*, suit une distribution beta. Un exemple concret serait par exemple la fraction du temps effectué à pied dans un déplacement multimodal. La distribution de beta a la fonction de densité suivante : 

\begin{equation}\footnotesize f(x;\alpha;\beta) = \frac{1}{\mathrm{B}(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}
(\#eq:beta)
\end{equation}

Elle a donc deux paramètres $\alpha$ et $\beta$ contrôlant tous les deux la forme de la distribution. Cette caractéristique lui permet d’avoir une très grande flexibilité et même d’adopter des formes bimodales. $B$ correspondant à la fonction mathématique Beta, à ne pas confondre avec la distribution de Beta et le paramètre Beta ($\beta$) de cette même distribution.

```{r fig268, echo=FALSE, fig.align='center', fig.cap="La distribution de Beta", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
parametres <- list(shape1 = c(0.5,5,2,2),
                  shape2 = c(0.5,1,2,5))
real_names <- c("alpha","beta")
colors <- c("#ee6c4d","#98c1d9","#293241","#8C4F47")
xlim <- c(0,1)
generate_plot(dbeta,parametres,real_names,xlim,colors)
```

#### La distribution de Weibull
La distribution de Weibull est directement liée à la distribution exponentielle, cette dernière étant en fait un cas particulier de distribution de Weibull. Elle sert donc à représenter une quantité *x* (souvent le temps) à accumuler pour qu’un évènement se produise. La distribution de Weibull est définie sur l’intervalle [0 ; +$\infty$[ et a la fonction de densité suivante : 

\begin{equation}\footnotesize f(x;\lambda) = \frac{k}{\lambda} (\frac{x}{\lambda})^{k-1} e^{-(\frac{x}{\lambda})^k}
(\#eq:weibull)
\end{equation}

$\lambda$ est le paramètre de dispersion (analogue a celui d’une distribution exponentielle classique) et *k* le paramètre de forme. Pour bien comprendre le rôle de *k*, prenons un exemple : la propagation d’un champignon d’un arbre à son voisin. Si $k<1$, cela signifie que la probabilité que l’évènement modélisé se produise diminue avec le temps. En d’autres termes, dans de nombreux cas la contamination se fait rapidement. Si $k=1$, alors la probabilité que l’évènement se produise reste stable dans le temps. Si $k > 1$, alors la probabilité que l’évènement se produisent augmente avec le temps, ce qui signifie une augmentation des risques de contamination à mesure que les deux arbres restent à proximité. La distribution de Weibull est très utilisée en analyse de survie, en météorologie, en ingénierie des matériaux et dans la théorie des valeurs extrêmes.

```{r fig269, echo=FALSE, fig.align='center', fig.cap="La distribution de Weibull", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
parametres <- list(shape = c(1,0.5,5),
                  scale = c(1,3,10))
real_names <- c("k","lambda")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(0,15)
generate_plot(dweibull,parametres,real_names,xlim,colors)
```

#### La distribution de Pareto
La distribution de Pareto est à la distribution exponentielle ce que la distribution log-normal est à la distribution gaussienne : la distribution de l’exponentiel (e) de cette distribution originale. Elle est définie sur l’intervalle  $[x_m ; +\infty[$ avec la fonction de densité suivante : 

\begin{equation}\footnotesize f(x;x_m;k) = (\frac{x_m}{x})^k
(\#eq:pareto)
\end{equation}

Elle comprend donc deux paramètres, $x_m$ étant un paramètre de localisation (décalant la distribution vers la droite ou vers la gauche) et $k$ un paramètre de forme. Plus $k$ augmente, plus la probabilité prédite par la distribution décroît rapidement.

```{r fig270, echo=FALSE, fig.align='center', fig.cap="La distribution de Pareto", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
parametres <- list(scale = c(4,3,1),
                  shape = c(5,3,1))
real_names <- c("mx","k")
colors <- c("#ee6c4d","#98c1d9","#293241")
xlim <- c(0,15)
generate_plot(VGAM::dpareto,parametres,real_names,xlim,colors)
```

Originalement, le mathématicien Pareto a utilisé cette fonction pour décrire la répartition du capital parmi la population puisqu’une large partie du capital est détenue par une petite fraction de la population. Elle peut également être utilisée pour décrire la répartition de la taille des villes [@William_pareto_ville], [la popularité des hommes sur tinder](https://medium.com/@worstonlinedater/tinder-experiments-ii-guys-unless-you-are-really-hot-you-are-probably-better-off-not-wasting-your-2ddf370a6e9a){target="_blank"} ou la taille des fichiers échangés sur internet [@William_pareto]. Pour ces trois exemples, nous avons une situation avec : de nombreuses petites villes, profils peu attractifs, petits fichiers échangés et à l'inverse très peu de grandes villes, profils très attractifs, gros fichiers échangés.

#### Cas particuliers
Sachez également qu’il existe des formes « plus exotiques » de distributions que nous n’abordons pas ici, mais auxquelles vous pourriez être confrontés un jour :

* Les distributions sphériques, servant à décrire des données dont le 0 est équivalent à la valeur maximale. Par exemple, des angles puisque 0 et 360 degrés sont identiques.
* Les mixtures de distributions, décrivant des combinaisons de distributions. Par exemple, la distribution de la taille de tous les humains est en réalité un mixte entre deux distributions gaussiennes, une pour chaque sexe, puisque ces deux sous-distributions n’ont pas la même moyenne ni le même écart-type.
* Les distributions multivariées, permettant de décrire des phénomènes multidimensionnels. Par exemple, la réussite des élèves en français et en mathématique pourrait être modélisée comme une distribution gaussienne bivariée plutôt que deux distributions distinctes.
* Les distributions censurées décrivant des variables pour lesquels des valeurs sont possibles au-delà d’une certaine limite mais que l’on est incapable de mesurer. Un bon exemple serait la mesure de la pollution sonore avec un capteur incapable de détecter des niveaux sonores en dessous de 55 décibels. Il arrive parfois en ville que les niveaux sonores soient si faibles, mais les données collectées ne le montrent pas. Dans ce contexte, il est important d’utiliser des versions censurées des distributions présentées précédemment. Les observations au-delà de la limite sont conservées dans l’analyse, mais nous ne disposons que d’une information partielle à leur égard.
* Les distributions tronquées, souvent confondues avec les distributions censurées, décrivent des situations ou des données qui au-delà d’une certaine limite sont retirées simplement de l’analyse.

### Conclusion sur les distributions

Voilà qui conclut cette exploration des principales distributions à connaître. L’idée n’est bien sûr pas de toutes les retenir par cœur (et encore moins les formules mathématiques), mais plutôt de se rappeler dans quels contextes elles peuvent être utiles; et de revenir au besoin sur ce chapitre. Vous aurez certainement besoin de le relire avant d’aborder le chapitre portant sur les modèles linéaires généralisés (GLM).
Wikipédia dispose d’informations très détaillées sur chaque distribution si vous avez besoin d’informations complémentaires. Pour un tour d’horizon plus exhaustif des distributions, vous pouvez aussi faire un tour sur les projets [probonto](https://sites.google.com/site/probonto/screenshots){target="_blank"} et [the ultimate probability distribution explorer](https://blog.wolfram.com/2013/02/01/the-ultimate-univariate-probability-distribution-explorer/){target="_blank"}. 


## Statistiques descriptives sur des variables quantitatives {#sect025}

### Les paramètres de tendance centrale {#sect0251}

Trois mesures de tendance centrale permettent de résumer rapidement une variable quantitative :

* la **moyenne arithmétique** est simplement la somme des données d'une variable divisée par le nombre d'observations ($n$), soit $\frac{\sum_{i=1}^n x_i}{n}$ notée $\mu$ (prononcez *mu*) pour des données pour une population et $\bar{x}$ (prononcez *x barre*) pour un échantillon.
* la **médiane** est la valeur qui coupe la distribution d'une variable d'une population ou d'un échantillon en deux parties égales. Autrement dit, 50% des valeurs des observations lui sont supérieures et 50% lui sont inférieures.
* le **mode** est la valeur la plus fréquente parmi un ensemble d'observations pour une variable. Il s'applique ainsi à des variables discrètes (avec un nombre fini de valeurs discrètes dans un intervalle donné) et non à des variables continues (avec un nombre infini de valeurs réelles dans un intervalle donné). Prenons deux variables, l'une discrète relative au nombre d'accidents par intersection (avec $X \in \left[0,20\right]$) et l'autre continue relative à la distance de dépassement (en mètres) d'un cycliste par un véhicule motorisé (avec $X \in \left[0,5\right]$). Pour la première, le mode – la valeur la plus fréquente – est certainement 0. Pour la seconde, identifier le mode n'est pas pertinent puisqu'il peut y avoir un nombre infini de valeurs entre 0 et 5 mètres.

Il convient de ne pas confondre moyenne et médiane ! Dans le tableau \@ref(tab:tableRevMoyMed), nous avons reporté les valeurs moyennes et médianes des revenus des ménages pour les municipalités de l'île de Montréal en 2015. Par exemple, les 8685 ménages résidant à Wesmount disposaient en moyenne d'un revenu de 295099\$; la moitié de ces 8685 ménages avaient un revenu inférieur à 100153\$  et l'autre moitié un revenu supérieur à cette valeur (médiane). Cela démontre clairement que la moyenne peut être grandement affectée par des valeurs extrêmes (faibles ou fortes); autrement dit, plus l'écart entre les valeurs de la moyenne et la médiane est importante, plus les données de la variable sont inégalement réparties. À Westmount, soit la municipalité la plus nantie de l'île de Montréal, les valeurs extrêmes sont des ménages avec des revenus très élevés tirant fortement la moyenne vers le haut. À l'inverse, le faible écart entre les valeurs moyenne et médiane dans la municipalité de Montréal-Est (58594\$ versus 50318\$) soulignent que les revenus des ménages sont plus également répartis. Cela explique que pour comparer les revenus totaux ou d'emploi entre différents groupes (selon le sexe, le groupe d'âge, le niveau d'éducation, la municipalité ou région métropolitaine, etc.), on prévilégie habituellement l'utilisation des revenus médians.

```{r tableRevMoyMed, echo=FALSE, message=FALSE, warning=FALSE}
df <-  read.csv("data/univariee/revenu.csv")
df2 <- df[, c("Muni","NMenages","RevMoyM","RevMedM")]

show_table(df2,
           col.names = c("Municipalité","Nombre de ménages", "Revenu moyen","Revenu médian"),
            caption = "Revenus moyens et médians des ménages en dollars, municipalités de l'île de Montréal, 2015"
           )
```

### Les paramètres de position {#sect0252}

Les paramètres de position permettent de diviser une distribution en _n_ parties égales.

* Les **quartiles** qui divisent une distribution en quatre parties (25%) :
  + Q1 (25%), soit le quartile inférieur ou premier quartile;
  + Q2 (50%), soit la médiane;
  + Q3 (75%), soit le quartile supérieur ou troisième quartile.
* Les **quintiles** qui divisent une distribution en cinq parties égales (20%).
* Les **déciles** (de D1 à D9) qui divisent une distribution en dix parties égales (10%).
* Les **centiles** (de C1 à C99) qui divisent une distribution en cent parties égales (1%).

En cartographie, les quartiles et les quintiles sont souvent utilisés pour discrétiser une variable quantitative (continue ou discrète) en quatre ou cinq classes et plus rarement, en huit ou dix classes. Avec les quartiles, les bornes des classes qui comprendront chacune 25% des unités spatiales seront ainsi définies comme suit : [Min à Q1], [Q1 à Q2], [Q2 à Q3] et [Q3 à Max]. La méthode de discrétisation selon les quartiles ou quintiles permet alors de repérer, en un coup d'œil, à quelle tranche de 25% ou 20% des données appartient chacune des unités spatiales. Cette méthode de discrétisation est aussi utile pour comparer plusieurs cartes et vérifier si deux phénomènes sont ou non colocalisés [@pumain1994]. En guise d'exemple, les pourcentages de personnes à faible revenu et de locataires par secteur de recensement ont clairement des distributions spatiales très semblables dans la région métropolitaine de Montréal en 2016 (figure \@ref(fig:figunivarie2)).



```{r figunivarie2, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Exemples de cartographie avec une discrétisation selon les quantiles",  out.width='85%'}
knitr::include_graphics('images/univariee/figure2.jpg', dpi = NA)
```

Une lecture attentive des valeurs des centiles permet de repérer la présence de valeurs extrêmes voire aberrantes dans un jeu de données. Il n'est donc pas rare de les voir reportées dans un tableau de statistiques descriptives d'un article scientifique, et ce, afin de décrire succinctement les variables à l'étude. Par exemple, dans une étude récente comparant les niveaux d'exposition au bruit des cyclistes dans trois villes  [@2020_1], les auteurs reportent à la fois les valeurs moyennes et celles de plusieurs centiles. Globalement, la lecture des valeurs moyennes permet de constater que, sur la base des données collectées, les cyclistes sont plus exposés au bruit à Paris qu'à Montréal et Copenhague (73,4 dB(A) contre 70,7 et 68,4, tableau \@ref(tab:tableCentiles)). Compte tenu de l'échelle logarithmique du bruit, la différence de 5 dB(A) entre les valeurs moyennes du bruit de Copenhague et de Paris peut être considérée comme une multiplication de l'énergie sonore par plus de 3. Pour Paris, l'analyse des quartiles montre que durant 25% du temps des trajets à vélo (plus de 63 heures de collecte), les participants ont été exposés à des niveaux de bruit soit inférieurs à 69,1 dB(A) (premier quartile), soit supérieurs à 74 dB(A). Quant à l'analyse des centiles, elle permet de constater que durant 5% et 10% du temps, les participants étaient exposés à des niveaux de bruit très élevés, dépassant 77 dB(A) (C90=76 et C90=77,2).

```{r tableCentiles, echo=FALSE, message=FALSE, warning=FALSE}
options(knitr.kable.NA = '')
df <- data.frame(
  Centiles = c("N","Moyenne de bruit","Centiles","1","5","10", "25 (premier quartile)", "50 (médiane)", "75 (troisième quartile)", "90", "95", "99"),
  C = c(6212,68.4,NA,57.5,59.1,60.3,62.7,66.0,69.2,71.9,73.3,76.5),
  M = c(4723,70.7,NA,59.2,61.1,62.3,64.5,67.7,71.0,73.7,75.2,78.9),
  P = c(3793,73.4,NA,62.3,65.0,66.5,69.1,71.6,74.0,76.0,77.2,81.0))

show_table(df, 
           col.names = c("Statistiques","Copenhague", "Montréal","Paris"),
            caption = "Stastistiques descriptives de l'exposition au bruit des cyclistes par minute dans trois villes (dB(A), Laeq 1min)")
```


### Les paramètres de dispersion {#sect0253}
Cinq principales mesures de dispersion permettent d'évaluer la variabilité des valeurs d'une variable quantitative : l'étendue, l'écart interquartile, la variance, l'écart-type et le coefficient de variation. Notez d'emblée que cette dernière mesure ne s'applique pas à des variables d'intervalle (section \@ref(sect02122)).

* **L'étendue** est la différence entre les valeurs minimale et maximale d'une variable, soit l'intervalle des valeurs dans lequel elle a été mesurée. Il convient d'analyser avec prudence cette mesure puisqu'elle inclut dans son calcul des valeurs potentiellement extrêmes voire aberrantes (faibles ou fortes).

* **L'intervalle ou écart interquartile** est la différence entre les troisième et premier quartiles ($Q3 − Q1$). Il représente ainsi une mesure de la dispersion des valeurs de 50% des observations centrales de la distribution. Plus la valeur de l'écart interquartile est élevée, plus la dispersion des 50% des observations centrales est forte. Contrairement à l'étendue, cette mesure élimine l'influence des valeurs extrêmes puisqu'elle ne tient pas compte des 25% des observations les plus faibles [Min à Q1] et des 25% des observations les plus fortes [Q3 à Max]. Graphiquement, l'intervalle interquartile est représenté à l'aide d'une boîte à moustaches (*boxplot* en anglais) : plus l'intervalle interquartile sera grand, plus la boîte sera allongée (figure \@ref(fig:figunivarie3))

```{r figunivarie3, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Graphique en violon, boîte à moustaches et intervalle interquartile",  out.width='30%'}
knitr::include_graphics('images/univariee/figure3.jpg', dpi = NA)
```

* **La variance** est la somme des déviations à la moyenne au carré (numérateur) divisée par le nombre d'observations pour une population ($\sigma^2$) ou divisée par le nombre d'observations moins une ($s^2$) pour un échantillon (eq. \@ref(eq:variance)). Puisque les déviations à la moyenne sont mises au carré, la valeur de la variance (tout comme celle de l'écart-type) sera toujours positive. Plus sa valeur est élevée, plus les observations sont dispersées autour de la moyenne. La variance représente ainsi l'écart au carré moyen des observations à la moyenne. 

\begin{equation}\footnotesize  
\sigma^2=\frac{\sum_{i=1}^n (x_{i}-\mu)^2}{n} \text{ ou } s^2=\frac{\sum_{i=1}^n (x_{i}-\bar{x})^2}{n-1}
(\#eq:variance)
\end{equation}

* **L'écart-type** est la racine carrée de la variance (eq. \@ref(eq:ecartype)). Rappelez-vous que la variance est calculée à partir des déviations à la moyenne mises au carré. Étant donné que l'écart-type est la racine carrée de la variance, il est donc évalué dans les mêmes unités que la variable, contrairement à la variance. Bien entendu, comme pour la variance, plus la valeur de l'écart-type est élevée, plus la distribution des observations autour de la moyenne est dispersée.

\begin{equation}\footnotesize  
\sigma=\sqrt{\sigma^2}=\sqrt{\frac{\sum_{i=1}^n (x_{i}-\mu)^2}{n}} \text{ ou } s=\sqrt{s^2}=\sqrt{\frac{\sum_{i=1}^n (x_{i}-\bar{x})^2}{n-1}}
(\#eq:ecartype)
\end{equation}

::: {.bloc_notes data-latex=""}
Les formules des variances et des écart-types pour une population et un échantillon sont très similaires : seul le dénominateur change avec $n$ *versus* $n-1$ observations. Par conséquent, plus le nombre d'observations de votre jeu de données sera important, plus l'écart entre ces deux mesures de dispersion pour une population et un échantillon sera minime.

Comme dans la plupart des logiciels de statistique, les fonctions de base `var` et `sd` de R calculent la variance et l'écart-type pour un échantillon ($n-1$ au dénominateur). Si vous souhaitez les calculer pour une population, adaptez la syntaxe ci-dessous dans laquelle `df$var1` représente la variable intitulée `var1` présente dans un *dataframe* nommé `df`.

`var.p <- mean((df$var1 - mean(df$var1))^2)` 

`sd.p <- sqrt(mean((df$var1 - mean(df$var1))^2))` 
:::


* **Le coefficient de variation (CV)** est le rapport entre l'écart-type et la moyenne, représentant ainsi une standardisation de l'écart-type ou, en d'autres termes, une mesure de dispersion relative (eq. \@ref(eq:cv)). L'écart-type étant exprimé dans l'unité de mesure de la variable, il ne peut pas être utilisé pour comparer les dispersions de variables exprimées des unités de mesure différentes (par exemple, en pourcentage, en kilomètres, en dollars, etc.). Pour y remédier, on utilisera le coefficient de variation : une variable est plus dispersée qu'une autre si la valeur de son CV est plus élevée. Certains préfèreront multiplier la valeur du CV par 100 : l'écart-type est alors exprimé en pourcentage de la moyenne.


\begin{equation}\footnotesize  
CV=\frac{\sigma}{\mu} \text{ ou } CV=\frac{s^2}{\bar{x}}
(\#eq:cv)
\end{equation}


Illustrons comment calculer les cinq mesures de dispersion précédemment décrites à partir de valeurs fictives pour huit observations (colonne intitulée $x_i$ au tableau \@ref(tab:datavar)). Les différentes statistiques reportées dans ce tableau sont calculées comme suit :

* La **moyenne** est la somme divisée par le nombre d'observations, soit $248/8=31$.
* L'**étendue** est la différence entre les valeurs maximale et minimale, soit $40-22=30$.
* Les quartiles coupent la distribution en quatre parties égales. Avec huit observations triées par ordre croissant, **le premier quartile** est égale à la valeur de la 2^e^ observation (soit 25), la **médiane** à celle de la 4^e^ (30), le **troisième quartile** à celle de la 6^e^ (35).
* **L'écart interquartile** est la différence entre Q3 et Q1, soit $35-25=10$.

* La seconde colonne du tableau est l'écart à la moyenne ($x_i-\bar{x}$), soit $22 - 31 = -9$ pour l'observation *1*; la somme de ces écarts est toujours égale à 0. La troisième colonne est cette déviation mise au carré ($(x_i-\bar{x})^2$), soit $-9^2 = 81$, toujours pour l'observation *1*. La somme de ces déviations à la moyenne au carré ($268$) représente le numérateur de la variance (eq. \@ref(eq:variance)). En divisant cette somme par le nombre d'observations, on obtient la **variance pour une population** ($268/8=33,5$) tandis que la **variance d'un échantillon** est égale à $268/(8-1)=38,29$.

* L'écart-type est la racine carrée de la variance (eq. \@ref(eq:ecartype)), soit $\sigma=\sqrt{33,5}=5,79$ et $s=\sqrt{38,29}=6,19$.

* Finalement, les valeurs des coefficients de variation (eq. \@ref(eq:cv)) sont de $5,79/31=0,19$ pour une population et $6,19/31=0,20$ pour un échantillon. 


```{r datavar, echo=FALSE, message=FALSE, warning=FALSE}
a <- c(22,25,27,30,32,35,37,40)
a <- sort(a)
n <- length(a)
df <- data.frame(
    id = as.character(c(1:n)),
    x = round(a,2),
    xi_mean = a - mean(a),
    numer = (a - mean(a))^2
)
df[n+1,1] <- "**Statistique**"
n <- n+1
df[n+1,1] <- "N"
df[n+2,1] <- "Somme"
df[n+3,1] <- "Moyenne ($\\bar{x}$ ou $\\mu$)"
df[n+1,2] <- length(a)
df[n+2,2] <- sum(a)
df[n+2,4] <- sum(df$numer, na.rm = TRUE)
df[n+2,3] <- sum(df$xi_mean, na.rm = TRUE)
df[n+3,2] <- mean(a)
df[n+3,3] <- sum(df$xi_mean, na.rm = TRUE)/n
df[n+3,4] <- round(mean((a - mean(a))^2),2)
df[n+4,1] <- "Étendue"
df[n+4,2] <- max(a)-min(a)
df[n+5,1] <- "Premier quartile"
df[n+6,1] <- "Troisième quartile"
df[n+5,2] <- quantile(a, type = 1)[2]
df[n+6,2] <- quantile(a, type = 1)[4]
df[n+7,1] <- "Intervalle interquartile"
df[n+7,2] <- quantile(a, type = 1)[4]-quantile(a, type = 1)[2]
df[n+8,1] <- "Variance (population, $\\sigma^2$)"
df[n+9,1] <- "Écart-type (population, $\\sigma$)"
df[n+10,1] <- "Variance (échantillon, $s^2$)"
df[n+11,1] <- "Écart-type (échantillon, $s$)"
df[n+8,2] <- round(mean((a - mean(a))^2),2)
df[n+9,2] <- round(sqrt(mean((a - mean(a))^2)),2)
df[n+10,2] <- round(var(a),2)
df[n+11,2] <- round(sd(a),2)
df[n+12,1] <- "Coefficient de variation ($\\sigma / \\mu$)"
df[n+13,1] <- "Coefficient de variation ($s / \\bar{x}$)"
df[n+12,2] <- round(sqrt(mean((a - mean(a))^2))/mean(a),2)
df[n+13,2] <- round(sd(a)/mean(a),2)
opts <- options(knitr.kable.NA = "")

show_table(df,
           col.names = c("Observation","$x_i$","$x_i-\\bar{x}$","$(x_i-\\bar{x})^2$"),
            caption = "Calcul des mesures de dispersion sur des données fictives"
           )
``` 

Le tableau \@ref(tab:datavar2) vise à démontrer à partir de trois variables comment certaines mesures de dispersion sont sensibles à l'unité de mesure et/ou aux valeurs extrêmes. 

Concernant **l'unité de mesure**, nous avons créé deux variables *A* et *B*, avec *B* étant simplement *A* multiplié par 10. Pour *A*, les valeurs de la moyenne, l'étendue et l'intervalle interquartile sont respectivement de 31, 18 et 10. Sans surprise, celles de B sont multipliées par 10 (310, 180, 100). La variance étant la moyenne des déviations à la moyenne au carré, elle est égale à 33,50 pour *A* et donc à $33,50\times10^2=3350$ pour *B*; l'écart-type de *B* est égal à celui de *A* multiplié par 10. Cela démontre que l'étendue, l'intervalle interquartile, la variance et l'écart-type sont des mesures de dispersion dépendantes de l'unité de mesure. Par contre, le coefficient de variation (CV) étant le rapport de l'écart-type avec la moyenne, il a la même valeur pour *A* et *B*, ce qui démontre que CV est bien une mesure de dispersion relative permettant de comparer des variables exprimées dans des unités de mesure différentes.

Concernant **la sensibilité aux valeurs extrêmes**, nous avons créé la variable *C* pour laquelle seule la huitième observation a une valeur différente (40 pour *A* et *105* pour B). Cette valeur de 105 pourrait être soit une valeur extrême positive mesurée, soit une valeur aberrante (par exemple, si l'unité de mesure était un pourcentage variant de 0 à 100%). Cette valeur a un impact important sur la moyenne (31 contre 39,12) et l'étendue (18 contre 83) et corollairement sur la variance (33,50 contre 641,86), l'écart-type (5,79 contre 25,33) et le coefficient de variation (0,19 contre 0,65). Par contre, l'intervalle interquartile étant calculé sur 50% des observations centrales ($Q3-Q1$), il n'est pas affecté par cette valeur extrême.


```{r datavar2, echo=FALSE, message=FALSE, warning=FALSE}
a <- c(22,25,27,30,32,35,37,40)
c <- c(22,25,27,30,32,35,37,105)
b <- a*10
var.a <- round(mean((a - mean(a))^2),2)
var.b <- round(mean((b - mean(b))^2),2)
var.c <- round(mean((c - mean(c))^2),2)
sd.a <- round(sqrt(mean((a - mean(a))^2)),2)
sd.b <- round(sqrt(mean((b - mean(b))^2)),2)
sd.c <- round(sqrt(mean((c - mean(c))^2)),2)
a <- sort(a)
b <- sort(b)
c <- sort(c)
n <- length(a)
df <- data.frame(
  id = as.character(c(1:n)),
  A = round(a,2),
  B = round(b,2),
  C = round(c,2)
)
df[n+1,1] <- "**Statistique**"
n <- n+1
df[n+1,1] <- "Moyenne ($\\mu$)"
df[n+2,1] <- "Étendue"
df[n+3,1] <- "Intervalle interquartile"
df[n+4,1] <- "Variance (population, $\\sigma^2$)"
df[n+5,1] <- "Écart-type (population, $\\sigma$)"
df[n+6,1] <- "Coefficient de variation ($\\sigma / \\mu$)"
df[n+1,2] <- mean(a)
df[n+2,2] <- max(a)-min(a)
df[n+3,2] <- quantile(a, type = 1)[4]-quantile(a, type = 1)[2]
df[n+4,2] <- round(var.a,2)
df[n+5,2] <- round(sd.a,2)
df[n+6,2] <- round(sd.a/mean(a),2)
df[n+1,3] <- mean(b)
df[n+2,3] <- max(b)-min(b)
df[n+3,3] <- quantile(b, type = 1)[4]-quantile(b, type = 1)[2]
df[n+4,3] <- round(var.b,2)
df[n+5,3] <- round(sd.b,2)
df[n+6,3] <- round(sd.b/mean(b),2)
df[n+1,4] <- round(mean(c),2)
df[n+2,4] <- max(c)-min(c)
df[n+3,4] <- quantile(c, type = 1)[4]-quantile(c, type = 1)[2]
df[n+4,4] <- round(var.c,2)
df[n+5,4] <- round(sd.c,2)
df[n+6,4] <- round(sd.c/mean(c),2)
opts <- options(knitr.kable.NA = "")

show_table(df, 
           col.names = c("Observation","A","B","C"), 
           caption = "Illustration de la sensibilité des mesures de dispersion à l'unité de mesure et aux valeurs extrêmes")
```


```{r resume, echo=FALSE, message=FALSE, warning=FALSE}
a <- c("Moyenne", "Étendue",
       "Intervalle interquartile",
       "Variance", "Écart-type", "Coefficient de variation")
b <- c("X","X","X","X","X","")
c <- c("X","X","","X","X","X")
df <- data.frame(
  Stat = a,
  Unite = b,
  Outlier = c
)
opts <- options(knitr.kable.NA = "")

show_table(df, 
          col.names = c("Statistique","Unité de mesure","Valeurs extrêmes"),
          caption = "Résumé de la sensibilité de la moyenne et des mesures de dispersion")
```


### Les paramètres de forme {#sect0254}

#### Vérifier la normalité d'une variable quantitative

::: {.bloc_objectif data-latex=""}
De nombreuses méthodes statistiques qui seront abordées dans les chapitres suivants – entre autres, la corrélation de Pearson, les test *t* et l'analyse de variance, les régressions simple et multiple – requièrent que la variable quantitative suive une **distribution normale** (nommée aussi **distribution gaussienne**).

Dans cette sous-section, nous décrirons trois démarches pour vérifier si la distribution d'une variable est normale : les coefficients d'asymétrie et d'applatissement (*skewness* et *kurtosis* en anglais), les graphiques (histogramme avec courbe normale, diagramme quantile-quantile), les tests de normalité (tests de Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors, Anderson-Darling et Jarque-Bera).

**Il est vivement recommandé de réaliser les trois démarches !**
:::

Une distribution est normale quand elle est symétrique et mésokurtique (figure \@ref(fig:figFormeDistr)).

```{r figFormeDistr, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Formes d'une distribution et les coefficients d'asymétrie et d'aplatissement",  out.width='70%'}
knitr::include_graphics('images/univariee/FormeDistribution.jpg', dpi = NA)
```

##### Vérifier la normalité avec les coefficients d'asymétrie et d'applatissement

**Une distribution est dite symétrique** quand la moyenne arithmétique est au centre de la distribution, c'est-à-dire que les observations sont bien réparties de part et d'autre de la moyenne qui sera alors égale à la médiane et au mode (on utilisera uniquement le mode pour une variable discrète et non pour une variable continue). Pour évaluer l'asymétrie, on utilise habituellement le coefficient d'asymétrie (*skewness* en anglais). 

Sachez toutefois qu'il existe trois façons (formules) pour le calculer [@joanes1998comparing] : $g_1$ est la formule classique (eq. \@ref(eq:SkewType1), disponible dans R avec la fonction `skewness` du *package* **moments**), $G_1$ est une version ajustée (eq. \@ref(eq:SkewType2), utilisée dans les logiciels SAS et SPSS notamment) et $b_1$ est une autre version ajustée (eq. \@ref(eq:SkewType3), utilisée par les logiciels MINITAB et BMDP). Nous verrons qu'avec les *packages* **DescTools** ou **e1071**, il possible de calculer ces trois méthodes. Aussi, pour des grands échantillons ($n>100$), il y a très peu de différences entre les résultats produits par ces trois formules [@joanes1998comparing]. Quelle que soit la formule utilisée, le coefficient d'assymétrie s'interprète comme suit (figure \@ref(fig:asymetrie)) :

* quand la valeur du *skewness* est négative, la **distribution est asymétrique négative**. La distribution est alors tirée à gauche par des valeurs extrêmes faibles, mais peu nombreuses. On emploie souvent l'expression *la queue de distribution* est étirée vers la gauche. La moyenne est alors inférieure à la médiane.
* quand la valeur du *skewness* est égale à 0, **la distribution est symétrique** (la médiane sera égale à la moyenne). Pour une variable discrète, les valeurs du mode, de la moyenne et de la médiane seront égales.
* quand la valeur du *skewness* est positive, la **distribution est symétrique positive**. La distribution est alors tirée à droite par des valeurs extrêmes fortes, mais peu nombreuses. La queue de distribution est alors étirée vers la droite. La moyenne est alors supérieure à la médiane. En sciences sociales, les variables de revenu (totaux ou d'emploi, des individus ou des ménages) ont souvent des distributions asymétriques positives : la moyenne est affectée par quelques observations avec des valeurs de revenu très élevées et est ainsi supérieure à la médiane. En études urbaines, la densité de population pour des unités géographiques d'une métropole donnée (secteur de recensement par exemple) a aussi souvent une distribution asymétrique positive : quelques secteurs de recensement au centre de la métropole sont caractérisés par des valeurs de densité très élevées qui tirent la distribution vers la droite.

\begin{equation}\footnotesize  
g_1=\frac{ \frac{1}{n} \sum_{i=1}^n(x_i-\bar{x})^3} { \left[\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\right] ^\frac{3}{2}}
(\#eq:SkewType1)
\end{equation}

\begin{equation}\footnotesize  
G_1= \frac{\sqrt{n(n-1)}}{n-2} g_1
(\#eq:SkewType2)
\end{equation}

\begin{equation}\footnotesize  
b_1= \left( \frac{n-1}{n} \right) ^\frac{3}{2} g_1
(\#eq:SkewType3)
\end{equation}


```{r asymetrie, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Asymétrie d'une distribution", out.width='100%'}
library(DescTools)
library(ggplot2)
library(ggpubr)
library(SimDesign)
# Générer des distributions asymétriques
df <- data.frame(
  Normale  = rnorm(1500,0,1),
  Skewed_L = rValeMaurelli(1500, mean=0, sigma=1, skew=-1.4, kurt=3),
  Skewed_R = rValeMaurelli(1500, mean=0, sigma=1, skew=1.4, kurt=3),
  L = rValeMaurelli(1500, mean=0, sigma=1, skew=0, kurt=7),
  P = rValeMaurelli(1500, mean=0, sigma=1, skew=0, kurt=-1)
)
statsL <- c(mean(df$Skewed_L),median(df$Skewed_L),Skew(df$Skewed_L),Kurt(df$Skewed_L))
statsR <- c(mean(df$Skewed_R),median(df$Skewed_R),Skew(df$Skewed_R),Kurt(df$Skewed_R))
statsN <- c(mean(df$Normale),median(df$Normale),Skew(df$Normale),Kurt(df$Normale))
CaptionL <- paste("Moyenne = ",  round(statsL[1],2), 
                  "\nMédiane = ",  round(statsL[2],2),  
                  "\nSkewness = ",  round(statsL[3],2), 
                  sep="")
CaptionR <- paste("Moyenne = ",  round(statsR[1],2), 
                  "\nMédiane = ",  round(statsR[2],2),  
                  "\nSkewness = ",  round(statsR[3],2), 
                  sep="")
CaptionN <- paste("Moyenne = ",  round(statsN[1],2), 
                  "\nMédiane = ",  round(statsN[2],2),  
                  "\nSkewness = ",  round(statsN[3],2), 
                  sep="")
Gl <- ggplot(data = df, mapping = aes(x=Skewed_L))+
      geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
      stat_function(fun = dnorm, args = list(mean = mean(df$Skewed_L), sd = sd(df$Skewed_L)), color="black",size=1)+
      labs(title ="a. Asymétrie négative",
           subtitle = "Moyenne < Médiane",
           x="", 
           y="Densité",
           caption = CaptionL)+
      geom_vline(xintercept = statsL[1],color="cadetblue4", size=.8)+
      geom_vline(xintercept = statsL[2],color="coral4", size=.8)+
      theme(
      plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
      plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
      )
  
Gr <- ggplot(data = df, mapping = aes(x=Skewed_R))+
      geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
      stat_function(fun = dnorm, args = list(mean = mean(df$Skewed_R), sd = sd(df$Skewed_R)), color="black",size=1)+
      labs(title ="b. Asymétrie positive",
           subtitle = "Moyenne > Médiane",
           x="", y="",
           caption = CaptionR)+
      geom_vline(xintercept = statsR[1],color="cadetblue4", size=.8)+
      geom_vline(xintercept = statsR[2],color="coral4", size=.8)+
      theme(
      plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
      plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
      )
Gn <- ggplot(data = df, mapping = aes(x=Normale))+
      geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
      stat_function(fun = dnorm, args = list(mean = mean(df$Normale), sd = sd(df$Normale)), color="black",size=1)+
      labs(title ="c. Asymétrie nulle", 
           subtitle = "Moy. et méd. très semblables",
           x="", y="",
           caption = CaptionN)+
      geom_vline(xintercept = statsN[1],color="cadetblue4", size=.8)+
      geom_vline(xintercept = statsN[2],color="coral4", size=.8)+
      theme(
      plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
      plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
      )
ggarrange(Gl, Gn, Gr, ncol = 2, nrow=2)
```


**Pour évaluer l'applatissement d'une distribution**, on utilisera le coefficient d’aplatissement (*kurtosis* en anglais). Là encore, il existe trois formules pour le calculer (eq. \@ref(eq:KurtType1), \@ref(eq:KurtType2), \@ref(eq:KurtType3)) qui renverront des valeurs très sembables pour de grands échantillons [@joanes1998comparing]. Cette mesure s'interprète comme suit (figure \@ref(fig:asymetrie)) :

* quand la valeur du *kurtosis* est négative, la **distribution est platikurtique**. La distribution est dite plate, c'est-à-dire que la valeur de l'écart-type est importante (comparativement à une distribution normale), signalant une grande dispersion des valeurs de part et d'autre la moyenne.
* quand la valeur du *kurtosis* est égale à 0, **la distribution est mésokurtique**, ce qui est typique d'une distribution normale.
* quand la valeur du *kurtosis* est positive, la **distribution est leptokurtique**, signalant que l'écart-type (la dispersion des valeurs) est plutôt faible. Autrement dit, la dispersion des valeurs autour de la moyenne est faible.

\begin{equation}\footnotesize  
g_2=\frac{\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^4} {\left( \frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\right)^2}-3
(\#eq:KurtType1)
\end{equation}

\begin{equation}\footnotesize  
G_2 = \frac{n-1}{(n-2)(n-3)} \{(n+1) g_2 + 6\}
(\#eq:KurtType2)
\end{equation}

\begin{equation}\footnotesize  
b_2 = (g_2 + 3) (1 - 1/n)^2 - 3
(\#eq:KurtType3)
\end{equation}

```{r kurtosis, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Applatissement d'une distribution", out.width='100%'}
library(DescTools)
library(ggplot2)
library(ggpubr)
library(SimDesign)
# Générer des distributions asymétriques
statsM <- c(mean(df$Normale),median(df$Normale),Skew(df$M),Kurt(df$Normale))
statsL <- c(mean(df$L),median(df$L),Skew(df$L),Kurt(df$L))
statsP <- c(mean(df$P),median(df$P),Skew(df$P),Kurt(df$P))
CaptionN <- paste("\nKurtosis = ",  as.character(round(Kurt(df$Normale),2)), sep="")
CaptionL <- paste("\nKurtosis = ",  as.character(round(Kurt(df$L),2)), sep="")
CaptionP <- paste("\nKurtosis = ",  as.character(round(Kurt(df$P),2)), sep="")
Gl <- ggplot(data = df, mapping = aes(x=L))+
  geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$L), sd = sd(df$L)), color="black",size=1)+
  labs(title ="a. Distribution leptokurtique",
       x="",
       y="Densité",
       subtitle = CaptionL)+
  geom_vline(xintercept = statsL[1],color="cadetblue4", size=.8)+
  theme(
    plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
    plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
  )
Gm <- ggplot(data = df, mapping = aes(x=Normale))+
  geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$Normale), sd = sd(df$Normale)), color="black",size=1)+
  labs(title ="b. Distribution mésokurtique",
       x="", y="",
       subtitle = CaptionN)+
  geom_vline(xintercept = statsM[1],color="cadetblue4", size=.8)+
  theme(
    plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
    plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
  )
Gp <- ggplot(data = df, mapping = aes(x=P))+
  geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$P), sd = sd(df$P)), color="black",size=1)+
  labs(title ="c. Distribution platikurtique",
       x="", y="",
       subtitle = CaptionP)+
  geom_vline(xintercept = statsN[1],color="cadetblue4", size=.8)+
  theme(
    plot.title = element_text(hjust = 0.5,  size = 9, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5,  size = 8, face = "plain"),
    plot.caption = element_text(hjust = 0.5,   size = 8, face = "plain")
  )
ggarrange(Gp, Gm, Gl, ncol = 2, nrow=2)
```

::: {.bloc_attention data-latex=""}
Regardez attentivement les équations \@ref(eq:KurtType1), \@ref(eq:KurtType2), \@ref(eq:KurtType3); vous remarquez que pour $g_2$ et $b_2$, il y a une soustraction de $-3$ et une addition $+6$ pour $G_2$. On parle alors de *kurtosis* normalisé (*excess kurtosis* en anglais). Pour une distribution normale, il prendra la valeur de 0, comparativement à la valeur de 3 pour un *kurtosis* non normalisé. Par conséquent, avant de calculer du *kurtosis*, il convient de s'assurer que la fonction que vous utilisez implémente une méthode de calcul normalisée (donnant une valeur de 0 pour une distribution normale). Par exemple, la fonction `Kurt` du *package* **DescTools** calcule les trois formules normalisées tandis que la fonction `kurtosis` du *package* **moments** renvoie un *kurtosis* non normalisé.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(DescTools)
library(moments)
#Générer une variable normalement distribuée avec 1000 observations
Normale <- rnorm(1500,0,1)
round(DescTools::Kurt(Normale),3)
round(moments::kurtosis(Normale),3)
```

:::

##### Vérifier la normalité avec des graphiques

Les graphiques sont un excellent moyen de vérifier visuellement si une distribution est normale ou pas. Bien entendu, les histogrammes, que nous avons déjà largement utilisés, sont un incontournable; à titre de rappel, ils permettent de représenter la forme de la distribution des données (figure \@ref(fig:CourbeNormale)). Un autre type de graphique intéressant est le **diagramme  quantile-quantile** (*Q-Q plot* en anglais) qui permet de comparer la distribution d'une variable avec une distribution gaussienne (normale). Trois éléments composent ce graphique tel qu'illustré à la figure \@ref(fig:qqplot) :

* les points, représentant les observations de la variable
* la distribution gaussienne (normale), représentée par une ligne
* l'intervalle de confiance à 5% de la distribution normale (en orange sur la figure).

Quand la variable est normale distribuée, les points seront situés le long de la ligne. Plus les points localisés en dehors de l'intervalle de confiance (bande orange) seront nombreux, plus la variable sera alors anormalement distribuée.


```{r CourbeNormale, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Distributions et courbe normale", out.width='90%'}
library(ggplot2)
library(reshape2)
library(qqplotr)
melted_dist <- melt(df,
                    distributions = c("Normale", "Skewed_L",
                                      "Skewed_R","student", "L", "P"))
names(melted_dist) <- c("distribution", "valeur")
melted_dist$distribution <- factor(melted_dist$distribution,
                          levels = c("Normale","Skewed_L","Skewed_R","L","P"),
                          labels = c("Normale",
                                     "Asymétrie négative",
                                     "Asymétrie positive",
                                     "Leptokurtique",
                                     "Platikurtique"))
ggplot(data = melted_dist, mapping = aes(x=valeur))+
  labs(caption = paste0("Skewness", "Kurtosis", sep=""))+
  geom_histogram(color="white",fill="bisque3",aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(melted_dist$valeur), sd = sd(melted_dist$valeur)), color="black",size=1)+
  geom_vline(xintercept = mean(melted_dist$valeur),color="cadetblue4", size=.8)+
  labs(y="Densité", x="", title="", caption="")+
  facet_wrap(vars(distribution), ncol=2, scales = "free")
```

```{r qqplot, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Diagrammes quantile-quantile", out.width='90%'}
ggplot(data = melted_dist, aes(sample=valeur))+
    stat_qq_band(fill="bisque3")+
    stat_qq_line(color="black", size=.3) +
    stat_qq(color="black", size=1)+
    labs(y="Échantillon", x="théorique", title="", caption="")+
  facet_wrap(vars(distribution), ncol=2, scales = "free")
```

##### Vérifier la normalité avec des tests de normalité {#sect025413}

Cinq principaux tests d'hypothèse permettent de vérifier la normalité d'une variable : les tests de **Kolmogorov-Smirnov** (KS), **Lilliefors** (LF), **Shapiro-Wilk** (SW), **Anderson-Darling**, et de **Jarque-Bera** (JB); sachez toutefois qu'il y en a d'autres non discutés ici (tests de D’Agostino–Pearson, Cramer–von Mises, de Ryan-Joiner, Shapiro–Francia, etc.). Pour les formules et une description détaillée de ces tests, vous pouvez consulter Razali et al. [-@razali2011power] ou Yap et Sim [-@yap2011comparisons]. **Quel test choisir ?** Plusieurs auteurs ont comparé ces différents tests à partir de plusieurs échantillons, et ce, en faisant varier la forme de la distribution et le nombre d'observations [@razali2011power;@yap2011comparisons]. Selon Razali et al. [-@razali2011power], le meilleur test semble être celui de Shapiro-Wilk, puis ceux de Anderson-Darling, Lilliefors et Kolmogorov-Smirnov. Yap et Sim [-@yap2011comparisons] concluent aussi que le Shapiro-Wilk semble être le plus performant.

Quoi qu'il en soit, ces cinq tests postulent que la variable suit une distribution gaussienne (hypothèse nulle, h<sub>0</sub>). Cela signifie que si la valeur de P associée à la valeur de chacun des tests est supérieure au seuil alpha choisi (habituellement $\alpha=0,05$), la distribution est normale. À l'inverse, si $P<0,05$, on choisit l'hypothèse alternative (h<sub>1</sub>), c'est-à-dire que la distribution est anormale.

```{r testnormalites, echo=FALSE, message=FALSE, warning=FALSE}
options(knitr.kable.NA = '')

df <- data.frame(
  Test = c("Kolmogorov-Smirnov",
           "Lilliefors",
           "Shapiro-Wilk",
           "Anderson-Darling",
           "Jarque-Bera"),
  Interpretation =
          c("Plus sa valeur est proche de zéro, plus la distribution est normale.  L'avantage de ce test est qu'il peut être utilisé pour vérifier si une variable suit la distribution de n'importe quelle loi (autre que la loi normale).",
           "Ce test est une adaptation du test de Kolmogorov-Smirnov. Plus sa valeur est proche de zéro, plus la distribution est normale.",
           "Si la valeur de la statistique de Shapiro-Wilk est proche de 1, alors la distribution est normale; et anormale quand elle est inférieure à 1.",
           "Ce test est une modification du test de Cramer-von Mises (CVM). Il peut être aussi utilisé pour tester d'autres distributions (uniforme, log-normale, exponentielle, Weibull, distribution de pareto généralisée, logistique, etc.).",
           "Basé sur un test du type multiplicateur de Lagrange, il utilise dans son calcul les valeurs du *Skewness* et du *Kurtosis*. Plus sa valeur s'approche de 0, plus la distribution est normale. Ce test est surtout utilisé pour vérifier si les résidus d'un modèle de régression linéaire sont normalement distribués, nous y reviendrons dans le chapitre sur la régression multiple. Il s'écrit $JB=\\frac{1}{6} \\left({g_1}^2+\\frac{{g_1}^2}{4} \\right)$ avec $g_1$ et $g_2$ qui sont respectivement les valeurs du *skewness* et du *kurtosis* de la variable (voir plus haut les équations \\@ref(eq:SkewType1) et \\@ref(eq:KurtType1))."),
  Fonction= c("`ks.test` du *package* **stats**",
           "`lillie.test` du *package* **nortest**",
           "`shapiro.test` du *package* **stats**",
           "`ad.test` du *package* **stats**",
           "`JarqueBeraTest` du *package* **DescTools**")
  )

show_table(
   df,
   col.names = c("Test","Propriétés et interprétation", "Fonction R"),
   caption = "Les différents tests d'hypothèse pour la normalité",
   col.to.resize = c(2,3),
   col.width = "6cm"
)
```

Dans le tableau ci-dessous sont reportées les valeurs des différents tests pour les cinq types de distribution générées à la figure \@ref(fig:CourbeNormale). Sans surprise, pour l'ensemble des tests, la valeur de *P* est inférieur à 0,05 pour la distribution normale.

```{r calcultestnormalites, echo=FALSE, message=FALSE, warning=FALSE}
library(DescTools)
library(nortest)
df <- data.frame(
  Normale  = rnorm(500,0,1),
  Skewed_L = rValeMaurelli(500, mean=0, sigma=1, skew=1.4, kurt=3),
  Skewed_R = rValeMaurelli(500, mean=0, sigma=1, skew=-1.4, kurt=3),
  L = rValeMaurelli(500, mean=0, sigma=1, skew=0, kurt=7),
  P = rValeMaurelli(500, mean=0, sigma=1, skew=0, kurt=-1)
)
vars <- names(df)
S <- c()
K <- c()
KS <- c()
KS.p <- c()
LF <- c()
LF.p <- c()
SW <- c()
SW.p <- c()
AD <- c()
AD.p <- c()
JB <- c()
JB.p <- c()
i <- 1
for (e in vars){
  ksmirnov <- ks.test(df[[e]], "pnorm", mean=mean(df[[e]]), sd=sd(df[[e]]))
  lillie <- lillie.test(df[[e]])
  shapiro <- shapiro.test(df[[e]])
  AndDarling <- ad.test(df[[e]])
  JarqueB <- JarqueBeraTest(df[[e]], robust = TRUE)
  
  S[i] <- Skew(df[[e]])
  K[i] <- Kurt(df[[e]])
  
  KS[i] <- ksmirnov$statistic
  KS.p[i] <- ksmirnov$p.value 
  
  LF[i] <- lillie$statistic
  LF.p[i] <- lillie$p.value  
  
  SW[i] <- shapiro$statistic
  SW.p[i] <- shapiro$p.value
  AD[i] <- AndDarling$statistic
  AD.p[i] <- AndDarling$p.value
  
  JB[i] <- JarqueB$statistic
  JB.p[i] <- JarqueB$p.value
  i <- i+1
}
Tests <- data.frame(
  "S" = round(S,3),
  "K" = round(K,3),
  "KS" = round(KS,3),
  "LF" = round(LF,3),
  "SW" = round(SW,3),
  "AD" = round(AD,3),
  "JB" = round(JB,3),
 "KS.p" = round(KS.p,3),
 "LF.p" = round(LF.p,3),
 "SW.p" = round(SW.p,3),
 "AD.p" = round(AD.p,3),
 "JB.p" = round(JB.p,3)
)
Tests <- rbind(c("Skewness","Kurtosis",
                      "Kolmogorov-Smirnov (KS)",
                      "Lilliefors (LF)",
                      "Shapiro-Wilk (SW)",
                      "Anderson-Darling (AD)",
                      "Jarque-Bera (JB)",
                      "KS (valeur p)",
                      "LF (valeur p)",
                      "SW (valeur p)",
                      "AD (valeur p)",
                      "JB (valeur p)"),
               Tests)
Tests <- data.frame(t(Tests))

show_table(Tests,
           col.names = c("","Normale","Asymétrie négative",
                           "Asymétrie positive","Leptokurtique", "Platikurtique"),
           caption = "Calculs des tests de normalité pour différentes distributions")
```

::: {.bloc_attention data-latex=""}
**Attention** ! La plupart des auteurs s'entendent sur le fait que ces tests sont très restrictifs : plus la taille de votre échantillon ($n$) est importante, plus les tests risquent de vous signaler que vos distributions sont anormales (à la lecture des valeurs de P).

Certains conseillent même de ne pas les utiliser quand $n>200$ et de vous fier uniquement aux graphiques (histogramme et diagramme Q-Q) !
:::

::: {.bloc_astuce data-latex=""}
Bref, vérifier la normalité d'une variable n'est pas une tâche si simple. De nouveau, nous vous conseillons vivement de :

* construire les graphiques pour analyser visuellement la forme de la distribution (histogramme avec courbe normale et diagramme Q-Q)
* calculer le *skewness* et le *kurtosis*, 
* calculer plusieurs tests (minimalement Shapiro-Wilk et Kolmogorov-Smirnov)
* accorder une importance particulière aux graphiques lorsque vous traitez des grands échantillons ($n>200$).
:::


#### Vérifier d'autres formes de distributions

Comme nous l'avons vu, la distribution normale n'est que l'une des multiples distributions existantes. Dans de nombreuses situations, elle ne sera pas adaptée pour décrire vos variables. La démarche à adopter pour trouver une distribution adaptée est la suivante : 

1. Définissez la nature de votre variable, identifier si elle est discrète ou continue et l'intervalle dans lequel elle est définie. Une variable dont les valeurs sont positives ou négatives ne pourra pas être décrite avec une distribution Gamma par exemple (à moins de la décaler).
2. Explorez votre variable, affichez son histogramme et son graphique de densité pour avoir une vue générale de sa morphologie.
3. Présélectionnez un ensemble de distributions candidates compte tenu des observations précédentes. Vous pouvez également vous reporter à la littérature existante sur votre sujet d'étude pour inclure d'autres distributions. Soyez flexible ! Une variable strictement positive pourrait tout de même avoir une forme normale. De même, une variable décrivant des comptages suffisamment grands pourrait être mieux décrite par une distribution normale qu'une distribution de poisson.
4. Tentez d'ajuster chacune des distributions retenues à vos données et comparez les qualités d'ajustements pour retenir la plus adaptée.

Pour ajuster une distribution à un jeu de données, il faut trouver les valeurs des paramètres de cette distribution qui lui permettront d'adopter une forme la plus proche possible des données. On appelle cette opération **ajuster un modèle**, puisque la distribution théorique est utilisée pour modéliser les données. L'ajustement des paramètres est un problème d'optimisation que plusieurs algorithmes sont capables de résoudre (*gradient descent*, *Newton-Raphson method*, *Fisher scoring*, etc.). Dans R, le *package* **fitdistrplus** permet d'ajuster pratiquement n'importe quelle distribution à des données en offrant plusieurs stratégies d'optimisation grâce à la fonction `fitdist`. Il suffit de disposer d'une fonction représentant la distribution de densité ou de masse de la distribution en question, généralement noté `dnomdeladistribution` (`dnorm`, `dgamma`, `dpoisson`, etc.) dans R. Notez que certains *packages* comme **VGAM** ou **gamlss.dist** ajoutent un grand nombre de fonctions de densité et de masse à celles déjà disponibles de base dans R.

Pour comparer l'ajustement de plusieurs distributions théoriques à des données, trois approches doivent être combinées : 

* Observer graphiquement l'ajustement de la courbe théorique à l'histogramme des données. Cela permet d'éliminer au premier coup d'œil les distributions qui ne correspondent pas.
* Comparer les *loglikelihood*. Le *loglikelihood* est un score d'ajustement des distributions aux données. Pour faire simple, plus le *loglikelihood* est grand, plus la distribution théorique est proche des données. Référez-vous à l'encadré suivant pour une description plus en profondeur du *loglikelihood*.
* Utiliser le test de Kolmogorov-Smirnov pour déterminer si une distribution particulière est mieux ajustée pour les données.

::: {.bloc_aller_loin data-latex=""}
**Qu'est-ce-que le loglikelihood** ? 

Le *loglikelihood* est une mesure de l'ajustement d'un modèle à des données. Il est utilisé à peu près partout en statistique. Comprendre sa signification est donc un exercice important pour développer une meilleure intuition du fonctionnement général de nombreuses méthodes. Si les concepts de fonction de densité et de fonction de masse vous semblent encore flous, reportez-vous à la section \@ref(sect024) sur les distributions dans un premier temps.

Admettons que nous disposons d'une variable continue *v* que nous avons tenté de modéliser avec une distribution *d* (il peut s'agir de n'importe quelle distribution). *d* a une fonction de densité avec laquelle il est possible de calculer pour chacune des valeurs de *v* sa probabilité d'être observée selon le modèle *d*.

Prenons un exemple concret dans R. Admettons que nous avons une variable comprenant 10 valeurs (oui, c'est un petit échantillon, mais c'est pour faire un exemple simple).

```{r}
v <- c(5,8,7,8,10,4,7,6,9,7)
moyenne <- mean(v)
ecart_type <- sd(v)
```

En calculant sa moyenne et son écart type, nous obtenons les paramètres d'une distribution normale que nous pouvons utiliser pour représenter les données observées. En utilisant la fonction `dnorm` (la fonction de densité de la distribution normale), nous pouvons calculer la probabilité d'observer chacune des valeurs de *v* selon cette distribution normale.

```{r}
probas <- dnorm(v, moyenne, ecart_type)
df <- data.frame(valeur = v,
                 proba = probas)
print(df)
```
On observe ainsi que les valeurs 7 et 8 sont très probables selon le modèle alors que la valeur 10 est très improbable.

Le *likelihood* est simplement le produit de toutes ces probabilités. Il s'agit donc de **la probabilité conjointe** d'avoir observé toutes les valeurs de *v* **sous l'hypothèse** que *d* est la distribution produisant ces valeurs. Si *d* décrit efficacement *v*, alors le *likelihood* est plus grand que si *d* ne décrit pas efficacement *v*. Il s'agit d'une forme de raisonnement par l'absurde : après avoir observé *v*, on calcule la probabilité d'avoir observé *v* (*likelihood*) si notre modèle *d* était vrai. Si cette probabilité est très basse, alors c'est que notre modèle est mauvais puisqu'on a bien observé *v*.

```{r}
likelihood_norm <- prod(probas)
print(likelihood_norm)
```
Cependant, multiplier un grand nombre de valeurs inférieures à zéro tend à produire des chiffres infiniment petits et donc à complexifier grandement le calcul. On préfère donc utiliser le *loglikelihood*. L'idée étant  transformer les probabilités obtenues avec la fonction *log* puis d'additionner leurs résultats, puisque $log(xy) = log(x)+log(y)$. 

```{r}
loglikelihood_norm <- sum(log(probas))
print(loglikelihood_norm)
```
Comparons ce *loglikelihood* a celui d'un second modèle dans lequel nous utilisons toujours la distribution normale, mais avec une moyenne différente (faussée en rajoutant +3) : 

```{r}
probas2 <- dnorm(v, moyenne+3, ecart_type)
loglikelihood_norm2 <- sum(log(probas2))
print(loglikelihood_norm2)
```
Ce second *loglikehood* est plus faible, indiquant clairement que le premier modèle est plus adapté aux données.
:::

Passons à la pratique avec deux exemples.

##### Temps de retard des bus de la ville de Toronto

Analysons les temps de retard pris par les bus de la ville de Toronto lorsqu'un évènement perturbe la circulation. Ce jeu de données est disponible sur le site de l'[Open Data](https://open.toronto.ca/catalogue/?search=bus%20delay&sort=score%20desc){target="_blank"} de la ville de Toronto. Compte tenu de la grande quantité d'observations, nous avons fait le choix de nous concentrer sur les évènements ayant eu lieu durant le mois de janvier 2019. Puisque la variable étudiée est une durée exprimée en minutes, elle est strictement positive (supérieure à 0), car un bus avec zéro minute de retard est à l'heure. Nous considérons également qu'un bus ayant plus de 150 minutes de retard (2h30) n'est tout simplement pas passé (personne ne risque d'attendre 2h30 pour prendre son bus). Commençons par charger les données et observer leur distribution empirique.

```{r figbustrt, fig.align='center', fig.cap="Distribution empirique des temps de retard des bus à Toronto en janvier 2019", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='65%'}
library(ggplot2)
# charger le jeu de données
data_trt_bus <- read.csv('data/univariee/bus-delay-2019_janv.csv', sep =';')
# retirer les observations aberrantes
data_trt_bus <- subset(data_trt_bus, data_trt_bus$Min.Delay > 0 &
                         data_trt_bus$Min.Delay < 150)
# représenter la distribution empirique du jeu de données
ggplot(data = data_trt_bus) + 
  geom_histogram(aes(x=Min.Delay, y = ..density..), bins = 40)+
  geom_density(aes(x=Min.Delay), color = 'blue', bw = 2, size = 0.8)+
  labs(x = 'temps de retard (min)',
       y = '')
```

Compte tenu de la forme de la distribution empirique et de sa nature, quatre distributions sont envisageables : 

* La distribution Gamma, strictement positive et asymétrique, elle est aussi une généralisation de la distribution exponentielle utilisée pour modéliser des temps d'attente. Pour des raisons similaires, on peut aussi retenir la distribution de Weibull et la distribution log-normale. Nous écartons ici la distribution skew-normale puisque le jeu de données n'a clairement pas une forme normale au départ.
* La distribution de Pareto, strictement positive et permettant de représenter ici le fait que la plupart des retards durent moins de 10 minutes, mais que quelques retards sont également beaucoup plus longs.

Commençons par ajuster les quatre distributions avec la fonction `fitdist` du *package* **fitdistrplus** et représentons-les graphiquements pour éliminer les moins bons candidats. Nous utilisons également le *package* **actuar** pour la fonction de densité de Pareto (`dpareto`).

```{r figbustrt2, fig.align='center', fig.cap="Comparaison des distributions ajustées aux données de retard des bus", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='65%'}
library(fitdistrplus)
library(actuar)
library(ggpubr)
# ajustement des modèles
model_gamma <- fitdist(data_trt_bus$Min.Delay, distr = "gamma")
model_weibull <- fitdist(data_trt_bus$Min.Delay, distr = "weibull")
model_lognorm <- fitdist(data_trt_bus$Min.Delay, distr = "lnorm")
model_pareto <- fitdist(data_trt_bus$Min.Delay, distr = "pareto", 
                        start = list(shape = 1, scale = 1),
                        method = "mse") # différentes méthodes d'optimisations
# réalisation des graphiques
plot1 <- ggplot(data = data_trt_bus) + 
  geom_histogram(aes(x=Min.Delay, y = ..density..), bins = 40)+
  stat_function(fun = dgamma, color = 'red', size = 0.8, 
                args = as.list(model_gamma$estimate))+
  labs(x = 'temps de retard (min)',
       y = '',
       subtitle = "modèle Gamma")
plot2 <- ggplot(data = data_trt_bus) + 
  geom_histogram(aes(x=Min.Delay, y = ..density..), bins = 40)+
  stat_function(fun = dweibull, color = 'red', size = 0.8, 
                args = as.list(model_weibull$estimate))+
  labs(x = 'temps de retard (min)',
       y = '',
       subtitle = "modèle Weibull")
plot3 <- ggplot(data = data_trt_bus) + 
  geom_histogram(aes(x=Min.Delay, y = ..density..), bins = 40)+
  stat_function(fun = dlnorm, color = 'red', size = 0.8, 
                args = as.list(model_lognorm$estimate))+
  labs(x = 'temps de retard (min)',
       y = '',
       subtitle = "modèle log-normal")
plot4 <- ggplot(data = data_trt_bus) + 
  geom_histogram(aes(x=Min.Delay, y = ..density..), bins = 40)+
  stat_function(fun = dpareto, color = 'red', size = 0.8, 
                args = as.list(model_pareto$estimate))+
  labs(x = 'temps de retard (min)',
       y = '',
       subtitle = "Modèle Pareto")
ggarrange(plotlist = list(plot1, plot2, plot3, plot4),
          ncol = 2, nrow = 2)
```

Visuellement, on constate que la distribution de Pareto est un mauvais choix. Pour les trois autres distributions, la comparaison des *loglikelihood* s'impose.

```{r tabledistribs, message=FALSE, warning=FALSE, out.width='50%'}
df <- data.frame(model = c("Gamma","Weibull",
                           "log-normal"), 
                 loglikelihood = c(model_gamma$loglik, 
                 model_weibull$loglik,
                 model_lognorm$loglik))
show_table(df, 
      col.names = c("Distributon","LogLikelihood"),
      caption = 'Comparaison des LogLikekelihood des trois distributions',
           )
```
Le plus grand *logLikelihood* est obtenu par la distribution de Gamma qui s'ajuste donc le mieux à nos données. Pour finir, nous pouvons tester formellement avec le test de Kolmogorov-Smirnov si nos données proviennent bien de cette distribution de Gamma.

```{r message=FALSE, warning=FALSE}
params <- as.list(model_gamma$estimate)
ks.test(data_trt_bus$Min.Delay,
        y = pgamma, shape = params$shape, rate = params$rate)
```

La valeur de *p* est inférieure à 0,05, on ne peut donc pas accepter l'hypothèse que notre jeu de données suit effectivement un loi de Gamma. Considérant le nombre d'observations et le fait que de nombreux temps d'attente sont identiques (ce à quoi le test est très sensible), ce résultat n'est pas surprenant. La distribution de Gamma reste cependant la distribution qui représente le mieux nos données. Nous pouvons estimer grâce à cette distribution la probabilité qu'un bus ait un retard de plus de 10 minutes de la façon suivante : 

```{r message=FALSE, warning=FALSE}
pgamma(10, shape = params$shape, rate = params$rate, lower.tail = F)
```

ce qui correspond à 54% de chance. 

Pour moins de 10 minutes : 
```{r message=FALSE, warning=FALSE}
pgamma(10, shape = params$shape, rate = params$rate, lower.tail = T)
```

soit 46%.

Uun dernier exemple avec la probabilité qu'un retard dépasse 45 minutes : 

```{r message=FALSE, warning=FALSE}
pgamma(45, shape = params$shape, rate = params$rate, lower.tail = F)
```
Soit seulement 1,3%. 

Par conséquent, si un matin à Toronto votre bus a plus de 45 minutes de retard, bravo vous êtes tombé sur une des très rares occasions où un tel retard se produit

##### Les accidents de vélo à Montréal

Le second jeu de données représente le nombre d'accidents de la route impliquant un vélo sur les intersections dans les quartiers centraux de Montréal. Le jeu de données complet est disponible sur le site des [données ouvertes](http://donnees.ville.montreal.qc.ca/dataset/collisions-routieres){target="_blank"} de la ville de Montréal. Puisque ces données correspondent à des comptages, la première distribution à envisager est la distribution de poisson. Cependant, puisque nous aurons également un grand nombre d'intersections sans accident, il serait judicieux de tester la distribution de poisson avec excès de zéro.

```{r figaccmtl, fig.align='center', auto_pdf = TRUE, fig.cap="Distribution empirique du nombre d'accidents par intersection impliquant un cycliste à Montréal en 2017 dans les quartiers centraux",  out.width='65%'}
library(ggplot2)
# charger le jeu de données
data_accidents <- read.csv('data/univariee/accidents_mtl.csv', sep =',')
counts <- data.frame(table(data_accidents$nb_accident))
names(counts) <- c("nb_accident",'frequence')
counts$nb_accident <- as.numeric(as.character(counts$nb_accident))
counts$prop <- counts$frequence / sum(counts$frequence)
# représenter la distribution empirique du jeu de donnée
ggplot(data = counts) + 
  geom_bar(aes(x=nb_accident, weight = frequence), width = 0.5)+
  labs(x = "nombre d'accidents",
      y = 'fréquence')
```
Nous avons effectivement de nombreux zéros ici, essayons d'ajuster nos deux distributions à ce jeu de données. Dans le graphique suivant, les barres grises représentent la distribution empirique du jeu de données et les barres rouges les distributions théoriques ajustées. Nous utilisons ici le *package* **gamlss.dist** pour avoir la fonction de masse d'une distribution de poisson avec excès de zéros.

```{r figaccmtldist, fig.align='center', fig.cap="Ajustement des distributions de poisson et poisson avec excès de zéros", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='65%'}
library(gamlss.dist)
#ajuster le modèle de poisson
model_poisson <- fitdist(data_accidents$nb_accident, distr = "pois")
#ajuster le modèle de poisson avec excès de zéros
model_poissonzi <- fitdist(data_accidents$nb_accident, "ZIP",
    start = list(mu = 4, sigma = 0.15), # valeurs pour faciliter la convergence
    optim.method = "L-BFGS-B", # méthode d'optimisation recommandée dans la doc
    lower = c(0.00001, 0.00001),# valeurs minimales des deux paramètres
    upper = c(Inf, 1)# valeurs maximales des deux paramètres
    )
dfpoisson <- data.frame(x=c(0:10),
                        y=dpois(0:10, model_poisson$estimate)
                        )
plot1 <- ggplot() + 
  geom_bar(aes(x=nb_accident, weight = prop), width = 0.6, data = counts)+
  geom_bar(aes(x=x, weight = y), width = 0.15, data = dfpoisson, fill = "red")+
  scale_x_continuous(limits = c(-0.5,7), breaks = c(0:7))+
  labs(subtitle = "modèle poisson",
       x = "nombre d'accidents",
       y = "")
dfpoissonzi <- data.frame(x=c(0:10),
                        y=dZIP(0:10, model_poissonzi$estimate[[1]],
                               model_poissonzi$estimate[[2]])
                        )
plot2 <- ggplot() + 
  geom_bar(aes(x=nb_accident, weight = prop), width = 0.6, data = counts)+
  geom_bar(aes(x=x, weight = y), width = 0.15, data = dfpoissonzi, fill = "red")+
  scale_x_continuous(limits = c(-0.5,7), breaks = c(0:7))+
  labs(subtitle = "modèle poisson avec excès de zéro",
       x = "nombre d'accident",
       y = "")
ggarrange(plotlist = list(plot1,plot2), ncol = 2)
```

Visuellement, le modèle avec excès de zéro semble s'imposer. Nous pouvons vérifier cette impression avec la comparaison des *loglikelihood*.
```{r}
print(model_poisson$loglik)
print(model_poissonzi$loglik)
#afficher les paramètres ajustés
model_poissonzi$estimate
```
Nous avons donc la confirmation que le modèle de poisson avec excès de zéros est mieux ajusté. Nous apprenons donc que 70% (sigma = 0,70) des intersections sont en fait exclues du phénomène étudié (probablement parce que très peu de cyclistes les utilisent ou parce qu'elles sont très peu accidentogènes) et que pour les autres, le taux d'accidents par année en 2017 était de 0,67 (mu = 0,669, mu signifiant $\lambda$ pour le *package* **gamlss**). À nouveau, nous pouvons effectuer un test formel avec le fonction `ks.test`.

```{r message=FALSE, warning=FALSE}
params <- as.list(model_poissonzi$estimate)
ks.test(data_accidents$nb_accident,
        y = pZIP, mu = params$mu, sigma = params$sigma)
```

Encore une fois, on doit rejeter l'hypothèse selon laquelle le test suit une distribution de poisson avec excès de zéros. Ces deux exemples montrent à quel point ce test est restrictif.

### La transformation des variables {#sect0255}

#### Les transformations visant à atteindre la normalité  {#sect02551}

Comme énoncé au début de cette section, plusieurs méthodes statistiques nécessitent que la variable quantitative soit normalement distribuée. C'est notamment le cas de l'analyse de variance et des tests *t* (abordés dans les chapitres suivants) qui fourniront des résultats plus robustes lorsque la variable est normalement distribuée. Plusieurs transformations sont possibles, les plus courantes étant la racine carrée, le logarithme et l'inverse de la variable. Selon plusieurs auteurs (notamment, Tabacknick et *et al.* [-@tabachnick2007, p. 89]), en fonction du type (positive ou négative) et du degré d'asymétrie, les transformations suivantes sont possibles afin d'améliorer la normalité de la variable :

* Asymétrie positive modérée : la racine carrée de la variable *X* avec la fonction `sqrt(df$x)`. 
* Asymétrie positive importante : le logarithme de la variable avec `log10(df$x)`
* Asymétrie positive sévère : l'inverse de la variable avec `1/(df$x)`

::: {.bloc_astuce data-latex=""}
Attention, pour une valeur égale ou inférieure à 0, on ne peut pas calculer une racine carrée ou un logarithme. Par conséquent, il convient de décaler simplement la distribution vers la droite afin de s'assurer qu'il n'y ait plus de valeurs négative ou égale à 0 :

* `sqrt(df$x - min(df$x+1))`avec pour une asymétrie positive avec des valeurs négatives ou égales à 0
* `log(df$x - min(df$x+1))`pour une asymétrie positive avec des valeurs négatives ou égales à 0

Par exemple, si la valeur minimale de la variable est égale à -10, la valeur minimale de variable décalée sera ainsi de 11.


* Asymétrie négative modérée : `sqrt(max(df$x+1) - df$x)`. 
* Asymétrie négative importante :`log(max(df$x+1) - df$x)`
* Asymétrie négative sévère : `1/(max(df$x+1) - df$x)`
::: 

::: {.bloc_attention data-latex=""}

**Transformation des variables pour atteindre la normalité : ce n'est pas toujours la panacée !**

La transformation des données fait et fera encore longtemps débat à la fois parmi les statisticiens, les débutants et utilisateurs avancés des méthodes quantitatives. Field et al. [-@field2012discovering, pp. 193] résument le tout avec humour : « To transform or not transform, that is the question ».

**Avantages de la transformation**

* L'obtention de *résultats plus robustes*.
* Dans une régression linéaire multiple, la transformation de la variable dépendante peut *remédier au non-respect des hypothèses de base liées à la régression* (linéarité et homoscédasticité des erreurs, absence des valeurs aberrantes, etc.).

**Inconvénients de la transformation**

* *Une variable transformée est plus difficile à interpréter* puisque cela change l'unité de mesure de la variable. Prenons un exemple concret : vous souhaitez comparer les moyennes de revenu de deux groupes *A* et *B*. Vous obtenez une différence de 15000$, soit une valeur facile à interpréter. Par contre, si la variable a été préalablement transformée en logarithme, il est possible que vous obteniez une différence de 9, ce qui est beaucoup moins parlant. Aussi, en transformant la variable en *log*, vous ne comparez plus les moyennes arithmétiques des deux groupes, mais plutôt leurs moyennes géométriques [@field2012discovering, pp. 193].

* *Pourquoi perdre la forme initiale de la distribution du phénomène à expliquer ?* Il est possible pour de nombreuses méthodes de choisir la distribution que l'on souhaite utiliser, il n'est donc pas nécessaire de toujours se limiter à la distribution normale. Par exemple, dans les modèles de régression généralisés (GLM), on pourrait indiquer que notre variable indépendante suit une distribution de *Student* plutôt que de vouloir à tout prix la rendre normale. De même, certains tests non-paramétriques permettent d'analyser des variables ne suivant pas une distribution normale.

**Démarche à suivre avant et après la transformation**

* *La transformation est-elle nécessaire ?* Ne transformez jamais une variable sans avoir analyser rigoureusement sa forme (histogramme avec courbe normale, *skewness* et *kurtosis*, tests de normalité).

* *D'autres options à la transformation d'une variable dépendante (VD) sont-elles envisageables ?* Identifiez la forme de la distribution de la VD et utilisez au besoin un modèle GLM adapté à cette distribution. Autrement dit, ne transformez pas automatiquement votre VD pour simplement pouvoir l'introduire dans une régression linéaire multiple.

* *La transformation a-t-elle un apport significatif ?* Premièrement, vérifiez si la transformation utilisée (logarithme, racine carrée, inverse, etc.) améliore la normalité de la variable. Ce n'est toujours le cas, pourquoi c'est pire ! Prenez soin de comparer les histogrammes, les valeurs de *skewness*, *kurtosis* et des différents tests de normalité avant et après la transformation. Deuxièmement, comparez les résultats de vos analyses statistiques sans et avec transformation, et ce, dans une démarche coût-avantage. Vos résultats sont-ils bien plus robustes? Par exemple, un R^2^ qui passe de 0,597 à 0,602 avant et après la transformation des variables avec des associations significatives similaires, mais plus difficiles à interpréter (du fait des transformations), n'est pas forcément un gain significatif. La modélisation en sciences sociales ne vise pas à prédire la trajectoire d'un satellite ou l'atterrissage d'un engin sur Mars ! La précision à la quatrième décimale n'est pas une condition ! Par conséquent, un modèle un peu moins robuste, mais plus facile à interpréter est parfois préférable.
:::

#### Autres types de transformations  {#sect02552}

Les trois transformations les plus couramment utilisées sont :

* **La côte $z$** (*z score* en anglais) qui consiste à soustraire à chaque valeur sa moyenne (soit un centrage), puis à la diviser par son écart-type (soit une réduction) (eq. \@ref(eq:scorez)). Par conséquent, on parle aussi de variable centrée-réduite qui a comme propriétés intéressantes une moyenne égale à 0 et un écart-type égale à 1 (la variance est aussi égale à 1 puisque $1^2=1$). Nous verrons que cette transformation est largement utilisée dans les méthodes de classification (chapitre \@ref(chap09)) et les méthodes factorielles (chapitre \@ref(chap10)).

\begin{equation}\footnotesize  
z= \frac{x_i-\mu}{\sigma}
(\#eq:scorez)
\end{equation}

* **La transformation en rangs** qui consiste simplement à trier une variable en ordre croissant, puis à affecter le rang de chaque observation de 1 à $n$. Cette transformation est très utilisée quand la variable est très anormalement distribuée, notamment pour calculer le coefficient de corrélation de Spearman (section \@ref(sect04133)) et certains tests non-paramétriques (sections \@ref(sect0432) et \@ref(sect0442)). 

* **La transformation sur une échelle de 0 à 1** (ou de 0 à 100) qui consiste à soustraite à chaque observation la valeur minimale et à diviser le tout par l'étendue (eq. \@ref(eq:t01)). 

\begin{equation}\footnotesize  
X_{\in\lbrack0-1\rbrack}= \frac{x_i-max}{max-min} \text{ ou } X_{\in\lbrack0-100\rbrack}= \frac{x_i-min}{max-min}\times100
(\#eq:t01)
\end{equation}


```{r AutresTransformation, echo=FALSE, message=FALSE, warning=FALSE}
a <- c(22,27,25,30,37,32,35,40)
b <- (a-mean(a))/sd(a)
d <- (a-min(a))/(max(a)-min(a))

df <- data.frame(
  id = as.character(c(1:length(a))),
  A = round(a,2),
  B = round(b,2),
  C = rank(a),
  D = round(d,2)
)
df[9,1] <- "Moyenne"
df[10,1] <- "Écart-type"
df[9,2] <- round(mean(a),2)
df[10,2] <- round(sd(a),2)
df[9,3] <- round(mean(b),2)
df[10,3] <- round(sd(b),2)
opts <- options(knitr.kable.NA = "")

show_table(df, 
  col.names = c("Observation","$x_i$","Côte $z$","Rang","0 à 1"),
  caption = "Illustration des trois tranformations",
           )
```

Pour un *dataframe* nommé *df* comprenant une variable *x*, la syntaxe ci-dessous illustre comment obtenir quatre transformations (côte $z$, rangs, 0 à 1 et 0 à 100).

```{r AutresTransformation2, echo=TRUE, message=FALSE, warning=FALSE}
df2 <- data.frame(x = c(22,27,25,30,37,32,35,40))

# Transformation centrée-réduite : côte Z
df2$zx <- (df2$x-mean(df2$x))/sd(df2$x)

# Transformation en rangs avec la fonction rank
df2$rz <- rank(df2$x)

# Transformation en rangs de 0 à 1
df2$x01 <- (df2$x-min(df2$x))/(max(df2$x)-min(df2$x))

# Transformation en rangs de 0 à 100
df2$x0100 <- (df2$x-min(df2$x))/(max(df2$x)-min(df2$x))*100
```

::: {.bloc_aller_loin data-latex=""}
Ces trois transformations sont parfois utilisées pour générer un indice composite à partir de plusieurs variables ou encore dans une analyse de sensibilité avec les indices de Sobol [-@Sobol1993].
:::


### Mise en œuvre dans R {#sect0256}
Il existe une multitude de *packages* dédiés au calcul des statistiques descriptives univariées. Par parcimonie, nous en utiliserons uniquement trois : `DescTools`, `nortest` et `stats`. Libre à vous de faire vos recherches sur Internet pour utiliser d'autres *packages* au besoin. Les principales fonctions que nous utilisons ici sont :

* `summary` : pour obtenir un résumé sommaire des statistiques descriptives (minimum, Q1, Q2 Q3, Maximum)
* `mean` : moyenne
* `min` : minimum
* `max` : maximum
* `range` : minimum et maximum
* `quantile` : quartiles
* `quantile((x, probs = seq(.0, 1, by = .2))` : quintiles
* `quantile((x, probs = seq(.0, 1, by = .1))` : déciles
* `var` : variance
* `sd` : écart-type
* `Skew` du *package* `DescTools` : coefficient d'asymétrie
* `Kurt` du *package* `DescTools` : coefficient d'applatissement
* `ks.test(x, "pnorm", mean=mean(x), sd=sd(x))` du *package* `nortest` : test de Kolmogorov-Smirnov
* `shapiro.test` du *package* `DescTools` : test de Shapiro-Wilk
* `lillie.test` du *package* `DescTools` : du package **nortest** : test de Lilliefors
* `ad.test` du *package* `DescTools` : test d'Anderson-Darling
* `JarqueBeraTest` du *package* `DescTools` : test de Jarque-Bera


#### Application à une seule variable {#sect02561}

Admettons que vous voulez obtenir des statistiques pour une seule variable présente dans un *dataframe* (`dataMTL$PctFRev`) :

```{r StatDesc1, echo=TRUE, message=FALSE, warning=FALSE}

library(DescTools)
library(stats)
library(nortest)

# Importation du fichier csv dans un dataframe
dataMTL <- read.csv("data/univariee/DataSR2016.csv")
# Tableau sommaire pour la variable PctFRev
summary(dataMTL$PctFRev)

# PARAMÈTRES DE TENDANCE CENTRALE
mean(dataMTL$PctFRev)   # Moyenne
median(dataMTL$PctFRev)   # Médiane


# PARAMÈTRES DE POSITION
# Quartiles
quantile(dataMTL$PctFRev)
# Quintiles
quantile(dataMTL$PctFRev, probs = seq(.0, 1, by = .2))
# Déciles
quantile(dataMTL$PctFRev, probs = seq(.0, 1, by = .1))
# Percentiles personnalisés avec apply
quantile(dataMTL$PctFRev, probs = c(0.01,.05,0.10,.25,.50,.75,.90,.95,.99))

# PARAMÈTRES DE DISPERSION
range(dataMTL$PctFRev)  # Min et Max
# Étendue
max(dataMTL$PctFRev)-min(dataMTL$PctFRev)
# Écart interquartile
quantile(dataMTL$PctFRev)[4]-quantile(dataMTL$PctFRev)[2]

var(dataMTL$PctFRev) # Variance
sd(dataMTL$PctFRev)  # Écart-type
sd(dataMTL$PctFRev) / mean(dataMTL$PctFRev) # CV

# PARAMÈTRES DE FORME
Skew(dataMTL$PctFRev)    # Skewness
Kurt(dataMTL$PctFRev)    # Kurtosis

# TESTS D'HYPOTHÈSE SUR LA NORMALITÉ
# K-Smirnov
ks.test(dataMTL$PctFRev, "pnorm", mean=mean(dataMTL$PctFRev), sd=sd(dataMTL$PctFRev))
shapiro.test(dataMTL$PctFRev) 
lillie.test(dataMTL$PctFRev) 
ad.test(dataMTL$PctFRev) 
JarqueBeraTest(dataMTL$PctFRev) 
```

Pour construire un histogramme avec la courbe normale, vous pourez consulter la section \@ref(sect03213) ou la syntaxe ci-dessous.

```{r GcourbeNormale, fig.align='center', auto_pdf = TRUE, fig.cap="Histogramme avec courbe normale",  out.width='65%'}
moyenne <- mean(dataMTL$PctFRev)
ecart_type <- sd(dataMTL$PctFRev)

ggplot(data = dataMTL) +
  geom_histogram(aes(x = PctFRev, y = ..density..),
                 bins = 30, color = "#343a40", fill = "#a8dadc") +
    labs(y = "densité")+
  stat_function(fun = dnorm, args = list(mean = moyenne, sd = ecart_type), 
                color = "#e63946", size = 1.2, linetype = "dashed")
```

#### Application à plusieurs variables {#sect02562}

Pour obtenir des sorties de statistiques descriptives pour plusieurs variables, nous vous conseillons :

* de créer un vecteur avec les noms de variables (*VarsSelect* dans la syntaxe ci-dessous)
* d'utiliser ensuite les fonctions `sapply` et `apply.`

```{r StatDesc2, echo=TRUE, message=FALSE, warning=FALSE}
# Noms des variables du dataframe
names(dataMTL)

# Vecteur pour trois variables
VarsSelect <- c("HabKm2", "TxChomage", "PctFRev" )

# Tableau sommaire pour les 3 variables
summary(dataMTL[VarsSelect])

# PARAMÈTRES DE TENDANCE CENTRALE
sapply(dataMTL[VarsSelect], mean)   # Moyenne
sapply(dataMTL[VarsSelect], median) # Médiane

# PARAMÈTRES DE POSITION
# Quartiles
sapply(dataMTL[VarsSelect], quantile)
# Quintiles
apply(dataMTL[VarsSelect], 2, function(x) quantile(x, probs = seq(.0, 1, by = .2)))
# Déciles
apply(dataMTL[VarsSelect], 2, function(x) quantile(x, probs = seq(.0, 1, by = .1)))
# Percentiles personnalisés avec apply
apply(dataMTL[VarsSelect], 2, 
      function(x) quantile(x, probs = c(0.01,.05,0.10,.25,.50,.75,.90,.95,.99)))

# PARAMÈTRES DE DISPERSION
sapply(dataMTL[VarsSelect], range)  # Min et Max
# Étendue
sapply(dataMTL[VarsSelect], max) - sapply(dataMTL[VarsSelect], min)
# Écart interquartile
sapply(dataMTL[VarsSelect], quantile)[4,] - sapply(dataMTL[VarsSelect], quantile)[2,]

sapply(dataMTL[VarsSelect], var)    # Variance
sapply(dataMTL[VarsSelect], sd)     # Écart-type
# Coefficient de variation
sapply(dataMTL[VarsSelect], sd) / sapply(dataMTL[VarsSelect], mean)

# PARAMÈTRES DE FORME
sapply(dataMTL[VarsSelect], Skew)    # Skewness
sapply(dataMTL[VarsSelect], Kurt)    # Kurtosis

# TESTS D'HYPOTHÈSE POUR LA NORMALITÉ
# K-Smirnov
apply(dataMTL[VarsSelect], 2, function(x) ks.test(x, "pnorm", mean=mean(x), sd=sd(x)))
sapply(dataMTL[VarsSelect], shapiro.test)       # Shapiro-Wilk
sapply(dataMTL[VarsSelect], lillie.test)       # Lilliefors
sapply(dataMTL[VarsSelect], ad.test)           # Anderson-Darling
sapply(dataMTL[VarsSelect], JarqueBeraTest)    # Jarque-Bera
```

#### Transformer une variable dans R {#sect02563}

La syntaxe ci-dessous illustre trois exemples de transformation (logarithme, racine carrée et inverse de la variable). Rappelez-vous qu'il faut compare les valeurs de forme (*skewness* et *kurtosis*) et de forme (tests de Shapiro-Wilk) avant et après les transformations pour identifier celle qui est la plus efficace.

```{r GTranf, fig.align='center', auto_pdf = TRUE, fig.cap="Histogramme des transformations",  out.width='75%'}

library(ggpubr)

# Importation du fichier csv dans un dataframe
dataMTL <- read.csv("data/univariee/DataSR2016.csv")

# Noms des variables du dataframe
names(dataMTL)

# Transformations
dataMTL$HabKm2_log <-  log10(dataMTL$HabKm2)
dataMTL$HabKm2_sqrt <-  sqrt(dataMTL$HabKm2)
dataMTL$HabKm2_inv <-  1/dataMTL$HabKm2

# Vecteur pour la variable et les trois transformations
VarsSelect <- c("HabKm2", "HabKm2_log", "HabKm2_sqrt", "HabKm2_inv")

# paramètres de forme
sapply(dataMTL[VarsSelect], Skew)    # Skewness
sapply(dataMTL[VarsSelect], Kurt)    # Kurtosis

# TESTS D'HYPOTHÈSE SUR LA NORMALITÉ
sapply(dataMTL[VarsSelect], shapiro.test) 

# Histogrammes avec courbe normale
Graph1 <- ggplot(data = dataMTL) +
          geom_histogram(aes(x = HabKm2, y = ..density..),
                 bins = 30, color = "#343a40", fill = "#a8dadc") +
          labs(x="Habitants au km2", y = "densité")+
          stat_function(fun = dnorm, 
                        args = list(mean = mean(dataMTL$HabKm2), 
                                    sd = sd(dataMTL$HabKm2)), 
                        color = "#e63946", size = 1.2)

Graph2 <- ggplot(data = dataMTL) +
          geom_histogram(aes(x = HabKm2_log, y = ..density..),
                         bins = 30, color = "#343a40", fill = "#a8dadc") +
          labs(x="habitants au km2 (logarithme)", y = "densité")+
          stat_function(fun = dnorm, 
                        args = list(mean = mean(dataMTL$HabKm2_log), 
                                    sd = sd(dataMTL$HabKm2_log)), 
                        color = "#e63946", size = 1.2)

Graph3 <- ggplot(data = dataMTL) +
          geom_histogram(aes(x = HabKm2_sqrt, y = ..density..),
                         bins = 30, color = "#343a40", fill = "#a8dadc") +
          labs(x="habitants au km2 (racine carrée)", y = "densité")+
          stat_function(fun = dnorm, 
                        args = list(mean = mean(dataMTL$HabKm2_sqrt), 
                                    sd = sd(dataMTL$HabKm2_sqrt)), 
                        color = "#e63946", size = 1.2)

Graph4 <- ggplot(data = dataMTL) +
          geom_histogram(aes(x = HabKm2_inv, y = ..density..),
                         bins = 30, color = "#343a40", fill = "#a8dadc") +
          labs(x="habitants au km2 (inverse)", y = "densité")+
          stat_function(fun = dnorm, 
                        args = list(mean = mean(dataMTL$HabKm2_inv), sd = sd(dataMTL$HabKm2_inv)), 
                        color = "#e63946", size = 1.2)

ggarrange(plotlist = list(Graph1, Graph2, Graph3, Graph4), ncol = 2, nrow=2)
```

La variable *HabKm2* est asymétrique positive et leptokurtique. Tant les valeurs des statistiques de forme, du test de Shapiro-Wilk que les histogrammes semblent démontrer que la transformation la plus efficace est la racine carrée. Si la variable originale est asymétrique positive, sa transformation logarithme est par contre asymétrique négative. Cela démontre que la transformation logarithmique n'est pas toujours la panacée.

## Statistiques descriptives sur des variables qualitatives et semi-qualitatives {#sect026}

### Les fréquences {#sect0261}

En guise de rappel, les variables nominales, ordinales et semi-quantitatives comprennent plusieurs modalités pour lesquelles plusieurs types de fréquences sont généralement calculées. Pour illustrer le tout, nous avons extrait du recensement de 2016 de Statistique Canada les effectifs des modalités de la variable sur le principal mode de transport utilisé pour les déplacements domicile-travail, et ce, pour la subdivision de recensement (MRC) de l'île de Montréal (tableau \@ref(tab:Frequences)). Les différents types de fréquences sont les suivantes :

* les fréquences absolues simples (**FAS**) ou fréquences observées représentent le nombre d'observations pour chacune des modalités. Par exemple, sur 857 540 navetteurs domicile-travail (ligne totale), seulement 30 645 optent pour le vélo, alors que 427 530 conduisent un véhicule motorisé (automobile, camion ou fourgonnette) comme principal mode de transport.

* les fréquences relatives simples (**FRS**) sont les proportions de chaque modalité sur le total ($30645/857540=0,036$); leur somme est égale à 1. Elles peuvent bien entendu être exprimées en pourcentage ($30645/857540 \times 100=3,57$); leur somme est alors égale à 100%. Par exemple, 3,7% des navetteurs utilisent le vélo comme mode de transport principal.

* les fréquences absolues cumulées (**FAC**) représentent la fréquence observée (FAS) de la modalité auxquelles sont additionnées celles qui la précèdent. La valeur de la FAC pour la dernière est donc égale au total.

* À partir des fréquences absolues cumulées (FAC), il est alors possible de calculer les fréquences relatives cumulées (**FRC**) en proportion ($453930 / 857540 = 0,529$) et en pourcentage ($453930 / 857540 \times 100= 52,93$). Par exemple, plus de la moitié des navetteurs utilisent l'automobile comme mode de transport principal (passager ou conducteur).

```{r Frequences, echo=FALSE, message=FALSE, warning=FALSE}
M <- c("Véhicule motorisé (conducteur)",
       "Véhicule motorisé (passager)",
       "Transport en commun",
       "À pied",
       "Bicyclette",
       "Autre moyen")
Freq <- c(427530, 26400,295860,69410,30645,7695)
df <- data.frame(
  Mode = M,
  FAS = Freq
)
# Somme des fréquences aboslues simples
sumFAS <-  sum(df$FA)
# Fréquences relatives simples
df$FRS <- round(df$FAS / sum(df$FAS),3)
sumFRS <-  sum(df$FAS / sum(df$FAS))
# Fréquences relatives simples en pourcentages
df$FRSpct <- round(df$FAS / sum(df$FAS)*100,2)
sumFRSpct <-  sum(df$FAS / sum(df$FAS)*100)
# Fréquences absolues cumulées
df$FAC <- cumsum(df$FAS)
# Fréquences relatives cumulées
df$FRC <- round(cumsum(df$FAS)/sum(df$FAS),3)
# Fréquences relatives cumulées en pourcentages
df$FRCpct <- round(cumsum(df$FAS)/sum(df$FAS)*100,2)
n <- nrow(df)
df[n+1,1] <- "Total"
df[n+1,2] <-  sumFAS
df[n+1,3] <-  sumFRS
df[n+1,4] <-  sumFRSpct
opts <- options(knitr.kable.NA = "")

show_table(df, 
           col.names = c("Mode de transport","FAS","FRS","FRS (%)","FAC", "FRC", "FRC (%)"), 
           caption = "Les différents types de fréquences sur une variable qualitative ou semi-qualitative"
           )
```


::: {.bloc_attention data-latex=""}
**Les fréquences cumulées : peu pertinentes pour les variables nomimales**

Le calcul et l'analyse des fréquences cumulées (absolues et relatives) sont très souvent inutiles pour les variables nominales.

Par exemple, au tableau \@ref(tab:Frequences), la fréquence cumulée relative (en %) est de 87,43% pour la troisième ligne. Cela signifie que 87,43% des navetteurs se déplacent en véhicule motorisé (conducteur ou passager) ou en transport en commun. Par contre, si la troisième modalité avait été *à pied*, le pourcentage aurait été de 61,02 ($52,93+8,09$). Si vous souhaitez calculer les fréquences cumulées sur une variable nominale, assurez-vous que l'ordre des modalités vous convient et de le modifier au besoin. Sinon, abstenez-vous de les calculer!


**Les fréquences cumulées : très utiles pour l'analyse pour des variables ordinales ou semi-quantitatives**

Pour des modalités hiérarchisées (variable ordinale ou semi-quantitative), l'analyse des fréquences cumulées (absolues et relatives) est par contre très intéressante. Par exemple, au tableau \@ref(tab:Frequences2), elle permet de constater rapidement que sur l'île de Montréal, un peu moins du très de la population à moins de 25 ans (35,95%) et 83,33% moins de 65 ans.
:::

```{r Frequences2, echo=FALSE, message=FALSE, warning=FALSE}
M <- c("0 à 14 ans",
       "15 à 24 ans",
       "25 à 44 ans",
       "45 à 64 ans",
       "65 à 84 ans",
       "85 ans et plus")
Freq <- c(304470,237555,582150,494205,271560,52100)
df <- data.frame(
  Mode = M,
  FAS = Freq
)
# Somme des fréquences aboslues simples
sumFAS <-  sum(df$FA)
# Fréquences relatives simples
df$FRS <- round(df$FAS / sum(df$FAS),3)
sumFRS <-  sum(df$FAS / sum(df$FAS))
# Fréquences relatives simples en pourcentages
df$FRSpct <- round(df$FAS / sum(df$FAS)*100,2)
sumFRSpct <-  sum(df$FAS / sum(df$FAS)*100)
# Fréquences absolues cumulées
df$FAC <- cumsum(df$FAS)
# Fréquences relatives cumulées
df$FRC <- round(cumsum(df$FAS)/sum(df$FAS),3)
# Fréquences relatives cumulées en pourcentages
df$FRCpct <- round(cumsum(df$FAS)/sum(df$FAS)*100,2)
n <- nrow(df)
df[n+1,1] <- "Total"
df[n+1,2] <-  sumFAS
df[n+1,3] <-  sumFRS
df[n+1,4] <-  sumFRSpct
opts <- options(knitr.kable.NA = "")
show_table(df, 
           col.names = c("Groupes d'âge","FAS","FRS","FRS (%)","FAC", "FRC", "FRC (%)"),
           caption = "Les différents types de fréquences sur une variable semi-qualitative"
           )
# knitr::kable(df, 
#   format.args = list(decimal.mark = ",", big.mark = " "),           
#   col.names = c("Groupes d'âge","FAS","FRS","FRS (%)","FAC", "FRC", "FRC (%)"),
#   caption = "Les différents types de fréquences sur une variable semi-qualitative",
#   booktabs = TRUE, valign = 't', row.names = FALSE) %>%
#   kableExtra::kable_styling(font_size = font_size_table)
```

Différents graphiques peuvent être construits pour illustrer la répartition des observations : les graphiques en barres (verticales et horizontales) avec les fréquences absolues,  les diagrammes circulaires ou en anneau pour les fréquences relatives (figure \@ref(fig:GraphiquesFreq1)). Ces graphiques seront présentés plus en détails dans le chapitre suivant.

```{r GraphiquesFreq1, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Différents graphiques pour représenter les fréquences absolues et relatives", out.width='75%'}
library(dplyr)
Code <- c("A","B","C","D","E","F")
M <- c("0 à 14 ans",
       "15 à 24 ans",
       "25 à 44 ans",
       "45 à 64 ans",
       "65 à 84 ans",
       "85 ans et plus")
Freq <- c(304470,237555,582150,494205,271560,52100)
df <- data.frame(
  Groupe = M,
  FAS = Freq
)
df <- data.frame(
  Code = Code,
  Groupe = M,
  FAS = Freq
)
df$FRSpct <- round(df$FAS / sum(df$FAS),3)*100
mycols <- c("#0c2c84", "#67000d",  "#99000d", "#ef3b2c","#006d2c", "#41ab5d")
# BARRRES
options(scipen = 999)
G1 <- ggplot(data = df)+
  geom_bar(aes(x = Code, weight = FAS, fill=Groupe))+
  scale_fill_manual(values = mycols) +
  labs(x = "Groupe d'âge",
       y = 'Navetteurs')
# BARRRES
G2 <- ggplot(data = df)+
  geom_bar(aes(y = Code, weight = FAS, fill=Groupe))+
  scale_fill_manual(values = mycols) +
  labs(x = "Groupe d'âge",
       y = 'Navetteurs')
# DIAGRAMME CIRCULAIRE
# Ajouter la position de l'étiquette
df <- df %>%
  arrange(desc(Groupe)) %>%
  mutate(ypos = cumsum(FRSpct) - 0.5*FRSpct)
G3 <- ggplot(df, aes(x="", y=FRSpct, fill=Groupe)) +
  geom_bar(width = 1, stat = "identity", color = "white") +
  coord_polar("y", start=0) +
  theme_void() +
  geom_text(aes(y = ypos, label = FRSpct), color = "white", size=3) +
  scale_fill_manual(values = mycols)
# ANNEAU
df$ymax <- cumsum(df$FRSpct)
df$ymin <-  c(0, head(df$ymax, n=-1))
G4 <- ggplot(df, aes(ymax=ymax, ymin=ymin,
                        xmax=4, xmin=3,
                        y=FRSpct, fill=Groupe)) +
  geom_rect(stat="identity", color="white") +
  coord_polar("y", start=0) +
  theme_void() +
  geom_text(aes(x = 3.5,y = ypos, label = FRSpct), color = "white", size=3) +
  scale_fill_manual(values = mycols) +
  xlim(c(2,4))
list_plot <- list(G1, G2, G3, G4)
ggarrange(plotlist = list_plot, ncol = 2, nrow=2,
                      common.legend = TRUE, legend = "bottom")
options(scipen = 0)
```

### Mise en œuvre dans R {#sect0262}

La syntaxe ci-dessous permet de calculer les différentes fréquences présentées au tableau \@ref(tab:Frequences2). Notez que pour les fréquences cumulées, nous utilisons la fonction `cumsum`.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Vecteur pour les noms des modalités
Modalite <- c("0 à 14 ans",
             "15 à 24 ans",
             "25 à 44 ans",
             "45 à 64 ans",
             "65 à 84 ans",
             "85 ans et plus")
# Vecteur pour les fréquences absolues simples (FAS)
Navetteurs <- c(304470,237555,582150,494205,271560,52100)
# Somme des FAS
sumFAS <-  sum(Navetteurs)
# Construction du dataframe avec les deux vecteurs
df <- data.frame(
  GroupeAge = Modalite, 
  FAS = Navetteurs,
  FRS = Navetteurs / sumFAS, 
  FRSpct = Navetteurs / sumFAS * 100,
  FAC = cumsum(Navetteurs),
  FRC = cumsum(Navetteurs) / sumFAS,
  FRCpct = cumsum(Navetteurs) / sumFAS * 100
  )
df
```

## Pour aller un peu plus loin : les statistiques descriptives pondérées {#sect027}

Dans la section \@ref(sect025), les différentes statistiques descriptives sur des variables quantitatives – paramètres de tendance centrale, de position, de dispersion et de forme – ont été largement abordées. Il est possible de calculer ces différentes statistiques en tenant compte d’une pondération. La statistique descriptive pondérée la plus connue est certainement la moyenne arithmétique pondérée. Son calcul est très simple; pour chaque observation, deux valeurs sont disponibles : 

* $x_i$, soit la valeur de la variable $X$ pour l'observation $i$
* $w_i$, soit la valeur de la pondération pour $i$.

Prenez soin de comparer les deux équations ci-dessous (à gauche, la moyenne arithmétique; à droite, la moyenne arithmétique pondérée). Vous constaterez rapidement qu'il suffit simplement de multiplier chaque observation par sa pondération (numérateur) et de diviser ce produit par la somme des pondérations (dénominateur; et non par $n$, soit le nombre d’observations comme pour la moyenne arithmétique non pondérée). 

\begin{equation}\footnotesize  
\bar{x}=\frac{\sum_{i=1}^n x_i}{n} \text { versus } \bar{m}=\frac{\sum_{i=1}^n  w_ix_i}{\sum_{i=1}^nw_i}
(\#eq:moypond)
\end{equation} 


```{r MoyPondCalcul, echo=FALSE, message=FALSE, warning=FALSE}
x <- c(200,225,275,300)
w <- c(20,80,50,200)

df <- data.frame(
  id = as.character(c(1:length(x))),
  x = round(x,0),
  w = round(w,0),
  wx = x*w
)
df[5,1] <- "Somme"
df[6,1] <- "Moyenne"
df[7,1] <- "Moyenne pondérée"

df[5,2] <- sum(x)
df[5,3] <- sum(w)
df[5,4] <- sum(x*w)
df[6,2] <- round(mean(x),0)
df[7,4] <- round(sum(x*w)/sum(w),0)

opts <- options(knitr.kable.NA = "")
show_table(df,
           col.names = c("Observation","$x_i$","$w_i$","$x_i \\times w_i$"),
           caption = "Calcul de la moyenne pondérée"
           )
```

::: {.bloc_notes data-latex=""}
**Calcul d’autres statistiques descriptives pondérées**

Nous n’allons pas reporter ici les formules des versions pondérées de toutes les statistiques descriptives. Retenez toutefois le principe suivant permettant de les calculer à partir de l’exemple du tableau \@ref(tab:MoyPondCalcul). Pour la variable *X*, dupliquons respectivement 20, 80, 50, 200 fois les observations 1 à 4. Si nous calculons la moyenne arithmétique sur ces valeurs dupliquées, alors cette valeur sera identique à la celle de la moyenne arithmétique pondérée. Le même principe reposant sur la duplication des valeurs s'applique à l'ensemble des statistiques descriptives.
:::

Dans un article récent, Alvarenga et al. [-@de2018accessibilite] évaluent l'accessibilité aux aires de jeux dans les parcs de la Communauté métropolitaine de Montréal (CMM). Pour les 881 secteurs de recensement de la CMM, ils ont calculé la distance à l'aire de jeux la plus proche à travers le réseau de rues. Ce résultat, cartographié à la figure \@ref(fig:FigParcCMM), permet d'avancer le constat suivant : « la quasi-totalité des secteurs de recensement de l’agglomération de Montréal présente des distances de l’aire de jeux la plus proche inférieures à 500 m, alors que les secteurs situés à plus d’un kilomètre d’une aire de jeux sont très majoritairement localisés dans les couronnes
nord et sud de la CMM » [@de2018accessibilite, p. 238].

Pour chaque secteur de recensement, Alvarenga et al. [-@de2018accessibilite] disposent des données suivantes  :

* $x_i$, soit la distance à l'aire de jeux la plus proche pour le secteur de recensement *i* et
* $w_i$, la pondération, soit le nombre d'enfants de moins de dix ans.

Il est alors possible de calculer les statistiques descriptives de la proximité à l'aire de jeux la plus proche en tenant compte du nombre d'enfants résidant dans chaque secteur de recensement (tableau \@ref(tab:MoyPondParc)). Cet exercice permet de conclure que : « [...] globalement, les enfants ont une bonne accessibilité aux aires de jeux sur le territoire de la CMM. [...] Les enfants sont en moyenne à un peu plus de 500 m de l’aire de jeux la plus proche (moyenne = 559 ; médiane = 512). Toutefois, les valeurs percentiles extrêmes signalent que respectivement 10% et 5% des enfants résident à près de 800 m et à plus de 1000 m de l’aire de jeux la plus proche » [-@de2018accessibilite, p. 236].

```{r FigParcCMM, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Accessibilité aux aires de jeux par secteur de recensement, Communauté métropolitaine de Montréal, 2016",  out.width='85%'}
knitr::include_graphics('images/bivariee/BivarieeFigureParc.jpg', dpi = NA)
```

```{r MoyPondParc, echo=FALSE, message=FALSE, warning=FALSE}
parc <- c(881,559,282,327,408,512,640,799,1006)
stat <- c("N","Moyenne","P5","P10","Q1","Médiane","Q3","P90","P95")

df <- data.frame(
  "Parc" = parc
)
df <- data.frame(t(df))

opts <- options(knitr.kable.NA = "")
show_table(df, 
           caption = "Statistiques de l'aire de jeux la plus proche par secteur de recensement pondérées par la population de moins de 10 ans",
           col.names = stat
           )
```

De nombreux *packages* sont disponibles pour calculer des statistiques pondérées, dont notamment `Weighted.Desc.Stat` et `Hmisc` utilisés dans la syntaxe ci-dessous.

```{r CalculStatPond2, echo=TRUE, message=FALSE, warning=FALSE}
library(foreign)
library(Hmisc)
library(Weighted.Desc.Stat)

df <- read.dbf("data/bivariee/SR_AireJeux_PopMoins10.dbf")

head(df, n = 5)

# xi (variable) et wi (pondération)
x <- df$AireJeux
w <- df$PopMoins10

# Calcul des paramètres de position
# Moyenne
Hmisc::wtd.mean(x, w)
Weighted.Desc.Stat::w.mean(x, w)
# Quartiles et percentile
Hmisc::wtd.quantile(x, weights=w, probs=c(.05, .10, .25, .50, .75, .90, .95))

# Paramètres de dispersion avec le package Weighted.Desc.Stat
# Variance, écart-type et coefficient de variation
w.var(x,w)
w.sd(x,w)
w.cv(x,w)

# Paramètres de forme avec le package Weighted.Desc.Stat
# Skewness et kurtosis 
w.skewness(x, w)
w.kurtosis(x, w)
```


<!--chapter:end:02-univarie.Rmd-->

# La magie des graphiques {#chap03}

Dans ce chapitre, nous découvrirons les incroyables capacités graphiques de R. Pour ce faire, nous couvrirons en profondeur les capacités du *package* **ggplot2** du **tidyverse**. Il s'agit de loin du meilleur *package* pour réaliser des graphiques selon nous.

::: {.bloc_package data-latex=""}
Dans cette section, nous utiliserons principalement les packages suivants : 

* Pour créer des graphiques :
  - **ggplot2**, le seul, l'unique
  - **ggpubr** pour combiner des graphiques
  - **ggthemes** thèmes complémentaires pour les graphiques  
  
* Pour les couleurs : 
  - **rcolorbrewer** pour avoir accès des palettes de couleurs

* Pour les graphiques spéciaux : 
  - **chorddiag** pour construire des graphiques d'accord
  - **fmsb** pour construire des graphiques ren radar
  - **treemap** pour construire un graphique *treemap*
  - **wordcloud2** pour construire un nuage de mots
fmsb
* Pour les cartes : 
  - **classInt** pour calculer les intervalles des classes
  - **ggsn** pour afficher une échelle
  - **tmap** pour la cartographie


* Autres *packages* : 
  - **dplyr** et **reshape2** pour manipuler des données
  - **pdftools** pour extraire les textes des fichiers pdf
  - **udpipe** pour obtenir des dictionnaires linguistiques
  - **sf** pour manipuler des *simple feature collection*
:::

::: {.bloc_notes data-latex=""}
**Qu'est-ce que la visualisation de données ?**

La représentation visuelle de données consiste à transposer des informations en une représentation graphique facilitant la lecture de ces dernières. Il s'agit autant d'un ensemble de méthodes, d'un art que d'un moyen de communication. Voici deux exemples marquants avant de détailler ce propos.

Cette première illustration permet de visualiser le volume de plastique que représente la consommation d'eau en bouteille : 480 milliards de bouteilles vendues en 10 ans ! Ce chiffre astronomique est inimaginable. En revanche, une [montagne de plastique](https://graphics.reuters.com/ENVIRONMENT-PLASTIC/0100B275155/index.html){target='_blank'} de 2400 mètres surplombant Manhattan marque davantage les esprits.

```{r fig3A, echo=FALSE, fig.align='center', fig.cap="Noyer dans le plastique selon Reuters Graphics", fig.pos="H", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='90%'}
library(dplyr)
knitr::include_graphics('images/magie_graphiques/Drowning-in-Plastic.jpg', dpi = NA)
```

Ce [second graphique](https://www.maplecroft.com/insights/analysis/84-of-worlds-fastest-growing-cities-face-extreme-climate-change-risks/){target='_blank'} représente quatre informations pour 234 villes à travers le monde : 

* la croissance démographique (axe des abcisses)
* la vulnérabilité au changement climatique (axes de ordonnées)
* la taille des villes (taille des cercles)
* le continent sur lequel est localisé chaque ville (couleur des cercles).

```{r fig3B, echo=FALSE, fig.align='center', fig.cap="Changement climatique et vulnérabilité par Verisk Maplecroft", auto_pdf=TRUE, out.width='70%', fig.pos = "H"}
knitr::include_graphics('images/magie_graphiques/climate_change_exposure.png', dpi = NA)
```

Le graphique est à la fois très accrocheur et esthétique. En un coup d'œil, on constate que les villes avec une forte croissance démographique sont aussi les plus vénérables (lecture des deux axes) et qu'elles sont surtout localisées en Afrique et secondairement en Asie (en rouge et orange), quelle que soit leur taille (taille du cercle). À l'inverse, les villes européennes et américaines (en bleu) sont beaucoup moins vulnérables aux changements climatiques et avec une croissance démographique plus faible, qu'elles soient des petites, moyennes ou grandes villes.

Souvent négligée, la visualisation de données est perçue comme un tâche triviale : il s'agit simplement de représenter une donnée sous forme d'un graphique car c'est l'option la plus pratique ou prenant le moins de place. Pourtant, les avantages de la visualisation des données sont limites. Par exemple, la visualisation de données intègre aujourd'hui des supports dynamiques comme des animations, des figures interactives ou des applications webs. R offre d'ailleurs des possibilités très intéressantes en la matière avec des *packages* comme **shiny**, **plotly**, ou **leaflet**. Toutefois, nous ne couvrirons pas ici ces méthodes plus récentes en visualisation des données qui devraient faire l'objet d'un autre livre en tant quel tel.

**Les principaux avantages de la visualisation des données** : 

* **Analyse exploratoire des données** (*exploratory data analysis - EDA* en anglais). Visualiser des données est crucial pour détecter des problèmes en tout genre (données manquantes, valeurs extrêmes ou aberrantes, non respect de conditions d'application de tests statistiques, etc.), mais aussi pour repérer de nouvelles associations entre les variables.
* **Communication de vos résultats**. La raison d'être d'un graphique est de délivrer un message clair relatif à un résultat obtenu suite à une analyse rigoureuse de vos données. Si votre graphique n'apporte aucune information claire, il vaut mieux ne pas le présenter, le diffuser. Les représentations ne sont pas neutres. Les couleurs et les formes ont des significations particulières en fonction de la culture et du contexte. Posez-vous donc toujours la question : à quel public est destiné le message ? Évitez de surcharger vos visualisation de données, sinon l'essence du message sera perdu.
* **Aide à la décision**. Une illustration (graphique ou carte) peut être un outil facilitant la prise de décisions. 
:::

## Philosophie du ggplot2 {#sect031}

**ggplot2** fait partie du **tidyverse** et dispose donc d'une logique de fonctionnement particulière. Cette dernière se nomme *The Grammar of Graphics* (les deux G sont d'ailleurs à l'origine du nom **ggplot2**) et a été proposée par Hadley Wickham (le créateur du **tidyverse** !) dans un article intitulé *A layered grammar of graphics* [@wickham2010layered]. Nous proposons de synthétiser ici les concepts et principes centraux qui sous-tendent la production de graphiques avec **ggplot2**.

### Une grammaire {#sect0311}

Hadley Wickham propose une grammaire pour unifier la création de graphiques. L'idée est donc de dépasser les simples dénominations comme un nuage de points, un diagramme en boite, un graphique en ligne, etc, pour comprendre ce qui relie tous ces graphiques. Ces éléments communs et centraux sont les géométries, les échelles et systèmes de coordonnées, et les annotations (figure \@ref(fig:fig30)) : 

```{r fig30, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Trois composantes d'un graphique, adapté de @wickham2010layered",  out.width='70%'}
knitr::include_graphics('images/magie_graphiques/composantes.png', dpi = NA)
```

* Les **géométries** sont les formes utilisées pour représenter les données. Il peut s'agire de points, lignes, cercles, rectangles, arcs de cercles, etc. 
* Les **échelles et systèmes de coordonnées** permettent de contrôler la localisation des éléments dans un graphique en convertissant les données depuis leur échelle originale (dollars, kilomètres, pourcentages, etc.) vers l'échelle du graphique (pixels).
* Les **annotations** recoupent l'ensemble des informations complémentaires ajoutées au graphique comme son titre et sous-titre, la source des données, la mention sur les droits d'auteurs, etc.

En plus de ces trois éléments, il est bien sûr nécessaire de disposer de **données**. Ces dernières sont assignées à des dimensions du graphique pour être représentées (notamment les axes *X* et *Y* et la couleur). Cette étape est appelée **aesthetics mapping** dans **ggplot2**.

Lorsque l'on combine des données, leur assignation a des dimensions, un type de géométries, des échelles et un système de coordonnées, on obtient un **calque** (ou *layer*). Un graphique peut comprendre plusieurs calques comme nous le verrons dans les prochaines sections.

Prenons un premier exemple très simple et construisons un nuage de points à partir du jeu de données *iris* fourni de base dans R. Nous allons représenter la relation qui existe entre la longueur et la largeur des sépals de ces fleurs. Pour commencer, nous devons charger le package **ggplot2** et instancier un graphique avec la fonction `ggplot`.

```{r fig31, fig.align='center', auto_pdf = TRUE, fig.cap="Base d'un graphique",  out.width='65%'}
library(ggplot2)
data(iris)
names(iris)

ggplot()
```

Pour le moment, le graphique est vide. La seconde étape consiste à lui ajouter des données (au travers du paramètre `data`) et à définir les dimensions à associer aux données (avec le paramètre `mapping` et la fonction `aes()`). Dans notre cas, nous voulons utiliser les coodonnées *X* pour représenter la largeur des sépals, et les coordonnées *Y* pour représenter la longueur des sépals. Enfin, nous souhaitons représenter les observations par des points, nous utiliserons donc la géométrie `geom_point`.

```{r fig32, fig.align='center', auto_pdf = TRUE, fig.cap="Ajout des dimensions au graphique",  out.width='65%'}
ggplot(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  geom_point()
```

Ce graphique ne comprend qu'un seul calque avec une géométrie de type point. Chaque calque est ajouté avec l'opérateur `+` qui permet de superposer des calques, le dernier apparaissant au dessus des autres. Les arguments `mapping` et `data` sont définis ici dans la fonction `ggplot` et sont donc appliqués à tous les calques qui composeraient le graphique. Il est aussi possible de définir `mapping` et `data` au sein des fonctions des géométries : 

```{r fig33, fig.align='center', auto_pdf = TRUE, fig.cap="Autre spécification des arguments mapping et data",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris)
```

La troisième étape consiste à ajouter au graphique ses annotations. Pour notre cas, il faudrait ajouter un titre, un sous-titre, et des intitulés plus clairs pour les axes *X* et *Y*, ce qu'il est possible de faire avec la fonction `labs`.

```{r fig34, fig.align='center', auto_pdf = TRUE, fig.cap="Ajouter les annotations",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals")
```

### Les types de géométries {#sect0312}

**ggplot2** permet d'utiliser un très grand nombre de géométries différentes. Dans le tableau  \@ref(tab:tablegeometries), nous avons reporté les principales géométries disponibles afin que vous puissiez vous faire une idée du bestiaire existant. Il ne s'agit que d'un extrait des principales fonctions. Sachez qu'il existe aussi des *packages* proposant des géométries supplémentaires pour compléter **ggplot2**.

```{r tablegeometries, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(
    noms = c("point", "ligne", "chemin", "boite à moustache", "diagramme violon", "histogramme", "barre", "densité", "texte", "barre d'erreur", "surface"),
    fonctions = c("`geom_point`", "`geom_line`", "`geom_path`", "`geom_boxplot`", "`geom_violin`", "`geom_histogram`", "`geom_bar`", "`geom_density`", "`geom_label`", "`geom_errorbar`", "`geom_ribbon`")
  )

show_table(df,
           col.names = c("Géométrie","Fonction"),
           caption = 'Principales géométries proposée par **ggplot2**',
           )
```


### L'habillage {#sect0313}

Nous avons montré dans le premier exemple comment rajouter le titre, le sous-titre et le nom des axes sur un graphique. Il est également possible de rajouter un texte sous le graphique (généralement la source des données avec l'argument `caption`) et des annotations textuelles (`annotate`). Pour ces dernières, il convient de spécifier leur localisation (cordonnées `x` et `y`) et le texte à intégrer (`label`); elles sont ensuite ajoutées au graphique avec l'opérateur `+`. Ajoutons deux annotations pour identifier deux fleurs spécifiques.

```{r fig35, fig.align='center', auto_pdf = TRUE, fig.cap="Ajouter des annotations textuelles",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") +
  annotate("text", x = 6.7, y = 2.5, # position de la note
           label = "une virginica",  # texte de la note
           hjust = "left", vjust = "top", # ajustement
           size = 3, fontface = "italic") + 
  annotate("text", x = 5.7, y = 4.4, # position de la note
           label = "une setosa",  # texte de la note
           hjust = "left", vjust = "top", # ajustement
           size = 3, fontface = "italic") 
```

Comme vous pouvez le constater, de nombreux paramètres permettent de contrôler le style des annotations. Pour avoir la liste des arguments disponibles, n'hésitez pas à afficher l'aide de la fonction : `help(annotate)`.

En plus des annotations de type texte, il est possible d'ajouter des annotations de type géométrique. Nous pourrions ainsi délimiter une boite encadrant les fleurs de l'espère setosa.

```{r fig36, fig.align='center', auto_pdf = TRUE, fig.cap="Ajouter des annotations géométriques",  out.width='65%'}

setosas <- subset(iris, iris$Species == "setosa")
sepal.length_extent <- c(min(setosas$Sepal.Length),max(setosas$Sepal.Length))
sepal.width_extent <- c(min(setosas$Sepal.Width),max(setosas$Sepal.Width))

ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") +
  annotate("text", x = 6.7, y = 2.5, # position de la note
           label = "une virginica",  # texte de la note
           hjust = "left", vjust = "top", # ajustement
           size = 3, fontface = "italic") + 
  annotate("text", x = 5.7, y = 4.4, # position de la note
           label = "une setosa",  # texte de la note
           hjust = "left", vjust = "top", # ajustement
           size = 3, fontface = "italic") +
  annotate("rect", 
           ymin = sepal.width_extent[[1]],
           ymax = sepal.width_extent[[2]],
           xmin = sepal.length_extent[[1]],
           xmax = sepal.length_extent[[2]],
           fill = rgb(0,0,0,0), # remplissage transparent
           color = "green") # contour de couleur verte
```

Vous noterez que comme le dernier calque ajouté au graphique est le rectangle, il recouvre tous les calques existant, y compris les précédentes annotations. Pour corriger cela, il suffit de changer l'ordre des calques.

```{r fig37, fig.align='center', auto_pdf = TRUE, fig.cap="Gérer l'ordre des annotations",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") +
  annotate("rect", 
           ymin = sepal.width_extent[[1]],
           ymax = sepal.width_extent[[2]],
           xmin = sepal.length_extent[[1]],
           xmax = sepal.length_extent[[2]],
           fill = rgb(0,0,0,0),  # remplissage transparent
           color = "green") + # contour de couleur verte
  annotate("text", x = 6.7, y = 2.5, # position de la note
           label = "une virginica",  # texte de la note
           hjust = "left", vjust = "top", # ajustement
           size = 3, fontface = "italic") + 
  annotate("text", x = 5.7, y = 4.4, # position de la note
           label = "une setosa",  # texte de la note
           hjust = "left", vjust = "top", # ajustement
           size = 3, fontface = "italic")
```

### Utiliser des thèmes

De nombreux autres éléments peuvent être modifiés dans un graphique comme les paramètres des polices, l'arrière-plan, la grille de repères, etc. Il peut être fastidieux de paramétrer tous ces éléments. Une alternative intéressante est d'utiliser des thèmes déjà préconstruits. **ggplot2** propose une dizaine de thèmes, constatons leur impact sur le graphique précédent.

* Le thème classique
```{r fig38, fig.align='center', auto_pdf = TRUE, fig.cap="Thème classique",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") + 
  theme_classic()
```

* Le thème gris
```{r fig39, fig.align='center', auto_pdf = TRUE, fig.cap="Thème gris",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") + 
  theme_gray()
```

* Le thème noir et blanc
```{r fig310, fig.align='center', auto_pdf = TRUE, fig.cap="Thème noir et blanc",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") + 
  theme_bw()
```

* Le thème minimal
```{r fig311, fig.align='center', auto_pdf = TRUE, fig.cap="Thème minimal",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") + 
  theme_minimal()
```

Il est aussi possible d'utiliser le *package* **ggthemes** qui apporte des thèmes complémentaires intéressants dont : 

* Le thème tufte (à l'ancienne...)
```{r fig312, fig.align='center', auto_pdf = TRUE, fig.cap="Thème tufte",  out.width='65%'}
library(ggthemes)

ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") + 
  theme_tufte()
```

* Le thème economist (inspiré de la revue du même nom)
```{r fig313, fig.align='center', auto_pdf = TRUE, fig.cap="Thème economist",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") + 
  theme_economist()
```

* Le thème solarized (plus original)
```{r fig314, fig.align='center', auto_pdf = TRUE, fig.cap="Thème solarized",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") + 
  theme_solarized()
```

Il en existe bien d'autres et vous pouvez composer vos propres thèmes. N'hésitez pas à explorer la documentation de **ggplot2** et **ggthemes** pour en apprendre plus !

### Composer une figure avec plusieurs graphiques {#sect0314}

Il est très fréquent de vouloir combiner plusieurs graphiques dans une même figure. Deux cas se distinguent : 

1. Les données pour les différents graphiques proviennent du même *DataFrame* et peuvent être distinguées selon une variables catégorielle. L'objectif est alors de dupliquer le même graphique, mais pour des sous-groupes de données. Dans ce cas, nous recommandons d'utiliser la fonction `facet_wrap` de **ggplot2**.

2. Les graphiques sont complètement indépendants. Dans ce cas, nous recommandons d'utiliser la fonction `ggarrange` du package **ggpubr**.

#### **ggplot2** et ses facettes {#sect03141}

Nous pourrions souhaiter réaliser une figure composite avec le jeu de données iris et séparer notre nuage de points en trois graphiques distincts selon les espèces des iris. Pour cela, il faut au préalable convertir la variable espèce en facteur.

```{r fig315, fig.align='center', auto_pdf = TRUE, fig.cap="Graphique à facettes",  out.width='65%'}
iris$Species_fac <- as.factor(iris$Species)

ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") +
  facet_wrap(vars(Species_fac), ncol=2)

```
Notez que le nom de la variable (ici `Species_fac`) doit être spécifié au sein d'une sous-fonction `vars` : `vars(Species_fac)`. Nous aurions aussi pu réaliser le graphique sur une seule ligne en spécifiant `ncol = 3`.

```{r fig316, fig.align='center', auto_pdf = TRUE, fig.cap="Graphique à facette en une ligne",  out.width='65%'}
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris) +
  labs(title = "Morphologie des sépals des iris", subtitle = "n = 150",
       x = "Longueur des sépals", 
       y = "Largeur des sépals",
       caption = "source : iris dataset") +
  facet_wrap(vars(Species_fac), ncol=3)
```

#### Arranger des graphiques {#sect03142}

La solution avec les facettes est très pratique, mais également très limitée puisqu'elle ne permet pas de créer une figure avec des graphiques combinant plusieurs types de géométries. `ggarrange` du *package* **ggpubr** permet tout simplement de combiner des graphiques déjà existant. Créons trois nuages de points comparant plusieurs variables en fonction de l'espèce des iris, puis combinons les. Nous allons également attribuer aux points une couleur en fonction de l'espèce des fleurs afin de mieux les distinguer en associant la variable `Species` au paramètre `color`.

```{r fig317, fig.align='center', auto_pdf = TRUE, fig.cap="Figure avec plusieurs graphiques avec ggarrange",  out.width='65%'}
library(ggpubr)

plot1 <- ggplot(data = iris) +
  geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  labs(subtitle = "Caractéristiques des sépals",
       x = "Longueur", 
       y = "Largeur")

plot2 <- ggplot(data = iris) +
  geom_point(aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  labs(subtitle = "Caractéristiques des pétals",
       x = "Longueur", 
       y = "Largeur")

liste_plots <- list(plot1, plot2)
comp_plot <- ggarrange(plotlist = liste_plots, ncol = 2, nrow = 1,
          common.legend = TRUE, legend = "bottom") #gérer la légende

annotate_figure(comp_plot,
                top = text_grob("Morphologie des sépals et pétals chez des iris",
                                face = "bold", size = 12, just = "center"),
                bottom = text_grob("source : iris dataset",
                                face = "italic", size = 8, just = "left")
                
                )
```

Vous constaterez que quatre étapes sont nécessaires : 

1. Créer les graphiques et les enregistrer dans des objets (ici *plot1* et *plot2*).
2. Encapsuler ces objets dans une liste (ici *liste_plots*).
3. Composer la figure finale avec la fonction `ggarrange`.
4. Ajouter les annotations à la figure composite.

L'argument `common.legend` permet d'indiquer à la fonction `ggarrange` de regrouper les légendes des deux graphiques. Dans notre cas, les deux graphiques ont les mêmes légendes, il est donc judicieux de les regrouper. L'argument `legend` contrôle la position de la légende, et peut prendre les valeurs : *top*, *bottom*, *left*, *right* ou *none* (absence de légende). La fonction `annotate_figure` permet d'ajouter des éléments de texte au-dessus, au-dessous et sur les cotés de la figure composite.

### La couleur {#sect0315}

Dans un graphique, la couleur peut être utilisée à la fois pour représenter une variable quantitative (dégradé de couleur ou discrétisation), ou une variable qualitative (couleur par catégorie). Dans **ggplot2**, il est possible d'attribuer une couleur au contour des géométries avec l'argument `color` et au remplissage avec l'argument `fill`. Il est possible de spécifier une couleur de trois façons dans R : 

* En utilisant le nom de la couleur dans une chaîne de caractère : `"chartreuse4"`. R dispose 657 noms de couleurs prédéfinis. Pour tous les afficher, utilisez la fonction `colors()` qui permet de les visualiser (figure \@ref(fig:figcolors)) .

```{r figcolors, echo=FALSE, fig.align='center', fig.cap="Couleurs de base", auto_pdf=TRUE, out.width='85%'}
knitr::include_graphics('images/magie_graphiques/all_colors.png', dpi = NA)
```

* En indiquant le code hexadécimal de la couleur. Il s'agit d'une suite de six lettres et de chiffres précédée par un dièse : `"#99ff33"`

* En utilisant une notation RGB (rouge, vert, bleu). Cette notation doit contenir quatre nombres entre 0 et 1 (0% et 100%), le premier indiquant la quantité de rouge, le second de vert, le troisième de bleu, et le quatrième la transparence. Ces quatres nombres sont donnés comme argument à la fonction `rgb` : `rgb(0.6, 1, 0.2, 0)`.

Le choix des couleurs est un problème plus complexe que la manière de les spécifier. Il existe d'ailleurs tout un pan de la sémiologie graphique dédié à la question du choix et de l'association des couleurs. Une première ressource intéressante est  [ColorBrewer](https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3){target="_blank"}. Il s'agit d'une sélection de palettes de couleurs particulièrements efficaces et dont certaines sont mêmes adaptées pour les personnes daltoniennes. Il est possible d'accéder directement aux palettes dans R grâce au package **RColorBrewer** et la fonction `brewer.pal` : 

```{r figcolorBrewer, fig.align='center', auto_pdf = TRUE, fig.cap="palette de couleurs de ColorBrewer",  out.width='85%'}
library(RColorBrewer)

display.brewer.all()
```

Une autre ressource pertinente est le site web [coolors.co](https://coolors.co/palettes/trending){target="_blank"} qui propose de nombreuses palettes à portée de clic.

## Principaux graphiques {#sect032}

::: {.bloc_objectif data-latex=""}
Puisque vous êtes désormais initié aux bases de la grammaire des graphiques implémentées par **ggplot2**, vous apprendrez dans les sous-sections suivantes à construire les principaux graphiques que vous utiliserez régulièrement et/ou que vous retrouverez présenter dans un article scientifique.
:::

### Histogramme {#sect0321}

L'histogramme permet de décrire graphiquement la forme de la distribution d'une variable. Pour le réaliser, on utilise la fonction `geom_histogram`. Le paramètre le plus important est le nombre de barres (`bins`) qui composent l'histogramme. Plus ce nombre est grand, plus l'histogramme est précis et à l'inverse, plus il est petit, plus l'histogramme est simplifié. En revanche, il faut éviter d'utiliser un nombre de barres trop élevé comparativement au nombre d'observations disponibles dans le jeu de données, sinon votre histogramme sera plein de trous.

#### histogrammes simples {#sect03211}

Générons quatre variables ayant respectivement une distribution Gaussienne, Student, Gamma et Beta, puis réalisons un histogramme pour chacune de ces variables et combinons les avec la fonction `ggarrange`.

```{r fig318, fig.align='center', fig.cap="Histogrammes", message=FALSE, auto_pdf=TRUE, out.width='65%'}

distribs <- data.frame(
  gaussien = rnorm(1000, mean = 5, sd = 1.5),
  gamma = rgamma(1000, shape = 2, rate = 12),
  beta = rbeta(1000,shape1 = 5, shape2 = 1, ncp = 2),
  student = rt(1000,ncp = 20, df = 5)
)

plot1 <- ggplot(data = distribs) +
  geom_histogram(aes(x = gaussien), bins = 50, color = "#343a40", fill = "#e63946")+
  ylim(c(0,130))

plot2 <- ggplot(data = distribs) +
  geom_histogram(aes(x = gamma), bins = 50, color = "#343a40", fill = "#f1faee")+
  ylim(c(0,130))

plot3 <- ggplot(data = distribs) +
  geom_histogram(aes(x = beta), bins = 50, color = "#343a40", fill = "#a8dadc")+
  ylim(c(0,130))

plot4 <- ggplot(data = distribs) +
  geom_histogram(aes(x = student), bins = 50, color = "#343a40", fill = "#1d3557")+
  ylim(c(0,130))

histogrammes <- list(plot1, plot2, plot3, plot4)

ggarrange(plotlist = histogrammes, ncol = 2, nrow = 2)

```

Notez que cette syntaxe est très lourde. Dans le cas présent, il serait plus judicieux d'utiliser la fonction `facet_wrap`. Pour cela, nous devons au préalable empiler nos données, ce qui signifie changer la forme du *dataframe* actuel qui comprend quatre colonnes (gaussien, gamma, beta et student) et 1000 observations, pour qu'il n'ait plus que deux colonnes (la valeur originale et le nom de l'ancienne colonne) et 4000 observations. La figure \@ref(fig:fig319) décrit graphiquement ce processus qui peut être effectué avec la fonction `melt` du *package* **reshape2**.

```{r fig319, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Empiler les données d'un dataframe",  out.width='30%'}
knitr::include_graphics('images/magie_graphiques/melting.png', dpi = NA)
```

```{r fig320, fig.align='center', fig.cap="Histogrammes en facettes", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='65%'}
library(reshape2)

#faire fondre le jeu de données
melted_distribs <- melt(distribs, measure.vars = c("gaussien", "gamma",
                                                   "beta","student"))
#renommer les colonnes du nouveau dataframe
names(melted_distribs) <- c("distribution", "valeur")
#convertir la variable catégorielle en facteur
melted_distribs$distribution <- as.factor(melted_distribs$distribution)

ggplot(data = melted_distribs)+
  geom_histogram(aes(x = valeur, fill = distribution), bins = 50, color = "#343a40") +
  ylim(c(0,130)) + 
  labs(x = "valeur", 
       y = "fréquence")+
  scale_fill_manual(values = c("#e63946","#f1faee","#a8dadc","#1d3557"))+
  facet_wrap(vars(distribution), ncol=2, scales = "free")
```

#### histogrammes de densité {#sect03212}

Les histogrammes que nous venons de construire utilisent les fréquences des observations pour délimiter la hauteur des barres. Il est possible de changer ce comportement pour plutôt utiliser la densité. L'intérêt est notamment de se rapprocher encore de la définition d'une distribution puisqu'avec cette configuration la somme totale de la surface de l'histogramme est égale à 1. La hauteur de chaque barre représente alors la probabilité d'obtenir l'étendue de valeurs représentées par cette barre. Prenons pour exemple la variable avec la distribution normale que nous venons de voir.

```{r fig321, fig.align='center', auto_pdf = TRUE, fig.cap="Histogrammes de densité",  out.width='65%'}
plot1 <- ggplot(data = distribs) +
  geom_histogram(aes(x = gaussien, y = ..density..),
                 bins = 30, color = "#343a40", fill = "#1d3557")

plot2 <- ggplot(data = distribs) +
  geom_histogram(aes(x = gaussien, y = ..count..),
                 bins = 30, color = "#343a40", fill = "#a8dadc")

ggarrange(plotlist = list(plot1, plot2), ncol = 2)
```

Le graphique de droite (fréquence) nous indique donc que plus de 100 observations ont une valeur d'environ 5 (entre 4,76 et 5,34, compte tenu de la largeur de la barre), ce qui se traduit par une probabilité de presque 30% d'obtenir cette valeur en tirant une observation au hasard dans le jeu de données.

#### Histogramme avec courbe de distribution {#sect03213}

Les histogrammes sont souvent utilisés pour vérifier graphiquement si une distribution empirique s'approche d'une courbe normale. Pour cela, on rajoute sur l'histogramme de la variable empirique la forme qu'aurait une distribution normale parfaite en utilisant la moyenne et l'écart type de la distribution empirique. Pour créer cette figure dans **ggplot2**, il suffit d'utiliser la fonction `stat_function` pour créer un nouveau calque. Pour faire bonne mesure, il est aussi possible d'ajouter une ligne verticale (`geom_vline`) pour indiquer la moyenne de la distribution.

```{r fig322, fig.align='center', auto_pdf = TRUE, fig.cap="Histogrammes et courbe normale",  out.width='65%'}
moyenne <- mean(distribs$gaussien)
ecart_type <- sd(distribs$gaussien)

ggplot(data = distribs) +
  geom_histogram(aes(x = gaussien, y = ..density..),
                 bins = 30, color = "#343a40", fill = "#a8dadc") +
    labs(x = "gaussien", 
       y = "densité")+
  stat_function(fun = dnorm, args = list(mean = moyenne, sd = ecart_type), 
                color = "#e63946", size = 1.2, linetype = "dashed") +
  geom_vline(xintercept = moyenne, color = 'red', size = 1)+
  annotate("text", x = round(moyenne,2)+0.5, y = 0.31, hjust = 'left',
           label = paste('moyenne : ',round(moyenne,2),sep=''))
```

Dans notre cas, nous savons que notre variable est normalement distribuée (car produite avec la fonction `rnorm`), et l'on peut constater la grande proximité entre l'histogramme et la courbe normale.

#### Histogramme avec coloration des valeurs extrêmes {#sect03214}

Il peut être nécessaire d'attirer le regard sur certaines parties de l'histogramme, comme par exemple des valeurs extrêmes. Si nous reprenons notre distribution de Student, nous pouvons clairement distinguer un ensemble de valeurs fortes à droite de la distribution. On pourrait dans notre cas considérer que des valeurs au delà de 50 constituent des cas extrêmes que nous souhaitons représenter dans une autre couleur. Pour cela, nous devons créer une variable catégorielle nous permettant de distinguer ces cas particuliers.

```{r fig323, fig.align='center', auto_pdf = TRUE, fig.cap="Histogramme coloré",  out.width='65%'}
distribs$cas_extreme <- ifelse(distribs$student >=50, "extrême", "normal")

ggplot(data = distribs) +
  geom_histogram(aes(x = student, y = ..count.., fill = cas_extreme),
                 bins = 30, color = "#343a40")+
  scale_fill_manual('cas extrême', values = c("#a8dadc","#e63946"))+
  labs(title = 'Distribution de Student',x = "", y = "fréquence")

```

### Graphique de densité {#sect0322}

L'histogramme est utilisé pour approximer graphiquement la distribution d'une variable. Sa principale limite est de représenter la variable de façon discontinue. Une alternative intéressante est d'utiliser à la place de l'histogramme une version lissée de celui-ci : le graphique de densité. Cette opération de lissage est réalisée le plus souvent à partir de fonctions *kernel*. Reconstruisons notre figure avec les quatres distributions, mais en utilisant cette fois-ci des graphiques de densité.

```{r fig324, fig.align='center', auto_pdf = TRUE, fig.cap="Graphiques de densité en facette",  out.width='65%'}
ggplot(data = melted_distribs)+
  geom_density(aes(x = valeur, fill = distribution), color = "#343a40") +
  scale_fill_manual(values = c("#e63946","#f1faee","#a8dadc","#1d3557"))+
  facet_wrap(vars(distribution), ncol=2, scales = "free")
```

Les graphiques de densité sont souvent utilisés pour comparer la distribution d'une variable pour plusieurs sous groupe d'une population. Si nous reprenons le jeu de données iris, nous pouvons comparer les longueurs de sépals en fonction des espèces. On constate ainsi que les setosa ont une nette tendance à avoir des sépals plus courts et qu'à l'inverse, les virginca ont les sépals généralement les plus longs.

```{r fig325, fig.align='center', auto_pdf = TRUE, fig.cap="Graphiques de densité",  out.width='65%'}
ggplot(data = iris)+
  geom_density(aes(x = Sepal.Length, fill = Species), 
               color = "#343a40", alpha = 0.4)+
  labs(x = 'Longueur de sépals',
       y = '',
       fill = 'Espèce')
```


### Nuage de points {#sect0323}

Un nuage de points est un outil très intéressant pour visualiser la relation existante entre deux variables. Prenons un exemple concret et analysons le volume de CO<sub>2</sub> produit annuellement par habitant dans l'ensemble des pays à travers le monde en comparaison avec le niveau d'urbanisation de ces pays. Nous avons extrait ces données sur le site web de la  [Banque Mondiale](https://donnees.banquemondiale.org/indicateur){target="_blank"}, puis nous les avons structuré dans un fichier *csv*.

```{r}
data_co2 <- read.csv("data/graphique/world_urb_co2.csv")
names(data_co2)
```

#### Nuage de points simple {#sect03231}

Commençons par un nuage de points simple avec l'ensemble des données.

```{r fig326, fig.align='center', fig.cap="Nuage de points simple", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2)+
  geom_point(aes(x = Urbanisation, y = CO2t_hab))+
  labs(x = "Niveau d'urbanisation (%)",
       y = 'tonnes de CO2 annuelle / habitant',
       title = 'Relation entre urbanisation et production de CO2 par habitant')
```

À la première lecture de ce graphique, on observe immédiatement un ensemble de points étranges dont le volume de CO<sub>2</sub> par habitant annuel est compris entre 150 et plus de 350 tonnes et dont le niveau d'urbanisation est proche de 50%. Isolons ces données pour observer de quoi il s'agit.

```{r}
cas_etrange <- subset(data_co2, data_co2$CO2t_hab >= 150)
print(cas_etrange$Country.Name)
```

Il s'agit d'une petite île néerlandaise des Caraïbes nommée Aruba disposant d'une faible population mais avec des activités très polluantes (raffinerie et extraction d'or). Nous faisons ici le choix de retirer ces observations puisqu'elles sont assez peu représentatives de la tendance mondiale. <span class="revision_prop">Cette démarche si simple relève ainsi de l'analyse exploratoire des données ! Sans ce graphique, nous n'aurions probablement jamais identifié ces cas problématiques.</span>


```{r}
data_co2 <- subset(data_co2, data_co2$CO2t_hab <= 150)
```

Reconstruisons le nuage de points maintenant que ces données aberrantes ont été retirées.

```{r fig327, fig.align='center', fig.cap="Nuage de points simple sans données aberrantes", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2)+
  geom_point(aes(x = Urbanisation, y = CO2t_hab))+
  labs(x = "Niveau d'urbanisation (%)",
       y = 'tonnes de CO2 annuelle / habitant',
       title = 'Relation entre urbanisation et production de CO2 par habitant')
```

Voilà qui est mieux ! Cependant, le grand nombre de points restant rend la lecture du graphique assez difficile puisqu'ils se superposent. Une première option à envisager dans ce cas est à la fois d'ajouter de la transparence aux points et de réduire leur taille : 

```{r fig328, fig.align='center', fig.cap="Nuage de points simple avec transparence", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2)+
  geom_point(aes(x = Urbanisation, y = CO2t_hab), alpha = 0.2, size = 0.5)+
  labs(x = "Niveau d'urbanisation (%)",
       y = 'tonnes de CO2 annuelle / habitant',
       title = 'Relation entre urbanisation et production de CO2 par habitant')
```

#### Nuage de points avec densité {#sect03232}

Bien que la transparence nous aide un peu à distinguer les secteurs du graphique avec le plus de points, il serait plus efficace d'abandonner la géométrie des points pour la remplacer par une géométrie de densité en deux dimensions. Une première approche consiste à diviser l'espace du graphique en petits carrés et à compter le nombre de points tombant dans chaque carré (en somme, un histogramme en deux dimensions).

```{r fig329, fig.align='center', fig.cap="Densité en deux dimensions par carrés", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2)+
  geom_bin2d(aes(x = Urbanisation, y = CO2t_hab), bins = 50) +
  scale_fill_continuous(type = "viridis") +
  labs(x = "Niveau d'urbanisation (%)",
       y = 'tonnes de CO2 annuelle / habitant',
       title = 'Relation entre urbanisation et production de CO2 par habitant')
```

On observe ainsi une forte concentration dans le bas du graphique, les pays avec des rejets annuels de CO<sub>2</sub>  supérieurs à 15 tonnes par habitant sont relativement rares. Pour les personnes préférant les représentations plus élaborées, il est aussi possible de diviser l'espace du graphique avec des hexagones en utilisant le package **hexbin**.

```{r fig330, fig.align='center', fig.cap="Densité en deux dimensions par hexagones", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2)+
  geom_hex(aes(x = Urbanisation, y = CO2t_hab), bins = 50) +
  scale_fill_continuous(type = "viridis") +
  labs(x = "Niveau d'urbanisation (%)",
       y = 'tonnes de CO2 annuelle / habitant',
       title = 'Relation entre urbanisation et production de CO2 par habitant')
```

Enfin, il est aussi possible de réaliser une version lissée de ces graphiques avec une fonction *kernel* en deux dimensions (`stat_density_2d`) : 

```{r fig331, fig.align='center', fig.cap="Densité en deux dimensions lissée", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2)+
  stat_density_2d(aes(x = Urbanisation, y = CO2t_hab, fill = ..density..), 
                  geom = "raster", n = 50, contour = FALSE) +
  scale_fill_continuous(type = "viridis") +
  labs(x = "Niveau d'urbanisation (%)",
       y = 'tonnes de CO2 annuelle / habitant',
       title = 'Relation entre urbanisation et production de CO2 par habitant')+
  ylim(0,25)
```

#### Nuage de points et droite de régression {#sect03233}

Afin de faire ressortir une éventuelle relation entre les variables représentées sur les deux axes, il est possible d'afficher la droite de régression sur le graphique entre X et Y. Cette opération s'effectue avec la fonction `geom_smooth`.

```{r fig332, fig.align='center', fig.cap="Nuage de points avec droite de régression", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2)+
  geom_point(aes(x = Urbanisation, y = CO2t_hab), alpha = 0.2, size = 0.5)+
  geom_smooth(aes(x = Urbanisation, y = CO2t_hab), method = lm, color = "red")+
  labs(x = "Niveau d'urbanisation (%)",
       y = 'tonnes de CO2 annuelle / habitant',
       title = 'Relation entre urbanisation et production de CO2 par habitant')
```
Notez que l'argument `method = lm` permet d'indiquer que nous souhaitons utiliser une régression linéaire (*linear model*) pour tracer la géométrie (une droite de régression). La droite semble bien indiquer une relation positive entre les deux variables : une augmentation de l'urbanisation serait associée avec une augmentation de la production annuelle de CO<sub>2</sub> par habitant. On pourrait également vérifier si une relation non linéaire serait plus adaptée au jeu de données. Dans notre cas, une relation quadratique pourrait produire un meilleur ajustement.

```{r fig333, fig.align='center', fig.cap="Nuage de points avec droite de régression exponentielle", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2)+
  geom_point(aes(x = Urbanisation, y = CO2t_hab), alpha = 0.2, size = 0.7)+
  geom_smooth(aes(x = Urbanisation, y = CO2t_hab), method = lm, 
              color = "red", formula = y ~ I(x**2))+
  labs(x = "Niveau d'urbanisation (%)",
       y = 'tonnes de CO2 annuelle / habitant',
       title = 'Relation entre urbanisation et production de CO2 par habitant')
```

La régression quadratique (avec *x* au carré) nous indique ainsi que l'impact du niveau d'urbanisation est plus important à mesure que le niveau d'urbanisation augmente. Vous pouvez également constater que la courbe ne prédit pas de valeurs négatives comparativement à la droite précédente. Il est égalemment possible d'ajuster une courbe sans choisir au préalable sa forme (dans le cas précédent $x^2$) en utilisant une méthode d'ajustement local appelée *loess*.

```{r fig334, fig.align='center', fig.cap="Nuage de points avec droite de régression non linéaire", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2)+
  geom_point(aes(x = Urbanisation, y = CO2t_hab), alpha = 0.2, size = 0.5)+
  geom_smooth(aes(x = Urbanisation, y = CO2t_hab), method = loess, 
              color = "red")+
  labs(x = "Niveau d'urbanisation (%)",
       y = 'tonnes de CO2 annuelle / habitant',
       title = 'Relation entre urbanisation et production de CO2 par habitant')
```

La relation non linéaire révèle davantage d'informations : l'augmentation de l'urbanisation est associée à une augmentation de l'émission de CO<sub>2</sub> par habitant uniquement jusqu'à 75% d'urbanisation; au-delà de ce seuil, la relation ne tient plus. Ces résultats semblent cohérents avec l'évolution classique de l'économie d'un pays passant progressivement d'une économie agricole, à une économie industrialisée et finalement une économie de services.

### Graphique en lignes {#sect0324}

Un graphique en ligne permet de représenter l'évolution d'une variable, généralement dans le temps. Dans le jeu de données précédent, nous disposons des émissions de CO<sub>2</sub> par habitant de nombreux pays sur plusieurs années. Nous pouvons ainsi représenter l'évolution des émissions pour chaque pays avec un graphique en ligne. Pour éviter de le surcharger, cet exercice est réalisé uniquement sur les pays de l'Europe de l'Ouest.

```{r fig335, fig.align='center', auto_pdf = TRUE, fig.cap="Graphique en ligne",  out.width='65%'}
# conversion de la variable year textuelle en variable numérique
data_co2$an <- as.numeric(data_co2$year)
# extraction des données d'Europe de l'Ouest
data_europe <- subset(data_co2, data_co2$region23 == "Western Europe")
# choix des valeurs pour l'axe des x
x_ticks <- seq(1960,2020,10)

ggplot(data = data_europe)+
  geom_path(aes(x = an, y = CO2t_hab, color = Country.Name))+
  labs(x = "Années",
       y = 'Tonnes de CO2 annuelle / habitant',
       color = "Pays",
       title = 'Évolution de la production de CO2 par habitant') +
  scale_x_continuous(breaks = x_ticks, labels = x_ticks)+
  theme_tufte()

```

On remarque notamment qu'aucune donnée avant 2005 n'est disponible pour le Liechtenstein.

#### Barre d'erreur et en bande {#sect03241}

Sur un graphique, il est souvent pertinent de représenter l'incertitude que nous avons sur nos données. Cela peut être fait à l'aide de barres d'erreurs ou à l'aide de polygones délimitant les marges d'incertitude. En guise d'exemple, admettons que les données précédentes sont fiables à plus ou moins 10%. En d'autres termes, la valeur d'émission de CO<sub>2</sub> annuelle serait relativement incertaine et pourrait se situer dans un intervalle de 10% autour de la valeur fourni par la Banque Mondiale. Nous obtenons ainsi une borne inférieure (valeur donnée - 10%) et une borne supérieure (valeur donnée + 10%). Nous pouvons facilement calculer ces bornes et les faire apparaître dans notre graphique précédent.

```{r fig336, fig.align='center', auto_pdf = TRUE, fig.cap="Graphique en ligne avec barres d'erreur",  out.width='65%'}

data_europe$borne_basse <- data_europe$CO2t_hab - 0.1 * data_europe$CO2t_hab
data_europe$borne_haute <- data_europe$CO2t_hab + 0.1 * data_europe$CO2t_hab

ggplot(data = data_europe)+
  geom_point(aes(x = an, y = CO2t_hab, color = Country.Name), size = 0.7)+
  geom_errorbar(aes(x = an, ymin = borne_basse, ymax = borne_haute, color = Country.Name))+
  labs(x = "Années",
       y = 'Tonnes de CO2 annuelle / habitant',
       color = "Pays",
       title = 'Évolution de la production de CO2 par habitant') +
  scale_x_continuous(breaks = x_ticks, labels = x_ticks)+
  theme_tufte()
```

Ces barres d'erreurs indiquent notamment qu'il n'y a finalement aucun écart significatif entre la Belgique, les Pays-Bas et l'Allemagne à partir des années 1990. Une autre option de représentation est d'utiliser des polygones avec la fonction `geom_ribbon`.

```{r fig337, fig.align='center', auto_pdf = TRUE, fig.cap="Graphique en ligne avec marge d'erreur",  out.width='65%'}
ggplot(data = data_europe)+
  geom_path(aes(x = an, y = CO2t_hab, color = Country.Name), size = 0.7)+
  geom_ribbon(aes(x = an, ymin = borne_basse, ymax = borne_haute, fill = Country.Name), alpha = 0.4)+
  labs(x = "Années",
       y = 'Tonnes de CO2 annuelle / habitant',
       color = "Pays",
       title = 'Évolution de la production de CO2 par habitant') +
  scale_x_continuous(breaks = x_ticks, labels = x_ticks)+
  theme_tufte()+
  guides( fill = FALSE)
```

Le message du graphique est le même. Notez que nous avons utilisé ici la fonction `guides` pour retirer de la légende les couleurs associées au remplissage des marges d'erreurs. Ces couleurs sont les mêmes que celles des lignes et il n'est pas utile de dédoubler la légende. De nombreuses méthodes statistiques produisent des résultats accompagnés d'une mesure de l'incertitude associée à ces résultats. Représenter cette incertitude est crucial pour que le lecteur puisse délimiter la portée des conclusions de vos analyses.

### Boites à moustache {#sect0325}

Les boites à moustache (*box plot* en anglais) sont des graphiques permettant de comparer les moyennes et les intervalles interquartiles d'une variable continue selon plusieurs groupes d'une population. Si l'on reprend notre exemple précédent, nous pourrions comparer en fonction de la région du monde la moyenne de production annuelle de CO<sub>2</sub> par habitant. Pour cela, il suffit d'utiliser la fonction `geom_boxplot`.

```{r fig338, fig.align='center', auto_pdf = TRUE, fig.cap="Boite à moustache",  out.width='65%'}
#retirer les observations n'étant pas associées à une région
data_co2_comp <- subset(data_co2, is.na(data_co2$region7) == F)

ggplot(data = data_co2_comp)+
  geom_boxplot(aes(y = region7, x = CO2t_hab))+
  labs(x="Tonnes de CO2 par an et habitant", y="Région")
```

La barre centrale d'une boite représente la moyenne. Les extrêmités de la boite représentent le premier et le troisième quartile. Plus une boite est allongée, plus les situations sont diversifiées pour les observations appartenant au groupe représenté par la boite. Au contraire, une boite étroite indique un groupe homogène. Notez qu'en inversant les variables dans les axes *X* et *Y*, on obtiendrait des boites à moustache verticales. Cependant, les noms des régions étant assez longs, cela nécessiterait d'avoir un graphique très large. Améliorons quelque peu le rendu de ce graphique en ajoutant des titres.

```{r fig339, fig.align='center', fig.cap="Boite à moustache améliorée", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2_comp)+
  geom_boxplot(aes(y = region7, x = CO2t_hab))+
  xlim(c(0,50))+
  labs(title = "Niveaux d'émission de CO2 par habitant par région géographique",
       x = "Tonnes de CO2 par an et habitant",
       y = 'Région')
```

Les points noirs sur le graphique représentent des valeurs extrêmes, soit des observations situées à plus de 1,5 intervalle interquatile d'une extrêmité de la boite. Pour mieux rendre compte de la densité d'observations le long de chaque boite à moustache, il est possible de les représenter directement avec la fonction `geom_jitter`.

```{r fig340, fig.align='center', fig.cap="Boite à moustache avec observations", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2_comp)+
  geom_boxplot(aes(y = region7, x = CO2t_hab), outlier.shape = NA)+
  geom_jitter(aes(y = region7, x = CO2t_hab), size = 0.2, alpha = 0.2)+
  xlim(c(0,50))+
  labs(title = "Niveaux d'émission de CO2 par habitant par région géographique",
       x = "Tonnes de CO2 annuelle par habitant",
       y = 'Région')
```
Notez que pour éviter que les valeurs extrêmes identifiées par la fonction `geom_boxplot` se superposent avec les points représentant les observations, nous les avons supprimées avec l'argument `outlier.shape = NA`.

### Graphique en violons {#sect0326}

Les boites à moustaches donnent des informations pertinentes sur le centre et la dispersion d'une variable en fonction de sous groupes de la population. Cependant, une grande partie de l'information reste masquée par la représentation sous forme de boite. Une alternative est de  remplacer la simple boite par la distribution de la variable étudiée. On obtient ainsi des graphiques en violon (`geom_violin`). Considérant les très grands écarts entre les régions que nous avons observés avec les boites à moustache, il est préférable de tracer les graphiques en violon en excluant les régions Afrique Sub-Saharienne et Asie du Sud.

```{r fig341, fig.align='center', fig.cap="Graphique en violon", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
# retirons les observations de régions que nous ne souhaitons par garder
data_co2_comp <- subset(data_co2, (! data_co2$region7 %in% c("Sub-Saharan Africa", "South Asia")) 
                        & is.na(data_co2$region7)==FALSE)

ggplot(data = data_co2_comp)+
  geom_violin(aes(y = region7,x = CO2t_hab))+
  xlim(c(0,50))+
  labs(title = "Comparaison des niveaux d'émission de CO2 par habitant par région géographique",
       x = "Tonnes de CO2 annuelle / habitant",
       y = '')+
  geom_vline(xintercept = 12, linetype = 'dashed', color = 'blue')
```

Ces distributions permettent notamment de souligner que deux groupes distincts se retrouvent en Amérique du Nord. L'un dont les émissions annuelles de CO<sub>2</sub> par habitant sont inférieure à 12 tonnes (ligne bleue) et un autre groupe pour lequel elles sont supérieures. En explorant les données, on constate que les Bermudes sont inclus dans le groupe Amérique du Nord, mais ont des niveaux d'émission inférieurs à ceux du Canada et des États Unis, ce qui explique cette distribution bimodale. Cette information était masquée avec les boites à moustaches. Finalement, il est aussi possible de superposer graphique en violon et boite à moustaches pour bénéficier des avantages des deux.

```{r fig342, fig.align='center', fig.cap="Graphique en violon et boite à moustaches", warning=FALSE, auto_pdf=TRUE, out.width='65%'}
ggplot(data = data_co2_comp)+
  geom_violin(aes(y = region7,x = CO2t_hab))+
  geom_boxplot(aes(y = region7,x = CO2t_hab), width = 0.15)+
  xlim(c(0,50))+
  labs(title = "Comparaison des niveaux d'émission de CO2 par habitant par région géographique",
       x = "Tonnes de CO2 annuelle / habitant",
       y = '')
```

### Graphique en barres {#sect0327}

Les graphiques en barres permettent de représenter des quantités (hauteur des barres) réparties dans des catégories (une barre par catégorie). Nous proposons ici un exemple avec des données de déplacements issues de l'*enquête origine destination de Québec de 2017* au niveau des grands secteurs (figure \@ref(fig:fig342)).

```{r fig343, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Grands secteurs de Québec",  out.width='60%'}
knitr::include_graphics('images/magie_graphiques/carte_secteurs.jpg', dpi = NA)
```

Nous représentons pour chaque secteur le nombre déplacements moyen entrant et sortant un jour de semaine en heures de pointe. Les données sont présentées sous forme d'une matrice carrée (avec autant de lignes que de colonnes). L'intersection de la ligne A et de la colonne C indique le nombre de personnes partant du secteur A pour se rendre dans le secteur C. À l'inverse, l'intersection de la ligne C et de la colonne A indique le nombre de personnes partant du secteur C pour se rendre dans le secteur A. En sommant les valeurs de chaque ligne, on obtient le nombre total de départs par secteur tandis que le nombre d'arrivées est la somme de chaque colonne. Ces opérations peuvent simplement être effectuées avec les fonctions `rowSums` et `colSums`.

```{r fig344, fig.align='center', auto_pdf = TRUE, fig.cap="Graphiques en barres simples",  out.width='75%'}

# chargement des donneées
matriceOD <- read.csv('data/graphique/Quebec_2017_OD_MJ.csv',
                 header = FALSE, sep = ';') # fichier csv sans entête

# calcul des sommes en lignes et en colonnes
tot_depart <- rowSums(matriceOD)
tot_arrivee <- colSums(matriceOD)

# création d'un DataFrame avec les valeurs et les noms des secteurs
df <- data.frame(depart = tot_depart,
                 arrivee = tot_arrivee, 
                 secteur = c('Arr. de Beauport (Québec)',
                            'Arr. de Charlesbourg (Québec)',
                            'Arr. des Rivières (Québec)',
                            'Arr. de la Cité-Limoilou (Québec)',
                            'Arr. de la Haute-St-Charles (Québec)',
                            'Arr. de Sainte-Foy-Sillery- Cap-Rouge (Québec)',
                            'Arr.de Desjardins (Lévis)',
                            'Arr. des Chutes–de-la-Chaudière-Est (Lévis)',
                            'Arr. Les Chutes de la Chaudière-Ouest (Lévis)',
                            'Ceinture Nord',
                            'Ceinture Sud',
                            'Hors Territoire'),
                 code = c('A','B','C','D','E','F','G','H','I','J','K','X'))

# création des deux graphiques en barres
plot1 <- ggplot(data = df)+
  geom_bar(aes(x = code, weight = depart))+
  labs(subtitle = 'Départs',
       x = 'total',
       y = '')

plot2 <- ggplot(data = df)+
  geom_bar(aes(x = code, weight = arrivee))+
  labs(subtitle = 'Arrivées',
       x = 'total',
       y = '')

# stocker les graphiques dans une liste et composer une figure
list_plot <- list(plot1, plot2)
tot_plot <- ggarrange(plotlist = list_plot, ncol = 1)

# création d'une légende pour associer le code de chaque secteur 
# à son nom. Pour cela on concatène en premier les lettres et les noms, 
# on fusionne ensuite le tout en les séparant par le symbole \n représentant 
# un saut de ligne.
nom_secteurs <- paste(df$code, df$secteur, sep= ' : ')
string_names <- paste(nom_secteurs, collapse = '\n')

titre <- "Déplacements journaliers moyens en heures de pointe de la région de Québec"
# production finale de la figure
annotate_figure(tot_plot,
                top = text_grob(titre, face = "bold", size = 11, just = "left"),
                right = text_grob(string_names, face = "italic", size = 8,
                                  just = "left", x = 0.05) # position du texte
                )

```

Plutôt que de représenter les arrivées et les départs dans deux graphiques séparés, il est possible de les empiler dans un même graphique en barres. Nous devons au préalable « faire fondre nos données » avec la fonction `melt`.

```{r fig345, fig.align='center', auto_pdf = TRUE, fig.cap="Graphique en barres empilées",  out.width='65%'}
# faire fondre le jeu de données (empiler les colonnes depart et arrivee)
melted_df <- melt(df, id.vars = c('code'), measure.vars = c('depart','arrivee'))
names(melted_df) <- c('code','deplacement','effectif')
# ajouter les accents dans la colonne déplacement
melted_df$deplacement <- ifelse(melted_df$deplacement == 'depart', 'départ', 'arrivée')

# comparaison du format original et du format "fondu"
head(df)
head(melted_df)

# réalisation du graphique
plot1 <- ggplot(data = melted_df)+
  geom_bar(aes(x = code,weight = effectif, fill = deplacement),color = '#e3e3e3')+
  scale_fill_manual(values = c("#e63946","#1d3557"))+
  labs(title = titre,
       y = 'Effectifs',
       x = '',
       fill = 'Déplacements')

annotate_figure(plot1,right = text_grob(string_names, face = "italic", size = 7,
                                  just = "left", x = 0.05)) # position du texte)
```

### Graphique circulaire {#sect0328}

Une alternative directe au graphique en barre est le graphique ou diagramme circulaire, appelé aussi graphique en pointes de tarte (pour ceux à la dent sucrée) ou en camembert (pour les amateurs de fromage). Il est suffissamment connu et utilisé pour qu'aucune présentation ne s'impose. Pour être exact, un graphique en pointes de tarte n'est rien d'autre qu'un graphique en barres dont le système de coordonnées a été modifié. Cela impose cependant de calculer à l'avance la position des étiquettes que l'on souhaite ajouter sur le graphique. Reprenons les données de production mondiale de CO<sub>2</sub> et calculons les productions totales par région géographique en 2015.

```{r fig346, fig.align='center', fig.cap="Graphique en pointes de tarte", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='65%'}
library(dplyr)

# extraire les données de 2018 pour lesquelles on connait la région
data_co2_2015 <- subset(data_co2,data_co2$year == "2015" & ! is.na(data_co2$region7))

# effectuer la somme du CO2 par région
co2_2015 <- data_co2_2015 %>% 
  group_by(region7) %>% 
  summarise(total_co2 = sum(CO2_kt,na.rm = TRUE))

# attribuer un code à chaque région pour faciliter la lecture
co2_2015$code <- c("A","B","C","D","E","F","G")

# modifier l'ordre des données, calculer les proportions et la position des labels
df <- co2_2015 %>% 
  arrange(desc(code)) %>%
  mutate(prop = total_co2 / sum(co2_2015$total_co2) *100) %>%
  mutate(ypos = cumsum(prop)- 0.5*prop )

# préparer la légende (pourcentages et vrais noms)
nom_region <- rev(paste(df$code, " : ", df$region7, "(", round(df$prop,1),"%)"))
string_region <- paste(nom_region, collapse = '\n')

# construire le graphique
plot1 <- ggplot(df, aes(x="", y=prop, fill=code)) +
  geom_bar(stat="identity", width=1, color="white") +
  coord_polar("y", start=0) +
  theme_void() + 
  theme(legend.position="none") +
  geom_text(aes(y = ypos, label = code), color = "white", size=3) +
  scale_fill_grey()+
  labs(title = "Proportion du CO2 émis en 2015")

# ajouter la légende
annotate_figure(plot1,right = text_grob(string_region, face = "italic", size = 9,
                                  just = "left", x = 0.05)) # position du texte)

```

Si à la place de la géométrie `geom_bar`, vous utilisez `geom_rect`, vous pouvez convertir votre graphique en pointes de tarte en graphique en anneau (ou en beigne, pour ceux à la dent sucrée) : 

```{r fig347, fig.align='center', auto_pdf = TRUE, fig.cap="Graphique en anneau",  out.width='65%'}
# calculer la limite inférieure et supérieure du beigne
df$ymax <- cumsum(df$prop)
df$ymin <-  c(0, head(df$ymax, n=-1))

# construire le graphique
plot1 <- ggplot(df, aes(ymax=ymax, ymin=ymin, 
                        xmax=4, xmin=3,
                        y=prop, fill=code)) +
  geom_rect(stat="identity", color="white") +
  coord_polar("y", start=0) +
  theme_void() + 
  theme(legend.position="none") +
  geom_text(aes(x = 3.5,y = ypos, label = code), color = "white", size=3) +
  scale_fill_grey()+
  xlim(c(2,4))+
  labs(title = "Proportion du CO2 émis en 2015")

# ajouter la légende
annotate_figure(plot1,right = text_grob(string_region, face = "italic", size = 8,
                                  just = "left", x = 0.05)) # position du texte)

```

## Graphiques spéciaux {#sect033}

Dans cette dernière section, nous allons aborder des graphiques plus rarement utilisés. Ils sont toutefois très utiles dans certains contextes du fait de leur capacité à synthétiser des informations complexes.

### Graphique en radar {#sect0331}

Les graphiques en radar (ou en toile d'araignée) sont utilisés pour comparer une série de variables continues pour plusieurs observations ou groupe d'observations. Chaque variable est associée à un axe et chaque observation est représentée avec un polygone. Prenons comme exemple les données de logement par secteur de recensement dans la région métropolitaine de Montréal en 2016. On pourrait souhaiter comparer la moyenne des pourcentages des différents types de logements pour les régions des Laurentides, de la Montérégie, de Laval, de Longueuil et de Montréal. Malheureusement, **ggplot2** ne permet pas de dessiner des graphiques en radar satisfaisants, nous devrons donc utiliser le *package* **fmsb**.

```{r fig348, fig.align='center', auto_pdf = TRUE, fig.cap="Graphique en anneau",  out.width='65%'}
library(fmsb)

data <- read.csv('data/bivariee/sr_rmr_mtl_2016.csv', header = T, encoding = 'UTF-8')

# aggrégeons les données au niveau des régions en calculant la moyenne des pourcentages
variables <- c("MaisonIndi","App5Plus","MaisRangee","AppDuplex","Proprio","Locataire")

data_region <- data[c("Region",variables)] %>% 
  group_by(Region) %>%
  summarise_all(.funs = list(mean))

# gérer le nom des colonnes pour ajuster les données aux besoins de 
# la fonction radachart
new_names <- c("Region",paste(variables,"_mean",sep=""))
names(data_region) <- new_names
data_region <- data.frame(data_region)
rownames(data_region) <- data_region$Region
data_region$Region <- NULL

# ajouter deux lignes aux données avec les valeurs maximales et minimales 
# de chaque colonne. Ces informations aideront la fonction radachart à
# dessiner chacun des axes du radar
data_chart <- rbind(apply(data_region,MARGIN = 2, FUN = max),
                    apply(data_region,MARGIN = 2, FUN = min),
                    data_region
                    )

# choix des couleurs pour l'intérieur des polygones (avec transparence)
couleurs <- c(
  rgb(0.94, 0.28, 0.44, 0.25),
  rgb(1.00, 0.82, 0.40, 0.25),
  rgb(0.02, 0.84, 0.63, 0.25),
  rgb(0.07, 0.54, 0.70, 0.25),
  rgb(0.03, 0.23, 0.30, 0.25)
)

# choix des couleurs pour l'intérieur des polygones (sans transparence)
couleurs_contour <- c(
  rgb(0.94, 0.28, 0.44),
  rgb(1.00, 0.82, 0.40),
  rgb(0.02, 0.84, 0.63),
  rgb(0.07, 0.54, 0.70),
  rgb(0.03, 0.23, 0.30)
)

# dessin du graphique
radarchart(data_chart,
           title = "Comparaison des types de logements dans la RMR",
           pcol = couleurs_contour, pfcol = couleurs,
           plwd = 2, plty=1,
           cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
           vlcex=0.8,
           vlabels = c("maison individuelle", "immeuble d'appartements",
                       "maison \nen rangée", "duplex",
                       "propriétaire", "locataire")
           )

# ajout d'une légende
legend(x=1.3, y=1, legend = rownames(data_chart[-c(1,2),]), bty = "n", 
       pch=20 , col=couleurs , text.col = "black", cex=0.9, pt.cex=1.5)
```

À la lecture du graphique, on constate rapidement que l'île de Montréal a une situation très différentes des trois autres régions. Laval se ditingue également avec une part importante de logements dans des immeubles d'appartements. Ce type de graphique a pour objectif d'orienter le regard sur de potentielles différences dans un contexte multidimensionnelle, mais il comporte quelques inconvénients : 

* Les échelles de chaque axe sont différentes. Il est donc essentiel de se rapporter aux valeurs exactes pour estimer si les écarts sont importants en termes absolus.
* La superposition de plusieurs polygones peut rendre la lecture difficile. Une alternative envisageable est de réaliser un graphique par polygone, mais cela prend beaucoup de place dans un document.
* L'utilisation de polygones donne parfois de fausses impressions d'écarts. Dans le précédent graphique, l'œil est attiré en bas à gauche par le polygone de Montréal très différent des autres. Cependant, les écarts sur l'axe *maison en rangée* sont relativement petit comparativement à l'axe *locataire* situé à l'opposé.

### Diagramme d'accord {#sect0332}

Les diagrammes d'accord (*chord diagram* en anglais) sont utilisés pour représenter des échanges ou des connexions entre des entités. Il peut s'agir par exemple de marchandises importées / exportées entre pays, des messages envoyés entre utilisateurs de réseaux sociaux, de flux de population, etc. Reprenons nos données de l'*enquête origine destination de la région de Québec en 2017* pour illustrer le tout. Nous utiliserons le *package* **chorddiag**, très facile d'utilisation et produisant des graphiques interactifs, facilitant grandement la lecture de ce type de graphique. Cependant ce *package* ne fait pas partie du répertoire *CRAN*, nous devrons l'installer directement depuis *github* avec la fonction `devtools::install_github`.

```{r eval = FALSE}
devtools::install_github('mattflor/chorddiag')
```

```{r fig349, fig.align='center', fig.cap="Diagramme d'accord", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='100%'}
library(chorddiag)

# chargement des données
matriceOD <- read.csv('data/graphique/Quebec_2017_OD_MJ.csv',
                 header = FALSE, sep = ';') # fichier csv sans entête

# transformation du dataframe en matrice
matriceOD <- as.matrix(matriceOD)
codes <- c('A','B','C','D','E','F','G','H','I','J','K','X')
secteurs <- c('Arr. de Beauport',
            'Arr. de Charlesbourg',
            'Arr. des Rivières',
            'Arr. de la Cité-Limoilou',
            'Arr. de la Haute-St-Charles',
            'Arr. de Sainte-Foy-Sillery-Cap-Rouge',
            'Arr.de Desjardins',
            'Arr. des Chutes–de-la-Chaudière-Est',
            'Arr. Les Chutes de la-Chaudière-Ouest',
            'Ceinture Nord',
            'Ceinture Sud',
            'Hors Territoire')

# ajout de noms aux colonnes et aux lignes de la matrice
rownames(matriceOD) <- secteurs
colnames(matriceOD) <- secteurs

#on supprime les trois secteurs Ceinture Nord, Sud et Hors territoire 
#qui comprennent de toute façon peu de déplacements
mat <- matriceOD[1:8,1:8]

# choix aléatoire de couleurs pour les lignes
# col <- sample(colors(),nrow(mat),replace = F)

# choix de couleurs 
col <- c("#a491d3", "#818aa3", "#C5DCA0", "#F5F2B8",
         "#F9DAD0", "#F45B69", "#22181C", "#5A0001")

# réalisation du graphique : sortie html
if(knitr::is_html_output()){
  chorddiag(mat, groupColors = col, showTicks = F,
        type = 'bipartite', chordedgeColor = 'white',
        groupnameFontsize = 12, groupnamePadding = 5)
}

# pour la sortie pdf
if(knitr::is_latex_output()){
  knitr::include_graphics('images/magie_graphiques/chord_diagramme.png', dpi = NA)
}

```
Le graphique permet de remarquer que la plupart des flux s’effectuent au sein d’un même secteur. La majorité des déplacements se font au sein du secteur Sainte-Foy (segment rouge central). On peut cependant constater que les secteurs des Rivières, la cité Limoilou et Haute-Saint-Charle attirent une plus grande quantité et diversité de flux. Si vous lisez ce livre dans un navigateur web (et pas au format *pdf*), le graphique est interactif ! En plaçant votre souris sur un lien, vous verrez s'afficher le nombre de déplacements qu'il représente.

### Nuage de mots {#sect0333}

Un nuage de mots est un graphique utilisé en analyse de texte pour représenter les mots les plus importants d'un document. Mesurer l'importance des termes dans un document est une discipline à part entière (*Natural Language Processing*), nous proposons un simple exemple ici avec la méthode *TextRank* (basée sur la théorie des graphs) proposée par @mihalcea2004textrank et implémentée dans le *package* **textrank**. Nous aurons également besoin des *packages* **udpipe** (fournissant des dictionnaires linguistiques), **RColorBrewer** (pour sélectionner une palette de couleurs) et **wordcloud2** (pour générer le graphique). En guise d'exemple, nous avons choisi d'extraire les textes de deux Schémas d'Aménagement et de Développement (SAD), ceux des agglomérations de Québec et Montréal en vigueur en 2020. Il s'agit de deux documents de planification définissant les lignes directrices de l'organisation physique du territoire des municipalités régionales de comté (MRC) ou des agglomérations. Pour ces deux documents, nous nous concentrons sur le chapitre portant sur les grandes orientations d'aménagement et de développement, soit les pages 30 à 135 pour Québec et 30 à 97 pour Montréal. Pour extraire les textes des fichiers pdf, nous utilisons le *package* **pdftools**.

Nous devons donc réaliser les étapes suivantes pour produire le nuage de mots : 

1. Extraire les sections qui nous intéressent des fichiers *pdf*
2. Extraire le texte de ces sections
3. Retirer les caractères représentant les sauts de lignes et les sauts de paragraphes (`\n` et `\r`)
4. Concaténer tout le texte en une seule longue chaîne de caractère
5. Utiliser un dictionnaire pour déterminer la nature des mots du texte (noms, adjectifs, verbes, etc.)
6. Utiliser l'algorithme *TextRank* pour identifier les mots clefs
7. Nettoyer les erreurs potentielles parmi les mots clefs
8. Construire le nuage de mots.

Notez que toutes ces étapes de nettoyage ne seraient pas nécessaires si nous utilisions un simple fichier texte comme point de départ. Cependant, il est plus courant de rencontrer des fichiers *pdf*, cet exercice est donc davantage révélateur de la difficulté réelle de la réalisation d'un nuage de mots.

```{r echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
library(wordcloud2)
library(udpipe)
library(RColorBrewer)
library(pdftools)
library(textrank)

# Étape 1 : extraire les sections pertinentes des fichiers pdf
extrait_qc <- pdf_subset("data/graphique/SAD_quebec.pdf", pages = c(30:135),
                         output = "data/graphique/SAD_quebec_ext.pdf")
extrait_mtl <- pdf_subset("data/graphique/SAD_montreal.pdf", pages = c(30:97),
                          output = "data/graphique/SAD_montral_ext.pdf")

# Étape 2 : extraire le texte des fichiers pdf sous forme de vecteur de texte
file_qc <- pdf_text(extrait_qc)
file_mtl <- pdf_text(extrait_mtl)

# Étape 3 : retirer les saut de lignes et les paragraphes
file_qc <- gsub("\r","",x = file_qc)
file_qc <- gsub("\n","",x = file_qc)

file_mtl <- gsub("\r","",x = file_mtl)
file_mtl <- gsub("\n","",x = file_mtl)

# Étape 4 : créer une seule longue chaîne de caractères
# à partir des vecteurs de texte
text_qc <- paste(file_qc, collapse = " ")
text_mtl <- paste(file_mtl, collapse = " ")

# charger le modèle linguistique français
model <- udpipe_load_model('data/graphique/french-sequoia-ud-2.4-190531.udpipe')

# pour télécharger le modèle si ce n'est pas encore fait : 
# model <- udpipe_download_model("french-sequoia")
# model <- udpipe_load_model(model)

# Etape 5 : Analyse de la nature des mots du texte avec le dictionnaire fr
# On obtient des dataframes décrivant les mots des textes
annote_qc <- udpipe_annotate(model, text_qc)
df_qc <- data.frame(annote_qc)

annote_mtl <- udpipe_annotate(model, text_mtl)
df_mtl <- data.frame(annote_mtl)

# Etape 6 : Utilisation de la méthode TextRank
stats_qc <- textrank_keywords(df_qc$lemma,
                  relevant = df_qc$upos %in% c("NOUN", "ADJ"), ngram_max=2)

stats_mtl <- textrank_keywords(df_mtl$lemma,
                  relevant = df_mtl$upos %in% c("NOUN","ADJ"), ngram_max=2)

# Etape 7 : Nettoyer les coquilles dans les mots clefs 
# NB : nous faisons ici le choix de garder des mots clefs uniques (ngram == 1)
# il serait aussi possible de garder des associations de plusieurs mots
dfstats_qc <- subset(stats_qc$keywords, stats_qc$keywords$ngram == 1 &
                       nchar(stats_qc$keywords$keyword)>2)
dfstats_qc$keyword <- gsub("d’","",dfstats_qc$keyword,fixed = T)
dfstats_qc$keyword <- gsub("l’","",dfstats_qc$keyword,fixed = T)

dfstats_mtl <- subset(stats_mtl$keywords, stats_mtl$keywords$ngram == 1 &
                        nchar(stats_mtl$keywords$keyword)>2)
dfstats_mtl$keyword <- gsub("d’","",dfstats_mtl$keyword,fixed = T)
dfstats_mtl$keyword <- gsub("l’","",dfstats_mtl$keyword,fixed = T)

# Etape 8 : Réaliser les nuages de mots
couleurs <- sample(brewer.pal(12, "Paired")) # mise en désordre des couleurs

wordcloud2(data = dfstats_mtl[c("keyword", "freq")],
           color = couleurs, size = 0.5, shuffle = F)

wordcloud2(data = dfstats_qc[c("keyword", "freq")],
           color = couleurs, size = 0.6, shuffle = F)
```


```{r fig350, echo=FALSE, fig.align='center', fig.cap="Nuage de mots pour le SAD de Montréal", auto_pdf=TRUE, out.width='85%'}
knitr::include_graphics('images/magie_graphiques/nuage_mtl.png', dpi = NA)
```

```{r fig351, echo=FALSE, fig.align='center', fig.cap="Nuage de mots pour le SAD de Québec", auto_pdf=TRUE, out.width='85%'}
knitr::include_graphics('images/magie_graphiques/nuage_qc.png', dpi = NA)
```

Notez qu'à chaque génération du nuage de mots, vous obteindrez une disposition différente. N'hésitez pas à en essayer plusieurs jusqu'à trouver celle qui vous semble optimale.

### Treemaps {#sect0334}

Un *treemap* est un graphique permettant de représenter une quantité partagée entre plusieurs observations structurées dans une hiérarchie de groupe. Le jeu de données portant sur les émissions de CO2 se prête tout à fait à une représentation par *treemaps*. La variable de quantité est bien sûr les émissions de CO<sub>2</sub> par pays; ces pays sont regroupés dans un premier ensemble de régions (découpage en 23 régions), qui elles-mêmes sont regroupées dans des régions plus larges (découpage en sept régions). Pour construire un *treemap*, nous allons utiliser le *package* **treemap**.

```{r fig352, fig.align='center', fig.cap="Treemap", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='90%'}
library(treemap)
library(RColorBrewer)

# extraire les données de CO2 en 2015
data_co2_2015 <- subset(data_co2,data_co2$year == "2015" & ! is.na(data_co2$region7))

# construire le treemap

treemap(data_co2_2015, index=c("region7","region23"),
    vSize="CO2_kt", type="index",
    title = "CO2 rejetés par pays en 2015",
    fontsize.labels=c(12,8), # taille des étiquettes              
    fontcolor.labels=c("white","black"), # couleur des étiquettes
    fontface.labels=c(2,1), # style des polices
    bg.labels=c("transparent"), # arrière plan des étiquettes
    align.labels=list(
        c("center", "center"), 
        c("right", "bottom")
        ),  # localisation des étiquettes dans les boites
    overlap.labels=0.5, # tolérance de superposition
    inflate.labels=F, # agrandir la taille des étiquettes ou non
    palette = brewer.pal(7,'Paired')
)

```

## Les cartes {#sect034}

Toute comme un graphique, un carte est aussi une illustration visuelle, avec la généralisation des données géographiques, il peut être utile de savoir représenter ce type de données. Si R n'est pas un logiciel de cartographie, il est possible de réaliser des cartes assez facilement, directement avec **ggplot2**. Nous avons cependant une préférence pour le *package* **tmap** qui propose de nombreuses fonctionnalités. Pour tracer des cartes, **tmap** et **ggplot2** ont besoin d'utiliser un format de données comprenant la géométrie (polyones, lignes ou points), la localisation et le système de projection des entités spatiales étudiées. Le format de fichier le plus courant pour ce type de données est le *shapefile* (*.shp*), mais vous pourrez parfois croiser des fichiers *geojson* (.js), ou encore *geopackages* (.gpkg). Pour lire ces fichiers, il est possible d'utiliser la fonction *readOGR* du *package* **rgdal**, ou la fonction *st_read* du package **sf**. Notez ici que ces deux fonctions ne produisent pas des *DataFrame*, mais respectivement un *SpatialDataFrame* et un objet *sf* (*simple feature collection*). Sans rentrer dans les détails, sachez que deux *packages* permettent de manipuler des objets spatiaux dans R : le traditionnel **sp** (avec les *SpatialDataFrame*) et le plus récent **sf** (avec les objets du même nom). Il est assez facile de convertir un objet de **sp** vers **sf** (et inversement) et cette opération est souvent nécessaire car de nombreux *packages* dédiés à l'analyse spatiale utilisent l'un ou l'autre des formats. Dans le cas de **tmap**, des objets de **sp** et de **sf** peuvent être utilisés sans distinction. En revanche, pour cartographier directement avec **ggplot2**, il est plus facile d'utiliser un objet de type *sf*.  

Une carte thématique permet de représenter la répartition spatiale de variables qualitatives ou quantitatives. On les distingue des cartes topographiques dont l'objectif est de représenter la localisation d'objets spécifiques (route, habitation, rivière, lac, etc.). Les premières sont relativement faciles à construire dans R car elles se limitent à quelques symboles relativement peu complexes. Pour les secondes, on préferera généralement un logiciel comme [QGis](https://qgis.org/en/site/){target="_blank"}.

Créons une carte thématique à partir des données de densité de végétation sur l'Île de Montréal avec les packages **ggplot2** puis **tmap**.

Avec **ggplot2**, nous aurons aussi besoin des *packages* **classInt** pour calculer les intervalles des classes et de **ggsn** pour afficher une échelle.

```{r fig353, fig.align='center', fig.cap="Carte thématique avec ggplot2", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='85%'}
library(sf)
library(classInt)
library(ggsn)

# chargement des données
spatialdf <- st_read("data/bivariee/IlotsVeg2006.shp")

# création d'une discrétisation en 7 classes égales
values <- c(max(spatialdf$ArbPct)+0.01,spatialdf$ArbPct)

quant <- classIntervals(values, n = 7,
                        style = "quantile",
                        intervalClosure = 'right')

spatialdf$class_col <- cut(spatialdf$ArbPct, breaks = quant$brks, right = F)


# cartographie avec ggplot2
ggplot(data = spatialdf) + 
  geom_sf(aes(fill = class_col), color = rgb(0,0,0,0))+
  scale_fill_brewer(palette = "Greens")+
  labs(title = "Végétation dans les Îlos de recensement",
       'fill' = 'Densité de la canopée (%)')+
  theme(axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(), axis.title.y=element_blank(),
        panel.background=element_blank(),
        panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        legend.key.size = unit(0.5, "cm"))+
  scalebar(spatialdf, dist = 5, st.size=3, height=0.01, model = 'WGS84', 
           dist_unit = "km", transform = F, location = 'bottomright')

```

Il est possible d'arriver à un résultat similaire avec **tmap** avec moins de code ! 

```{r fig354, fig.align='center', fig.cap="Carte thématique avec tmap", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='85%'}
library(tmap)

colors <- brewer.pal(7,"Greens")

tm_shape(spatialdf) +
  tm_polygons("ArbPct", palette = colors, border.alpha = 0,
            n = 7, style = 'quantile',
            title = 'Densité de la canopée (%)')+
  tm_scale_bar(breaks = c(0,5,10)) + 
  tm_layout(title = "Végétation dans les Îlos de recensement",
            attr.outside = TRUE, frame = FALSE)
```

Les graphiques créés par **tmap** ne peuvent malheureusement pas être combinés avec la fonction `ggarrange`, mais **tmap** dispose de sa propre fonction `tmap_arrange` si vous souhaitez combiner plusieurs cartes.

```{r fig355, fig.align='center', fig.cap="Combiner des cartes avec tmap", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='85%'}
library(tmap)

colors <- brewer.pal(7,"Greens")

colors2 <- brewer.pal(7,"Reds")

carte1 <- tm_shape(spatialdf) +
  tm_polygons("ArbPct", palette = colors, border.alpha = 0,
            n = 7, style = 'quantile',
            title = 'Densité de la canopée (%)') +
  tm_scale_bar(breaks = c(0,5,10)) + 
  tm_layout(attr.outside = TRUE, frame = FALSE)

carte2 <- tm_shape(spatialdf) +
  tm_polygons("LogDens", palette = colors2, border.alpha = 0,
            n = 7, style = 'quantile',
            title = 'Densité de logement') + 
  tm_scale_bar(breaks = c(0,5,10)) + 
  tm_layout(attr.outside = TRUE, frame = FALSE)

tmap_arrange(carte1, carte2, ncol = 2)
```

## Exporter des graphiques {#sect035}

Tous les graphiques que nous avons construits dans ce chapitre peuvent être exportés assez facilement. Dans RStudio, vous pouvez directement cliquer sur le bouton *export* (figure \@ref(fig:fig355))) pour enregistrer votre figure au format image (raster) ou au format PDF (vecteur). Notez qu'avec la seconde option, vous pourrez retoucher votre graphique avec un logiciel externe comme *Inkscape* ou *Illustrator*, ce qui est souvent nécessaire.

```{r fig356, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Exporter un graphique dans RStudio",  out.width='40%'}
knitr::include_graphics('images/magie_graphiques/export.JPG', dpi = NA)
```

Lorsque vous créez un graphique avec **ggplot2**, il est aussi possible de l'exporter avec la fonction `ggsave`. Cette fonctionnalité est très pratique lorsque vous souhaitez automatiser la production de graphiques et ne pas avoir à tous les exporter à la main. Pour en apprendre plus sur l'automatisation de tâches dans R, référez vous au chapitre XXX.

```{r warning=FALSE, eval = FALSE}
data(iris)

plot1 <- ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris)

ggsave(filename = 'graphique.pdf',
       path = 'mon/dossier',
       plot = plot1,
       width = 10, height = 10, units = "cm")
```

Pour les graphiques n'étant pas réalisés avec **ggplot2**, l'alternative à la fonction `ggsave` est l'ensemble de fonctions `png`, `bmp`, `jpeg`, `tiff` et `pdf`, qui permettent d'exporter n'importe quel graphique dans ces différents formats. Le processus comprend trois étapes : 

1. Ouvrir une connexion vers le fichier dans lequel le graphique sera exporté avec une des fonctions `png`, `bmp`, `jpeg`, `tiff` et `pdf`.
2. Réaliser son graphique comme si on souhaitait l'afficher dans RSudio. Il n'apparaitra cependant pas, car il sera écrit dans le fichier en question à la place.
3. Fermer la connexion au fichier avec la fonction `dev.off` pour définitivement enregistrer le graphique.

```{r warning=FALSE, eval = FALSE}
data(iris)

# 1. Ouvrir la connexion
png(filename = 'mon/dossier/graphique.png')

# 2. Afficher le graphique
ggplot() +
  geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), data = iris)

# 3. fermer la connexion
dev.off()

```

## Conclusion sur les graphiques {#sect036}

Vous avez pu constater que les capacités de représentation graphique de R sont vastes et pourtant nous n'avons fait qu'observer la partie émergée de l'iceberg dans ce chapitre. Il est également possible de réaliser de la visualisation en 3D dans R (**plot3D**, **rgl**), d'animer des graphiques pour en faire des *GIF* ou des vidéos (**gganimate**), de rendre des graphiques interactifs, ou même de construire des plateformes de visualisation de données disponibles en ligne (**shiny**). Vous continuerez à découvrir de nouvelles formes de représentations au fur et à mesure de votre pratique, en apprenant de nouvelles méthodes nécessitant des visualisations spécifiques.

Voici également deux références trés utiles qui nous ont notamment aidé à construire ce chapitre : 

* [The R Graph Gallery](https://www.r-graph-gallery.com/){target="_blank"}, probablement **LE** site web proposant le plus de matériel sur comment réaliser des grahiques dans R.
* [Data to viz](https://www.data-to-viz.com/){target="_blank"}, si vous ne savez pas quel graphique pourrait le mieux correspondre à vos données, Data to viz est là pour vous aider. Vous y trouverez un arbre de décision pour vous indiquer quel graphique utiliser dans quelle situation, ainsi que de nombreux conseils sur la visualisation de données.


<!--chapter:end:03-magiedesgraphiques.Rmd-->

# (PART) Analyses bivariées {-} 

# Analyses bivariées {#chap04}

Dans ce chapitre, nous présentons les principales méthodes exploratoires et confirmatoires bivariées permettant d'évaluer la relation entre deux variables, et ce, en fonction de leur type : deux variables quantitatives, deux variables qualitatives ou encore une variable quantitative _versus_ une variable qualitative (comprenant deux modalités ou plus de deux modalités) (figure \@ref(fig:fig1)). 

```{r fig1, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Les principales méthodes bivariées",  out.width='65%'}
knitr::include_graphics('images/bivariee/figure1.jpg', dpi = NA)
```

Plus spécifiquement, nous présentrons puis mettrons en œuvre dans le logiciel  les méthodes suivantes : covariance, corrélation et régression linéaire simple (entre deux variables quantitatives, section \@ref(sect041)), tableau de contingence et test du khi^2^ (entre deux variables qualitatives, section \@ref(sect042)), t de student (test *t*) et test de Wilcoxon (entre une variable quantitative et une variable qualitative comprenant deux modalités, section \@ref(sect043)), et analyse de variance et test de Kruskal-Wallis (entre une variable quantitative et une variable qualitative comprenant plus de deux modalités, section \@ref(sect044)).

::: {.bloc_package data-latex=""}
Dans cette section, nous utiliserons principalement les *packages* suivants : 

* Pour créer des graphiques :
  - **ggplot2**, le seul, l'unique
  - **ggpubr**, pour combiner des graphiques et réaliser des diagrammes quantiles-quantiles
* Pour manipuler des données : 
  - **dplyr**, avec les fonctions *group_by*, *summarize* et les pipes *%>%*
* Pour les corrélations (section \@ref(sect0411)) : 
  - **correlation**, de l'ensemble de package **easy_stats**, offrant une large gamme de méthodes de corrélations
  - **boot** pour réaliser des corrélations avec *bootstrap* 
  - **Hmisc** pour calculer des corrélations de Pearson et Spearman
  - **ppcor**, notamment pour des corrélations partielles
  - **psych** pour obtenir une matrice de  corrélation (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de p.
  - **stargazer** pour créer des beaux tableaux d’une matrice de corrélation en Html ou en LaTeX ou en ASCII.
  - **corrplot**, pour créer des graphiques de matrices de corrélation
* Pour le tableau de contignence (section \@ref(sect0412)) :
  - **gmodels**, pour construire des tableaux de contingence et calculer les tests *t* et ses différentes variantes (section \@ref(sect0424))
  - **vcd**, pour construire un graphique pour un tableau de contigence ((section \@ref(sect0424)))
* Pour les test *t* : 
  - **sjstats** pour réaliser des test *t* pondérés
  - **effectsize**, pour calculer les tailles d'effet de tests de *t*
* Pour la section sur les ANOVA (section \@ref(sect0441)) : 
  - **car**, pour les ANOVA classiques
  - *lmtest* pour le test de Breusch-Pagan d'homogénéité des variances 
  - **rstatix**, intégrant de nombreux tests classiques (comme le test de Shapiro) avec **tidyverse**
:::

## Relation linéaire entre deux variables quantitatives {#sect041}

::: {.bloc_objectif data-latex=""}
**Deux variables continues varient-elles dans le même sens ou bien en sens contraire ?** Répondre à cette question est une démarche exploratoire classique en sciences sociales puisque les données socioéconomiques sont souvent associées linéairement. En d'autres termes, lorsque l'une des deux variables tant à augmenter, la seconde augmente également ou diminue systématiquement.

En études urbaines, on pourrait vouloir vérifier si certaines variables socioéconomiques sont associées positivement ou négativement à des variables environnementales jugées positives (comme la couverture végétale ou des mesures d’accessibilité spatiale aux parcs) ou négatives (pollutions atmosphériques et sonores). 

Par exemple, au niveau des secteurs de recensement d’une ville canadienne ou américaine, on pourrait vouloir vérifier si le revenu médian des ménages ou encore le coût moyen du loyer varient dans le même sens que la couverture végétale ; ou au contraire, en sens inverse des niveaux moyens de dioxyde d’azote ou de bruit routier.

Pour évaluer la linéarité entre deux variables continues, deux statistiques descriptives sont utilisées : la **covariance**\index{covariance} (section \@ref(sect0412)) et la **corrélation**\index{corrélation} (section \@ref(sect0413)).
:::


### Bref retour sur le postulat de la relation linéaire {#sect0411}

Vérifier le postulat de la linéarité consiste à évaluer si deux variables quantitatives varient dans le même sens ou bien en sens contraire. Toutefois, la relation entre deux variables quantitatives n’est pas forcément linéaire. En guise d'illustration, la figure \@ref(fig:fig2) permet de distinguer quatre types de relations :

* le cas **a** illustre une relation linéaire positive entre les deux variables puisqu’elles vont dans le même sens. Autrement dit, quand les valeurs de *X* augmentent, celles de *Y* augmentent aussi. En guise d'exemple, pour les secteurs de recensement d'une métropole donnée, il est fort probable que le coût moyen du loyer soit associé positivement avec le revenu médian des ménages. Graphiquement parlant, il est clair qu'une droite dans ce nuage de points résumerait efficacement la relation entre ces deux variables.

* le cas **b** illustre une relation linéaire négative entre les deux variables puisqu’elles vont en sens inverse. Autrement dit, quand les valeurs de *X* augmentent, celles de *Y* diminuent, et inversement. En guise d'exemple, pour les secteurs de recensement d'une métropole donnée, il est fort probable que le revenu médian des ménages soit associé négativement avec le taux de chômage. De nouveau, une droite résumerait efficacement cette relation.

* pour le cas **c**,  il y a une relation entre les deux variables, mais qui n’est pas linéaire. Le nuage de points entre les deux variables prend d’ailleurs une forme parabolique qui traduit une relation curvilinéaire. Concrètement, on observe une relation positive jusqu'à un certain seuil, puis une relation négative. 
 
* pour le cas **d**,  la relation entre les deux variables est aussi curvilinéaire; d'abord négative, puis positive.


```{r fig2, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Relations linéaires et curvilinéaires entre deux variables continues",  out.width='85%'}
knitr::include_graphics('images/bivariee/figure2.jpg', dpi = NA)
```

Prenons un exemple concret. Dans une étude portant sur l'équité environnementale et la végétation à Montréal, Pham *et al.* [-@PhamApparicioSeguin2012] ont montré qu'il existe une relation curvilinéaire entre l'âge médian des bâtiments résidentiels (axe des abscisses) et les couvertures végétales (axes des ordonnées) :

* la couverture de la végétation totale et celle des arbres augmentent quand l'âge médian des bâtiments croît jusqu'à atteindre un pic autour de 60 ans (autour de 1950). On peut supposer que les secteurs récemment construits, surtout ceux dans les banlieues, présentent des niveaux de végétation plus faibles. Au fur et au fur que le quartier vieillit, les arbres plantés lors du développement résidentiel deviennent matures — canopée plus importante –, d'où l'augmentation des valeurs de la couverture végétale totale et de celle des arbres.
*  Par contre, dans les secteurs développés avant les années 1950, la densité du bâti est plus forte, laissant ainsi moins de place pour la végétation, ce qui explique une diminution des variables relatives à la couverture végétale (figure \@ref(fig:fig3)).

```{r fig3, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Exemples de relations curvilinéaires",  out.width='65%'}
knitr::include_graphics('images/bivariee/figure3.jpg', dpi = NA)
```

Dans les sous-sections suivantes, nous décrirons deux statistiques descriptives et exploratoires – la covariance (section \@ref(sect0412)) et la corrélation (section \@ref(sect0413)) – utilisées pour évaluer la **relation linéaire** entre deux variables continues. Ces deux mesures permettent de mesurer le degré d'association entre deux variables, sans que l'une soit la variable dépendante (variable à expliquer) et l'autre, la variable dépendante (variable explicative). Puis, nous décrirons la régression linéaire simple (section \@ref(sect0414))  qui permet justement de prédire une variable dépendante (_Y_) à partir d'une variable indépendante (_X_).

### Covariance {#sect0412}

#### Formulation {#sect04121} 

La covariance\index{covariance} (eq. \@ref(eq:cov)), écrite $cov(x,y)$, est égale à la moyenne du produit des écarts des valeurs des deux variables par rapport à leurs moyennes respectives :


\begin{equation}\footnotesize 
cov(x,y) = \frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1} = \frac{covariation}{n-1}
(\#eq:cov)
\end{equation} 

avec $n$ étant le nombre d’observations; $\bar{x}$ et $\bar{y}$ (prononcez x et y barre) étant les moyennes respectives des variables *X* et *Y*.

#### Interprétation {#sect04122} 

Le numérateur de l'équation \@ref(eq:cov) représente la covariation\index{covariance}, soit la somme du produit des déviations des valeurs $x_{i}$ et $y_{i}$ par rapport à leurs moyennes respectives ($\bar{x}$ et $\bar{y}$). La covariance est donc la covariation divisée par le nombre d’observations, soit la moyenne de la covariation. Sa valeur peut être positive ou négative :  

* positive quand les deux variables varient dans le même sens, c'est-à-dire que lorsque les valeurs de la variable _X_ s'éloignent de la moyenne, les valeurs de _Y_ s'éloignent aussi dans le même sens; et négative pour une situation inverse. 
 * Quand la covariance est égale à 0, il n’y a pas de relation entre les variables _X_ et _Y_. Plus sa valeur absolue est élevée, plus la relation entre les deux variables *X* et *Y* est importante. 
 
Ainsi, la covariance\index{covariance} correspond à un centrage des variables, c’est-à-dire à soustraire à chaque valeur de la variable sa moyenne correspondante. L'inconvénient majeur de l'utilisation de la covariance est qu'elle est tributaire des unités de mesure des deux variables. Par exemple, si nous calculons la covariance entre le pourcentage de personnes à faible revenu et la densité de population (habitants au km^2^) au niveau des secteurs de recensement de la région métropolitaine de Montréal, nous obtenons une valeur de covariance de 34934. En revanche, si la densité de population est exprimée en milliers d'habitants au km^2^, la valeur de la covariance sera de 34,934, alors que la relation linéaire entre les deux variables reste la même tel qu'illustré à la figure \@ref(fig:fig4). Pour rémédier à ce problème, on privilégie l'utilisation du coefficient de corrélation.

```{r fig4, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Covariance et unités de mesure', out.width='75%'}
library("ggplot2")
library("ggpubr")
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
cov1 <- round(cov(df$FaibleRev,df$HabKm2),0)
cor1 <- round(cor(df$FaibleRev,df$HabKm2),3)
cov2 <- round(cov(df$FaibleRev,df$Hab1000Km2),3)
cor2 <- round(cor(df$FaibleRev,df$Hab1000Km2),3)
plot1 <- ggplot(data = df, mapping = aes(x=FaibleRev,y=HabKm2))+
  geom_point(colour="red")+
  labs(title=paste0("covariance = ", cov1), 
       subtitle = paste0("corrélation = ", cor1),
       caption = "Les traits pointillés indiquent les moyennes.")+
  xlab("Personnes à faible revenu (%)")+
  ylab(expression("Densité de population : habitants au"~km^{2}))+
  geom_vline(xintercept = mean(df$FaibleRev), colour="black", linetype="dashed", size=.5) +
  geom_hline(yintercept = mean(df$HabKm2), colour="black", linetype="dashed", size=.5) +
  stat_smooth(method="lm", se=FALSE)
plot2 <- ggplot(data = df, mapping = aes(x=FaibleRev,y=Hab1000Km2))+
  geom_point(colour="red")+
  labs(title=paste0("covariance = ", cov2), 
       subtitle = paste0("corrélation = ", cor2),
       caption = "Les traits pointillés indiquent les moyennes.")+
  xlab("Personnes à faible revenu (%)")+
  ylab(expression("Densité de population : 1000 habitants au"~km^{2}))+
  geom_vline(xintercept = mean(df$FaibleRev), colour="black", linetype="dashed", size=.5) +
  geom_hline(yintercept = mean(df$Hab1000Km2), colour="black", linetype="dashed", size=.5) +
  stat_smooth(method="lm", se=FALSE)
ggarrange(plot1, plot2, ncol = 2, nrow = 1)
```


### Corrélation {#sect0413}

#### Formulation {#sect04131} 
Le coefficient de corrélation de Pearson ($r$) est égal à la covariance (numérateur) divisée par le produit des écart-types des deux variables *X* et *Y* (dénominateur). Il représente une standardisation de la covariance. Autrement dit, le coefficient de corrélation repose sur un centrage (moyenne = 0) et une réduction (variance = 1) des deux variables, c’est-à-dire à soustraire à chaque valeur sa moyenne correspondante et à la diviser par son écart-type. Il correspond ainsi à la moyenne du produit des deux variables centrées réduites. Il s'écrit alors :

\begin{equation}\footnotesize 
r_{xy} = \frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{(n-1)\sqrt{\sum_{i=1}^n(x_i - \bar{x})^2(y_i - \bar{y})^2}}=\sum_{i=1}^n\frac{ZxZy}{n-1}
(\#eq:cor)
\end{equation}

La syntaxe  ci-dessous démontre que le coefficient de corrélation de Pearson est bien égal à la moyenne du produit de deux variables centrées-réduites.

```{r message=FALSE, warning=FALSE}
library("MASS")
N <- 1000      # nombre d'observations
moy_x <- 50   # moyenne de x
moy_y <- 40   # moyenne de y
sd_x <- 10    # écart-type de x
sd_y <- 8     # écart-type de y
rxy <- .80 # corrélation entre X et Y
## création de deux variables fictives normalement distribuées et corrélées entre elles
# Création d'une matrice de covariance
cov <- matrix(c(sd_x^2,  rxy*sd_x*sd_y, rxy*sd_x*sd_y, sd_y^2), nrow=2)
# Création du tableau de données avec deux variables
df <- as.data.frame(mvrnorm(N, c(moy_x, moy_y), cov))
# Centrage et réduction des deux variables
df$zV1 <- scale(df$V1, center = TRUE, scale = TRUE)
df$zV2 <- scale(df$V2, center = TRUE, scale = TRUE)
# Corrélation de Pearson
cor1 <- cor(df$V1, df$V2)
# Moyenne du produit des variables centrées-réduites
cor2 <- sum(df$zV1*df$zV2) / (nrow(df)-1)
cat("Corrélation de Pearson = ",round(cor1,5),
    "\nMoyenne du produit des variables centrées-réduites =", round(cor2,5))
```


#### Interprétation {#sect04132} 

Le coefficient de corrélation $r$ varie de −1 à 1 avec :

* 0 quand il n’y a pas de relation linéaire entre les variables _X_ et _Y_
* −1 quand il y relation linéaire négative parfaite
* et 1 quand il y a une relation linéaire positive parfaite. 


```{r fig5, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Relations entre deux variables continues et coefficients de corrélation de Pearson', out.width='75%'}
library("MASS")
library("ggplot2")
library("ggpubr")
N <- 1000      # nombre d'observations
moy_x <- 50   # moyenne de x
moy_y <- 40   # moyenne de y
sd_x <- 10    # écart-type de x
sd_y <- 8     # écart-type de y
rxy <- c(.90,-.85,0.01) # corrélation entre X et Y
 # Matrice de covariance
cov1 <- matrix(c(sd_x^2,  rxy[1]*sd_x*sd_y, rxy[1]*sd_x*sd_y, sd_y^2), nrow=2)
cov2 <- matrix(c(sd_x^2,  rxy[2]*sd_x*sd_y, rxy[2]*sd_x*sd_y, sd_y^2), nrow=2) 
cov3 <- matrix(c(sd_x^2,  rxy[3]*sd_x*sd_y, rxy[3]*sd_x*sd_y, sd_y^2), nrow=2) 
data1 <- mvrnorm(N, c(moy_x, moy_y), cov1)
data2 <- mvrnorm(N, c(moy_x, moy_y), cov2)
data3 <- mvrnorm(N, c(moy_x, moy_y), cov3)
plot1 <- ggplot(mapping = aes(x=data1[,1],y=data1[,2]))+
  geom_point(colour="red")+
  ggtitle("a. Relation linéaire positive", subtitle = paste0("Corrélation = ",round(cor(data1)[1,2],3)))+
  xlab("Variable X")+ylab("Variable Y")+
  stat_smooth(method="lm", se=FALSE)
plot2 <- ggplot(mapping = aes(x=data2[,1],y=data2[,2]))+
  geom_point(colour="red")+
  ggtitle("b. Relation linéaire négative", subtitle = paste0("Corrélation = ",round(cor(data2)[1,2],3)))+
  xlab("Variable X")+ylab("Variable Y")+
  stat_smooth(method="lm", se=FALSE)
plot3 <- ggplot(mapping = aes(x=data3[,1],y=data3[,2]))+
  geom_point(colour="red")+
  ggtitle("c. Absence de relation linéaire", subtitle = paste0("Corrélation = ",round(cor(data3)[1,2],3)))+
  xlab("Variable X")+ylab("Variable Y")+
  stat_smooth(method="lm", se=FALSE)
ggarrange(plot1, plot2, plot3, ncol = 2, nrow = 2)
```

Concrètement, le signe du coefficient de corrélation indique si la relation est positive ou négative et la valeur absolue du coefficient indique le degré d’association entre les deux variables. Reste à savoir comment déterminer qu’une valeur de corrélation est faible, moyenne ou forte. En sciences sociales, on utilise habituellement les intervalles de valeurs reportées au tableau \@ref(tab:tableIntervallesCorrelation). Toutefois, ces seuils sont tout à fait arbitraires. En effet, dépendamment de la discipline de recherche (sciences sociales, sciences de la santé, sciences physiques, etc.), et des variables à l’étude, l’interprétation d’une valeur de corrélation peut varier. Par exemple, en sciences sociales, une valeur de corrélation de 0,2 sera considérée comme très faible alors qu’en sciences de la santé, elle pourrait être considérée comme intéressante. À l’opposé, une valeur de 0,9 en sciences physiques pourrait être considérée comme faible. Il convient alors d’utiliser ces intervalles avec précaution.

```{r tableIntervallesCorrelation, echo=FALSE, message=FALSE, warning=FALSE}
df <- data.frame(
        Correlation = c("Faible","Moyenne", "Forte"), 
        Negative = c("de −0,3 à 0,0","de −0,5 à −0,3", "de −1,0 à −0,5"), 
        Positive = c("de 0,0 à 0,3","de 0,3 à 0,5", "de 0,5 à 1,0"))

show_table(df, 
           caption = 'Intervalles pour l’interprétation du coefficient de corrélation habituellement utilisés en sciences sociales',
           col.names = c("Corrélation","Négative","Positive"))
```

Le coefficient de corrélation mis au carré représente le coefficient de détermination et indique la proportion de la variance de la variable _Y_ expliquée par la variable _X_ et inversement. Par exemple, un coefficient de corrélation de −0,70 signale que 49% de la variance de la variable de _Y_ est expliquée par _X_ (figure \@ref(fig:fig6)).

```{r fig6, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Coefficient de corrélation et proportion de la variance expliquée', out.width='75%'}
library("ggplot2")
R <- seq(-1, 1, by=0.01)
R2 <- R^2
 ggplot(mapping = aes(y=R2,x=R)) +
   geom_rect(xmin=-1,xmax=-.5, ymin=0,ymax=1, size=0, fill="#91bfdb")+
   geom_rect(xmin=-.5,xmax=-.3, ymin=0,ymax=1, size=0, fill="#e0f3f8")+ 
   geom_rect(xmin=-.3,xmax=.3, ymin=0,ymax=1, size=0, fill="#ffffbf")+
   geom_rect(xmin=.3,xmax=.5, ymin=0,ymax=1, size=0, fill="#fee090")+  
   geom_rect(xmin=.5,xmax=1, ymin=0,ymax=1, size=0, fill="#fc8d59")+
   geom_point() +
   ggtitle("Corrélation et coefficient de détermination")+
   xlab("Corrélation de Pearson – R")+
   ylab(expression("Coefficient de détermination –"~ R^{2}))+
   geom_vline(xintercept = -1, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = -0.5, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = -0.3, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = 0.0, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = 0.3, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = 0.5, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = 1, colour="black", linetype="dashed", size=.5) +
   
   annotate(geom="text", x =-.75, y=0.8, label="Forte et \nnégative", color="black", hjust = 0.5, size = 3.5)+
   annotate(geom="text", x =.75, y=0.8, label="Forte et \npositive", color="black", hjust = 0.5, size = 3.5)+
   annotate(geom="text", x =-.4, y=0.8, label="Modérée \n et négative", color="black", hjust = 0.5, size = 3.5)+  
   annotate(geom="text", x =.4, y=0.8, label="Modérée \n et positive", color="black", hjust = 0.5, size = 3.5)+   
   annotate(geom="text", x =-.15, y=0.8, label="Faible et \n négative", color="black", hjust = 0.5, size = 3.5)+   
   annotate(geom="text", x =.15, y=0.8, label="Faible et \n positive", color="black", hjust = 0.5, size = 3.5)
```

**Condition d'application.** L'utilisation du coefficient de corrélation de Pearson nécessite que les deux variables continues soient normalement distribuées et qu'elles ne comprennent pas de valeurs aberrantes (extrêmes). D’ailleurs, plus le nombre d’observations sera réduit, plus la présence de valeurs aberrantes aura un impact important sur le résultat du coefficient de corrélation de Pearson. En guise d’exemple, dans le nuage de points à gauche de la figure \@ref(fig:fig7), il est possible d’identifier des valeurs extrêmes qui se démarquent nettement dans le jeu de données : six observations avec une densité de population supérieure à 20 000 habitants au km^2^ et deux observations avec un pourcentage de 65 ans et plus supérieur à 55%. Si l'on supprime ces observations (ce qui est défendable dans ce contexte) – soit moins d'un pourcent des observations du jeu de données initial –, la valeur du coefficient de corrélation passe de −0,158 à −0,194, signalant une augmentation du degré d'association entre les deux variables.

```{r fig7, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Illustation de l’effet des valeurs extrêmes sur le coefficient de Pearson', out.width='75%'}
library("ggplot2")
library("ggpubr")
library("moments")
df1 <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
cor1 <- cor(df1$HabKm2, df1$A65plus, method = "pearson")
df2 <- subset(df1, HabKm2 < 20000 & A65plus < 50)
cor2 <- cor(df2$HabKm2, df2$A65plus, method = "pearson")
plot1 <- ggplot(data=df1, mapping = aes(x=A65plus,HabKm2))+
  geom_point(colour="red")+
  ggtitle(paste0("N = ", nrow(df1)), subtitle = paste0("Corrélation = ",round(cor1,3)))+
  xlab("65 ans et plus (%)")+
  ylab(expression("Densité de population : 1000 habitants au"~km^{2}))+
  stat_smooth(method="lm", se=FALSE)+
  annotate(geom="text", x =60, y=50000, label=paste0("Skewness = ", round(skewness(df1$HabKm2),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=48000, label=paste0("Kurtosis = ", round(kurtosis(df1$HabKm2),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=46000, label=paste0("Shapiro = ", round(shapiro.test(df1$HabKm2)$statistic,3)), color="black", hjust = 1, size = 3.5)
plot2 <- ggplot(data=df2, mapping = aes(x=A65plus,HabKm2))+
  geom_point(colour="red")+
  ggtitle(paste0("N = ", nrow(df2)), subtitle = paste0("Corrélation = ",round(cor2,3)))+
  xlab("65 ans et plus (%)")+
  ylab(expression("Densité de population : 1000 habitants au"~km^{2}))+
  stat_smooth(method="lm", se=FALSE)+
  annotate(geom="text", x =60, y=20000, label=paste0("Skewness = ", round(skewness(df2$HabKm2),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=19200, label=paste0("Kurtosis = ", round(kurtosis(df2$HabKm2),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=18400, label=paste0("Shapiro = ", round(shapiro.test(df2$HabKm2)$statistic,3)), color="black", hjust = 1, size = 3.5)
ggarrange(plot1, plot2, ncol = 2, nrow = 1)
```


#### Corrélations pour des variables anormalement distribuées (coefficients de Spearman, Tau de kendall) {#sect04133} 

Lorsque les variables sont fortement anormalement distribuées, le coefficient de corrélation de Pearson est peu adapté pour analyser leurs relations linéaires. Il est alors conseillé d'utiliser deux statistiques non-paramétriques : principalement, le coefficient de corrélation de Spearman (_rho_) et secondairement, le coefficient de Kendall ($\tau$, prononcez Tau), qui varient aussi tous deux de −1 à 1. 
Calculé sur les rangs des deux variables, **le coefficient de Spearman** est le rapport entre la covariance des deux variables de rangs sur les écart-types des variables de rangs. En d'autres termes, il représente simplement le coefficient de Pearson calculé sur les rangs des deux variables :

\begin{equation}\footnotesize 
r_{xy} = \frac{cov(rg_{x},rg_{y})}{\sigma_{rg_{x}}\sigma_{rg_{y}}}
(\#eq:spearman)
\end{equation}

La syntaxe  ci-dessous démontre clairement que le coefficient de Spearman est bien le coefficient de Pearson calculé sur les rangs (\@ref(sect04131)).


```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
# Transformation des deux variables en rangs
df$HabKm2_rang <- rank(df$HabKm2)
df$A65plus_rang <- rank(df$A65plus)
# Coefficient de Spearman avec la fonction cor et la méthode spearman
cat("Coefficient de Spearman = ", 
    round(cor(df$HabKm2, df$A65plus, method = "spearman"),5))
# Coefficient de Pearson sur les variables transformées en rangs
cat("Coefficient de Pearson calculé sur les variables transformées en rangs = ", 
    round(cor(df$HabKm2_rang, df$A65plus_rang, method = "pearson"),5))
# Vérification avec l'équation
cat("Covariance divisée par le produit des écart-types sur les rangs :",
    round(cov(df$HabKm2_rang, df$A65plus_rang) / (sd(df$HabKm2_rang)*sd(df$A65plus_rang)),5))
```

Le **coefficient de Kendall** est une autre mesure non-paramétrique calculée comme suit :

\begin{equation}\footnotesize 
\tau = \frac{n_{c}-n_{d}}{\frac{1}{2}n(n-1)}
(\#eq:tau)
\end{equation}

avec $n_{c}$ et $n_{d}$ qui sont respectivement les nombres de paires d'observations **c**oncordantes et **d**iscordantes; et le dénominateur étant le nombre totale de paires d'observations. Des paires sont dites corcondantes quand les valeurs des deux observations vont dans les même sens pour les deux variables ($x_{i}>x_{j}$ et $y_{i}>y_{j}$ ou  $x_{i}<x_{j}$ et $y_{i}<y_{j}$), et discordantes quand elles vont en sens contraire ($x_{i}>x_{j}$ et $y_{i}<y_{j}$ ou $x_{i}<x_{j}$ et $y_{i}>y_{j}$). Contrairement au calcul du coefficient de Spearman, celui de Kendall peut être chronophage : plus le nombre d'observations sera élevé, plus les temps de calcul et la mémoire utilisée sont importants. En effet, avec _n_=1000, le nombre de paires d'observations (${0.5*n(n-1)}$) sera de 499500, contre près de 50 millions avec _n_=10000 (49 995 000).

```{r fig8, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Comparaison des coefficients de Pearson, Spearman et Kendall sur deux variables anormalement distribuées', out.width='75%'}
library("moments")
library("ggpubr")
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
df$HabKm2 <- df$HabKm2 / 1000
p <- round(cor(df$HabKm2, df$A65plus, method = "pearson"),3)
s <- round(cor(df$HabKm2, df$A65plus, method = "spearman"),3)
k <- round(cor(df$HabKm2, df$A65plus, method = "kendall"),3)
Plot1 <- ggplot(data =df, mapping = aes(x=HabKm2))+
  geom_histogram(color="white",fill="#B22222", aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$HabKm2), sd = sd(df$HabKm2)), color="blue",size=1.2)+
  labs(title="Histogramme")+
  xlab(expression("1000 habitants au"~km^{2}))+
  ylab("Densité")+
  annotate(geom="text", x =60, y=0.130, label=paste0("Skewness = ", round(skewness(df$HabKm2,na.rm=TRUE),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=0.124, label=paste0("Kurtosis = ", round(kurtosis(df$HabKm2,na.rm=TRUE),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=0.118, label=paste0("Shapiro = ", round(shapiro.test(df$HabKm2)$statistic,3)), color="black", hjust = 1, size = 3.5)
Plot2 <- ggplot(data = df, mapping = aes(x=A65plus))+
  geom_histogram(color="white",fill="#B22222", aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$A65plus), sd = sd(df$A65plus)), color="blue",size=1.2)+
  labs(title="Histogramme")+
  xlab("65 ans et plus (%)")+
  ylab("Densité")+
  annotate(geom="text", x =60, y=0.072, label=paste0("Skewness = ", round(skewness(df$A65plus,na.rm=TRUE),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=0.069, label=paste0("Kurtosis = ", round(kurtosis(df$A65plus,na.rm=TRUE),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=0.066, label=paste0("Shapiro = ", round(shapiro.test(df$A65plus)$statistic,3)), color="black", hjust = 1, size = 3.5)
Plot3 <- ggplot(data=df, mapping = aes(x=A65plus,HabKm2))+
  geom_point(colour="red")+
  labs(title="Nuage de points")+
  xlab("65 ans et plus (%)")+
  ylab(expression("1000 habitants au"~km^{2}))+
  stat_smooth(method="lm", se=FALSE, cor.coef = TRUE, cor.method = "pearson")+
  annotate(geom="text", x =60, y=50, label=paste0("Pearson = ", p), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=48, label=paste0("Spearman = ", s), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=46, label=paste0("Kendall = ", k), color="black", hjust = 1, size = 3.5)  
ggarrange(Plot1, Plot2, Plot3, ncol=3,nrow=1)
```

À la lecture des deux histogrammes ci-dessus, il est clair que les deux variables *densité de population* et *pourcentage de personnes ayant 65 ou plus* sont très anormalement distribuées. Dans ce contexte, l'utilisation du coefficient de Pearson peut nous amener à mésestimer la relation existant entre les deux variables. Notez que les coefficients de Spearman et de Kendall sont tous les deux plus faibles.

#### Corrélations robustes (*Biweight midcorrelation*, *Percentage bend correlation* et la corrélation *pi* de Shepherd) {#sect04134}

Dans l'exemple donné à la figure \@ref(fig:fig7), nous avions identifié des valeurs aberrantes que nous avons retirées du jeu de données. Cette pratique peut tout à fait se justifier quand les données sont erronées (un capteur de pollution renvoyant une valeur négative, un questionnaire rempli par un mauvais plaisantin, etc.), mais parfois, les cas extrêmes font partie du phénomène à analyser. Dans ce contexte, les identifier et les retirer peut paraître arbitraire. Une solution plus élégante est d'utiliser des méthodes dites **robustes**, c'est à dire moins sensibles aux valeurs extrêmes. Pour les corrélations, la *Biweight midcorrelation* [@wilcox1994percentage] est au coefficient de Pearson ce que la médiane est à la moyenne. Il est donc pertinent de l'utiliser dans des jeux de données présentant potentiellement des valeurs extrêmes. Elle est calculée comme suit : 

\begin{equation}\footnotesize
\begin{aligned}
&u_{i} = \frac{x_{i} - med(x)}{9 * (med(|x_{i} - med(x)|))} \text{ et } v_{i} = \frac{y_{i} - med(y)}{9 * (med(|y_{i} - med(y)|))}\\
&w_{i}^{(x)} = (1 - u_{i}^2)^2 I(1 - |u_{i}|) \text{ et } w_{i}^{(y)} = (1 - v_{i}^2)^2 I(1 - |v_{i}|)\\
&I(x) = 
\begin{cases}
1, \text{si} x = 1\\
0, \text{sinon}
\end{cases}\\
&\tilde{x}_{i} = \frac{(x_{i} - med(x))w_{i}^{(x)}}{\sqrt{(\sum_{j=1}^m)[(x_{j} - med(x))w_{j}^{(x)}]^2}} \text{ et } \tilde{y}_{i} = \frac{(y_{i} - med(y))w_{i}^{(y)}}{\sqrt{(\sum_{j=1}^m)[(y_{j} - med(y))w_{j}^{(y)}]^2}}\\
&bicor(x,y) = \sum_{i=1}^m \tilde{x_i}\tilde{y_i}
\end{aligned}
(\#eq:bicor)
\end{equation}

Comme le souligne l'équation \@ref(eq:bicor), la *Biweight midcorrelation* est basée sur les écarts à la médiane, plutôt que sur les écarts à la moyenne.

Assez proche de la *Biweight midcorrelation*, la *Percentage bend correlation* se base également sur la médiane des variables *X* et *Y*. Le principe général est de donner un poids plus faible dans le calcul de cette corrélation à un certain pourcentage des observations (20% est généralement recommandé) dont la valeur est éloignée de la médiane. Pour une description complète de la méthode, vous pourrez lire l'article de @wilcox1994percentage.

Enfin, une autre option est l'utilisation de la corrélation $pi$ de Sherphred [@Schwarzkopf2012]. Il s'agit simplement d'une méthode en deux étapes. Premièrement, les valeurs abberantes sont identifiées à l'aide d'une approche par *bootstrap* utilisant la distance de Mahalanobis (calculant les écarts multivariés entre les observations). Ensuite, le coefficient de *Spearman* est calculé sur les observations restantes.

Appliquons ces corrélations aux données précédentes. Notez que ce simple code d'une dizaine de lignes permet d'explorer rapidement la corrélation entre deux variables selon six mesures de corrélations.

```{r robcorr, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}
library("correlation")
df1 <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
methods <- c("pearson","spearman","biweight","percentage","shepherd")
rs <- lapply(methods,function(m){
  test <- correlation::cor_test(data = df1, x="Hab1000Km2",y="A65plus",method = m, ci=0.95)
  return(c(test$r,test$CI_low, test$CI_high))
  })
dfCorr <- data.frame(do.call(rbind,rs))
names(dfCorr) <- c("r","IC_05","CI_95")
dfCorr$method <- methods

show_table(dfCorr,
           digits = 2,
            caption = 'Comparaison de différentes corrélations pour les variables densité de population et pourcentage de personnes ayant 65 ans et plus',
           col.names=c("r","IC 5%","IC 95%", "Méthode")
           )
```

Il est intéressant de mentionner que ces trois corrélations sont rarement utilisées malgré leur pertinence dans de nombreux cas d'application. Nous faisons face ici à un cercle vicieux dans la recherche : les méthodes les plus connues sont les plus utilisées car elles sont plus facilement acceptées par les autres chercheurs. Des méthodes plus élaborées nécessitent davantage de justification et de discussion, ce qui peut conduire à de multiples sessions de corrections/resoumissions pour qu'un article soit accepté, malgré le fait qu'elles puissent être plus adaptées au jeu de données à l'étude.

#### Significativité des coefficients de corrélation {#sect04135} 

Quelle que soit la méthode utilisée, il convient de vérifier si le coefficient de corrélation est ou non statistiquement différent de 0. En effet, nous travaillons la plupart du temps avec des données d'échantillonage, et très rarement avec des populations complètes. En collectant un nouvel échantillon, aurions-nous obtenu des résultats différents ? Le calcul de ce degré de significativité nous permet de quantifier notre niveau de certitude quant à l'existance d'une corrélation entre nos deux variables, positive ou négative. Cet objectif est réalisé en calculant la valeur de _t_ et le nombre de degrés de liberté : $t=\sqrt{\frac{n-2}{1-r^2}}$ et $dl = n-2$ avec $r$ et $n$ étant le coefficient de corrélation et le nombre d'observations. De manière classique, on utilisera la table des valeurs critiques de la distribution de $t$ : si la valeur de $t$ est supérieure à la valeur critique (avec  _p_ = 0,05 et le nombre de degré de liberté), alors le coefficient est significatif à 5%. En d'autres termes, si la vraie corrélation entre les deux variables (calculable uniquement à partir des populations complètes) était 0, alors la probabilité de collecter notre échantillon aurait été inférieure à 5%. Dans ce contexte, on peut raisonnablement rejeter l'hypothèse nulle (corrélation réelle de 0). 

La courte syntaxe  illustre comment calculer la valeur de $t$, le nombre de degrés de liberté et la valeur de _p_ pour une corrélation donnée.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
r <- cor(df$A65plus, df$LogTailInc)     # Corrélation
n <- nrow(df)                           # Nombre d'observations
dl <- nrow(df)-2                        # degrés de liberté
t <-  r*sqrt((n-2)/(1-r^2))             # Valeur de T
p <- 2*(1-pt(abs(t),dl))                # Valeur de p
cat("\nCorrélation =", round(r, 4),       
    "\nValeur de t =", round(t, 4),
    "\nDegrés de liberté =", dl,
    "\np=", round(p, 4))        
```

Plus simplement, la fonction `cor.test` permet d'obtenir en une seule ligne de code le coefficient de corrélation, l'intervalle de confiance à 95% et les valeurs de _t_ et de _p_, tel qu'illustré dans la syntaxe  ci-dessous. Si l'intervalle de confiance est à cheval sur 0, c'est-à-dire que la borne inférieure est négative et la borne supérieure positive, alors le coefficient de corrélation n'est pas significatif au seuil choisi (95% habituellement). Dans l'exemple ci-dessous, le relation linéaire entre les deux variables est significativement négative avec une corrélation de Pearson de −0,158 (P=0,000) et un intervalle de confiance de [−0,219 −0,095].

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Intervalle de confiance à 95%
cor.test(df$HabKm2, df$A65plus, conf.level = .95)
# Vous pouvez accéder à chaque sortie de la fonction cor.test comme suit :
p <- cor.test(df$HabKm2, df$A65plus)
cat("Valeur de corrélation = ", round(p$estimate,3), "\n",
    "Intervalle à 95% = [", round(p$conf.int[1],3), " ", round(p$conf.int[2],3), "]", "\n",
    "Valeur de t = ", round(p$statistic,3), "\n",
    "Valeur de p = ", round(p$p.value,3), sep="")
# Corrélation de Spearman
cor.test(df$HabKm2, df$A65plus, method = "spearman")
# Corrélation de Kendall
cor.test(df$HabKm2, df$A65plus, method="kendall")
```

On pourra aussi modifier l'intervalle de confiance, par exemple à 90% ou 99%. Le choix de l'intervalle de confiance et du seuil de significativité doivent être définis avant l'étude. Il doit s'appuyer sur les standards de la littérature du domaine étudié, du niveau de preuve attendu et de la quantité de données.
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Intervalle à 90%
cor.test(df$HabKm2, df$A65plus, method ="pearson", conf.level = .90)
# Intervalle à 99%
cor.test(df$HabKm2, df$A65plus, method ="pearson", conf.level = .99)
```

**Corrélation et _bootstrap_.** Dans le premier chapitre (LIEN BOOTSTRAP), nous avons abordé la notion de _bootstrap_, soit des méthodes d'inférence statistique basées sur des réplications des données initiales par rééchantillonnage. Il est possible d'appliquer la même méthode aux corrélations afin d'obtenir un intervale de confiance avec _r_ réplications, tel qu'illustré à partir de la syntaxe  ci-dessous.


```{r fig9, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Histogramme pour les valeurs de corrélation issues du Bootstrap', out.width='75%'}
library("boot")
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
# Fonction pour la corrélation
correlation <- function(df, i, X, Y, cor.type="pearson"){
  # Paramètres de la fonction :
  # data : dataframe
  # X et Y : noms des variables X et Y
  # cor.type : type de corrélation : c("pearson","spearman","kendall")
  # i : indice qui sera utilisé par les réplications (à ne pas modifier)
  cor(df[[X]][i], df[[Y]][i], method=cor.type)
}
# Calcul du Bootstrap avec 5000 réplications
corBootstraped <- boot(data=df, # nom du tableau
                     statistic = correlation, # appel de la fonction à répliquer 
                     R=5000, # nombre de réplications
                     X = "A65plus",
                     Y ="HabKm2", 
                     cor.type="pearson")
# Histogramme pour les valeurs de corrélation issues du Bootstrap
plot(corBootstraped)
# Corrélation "bootstrapée"
corBootstraped
# Intervalle de confiance du bootstrap à 95%
boot.ci(boot.out = corBootstraped, conf = 0.95, type = "all")
# Comparaison de l'intervalle classique basé sur la valeur de T
p <- cor.test(df$HabKm2, df$A65plus)
cat(round(p$estimate,5), " [", round(p$conf.int[1],4), " ",round(p$conf.int[2],4), "]", sep="")
```

Le _bootstrap_ renvoie un coefficient de corrélation de Pearson de −0,158. Les intervalles de confiance obtenues à partir des différentes méthodes d'estimation (normale, basique, pourcentage et bca) ne sont pas à cheval sur 0, indiquant que le coefficient est significatif à 5%.

#### Corrélation partielle {#sect04136} 

::: {.bloc_objectif data-latex=""}

**Quelle est la relation entre deux variables continues une fois pris en compte une ou plusieurs variables dites de contrôle ?** En études urbaines, on pourrait vouloir vérifier si deux variables sont ou non associées une fois contrôlée la densité de population ou encore la distance au centre-ville.

La corrélation partielle\index{corrélation partielle} permet d'évaluer la relation linéaire entre deux variables quantitatives continues, une fois contrôlé une ou plusieurs autres variables quantitatives (dites variables de contrôle).


:::

Le coefficient de corrélation partielle peut être calculé pour les trois méthodes (Pearson, Spearman et Kendall). Variant aussi de −1 à 1, il est calculé comme suit :

\begin{equation}\footnotesize 
r_{ABC} = \frac{r_{AB}-r_{AC}r_{BC}}{\sqrt{(1-r_{AC}^2)(1-r_{BC}^2)}}
(\#eq:corpartielle)
\end{equation}

avec _A_ et _B_ étant les deux variables pour lesquelles on souhaite évaluer la relation linéaire, une fois contrôlée la variable _C_; $r$ étant le coefficient de corrélation (Pearson, Spearman ou Kendall) entre deux variables.

Dans l'exemple ci-dessous, nous voulons estimer la relation linéaire entre le pourcentage de personnes à faible revenu et la couverture végétale au niveau des îlots de l'île de Montréal, une fois contrôlée la densité de population. En effet, plus cette dernière sera forte, plus la couverture végétale sera faible ($r$ de Pearson = −0,603). La valeur du $r$ de Pearson s'élève à −0,546 entre le pourcentage de personnes à faible revenu dans la population totale de l'îlot et la couverture végétale. Une fois la densité de population contrôlée, il chute à −0,316. Pour calculer la corrélation partielle, on pourra utiliser la fonction `pcor.test` du package **ppcor**.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("foreign")
library("ppcor")
dfveg <- read.dbf("data/bivariee/IlotsVeg2006.dbf")
# Corrélation entre les trois variables
round(cor(dfveg[, c("VegPct", "Pct_FR","LogDens")], method="p"), 3)
# Corrélation partielle entre :
# la couverture végétale de l'îlot (%) et
# le pourcentage de personnes à faible revenu
# une fois contrôlée la densité de population
pcor.test(dfveg$Pct_FR, dfveg$VegPct, dfveg$LogDens, method="p")
# Calcul de la corrélation partielle avec la formule :
corAB <- cor(dfveg$VegPct, dfveg$Pct_FR, method = "p")
corAC <- cor(dfveg$VegPct, dfveg$LogDens, method = "p")
corBC <- cor(dfveg$Pct_FR, dfveg$LogDens, method = "p")
CorP  <- (corAB - (corAC*corBC)) / sqrt((1-corAC^2)*(1-corBC^2))
cat("Corr. partielle avec ppcor  = ", 
    round(pcor.test(dfveg$Pct_FR,  dfveg$VegPct, dfveg$LogDens, method="p")$estimate,5),
    "\nCorr. partielle (formule)  = ", round(CorP, 5))
```

#### Mise en œuvre dans R {#sect04137}

Comme vous l'aurez compris, il est possible d'arriver au même résultat dans  par différents moyens. Pour calculer les corrélations, nous avons utilisé jusqu'à présent les fonctions de base `cor` et `cor.test` . Il est aussi possible de recourir à des fonctions d'autres _packages_, dont notamment :

* **Hmisc** dont la fonction `rcorr` permet de calculer des corrélations de Pearson et Spearman (mais non celle de Kendall) avec la valeur de _p_.
* **psych** dont la fonction `corr.test` permet d'obtenir la matrice de corrélation (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de _p_.
* **stargazer** pour créer des beaux tableaux d'une matrice de corrélation en *Html* ou en *LaTeX* ou en ASCII.
* **apaTables**  pour créer un tableau avec une matrice de corrélation dans un fichier Word.
* **correlation** pour aller plus loin et explorer les corrélations bayesiennes, robustes, non-linéaires ou multiniveaux.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
library("Hmisc")
library("stargazer")
library("apaTables")
library("dplyr")
# Corrélations de Pearson et Spearman et valeurs de p 
# avec la fonction rcorr de Hmisc pour deux variables
Hmisc::rcorr(df$RevMedMen, df$Locataire, type="pearson")
Hmisc::rcorr(df$RevMedMen, df$Locataire, type="spearman")
# Matrice de corrélation avec la fonction rcorr de Hmisc pour plus de variables
# On crée un vecteur avec les noms des variables à sélectionner
Vars <- c("RevMedMen","Locataire", "LogTailInc","A65plus","ImgRec", "HabKm2", "FaibleRev")
Hmisc::rcorr(df[, Vars] %>% as.matrix())
# # Avec la fonction corr.test de psych pour avoir la matrice de corrélation
# # (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de p
# print(psych::corr.test(df[, Vars], 
#              method = "kendall", 
#              ci=TRUE, alpha = 0.05), short=FALSE) 
# Création d'un tableau pour une matrice de corrélation
# changer le paramètre type pour 'html' or 'LaTex'
p <- cor(df[, Vars], method="pearson")
stargazer(p, title="Correlation Matrix", type = "text")
# stargazer(p, title="Correlation Matrix", type = "html")
# stargazer(p, title="Correlation Matrix", type = "latex")
# Créer un tableau avec la matrice de corrélation 
# dans un fichier Word (.doc)
apaTables::apa.cor.table(df[, c("RevMedMen","Locataire","LogTailInc")], 
                         filename = "data/bivariee/TitiLaMatrice.doc",
                         show.conf.interval = TRUE,
                         landscape = TRUE)
```


::: {.bloc_astuce data-latex=""}
**Une image vaut mille mots, surtout pour une matrice de corrélation !** Le package **corrplot** vous permet justement de construire de belles figures avec une matrice de corrélation (figures \@ref(fig:figcorrplot1) et \@ref(fig:figcorrplot2)). L'intérêt de ce type de figure est de repérer rapidemment des associations intéressantes lorsque l'on calcule les corrélations entre un grand nombre de variables.
:::

```{r figcorrplot1, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Matrice de corrélation avec corrplot (chiffres)', out.width='60%'}
library("corrplot")
library("ggpubr")
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
Vars <- c("RevMedMen","Locataire", "LogTailInc","A65plus","ImgRec", "HabKm2", "FaibleRev")
p <- cor(df[, Vars], method="pearson")
couleurs <- colorRampPalette(c("#053061", "#2166AC","#4393C3", "#92C5DE",
                               "#D1E5F0", "#FFFFFF", "#FDDBC7", "#F4A582",
                               "#D6604D", "#B2182B", "#67001F"))
corrplot(p, addrect = 3, method="number", diag=FALSE, col=couleurs(100))
```

```{r figcorrplot2, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Matrice de corrélation avec corrplot (chiffres et ellipses)', out.width='60%'}
fig2 <- corrplot.mixed(p, lower="number", lower.col = "black", 
                      upper = "ellipse", upper.col=couleurs(100))
```


#### Comment rapporter des valeurs de corrélations? {#sect04138}

Bien qu'il n'y ait pas qu'une seule manière de reporter des corrélations, voici quelques lignes directrices pour vous guider : 

* Signaler si la corrélation est faible, modérée ou forte.
* Indiquer si la corrélation est positive ou négative. Toutefois, ce n'est pas une obligation car l'on peut rapidement le constater avec le signe du coefficient.
* Mettre le *r* et le *p* en italique et en minuscules.
* Deux décimales uniquement pour le $r$ (sauf si une plus grande précision se justifie dans le domaine d'étude).
* Trois décimales pour la valeur de *p*. Si elle est inférieure à 0,001, écrire plutôt *p* < 0,001.
* Indiquer éventuellement le nombre de degrés de liberté, soit $r(dl)=...$

Voici des exemples :

* La corrélation entre les variables revenu médian des ménages et pourcentage de locataire est fortement négative (*r* = −0,78, *p* < 0,001).
* La corrélation entre les variables revenu médian des ménages et pourcentage de locataire est forte (*r*(949) = −0,78, *p* < 0,001).
* La corrélation entre les variables densité de population et pourcentage de logements de taille est modérée (*r* = 0,48, *p* < 0,001).
* La corrélation entre les variables densité de population et pourcentage de 65 ans et plus n'est pas significative (*r* = −0,08, *p* = 0,07).

Pour un texte en anglais, référez-vous à : [https://www.socscistatistics.com/tutorials/correlation/default.aspx](https://www.socscistatistics.com/tutorials/correlation/default.aspx){target="_blank"}.

### Régression linéaire simple  {#sect0414}

::: {.bloc_objectif data-latex=""}
**Comment expliquer et prédire une variable continue en fonction d'une autre variable?** Répondre à cette question relève de la statistique inférentielle. Il s'agit en effet d'établir une équation simple du type $Y = a + bX$, pour expliquer et prédire les valeurs d'une variable dépendante (*Y*) à partir d'une variable indépendante (*X*). L'équation de la régression est construite grâce à un jeu de données (un échantillon). À partir de cette équation, il est possible de prédire la valeur attendue de *Y* pour n'importe quelle valeur de *X*. On appelle cette équation un modèle, car elle cherche à représenter la réalité de façon simplifiée.

La régression linéaire simple se distingue ainsi de la **covariance** (section \@ref(sect0412)) et de la **corrélation** (section \@ref(sect0413)), relevant de la statistique bivariée descriptive et exploratoire. 

Par exemple, la régression linéaire simple pourrait être utilisée pour expliquer les notes d'un groupe d'étudiants à un examen (variable dépendante *Y*) en fonction du nombre d'heures qu'ils ont consacrés à la révision des notes de cours (variable indépendante *X*). Une fois l'équation de régression déterminée et si le modèle est efficace, nous pourrons prédire les notes des étudiants inscrits au cours la session suivante en fonction du temps qu'ils prévoient de passer à étudier, et ce, avant même qu'ils aient passé l'examen. 

Formulons un exemple d'application de la régression linéaire simple en études urbaines. Dans le cadre d'une étude sur les îlots de chaleur urbains, la température de surface (variable dépendante) pourrait être expliquée par la proportion de la superficie de l'îlot couverte par de la végétation (variable indépendante). On supposerait alors que plus cette proportion est importante, plus la température est faible et inversement, soit une relation linéaire négative. Si le modèle est efficace, nous pourrions prédire la température moyenne des îlots d'une autre municipalité pour laquelle nous ne disposons pas d'une carte de température, et repérer ainsi les îlots de chaleur potentiels. Bien entendu, il est peu probable que nous arrivions à prédire efficacement la température moyenne des îlots avec uniquement la couverture végétale comme variable explicative. En effet, bien d'autres caractéristiques de la forme urbaine peuvent influencer ce phénomène comme la densité du bâti, la couleur des toits, les occupations du sol présentes, l'effet des canyons urbains, etc. Il faudrait alors inclure non pas une, mais plusieurs variables explicatives (indépendantes).

Ainsi, on distinguera la **régression linéaire simple** (une variable indépendante, explicative) de la **régression linéaire multiple** (plusieurs variables indépendantes); cette dernière sera largement abordée au chapitre \@ref(chap05).
:::

Dans cette section, nous décrirons succinctement la régression linéaire simple. Concrètement, nous verrons comment déterminer la droite de régression, interpréter ses différents paramètres du modèle et comment évaluer la qualité d'ajustement du modèle. Nous n'aborderons pas les hypothèses liées au modèle de régression linéaire des moindres carrés ordinaires (MCO), ni les conditions d'application. Ces éléments seront expliqués au chapitre \@ref(chap05) consacré à la régression linéaire multiple.

::: {.bloc_attention data-latex=""}
**Corrélation, régression simple et causalité : attention aux raccourcis !**

Si une variable *X* explique et prédit efficacement une variable *Y*, cela ne veut pas dire pour autant que *X* cause *Y*. Autrement dit, la corrélation, l'association entre deux variables ne signifie qu'il existe un lien de causalité entre elles.

Premièrement, la variable explicative (*X*, indépendante) doit absolument précéder la variable à expliquer (*Y*, dépendante). Par exemple, l'âge (*X*) peut influencer le sentiment de sécurité (*Y*). Mais, le sentiment de sécurité ne peut en aucun cas influencer l'âge. Par conséquent, l'âge ne peut conceptuellement pas être la variable dépendante dans cette relation.

Deuxièmement, bien qu'une variable puisse expliquer efficacement une autre variable, elle peut être un **facteur confondant**. Prenons deux exemples bien connus :

* Avoir les doigts jaunes est associé au cancer du poumon. Bien entendu, les doigts jaunes ne causent pas le cancer : c'est un facteur confondant puisque fumer augmente les risques du cancer du poumon et jaunit aussi les doigts.

* Dans un article intitulé *Chocolate Consumption, Cognitive Function, and Nobel Laureates*, Messerli [-@Messerli] a trouvé une corrélation positive entre la consommation de chocolat par habitant et le nombre de prix Nobel pour dix millions d'habitants pour 23 pays. Ce résultat a d'ailleurs été rapporté par de nombreux médias ([Radio Canada](https://ici.radio-canada.ca/nouvelle/582457/chocolat-consommateurs-nobels), [La Presse]("https://www.lapresse.ca/vivre/sante/nutrition/201210/11/01-4582347-etude-plus-un-pays-mange-de-chocolat-plus-il-a-de-prix-nobel.php"), [Le Point](https://www.lepoint.fr/insolite/le-chocolat-dope-aussi-l-obtention-de-prix-nobel-12-10-2012-1516159_48.php), etc.), sans pour autant que Messerli [-@Messerli] et les journalistes concluent à un lien de causalité entre les deux variables. Tout chercheur sait que la consommation de chocolat ne permet pas d'obtenir des résultats intéressants et de publier dans des revues prestigieuses; c'est plutôt le café ! Plus sérieusement, il est probable que les pays les plus riches investissent davantage dans la recherche et obtiennent ainsi plus de prix Nobel. Dans les pays les plus riches, il est aussi probable que l'on consomme plus de chocolat, considéré comme un produit de luxe dans les pays les plus pauvres.

Pour approfondir le sujet sur la confusion entre *corrélation, régression simple et causalité*, vous pourrez visionner cette courte [vidéo ludique](https://www.youtube.com/embed/A-_naeATJ6o) de vulgarisation.

L'association entre deux variables peut aussi être simplement le fruit du hasard. Si l'on explore de très grandes quantités de données (avec un nombre impressionnant d'observations et de variables), soit une démarche relevant du *data mining*, le hasard fera que l'on risque d'obtenir des corrélations surprenantes entre certaines variables. Prenons un exemple concret, admettons que l'on ait collecté 100 variables et que l'on calcule les corrélations entre chaque paire de variables. On obtient une matrice de corrélation de 100 x 100, à laquelle on peut enlever la diagonale et une moitié de la matrice, ce qui nous laisse un total de 4950 corrélations différentes. Admettons que l'on choisisse un seuil de significativité de 5%, on doit alors s'attendre à ce que le hasard produise des résultats significatifs dans 5% des cas. Sur 4950 corrélations, cela signifie qu'environ 247 corrélations seront significatives, et ce, indépendamment de la nature des données. Nous pouvons aisément l'illustrer avec la syntaxe  suivante :

```{r corraleatoire, message=FALSE, warning=FALSE, out.width='60%', eval = FALSE}
library("Hmisc")
nbVars <- 100 # nous utilisons 100 variables générées aléatoirement pour l'expérience
nbExperiment <- 1000 # nous reproduirons 1000 fois l'expérience avec les 100 variables
# Le nombre de variables significatives par expérience est enregistrée dans Results
Results <- c()
# iterons pour chaque expérimentation (1000 fois)
for(i in 1:nbExperiment){
  Datas <- list()
  # générons 100 variables aléatoires normalement distribuées
  for (j in 1:nbVars){
    Datas[[j]] <- rnorm(150)
  }
  DF <- do.call("cbind",Datas)
  # calculons la matrice de corrélation pour les 100 variables
  cor_mat <- rcorr(DF)
  # comptons combien de fois les corrélations étaient significatives
  Sign <- table(cor_mat$P<0.05)
  NbPairs <- Sign[["TRUE"]]/2
  # ajoutons les résultats dans Results
  Results <- c(Results,NbPairs)
}
# transformons Results en un dataframe
df <- data.frame(Values = Results)
# affichons le résultat
ggplot(df, aes(x = Values)) + 
  geom_histogram(aes(y =..density..), 
                 colour = "black", 
                 fill = "white") +
  stat_function(fun = dnorm, args = list(mean = mean(df$Values), 
                sd = sd(df$Values)),color="blue")+
  geom_vline(xintercept = mean(df$Values),color="red", size=1.2)+
  annotate("text", x=250, y = 0.028, 
           label = paste("Nombre moyen de corrélations significatives\n 
                         sur 1000 replications :",
           round(mean(df$Values),0),sep=""), hjust="left")+
  xlab("Nombre de corrélations significatives")+
  ylab("densité")
```

```{r replicationhist, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Corrélations significatives obtenues aléatoirement",  out.width='80%', fig.pos = "H", out.extra = ""}
knitr::include_graphics('images/bivariee/replication_hist.PNG', dpi = NA)
```

:::

#### Principe de base de la régression linéaire simple {#sect04141}
La régression linéaire simple vise à déterminer une droite (une fonction linéaire) qui résume le mieux la relation linéaire entre une variable dépendante (*Y*) et une variable indépendante (*X*) :
\begin{equation}\footnotesize 
\widehat{y_i} = \beta_{0} + \beta_{1}x_{i}
(\#eq:regsimple)
\end{equation}

avec $\widehat{y_i}$ et $x_{i}$ qui sont respectivement la valeur prédite de la variable indépendante et la valeur de la variable dépendante pour l'observation $i$. $\beta_{0}$ est la constante (*intercept* en anglais), soit la valeur prédite de la variable $Y$ quand $X$ est égale à 0. $\beta_{1}$ est le coefficient de régression pour la variable *X*, soit la pente de la droite. Ce coefficient nous informe sur la relation entre les deux variables : s’il est positif, la relation est positive; s’il est négatif, la relation est négative, et proche de 0, la relation est nulle (la droite sera alors horizontale). Plus la valeur absolue de $\beta_{1}$ est élevée, plus la pente est forte, et plus la variable *Y* varie à chaque changement d’une unité de la variable *X*.

Considérons un exemple fictif de dix municipalités d'une région métropolitaine pour lesquelles nous disposons de deux variables : le pourcentage d'actifs occupés se rendant au travail principalement à vélo et la distance de la municipalité au centre-ville (tableau \@ref(tab:regfictives)).


```{r regfictives, echo=FALSE, message=FALSE, warning=FALSE, out.width='85%'}
df <- read.csv("data/bivariee/Reg.csv", stringsAsFactors = F)
df1 <- df[1:5,]
df2 <- df[6:10,]

show_table(list(df1, df2),
           col.names = c("Municipalité","Vélo","KMCV"),
           caption = "Données fictives sur l'utilisation du vélo par municipalité"
           )
```

D'emblée, à la lecture du nuage de points (figure \@ref(fig:figreg)), on décèle une forte relation linéaire négative entre les deux variables : plus la distance au centre-ville augmente, plus le pourcentage de cyclistes est faible, ce qui est confirmé par le coefficient de corrélation (*r* = −0,86). La droite de régression (en rouge à la figure \@ref(fig:figreg)) qui résume le mieux la relation entre `Vélo` (variable dépendante) et `KmCV` (variable indépendante) s'écrit alors : **Vélo = 30,603 − 1,448 x KmCV**.

La valeur du coefficient de régression ($\beta_{1}$) est de −1,448. Le signe de ce coefficient décrit une relation négative entre les deux variables. Ainsi, à chaque ajout d'une unité de la distance au centre-ville (exprimée en kilomètres), le pourcentage de cyclistes diminue de 1,448. Retenez que l'unité de mesure de la variable dépendante est très importante pour bien interpréter le coefficient de régression. En effet, si la distance au centre-ville n'était pas exprimée en kilomètres, mais plutôt en mètres, $\beta_1$ sera égal à −0,001448. Dans la même optique, l'ajout de 10 km de distance entre une municipalité et le centre-ville fait diminuer le pourcentage de cyclistes de −14,48 points de pourcentage.

Avec, cette équation de régression, il est possible de prédire le pourcentage de cyclistes pour n'importe quelle municipalité de la région métropolitaine. Par exemple, pour des distances de 5, 10 ou 20 kilomètres, les pourcentages de cyclistes seraient de :

* $\widehat{y_i} = 30,603 + (-1,448 \times 5 km) = 23,363$
* $\widehat{y_i} = 30,603 + (-1,448 \times 10km) = 8,883$
* $\widehat{y_i} = 30,603 + (-1,448 \times 20km) = 1,643$

```{r figreg, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Relation linéaire entre l'utilisation du vélo et la distance au centre-ville", out.width='65%'}
library("ggplot2")
modele <- lm(Velo~KmCV, df)
b0 <- round(modele$coefficients[1],3)
b1 <- round(modele$coefficients[2],3)
eq <- paste0("Vélo = ", b0, " ", b1, " x KmCV")
r2 <- round(summary(modele)$r.squared,3)
pearson <- round(cor(df$Velo, df$KmCV),3)
ggplot(df, aes(x=KmCV, y=Velo)) +
  stat_smooth(method="lm", se=FALSE, col="red")+
  labs(title= eq,
     subtitle = paste0("r2 = ", r2, ", r =", pearson), sep="")+
  xlab("Distance au centre-ville (km)")+
  ylab("Vélo (%)")+
  geom_point(colour="black", size=5)
```  


#### Formulation de la droite de régression des moindres carrés ordinaires {#sect04142}
Reste à savoir comment sont estimés les différents paramètres de l'équation, soit $\beta_0$ et $\beta_1$. À la figure \@ref(fig:figreg2), les points noirs représentent les valeurs observées ($y_i$) et les points bleus les valeurs prédites ($\widehat{y_i}$) par l'équation du modèle. Les traits noirs verticaux représentent pour chaque observation $i$, l'écart entre la valeur observée et la valeur prédite, dénommé résidu ($\epsilon_i$, prononcez epsilon de _i_ ou plus simplement le résidu pour _i_, ou encore le terme d'erreur de _i_). Si un point est au-dessus de la droite de régression, la valeur observée sera alors supérieure à la valeur prédite ($y_i > \widehat{y_i}$) et inversement, si le point est au-dessous de la droite ($y_i < \widehat{y_i}$). Plus cet écart ($\epsilon_i$) est important, plus l'observation s'éloigne de la prédiction du modèle, et par extension moins bon est le modèle. Au tableau \@ref(tab:regfictives2), vous constaterez que la somme des résidus est égale à zéro. La méthode des moindres carrés ordinaires (MCO) vise à minimiser les écarts au carré entre les valeurs observées ($y_i$) et prédites ($\beta_0+\beta_1 x_i$, soit $\widehat{y_i}$) :

\begin{equation}\footnotesize 
min\sum_{i=1}^n{(y_i-(\beta_0+\beta_1 x_i))^2}
(\#eq:mco)
\end{equation}

Pour minimiser ces écarts, le coefficient de régression $\beta_1$ représente le rapport entre la covariance entre *X* et *Y* et la variance de *Y* (équation \@ref(eq:b1)), tandis que la constante $\beta_0$ est la moyenne de la variable *Y* moins le produit de la moyenne de *X* et de son coefficient de régression (équation \@ref(eq:b0)).

\begin{equation}\footnotesize 
\beta_1 = \frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2} = \frac{cov(X,Y)}{var(X)}
(\#eq:b1)
\end{equation}

\begin{equation}\footnotesize 
\beta_0 = \widehat{Y}-\beta_1 \widehat{X}
(\#eq:b0)
\end{equation}

```{r figreg2, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Droite de régression, valeurs observées, prédites et résidus", out.width='65%'}
df <- read.csv("data/bivariee/Reg.csv", stringsAsFactors = F)
modele <- lm(Velo~KmCV, df)
df$ypredit <- round(modele$fitted.values,3)
df$residus <- round(residuals(modele),3)
df$sqrtRes <- round(residuals(modele),3)^2
b0 <- round(modele$coefficients[1],3)
b1 <- round(modele$coefficients[2],3)
eq <- paste0("Vélo = ", b0, " ", b1, " x KmCV")
r2 <- round(summary(modele)$r.squared,3)
pearson <- round(cor(df$Velo, df$KmCV),3)
seg <- data.frame(x1 = df[3,3], x2 = 12, y1 = df[2,3], y2 = df[2,3]+5)
ggplot(df) +
  aes(
    x = KmCV,
    y = Velo,
    ymin = Velo,
    ymax = ypredit
  ) +
  stat_smooth(method="lm", se=FALSE, col="red")+
  labs(title= eq,
       subtitle = paste0("r2 = ", r2, ", r =", pearson), sep="")+
  xlab("Distance au centre-ville (km)")+
  ylab("Vélo (%)")+
  geom_segment(aes(xend = df[9,3]+.2, x = df[9,3]+2,
                   yend = df[9,2], y = df[9,2]+3),
                   size=1.2, linetype="solid", colour="brown4",
                   arrow = arrow(length = unit(0.2, "inches")))+
  geom_segment(aes(xend = df[9,3]+.05, x = df[9,3]+2,
                   yend = df[9,2]-abs(df[9,5]/2),
                   y = df[9,2]-abs(df[9,5]/2)),
                   size=1.2, linetype="solid", colour="brown4",
                   arrow = arrow(length = unit(0.2, "inches")))+
  geom_segment(aes(xend = df[9,3], x =df[9,3]-.5,
                   yend = df[9,4]-.5, y = df[9,4]-9),
                   size=1.2, linetype="solid", colour="brown4",
                   arrow = arrow(length = unit(0.2, "inches")))+
  annotate(geom="text", x =df[9,3]+2.2, y= df[9,2]+4,
           label="Valeur observée pour la municipalité I", color="black", hjust = 0, size = 5)+
  annotate(geom="text", x =df[9,3]+2.2, y=df[9,2]+3,
           label="x =5,225 km, et y = 25,3%", color="black", hjust = 0, size = 5)+
  annotate(geom="text", x =df[9,3]+2.2, df[9,2]-abs(df[9,5]/2)+.2,
           label="résidu = 25,3 - 23,038 = 2,262", color="black", hjust = 0, size = 5)+
  annotate(geom="text", x =df[9,3], y= df[9,4]-9.5,
           label="Valeur prédite pour I", color="black", hjust = 0.5, size = 5)+
  
    annotate(geom="text", x =df[9,3], y= df[9,4]-10.5,
           label="30,603 - 1,448 x 5,225 = 23,038", color="black", hjust = 0.5, size = 5)+
  geom_pointrange(colour="black", size=.8, linetype="solid", shape=16)+
  geom_point(colour="blue", size=3, aes(x=KmCV, y=ypredit))
```  

```{r regfictives2, echo=FALSE, message=FALSE, warning=FALSE}
df$sqrtRes <- round(df$residus^2,3)
sqrtres <- round(sum(df$sqrtRes),3)
df[11,1] <- "Somme"
df[11,5] <- 0
df[11,6] <- sqrtres
df1 <- rbind(df[1:5,],df[11,])
df2 <- rbind(df[6:10,],df[11,])
opts <- options(knitr.kable.NA = "--")

show_table(df,
           col.names = c("Municipalité","Vélo","KMCV","Valeur prédite","Résidu", "Résidu au carré"),
           caption = "Valeurs observées, prédites et résidus"
           )
``` 

#### Mesurer la qualité d'ajustement du modèle {#sect04143}
Les trois mesures les plus courantes pour évaluer la qualité d'ajustement d'un modèle de régression linéaire simple sont l'erreur quadratique moyenne (en anglais, *root-mean-square error*, *RMSE*), le coefficient de détermination ($R^2$) et la statistique *F* de Fisher. Pour mieux appréhender le calcul de ces trois mesures, rappelons que l'équation de régression s'écrit : 

\begin{equation}\footnotesize 
y_i = \beta_0 + \beta_1 x_1+ \epsilon_i \Rightarrow Y= \beta_0 + \beta_1 X + \epsilon
(\#eq:reg2)
\end{equation}

Elle comprend ainsi une partie de *Y* qui est expliquée par le modèle et une autre partie non expliquée : $\epsilon$ appelé habituellement le terme d'erreur. Ce terme d'erreur pourrait représenter d'autres variables explicatives qui n'ont pas été prises en compte pour prédire la variable indépendante ou une forme de variation aléatoire inexplicable présente lors de la mesure.

\begin{equation}\footnotesize 
Y  = \underbrace{\beta_0 + \beta_1 X}_{\mbox{partie expliquée par le modèle}}+ \underbrace{\epsilon}_{\mbox{partie non expliquée}}
(\#eq:reg3)
\end{equation}

Par exemple, pour la municipalité *A* au tableau \@ref(tab:regfictives2), nous avons : $y_A = \widehat{y}_A - \epsilon_A \Rightarrow 12.5 = 10.138+2.362$. Souvenez-vous que la variance d'une variable est la somme des écarts à la moyenne, divisée par le nombre d'observations. Par extension, il est alors possible de décomposer la variance de *Y* comme suit :

\begin{equation}\footnotesize 
\underbrace{\sum_{i=1}^n (y_{i}-\bar{y})^2}_{\mbox{variance de Y}} = \underbrace{\sum_{i=1}^n (\widehat{y}_i-\bar{y})^2}_{\mbox{var. expliquée}} + \underbrace{\sum_{i=1}^n (y_{i}-\widehat{y})^2}_{\mbox{var. non expliquée}} \Rightarrow 
SCT = SCE + SCR
(\#eq:reg4)
\end{equation}

avec :

* *SCT* est la somme des écarts au carré des valeurs observées à la moyenne (en anglais, _total sum of squares_)
* *SCE* est la somme des écarts au carré des valeurs prédites à la moyenne (en anglais, _regression sum of squares_)
* *SCR* est la somme des carrés des résidus (en anglais, _sum of squared errors_).

Autrement dit, la variance totale est égale à la variance expliquée plus la variance non expliquée. Au tableau \@ref(tab:computeR), vous pouvez repérer les valeurs de *SCT*, *SCE* et *SCR* et constater que 279,30 = 227,04 + 52,26 et 27,93 = 22,70 + 5,23.

```{r computeR, echo=FALSE, message=FALSE, warning=FALSE, out.width='85%'}
data <- read.csv("data/bivariee/Reg.csv", stringsAsFactors = F)
modele <- lm(Velo~KmCV, data)
data$yi <- data$Velo
data$ypredit <- modele$fitted.values
data$ei <- modele$residuals
data$Velo <- NULL
data$KmCV <- NULL
n <- nrow(data)
moy_y <- mean(data$yi)
sumy <- sum(data$yi)
data$yi_ymean2 <- (data$yi - moy_y)^2
data$ypredit_ymean2 <- (data$ypredit - moy_y)^2
data$ei2 <- (data$ei)^2
df <- data
df[11,1] <- "N"
df[11,2] <- n
df[12,1] <- "Somme"
df[13,1] <- "Moyenne"
for (i in c(2,4:7))
{
  df[12,i] <- sum(df[1:10,i])
  df[13,i] <- mean(df[1:10,i])
  df[i] <- round(df[i], 2)
}
df[3] <- round(df[3], 2)
opts <- options(knitr.kable.NA = "--")

show_table(df, 
           caption = "Calcul du coefficient de détermination",
           col.names = c("Municipalité", "$y_i$", "$\\widehat{y}_i$", "$\\epsilon_i$","$(y_i-\\bar{y})^2$","$(\\widehat{y}_i-y_i)^2$", "$\\epsilon_i^2$")
           )
```


**Calcul de l'erreur quadratique moyenne**

La somme des résidus au carré (*SCR*) divisée par le nombre d'observations représente donc le carré moyen des erreurs (en anglais, *mean square error - MSE*), soit la variance résiduelle du modèle ($52,26 / 10 = 5,23$). Plus sa valeur sera faible, plus le modèle sera efficace pour prédire la variable indépendante. L'erreur quadratique moyenne (en anglais, *root-mean-square error - RMSE*) est simplement la racine carrée de la somme des résidus au carré divisée par le nombre d'observations ($n$) :

\begin{equation}\footnotesize 
RMSE = \sqrt{\frac{\sum_{i=1}^n (y_{i}-\widehat{y})^2}{n}}
(\#eq:reg5)
\end{equation}

Elle représente ainsi une **mesure absolue des erreurs** qui est exprimée dans l'unité de mesure de la variable dépendante. Dans le cas présent, on a : $\sqrt{5,23}=2,29$. Cela signifie qu'en moyenne, l'écart absolu (ou erreur absolue) entre les valeurs observées et prédites est de 2,29 points de pourcentage. De nouveau, une plus faible valeur de **RMSE** indique un meilleur ajustement du modèle. Mais surtout, le RMSE permet d'évaluer avec quelle précision le modèle prédit la variable dépendante. Il est donc particulièrement important si l'objectif principal du modèle est de prédire des valeurs sur un échantillon d'observations pour lequel la variable dépendante est inconnue.

**Calcul du coefficient de détermination**

Nous avons largement démontré que la variance totale est égale à la variance expliquée plus la variance non expliquée. La qualité du modèle peut donc être évaluée avec le coefficient de détermination ($R^2$), soit le rapport entre les variances expliquée et totale : 

\begin{equation}\footnotesize 
R^2 = \frac{SCE}{SCT} \mbox{ avec } R^2 \in \left[0,1\right]
(\#eq:reg6)
\end{equation}

Comparativement au RMSE qui est une mesure absolue, le coefficient de détermination est une **mesure relative** qui varie de 0 à 1. Il exprime la proportion de la variance de *Y* qui est expliquée par la variable *X*; autrement dit, plus sa valeur est élevée, plus *X* influence / est capable de prédire *Y*. Dans le cas présent, on a : $R^2 = 227.04 / 279.3 = 0.8129$, ce qui signale que 81,3% de la variance du pourcentage de cyclistes est expliquée par la distance au centre-ville. Tel que signalé dans la section \@ref(sect04132), la racine carrée du coefficient de détermination ($R^2$) est égale au coefficient de corrélation ($r$) entre les deux variables. 

**Calcul de la statistique _F_ de Fisher**

La statistique _F_ de Fisher permet de vérifier la significativité globale du modèle.

\begin{equation}\footnotesize 
F = (n-2)\frac{R^2}{1-R^2} = (n-2)\frac{SCE}{SCR}
(\#eq:reg7)
\end{equation}

L'hypothèse nulle (h<sub>0</sub> avec $\beta_1=0$) est rejetée si la valeur calculée de *F* est supérieure à la valeur critique de la table *F* avec *(1, n-2)* degrés de liberté et un seuil $\alpha$ (*p*=0,05 habituellement) (voir le tableau des valeurs critiques de F, section \@ref(annexe2)). Notez qu'on utilise rarement la table *F* puisqu'avec la fonction `pf(f obtenu, 1, n-2, lower.tail = FALSE)` l'on obtient directement la valeur de *p* associée à la valeur de *F*. Concrètement, si le test _F_ est significatif (avec *p*<0,05), plus la valeur de *F* sera élevée, plus le modèle sera efficace (et plus le $R^2$ sera également élevé).

Notez que la fonction *summary* renvoie les résultats du modèle, dont notamment le test _F_ de Fisher.

```{r echo=TRUE, message=FALSE, warning=FALSE, out.width='85%'}
# utiliser la fonction summary
summary(modele)
```

Dans le cas présent, $F = (10 - 2)\frac{0.8129}{1-0.8129} = (10-2)\frac{227.04}{52.26} = 34.75$ avec une valeur de $p < 0.001$. Par conséquent, le modèle est significatif.

#### Mise en œuvre dans R {#sect04144}
Comment calculer une régression linéaire simple dans ? Rien de plus simple avec la fonction `lm(formula = y ~ x, data= dataframe)`.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/Reg.csv", stringsAsFactors = F)
## Création d'un objet pour le modèle
monmodele <- lm(Velo ~ KmCV, df)
## Sorties du modèle avec la fonction summary
summary(monmodele)
## Calcul du MSE et du RMSE
MSE <- mean(monmodele$residuals^2)
RMSE <- sqrt(MSE)
cat("MSE=", round(MSE, 2), "; RMSE=", round(RMSE,2), sep="")
```

#### Comment rapporter une régression linéaire simple {#sect04145}

Nous avons calculé une régression linéaire simple pour prédire le pourcentage d'actifs occupés utilisant le vélo pour se rendre au travail en fonction de la distance au centre-ville (en kilomètres). Le modèle obtient un *F* de Fisher significatif (*F*(1,8)= 34,75, *p* < 0,001) et un $R^2$ de 0,813. Le pourcentage de cyclistes peut être prédit par l'équation suivante : 30,603 - 1,448 x (distance au centre-ville en km).

## Relation entre deux variables qualitatives {#sect042}

::: {.bloc_objectif data-latex=""}
**Deux variables qualitatives sont-elles associées entre elles?** Plus spécifiquement, certaines modalités d'une variable qualitative sont-elles associées significativement à certaines modalités d'une variable qualitative.

Prenons l'exemple de deux variables qualitatives : l'une intitulée *groupe d'âge* comprenant trois modalités (15 à 29 ans, 30 à 44 ans, 45 à 64 ans); l'autre intitulée *mode de transport habituel pour se rendre au travail* comprenant quatre modalités (véhicule motorisé, transport en commun, vélo, marche). 

Comparativement aux deux autres groupes, on pourrait supposer que les jeunes se déplacent proportionnellement plus en modes de transport actifs (vélo et marche) et en transports en commun. À l'inverse, il est possible que les 45 à 64 ans se déplacent majoritairement en véhicule motorisé.

Pour vérifier l'existence d'associations significatives entre les modalités de deux variables qualitatives, il est possible de construire un **tableau de contingence**\index{tableau de contingence} (section \@ref(sect0421)), puis de réaliser le **test du khi^2^**\index{khi2} (section \@ref(sect0422)).
:::


### Construction de tableau de contingence {#sect0421}

Les données du tableau de contingence ci-dessous décrivent 279 projets d'habitation à loyer modique (HLM) dans l'ancienne ville de Montréal, croisant les modalités de la période de construction (en colonnes) et de la taille (en ligne) des projets HLM [@TheseApparicio]. Les différents éléments du tableau sont décrits ci-dessous.

* **Les fréquences observées**, nommées communément $f_{ij}$, correspondent aux observations appartenant à la fois à la *i^ème^* modalité de la variable en ligne et à la *j^ème^* modalité de la variable en colonne. À titre d’exemple, on compte 14 HLM construits entre 1985 et 1989 comprenant moins de 25 logements.

* **Les marges** du tableau sont les totaux pour chaque modalité en ligne ($n_{i.}$) et en colonne ($n_{j.}$). En guise d’exemple, sur les 279 projets HLM, 53 comprennent de 25 à 49 logements et 56 ont été construites entre 1968 et 1974. Bien entenu, la somme des marges en ligne ($n_{i.}$) est égale au nombre total d'observations ($n_{ij}$), tout comme la somme de marges en colonne ($n_{.j}$).

* **Trois pourcentages** sont disponibles (total, en ligne, en colonne). Ils sont respectivement la fréquence observée divisée par le nombre d'observations ($f_{ij}/n_{ij}\times100$), par la marge en ligne ($f_{ij}/n_{i.} \times 100$) et en colonne ($f_{ij}/n_{.j}\times100$). En guise d'exemple, 5% des 279 projets HLM ont été construits entre 1985 et 1989 et comprennent moins de 25 logements (pourcentage total, soit 14/279×100). Aussi, plus de la moitié des habitations de moins de 25 logements ont été construits entre 1990 et 1994 (pourcentage en ligne, 41/80×100). Finalement, près de 36% des logements construits avant 1975 ont 100 logements et plus (20/56×100).

* **Les fréquences théoriques**, représentent les valeurs que l'on devrait observer théoriquement s'il y avait indépendance entre les modalités des deux variables : si la répartition des deux modalités des deux variables étaient dûes au hasard. Pour le croisement de deux modalités, la fréquence théorique est égale au produit des marges divisé par le nombre total d'observations ($ft_{ij} = (n_{i.}n_{.j})/n_{ij}$). Par exemple, la fréquence théorique pour le croisement des modalité *moins de 25 logements* et *avant 1975* est égale à : (80×56)/279 = 16,06. Nous observons ici que la valeur théorique (16,06) est bien supérieure à la valeur réelle (6). On observe donc moins de HLM de moins de 25 logements avant 1975 que ce que l'on pourrait attendre du hasard.

* **La déviation** est la différence entre la fréquence observée et la fréquence théorique ($f_{ij}-ft_{ij}$). Plus la déviation est grande, plus on s'écarte d'une situation d'indépendance entre les deux modalités *i* et *j*. La somme des déviations sur une ligne ou sur une colonne est nulle. Si la déviation *ij* est nulle, la fréquence théorique est égale à la fréquence observée, ce qui signifie qu’il y a indépendance entre les modalités *i* et *j*. Une déviation positive traduit, quant à elle, une attraction entre les modalités *i* et *j*, ou autrement dit, une surreprésentation du phénomène *ij*; tandis qu’une déviation négative renvoie à une répulsion entre les modalités *i* et *j*, soit une sous-représentation du phénomène *ij*. Dans le cas précédent, on observait 6 habitations de moins de 25 logements construits avant 1975 et une fréquence théorique de 16,0. La déviation est donc -10,06, soit une sous-représentation du phénomène.


* **La contribution au khi^2^** est égale à la déviation au carré divisée par la fréquence théorique : $\chi_{ij}^2 = (f_{ij}-ft_{ij})^2/ft_{ij}$. Plus sa valeur est forte, plus il y a association entre les deux modalités. La somme des contributions au khi^2^ représente le *khi^2^* total pour l'ensemble du tableau de contingence (ici à 63,54) que nous abordons dans la section suivante.


```{r tablecontingence, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
library("gmodels")
TabKhi2 <- read.csv("data/bivariee/hlm.csv")
# Création d'un facteur pour les modalités de la période de construction
TabKhi2$Periode <- factor(TabKhi2$Periode, 
                              levels = c(1,2,3,4,5), 
                              labels = c("Av. 1975", "1975-79", "1980-84", "1985-89", "1990-94"))
# Création d'un facteur pour les modalités de la taille
TabKhi2$Taille <- factor(TabKhi2$Taille, 
                             levels = c(1,2,3,4), 
                             labels = c("< 25 log.", "25-49", "50-99", "100 et +"))
# Tableau de contingence en utilisant la fonction CrossTable du package gmodels
CrossTable(TabKhi2$Taille, TabKhi2$Periode,
           expected=TRUE, chisq = TRUE, resid = TRUE, digits = 2, format="SPSS")
```

### Test du khi^2^ {#sect0422}
Avec le test du khi^2^, on postule qu'il y a indépendance entre les modalités des deux variables qualitatives, soit l'hypothèse nulle (h<sub>0</sub>). Puis, on cacule le nombre de degrés de liberté : $DL = (n-1)(l-1)$ avec $l$ et $n$ étant respectivement les nombres de modalités en ligne et en colonne. Pour notre tableau de contingence, nous avons 12 degrés de liberté : $(4-1)(5-1)=12$. À partir du nombre de degré de liberté et d'un seuil critique de significativité (prenons 5% ici), nous pouvons trouver la valeur critique de khi^2^ dans le tableau des valeurs critiques du khi^2^ : 21,03 section \@ref(annexe1)). Puisque la valeur du khi^2^ calculé dans le tableau de contingence est bien supérieure à celle obtenue dans le tableau des valeurs critiques (63,54), on peut rejeter l'hypothèse d'indépendance au seuil de 5%. Autrement dit, si les deux variables n'étaient pas associées, nous aurions eu moins de 5% de chances de collecter des données avec ce niveau d'association, ce qui nous permet de rejeter l'hypothèse nulle (absence d'association). Notez que le test reste significatif avec des seuils de 1% (*p*=0,01) et 0,1% (*p*=0,001) puisque les valeurs critiques sont de 26,22 et 32,91.

Bien entendu, une fois que l'on connait le nombre de degrés de liberté, on peut directement calculer les valeurs critiques pour différents seuils de signification et éviter ainsi de recourir à la table ci-dessus. Dans la même veine, on peut aussi calculer la valeur de *p* d'un tableau de contingence en spécifiant le nombre de degrés de liberté et la valeur du khi^2^ obtenue.

```{r echo=TRUE, message=FALSE, warning=FALSE}
cat("Valeurs critiques du khi2 avec le nombre de degrés de liberté", "\n",
    round(qchisq(p=0.95,  df=12, lower.tail = FALSE),3), "avec p=0,05", "\n",
    round(qchisq(p=0.99,  df=12, lower.tail = FALSE),3), "avec p=0,01", "\n",
    round(qchisq(p=0.999, df=12, lower.tail = FALSE),3), "avec p=0,0001")
cat("Valeurs de p du Khi2 obtenu (63.54291) avec 12 degrés de liberté :", "\n",
    pchisq(q=63.54291, df=12, lower.tail = FALSE))
```

::: {.bloc_aller_loin data-latex=""}
Outre le khi^2^, d'autres mesures d'association permettent de mesurer le degré d'association entre deux variables qualitatives. Les plus courantes sont reportées au tableau ci-dessous. À des fins de comparaison, le khi^2^ décrit précédemment est aussi reporté sur la première ligne du tableau.

Statistique | Formule | Propriété et interprétation
-------- | ------- | -----------------------------
khi^2^  | $\chi^2 = \sum \frac{(f_{ij}-ft_{ij})^2}{ft_{ij}}$ | Mesure classique du Khi^2^ calculé à partir des différences entre les fréquences observées et attendues. Valeur de *p* disponible.
Ratio de vraissemblance du  khi^2^ | $G^2 = 2 \sum f_{ij} \ln{(\frac{f_{ij}}{ft_{ij}})}$ | Calculé à partir du ratio entre les fréquences observées et attendues. Valeur de *p* disponible.
khi^2^ de Mantel-Haenszel | $Q_{MH}=(N−1)r^2$ | avec $r$ étant le coefficient de corrélation entre les deux variables qualitatives; par exemple, entre les valeurs des modalités de 1 à 5 de la variable *période de construction* et celles de 1 à 4 de la variable *taille du projet* HLM. Ce coefficient est très utile quand les deux variables qualitatives ne sont pas nominales, mais **ordinales**. Valeur de *p* disponible.
Corrélation polychorique | obtenue itérativement par maximum de vraissemblance | Dans le même esprit que le khi^2^ de Mantel-Haenszel, la corrélation polychorique s'applique à deux variables ordinales. Plus spécifiquement, elle formule le postulat que deux variables théoriques normalement distribuées ont été mesurées de façon approximative avec deux échelles ordinales. Par exemple, en psychologie, le sentiment de bien être et le sentiment de sécurité peuvent être conceptualisés comme deux variables continues normalement distribuées. Cependant, les mesurer directement est très difficile, on a donc recours à des échelles de Likert allant de 1 à 10. Pour cet exemple, il serait pertinent d'utiliser la corrélation polychorique. Comme une corrélation de Pearson, la corrélation polychorique varie de -1 à 1, une valeur négative indiquant une relation inverse entre les deux variables théoriques et inversement. Une valeur de *p* peut être obtenue.
Coefficient Phi | $\phi=\sqrt{\frac{\chi^2}{n}}$ | Simplement le Khi^2^ divisé par le nombre d'observations. Si les deux variables qualitatives comprennent deux modalités chacune (tableau 2x2 dimensions) alors $\phi$ varie de −1 à 1; sinon de 0 à $min(\sqrt{c-1}, \sqrt{l-1})$ avec $c$ et $l$ étant le nombre de modalités en colonne et en ligne. Par conséquent, ce coefficient est peu utile pour les tableaux de plus de 2x2 dimensions. Pas de valeur de *p* disponible.
V de Cramer | $V=\sqrt{\frac{\chi^2/n}{min(c-1,l-1)}}$ | Il représente un ajustement du coefficient Phi et varie de 0 à 1. Plus sa valeur est forte plus les deux variables sont associées. À la lecture des deux formules, vous constaterez que pour un tableau de 2 x 2, la valeur du V de Carmer sera égale à celle du Coefficient Phi. Pas de valeur de *p* disponible.
:::

### Mise en œuvre dans  {#sect0423}

Pour calculer le Khi^2^ entre deux variables qualitatives, on utilise la fonction de base : `chisq.test(x = ..., y = ...)` qui renvoie le nombre de degré de liberté, les valeurs du Khi^2^ et de *p*.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Importation du csv
dataHLM <- read.csv("data/bivariee/hlm.csv")
# Calcul du Khi2 avec la fonction de base chisq.test
chisq.test(x = dataHLM$Taille, y = dataHLM$Periode)
```

Pour la construction du tableau de contingence, deux options sont possibles dépendamment de la structuration de votre tableau de données initial. Premier cas de figure, votre tableau comprend une ligne par observation avec les différentes modalités dans deux colonnes (ici *Periode* et *Taille*). Dans la syntaxe ci-dessous, pour chacune des deux variables qualitatives, on crée un facteur afin de spécifier un intitulé à chaque modalité (`factor(levels =c(....), labels = c(..)`). Puis, on utilise la fonction `CrossTable` du package **gmodels**. Pour obtenir les fréquences théoriques, les contributions locales au Khi^2^ et les déviations, on spécifie les options suivantes : `expected=TRUE, chisq=TRUE, resid=TRUE`.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("gmodels")
#Premiers enregistrements du tableau
head(dataHLM)
# La variable Periode comprend 5 modalités (de 1 à 5)
table(dataHLM$Periode)
# La variable Periode comprend 4 modalités (de 1 à 4)
table(dataHLM$Taille)
#Création d'un facteur pour les cinq modalités de la période de construction
dataHLM$Periode <- factor(dataHLM$Periode, 
                          levels = c(1,2,3,4,5), 
                          labels = c("<1975", 
                                     "1975-1979", 
                                     "1980-1984", 
                                     "1985-1989", 
                                     "1990-1994"))
#Création d'un facteur pour les quatre modalités de la taille des habitations
dataHLM$Taille <- factor(dataHLM$Taille, 
                         levels = c(1,2,3,4), 
                         labels = c("<25 log.", 
                                    "25-49", 
                                    "50-99", 
                                    "100 et +"))
# Pour construire un tableau de contingence on utilise la fonction CrossTable 
# (package gmodels) les deux lignes ci-dessous sont mises en commentaire 
# pour ne pas répéter le tableau
#CrossTable(x=dataHLM$Taille, y=dataHLM$Periode, digits = 2,
#           expected=TRUE, chisq = TRUE, resid = TRUE, format="SPSS")
```

Deuxième cas de figure, vous disposez déjà d'un tableau de contingence, soit les fréquences observées ($f_{ij}$). On n'utilise donc pas la fonction `CrossTable`, mais directement la fonction `chisq.test`.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Importation des données
df <-  read.csv("data/bivariee/data_transport.csv", stringsAsFactors = FALSE)
df # Visualisation du tableau
Matrice <- as.matrix(df[, c("Homme","Femme")])
dimnames(Matrice) <- list(unique(df$ModeTransport), Sexe=c("Homme","Femme"))
# Notez que vous pouvez saisir vos données directement si vous avez peu d'observations
Femme <- c(689400, 21315, 181435, 43715, 24295, 8395) # Vecteur de valeurs pour les femmes
Homme <- c(561830, 40010, 238330, 54360, 13765, 6970) # Vecteur de valeurs pour les hommes
Matrice <- as.table(cbind(Femme, Homme)) # Création du tableau
# Nom des deux variables et de leurs modalités respectives
dimnames(Matrice) <- list(transport=c("Automobile (conducteur)",
                                      "Automobile (passager)",
                                      "Transport en commun",                            
                                      "A pied",
                                      "Bicyclette",
                                      "Autre moyen"),
                          sexe=c("Homme","Femme"))
# Test du Khi2
test <- chisq.test(Matrice)
print(test)
# Fréquences observées (Fij)
test$observed
# Fréquences théoriques (FTij)
round(test$expected,0)
# Déviations (Fij - FTij)
round(test$observed-test$expected,0)
# Contributions au Khi2
round((test$observed-test$expected)^2/test$expected,2)
# Marges en lignes et en colonnes
colSums(Matrice)
rowSums(Matrice)
# Grand total
sum(Matrice)
# Pourcentages
round(Matrice/sum(Matrice)*100,2)
# Pourcentages en ligne
round(Matrice/rowSums(Matrice)*100,2)
# Pourcentages en colonne
round(Matrice/colSums(Matrice)*100,2)
```

Pour obtenir les autres mesures d'association, on pourra utiliser la syntaxe  suivante :

```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/hlm.csv")
# Fonction pour calculer les autres mesures d'association
AutresMesuresKhi2 <- function(x, y){
  testChi2 <- chisq.test(x, y) # Calcul du Chi2
  n <- sum(testChi2$observed)  # Nombre d'observations
  c <- ncol(testChi2$observed) # Nombre de colonnes
  l <- nrow(testChi2$observed) # Nombre de lignes
  dl <- (c-1)*(l-1)            # Nombre de degrés de libertés
  chi2 <- testChi2$statistic   # Khi2
  Pchi2 <- testChi2$p.value    # P pour le Khi2
  
  #Ratio de vraissemblance du khi2
  G <- 2*sum(testChi2$observed*log(testChi2$observed/testChi2$expected)) # G2
  PG <- pchisq(G, df=dl, lower.tail = FALSE) # P pour le G22
  
  # khi2 de Mantel-Haenszel avec la librarie DescTools
  MHTest <- DescTools::MHChisqTest(testChi2$observed)
  MH <- MHTest$statistic
  PMH <- MHTest$p.value
  
  # Coefficient de correlation Polychorique
  df <- data.frame("x" = as.factor(x),
                 "y" = as.factor(y))
  polychoricCorr <- correlation::cor_test(df,"x","y",method = "polychoric")
  polyR <- polychoricCorr$rho
  polyP <- polychoricCorr$p
  
  # Coefficient Phi et V de Cramer
  phi <- sqrt(chi2/n)
  vc <- sqrt(chi2/(n*min(c-1,l-1)))
  
  # Tableau pour les sorties
  dfsortie <- data.frame(
        Statistique = c("Khi2", 
                        "Ratio de vraissemblance du  khi", 
                        "Khi2 de Mantel-Haenszel",
                        "Corrélation Polychoric",
                        "Coefficient de Phi",
                        "V de Cramer"), 
        Valeur = round(c(Pchi2, G, MH, polyR, phi, vc),3), 
        P = round(c(Pchi2, PG, PMH, polyP , NA, NA),10))
  return(dfsortie)
}

dfkhi2 <- AutresMesuresKhi2(df$Periode, df$Taille)

show_table(dfkhi2,
           caption="Mesures d'association entre deux variables qualitatives",
           digits = 3
           )
```

### Interprétation d'un tableau de contingence {#sect0424}

Nous vous proposons une démarche très simple pour vérifier l'association entre deux variables qualitatives avec les étapes suivantes :

* On pose l'hypothèse nulle (h<sub>0</sub>), soit l'indépendance entre les deux variables. Si le Khi^2^ total du tableau de contingence est inférieur à la valeur critique du Khi^2^ avec *p*=0,05 et le nombre de degrés de liberté de la table *T*, alors il y a bien indépendance. La valeur de *p* sera alors supérieure à 0,05. L'analyse s'arrête donc là ! Autrement dit, il n'est pas nécessaire d'analyser le contenu de votre tableau de contingence puisqu'il n'y pas d'associations significatives entre les modalités des deux variables. Vous pouvez simplement signaler que : selon les résultats du test du Khi^2^, il n'y a pas d'association significative entre les deux variables ($\chi$ = ... avec *p*= ...).

* S'il y a dépendance ($khi_{observé}^2 > khi_{critique}^2$), trouver les cellules *ij* où les contributions au Khi^2^ sont les plus fortes, c'est-à-dire où les liens entre les modalités *i* de la variable en ligne et les modalités *j* de la variable en colonne sont les plus marqués. Pour ces cellules, le phénomène *ij* est surreprésenté si la déviation est positive ou sous-représenté si la déviation est négative. Commentez ces associations et utilisez les pourcentages en lignes ou en colonnes pour appuyer vos propos.

::: {.bloc_astuce data-latex=""}
Pour repérer rapidement les cellules où les contributions au Khi^2^ sont les plus fortes, vous pouvez construire un graphique avec la fonction **mosaic** du *package* **vcd**. À la figure \@ref(fig:figVDC), la taille des rectangles représentent les effectifs entre les deux modalités tandis que les associations sont représentées comme suit : 
en gris lorsqu'elles ne sont pas significatives, en rouge pour des déviations significatives et négatives et en bleu pour des déviations significatives et positives.

```{r figVDC, echo=TRUE, message=FALSE, warning=FALSE, out.width='65%', fig.pos="H"}
library(vcd)
mosaic(~ Taille+Periode, data=dataHLM,shade=TRUE, legend=TRUE)
```

:::

**Exemple d'interprétation.** « Les résultats du test du khi^2^ signale qu'il existe des associations entre les modalités de la taille et de la période de construction des projets d'habitation ($\chi$ =63,5, *p* < 0,001). Les fortes contributions au khi^2^ et le signe positif ou négatif des déviations correspondantes permettent de repérer cinq associations majeures entre les modalités de taille et de période de construction des projets HLM : **1)** la répulsion entre les projets d’habitation de moins de 25 logements et la période de construction 1964-1974; **2)** l’attraction entre les projets d’habitation de 100 logements et plus et la période de construction de 1969-1974; **3)** l’attraction entre les projets d’habitation de moins de 25 logements et la période de construction de 1990-1994; **4)** la répulsion entre les projets d’habitation de 50 à 99 logements et la période de construction 1990-1994; **5)** la répulsion entre les projets d’habitation de 100 logements et plus et la période de construction 1990-1994.
On observe donc une tendance bien marquée dans l’évolution du type de construction entre 1970 et 1994 : entre 1969 et 1974, on construit habituellement de grandes habitations dépassant souvent 100 logements; du milieu des années 1970 à la fin des années 1980, on privilégie la construction d’habitations de taille plus modeste, entre 50 et 100 logements; tandis qu’au début des années 1990, on opte plutôt pour des habitations de taille réduite (moins de 50 logements). Quelques chiffres à l’appui : sur les 56 habitations réalisées entre 1969 et 1974, 20 ont plus de 100 logements, 20 comprennent entre 50 et 99 logements et seules 10 ont moins de 25 logements. Près de la moitié des habitations construites entre 1975 et 1989 regroupent 50 à 99 logements (43,8% pour la période 1975-1979, 45,8% pour 1980-1984 et 44,7% pour 1985-1989). Par contre, 51% des logements érigés à partir de 1990 disposent de moins de 25 logements » (Apparicio, 2002, 117-118). Notez que cette évolution décroissante est aussi soutenue par le coefficient négatif de la corrélation polychorique.

Vous pouvez aussi construire un graphique pour appuyer vos constats, soit avec les pourcentages en ligne ou en colonne (figure \@ref(fig:fighlm) tirée de @apparicio2006).

```{r fighlm, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Taille des ensembles HLM selon la période de construction",  out.width='80%'}
knitr::include_graphics('images/bivariee/figurehlm.png', dpi = NA)
```

**Comment rapporter succinctement les résultats d'un Test du Khi^2^?**

Le test du Khi^2^ a été réalisé pour examiner la relation entre la taille et la période de construction des habitations HLM. Cette relation est significative : $\chi^2$(12, N = 279) = 63,5, *p* < 0,001. Plus les projets ont été construits récemment, plus ils sont de taille réduite.

Pour un texte en anglais, vous pourrez consulter [https://www.socscistatistics.com/tutorials/chisquare/default.aspx](https://www.socscistatistics.com/tutorials/chisquare/default.aspx){target="_blank"}.



## Relation entre une variable quantitative et une variable qualitative à deux modalités {#sect043}

::: {.bloc_objectif data-latex=""}

**Les moyennes de deux groupes de population sont-elles significativement différentes?** On souhaite ici comparer deux groupes de population en fonction d'une variable continue. 
Par exemple, pour deux échantillons respectivement d'hommes et de femmes travaillant dans le même secteur d'activité, on pourrait souhaiter vérifier si les moyennes des salaires des hommes et des femmes sont différentes et ainsi vérifier la présence ou l'absence d'une iniquité systématique. En études urbaines, dans le cadre d'une étude sur un espace public, on pourrait vouloir vérifier si la différence des moyennes du sentiment de sécurité des femmes et des hommes est significative (c'est-à-dire différente de 0).

**Pour un même groupe, la moyenne de la différence d'un phémonène donné mesuré à deux moments est-elle ou non égale à zéro?** Autrement dit, on cherche à comparer un même groupe d'individus avant et après une expérimentation, ou dans deux contextes différents. Prenons un exemple d'application en études urbaines. Dans le cadre d'une étude sur la perception des risques associés à la pratique du vélo en ville, 50 individus utilisant habituellement l'automobile pour se rendre au travail sont recrutés. L'expérimentation pourrait consister à leur donner une formation sur la pratique du vélo en ville et à les accompagner quelques jours durant leurs déplacements domicile-travail. On évaluera la différence de leurs perceptions des risques associés à la pratique du vélo sur une échelle de 0 à 100 avant et après l'expérimentation. On pourrait supposer que la moyenne des différences est significativement négative, ce qui indiquerait que la perception du risque a diminué après l'expérimentation; autrement dit, la perception du risque serait plus faible en fin de période. 
:::


### Test *t* et ses différentes variantes {#sect0431}

Le **t de student**, appelé aussi **test _t_** (*t-test* en anglais), est un test paramétrique permettant de comparer les moyennes de deux groupes (échantillons), qui peuvent être indépendantes ou non :

* **Échantillons indépendants (dits non appariés)**, les observations de deux groupes qui n'ont aucun lien entre eux. Par exemple, on souhaite vérifier si les moyennes du sentiment de sécurité des hommes et des femmes, ou encore si, les moyennes des loyers entre deux villes sont statistiquement différentes. Ainsi, les tailles des deux échantillons peuvent être différentes ($n_a \neq n_b$).

* **Échantillons dépendants (dits appariés)**, les individus des deux groupes sont les mêmes et  sont donc associés par paires. Autrement dit, on a deux séries de valeurs de taille identique $n_a = n_b$ et $n_{ai}$ est le même individu que $n_{bi}$. Ce type d'analyse est souvent utilisée en études cliniques : pour $n$ individus, on dispose d'une mesure quantitative de leur état de santé pour deux séries (l'une avant le traitement, l'autre une fois le traitement terminé). Cela permet de comparer les mêmes individus avant et après un traitement, une expérimentation; on parle alors d'étude, d'expérience et d'analyse pré-post. Concrètement, on cherche à savoir si la moyenne des différences des observations avant et après est significativement différente de 0. Si c'est le cas, on peut en conclure que l'expérimentation a eu un impact sur le phénomène mesuré (variable continue). Ce type d'analyse pré-post peut aussi être utilisé pour évaluer l'impact du réaménagement d'un espace public (rue commerciale, place publique, parc, etc.). Par exemple, on pourrait questionner le même échantillon de commerçants ou d'usagers avant et après le réaménagement d'une artère commerciale.

**Condition d'application**. Pour utiliser les tests de Student et de Welch, la variable continue doit être normalement distribuée. Si elle est fortement anormale, on utilisera le test non paramétrique de Wilcoxon (section \@ref(sect0432)). Il existe trois principaux tests pour comparer les moyennes de deux groupes :

* Test de Student (test *t*) avec échantillons indépendants et variances similaires (méthode *pooled*).  Les variances de deux groupes sont semblables quand leur ratio varie de 0,5 à 2 ($0,5< (S^2_{X_A}/S^2_{X_B})<2$).
* Test de Welch (appelé aussi Satterthwaite) avec échantillons indépendants quand les variances des deux groupes sont dissemblables.
* Test de Student (test *t*) avec échantillons dépendants.

Il s'agit de vérifier si les moyennes des deux groupes sont statistiquement différentes avec les étapes suivantes :

* On pose l'hypothèse nulle (H<sub>0</sub>), soit que les moyennes des deux groupes *A* et *B* ne sont pas différentes ($\bar{X}_{A}=\bar{X}_{B}$) ou autrement dit, la différence des deux moyennes est nulle ($\bar{X}_{A}-\bar{X}_{B}=0$). L'hypothèse alternative (H<sub>1</sub>) est donc $\bar{X}_{A}\ne\bar{X}_{B}$.
* On calcule la valeur de *t* et le nombre de degrés de liberté. La valeur de *t* sera négative quand la moyenne du groupe A est inférieure au groupe B et inversement.
* On compare la valeur absolue de *t* ($\mid T \mid$) avec celle issue de la table des valeurs critiques T avec le bon nombre de degrés de liberté et en choisissant un degré de signification (habituellement, p = 0,05). Si ($\mid t \mid$) est supérieure à la valeur *t* critique, alors les moyennes sont statistiquement différentes au degré de signification retenu.
* Si les moyennes sont statistiquement différentes, on peut calculer la taille de l'effet.


**Cas 1. Test de student pour des échantillons indépendants avec variances égales (méthode *pooled*).** La valeur de *t* est le ratio entre la différence des moyennes des deux groupes (numérateur) et l'erreur-type groupée des deux échantillons (dénominateur) :

$t = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sqrt{\frac{S^2_p}{n_A}+\frac{S^2_p}{n_B}}}$ avec 
$S^2_p = \frac{(n_A-1)S^2_{X_A}+(n_B-1)S^2_{X_B}}{n_A+n_B-2}$

avec $n_A$,$n_B$, $S^2_{X_A}$ et $S^2_{X_B}$ étant respectivement les nombres d'observations et les variances pour les groupes *A* et *B*, $S^2_p$ étant la variance groupée des deux échantillons et $n_A+n_B-2$ étant le nombre de degrés de liberté.

**Cas 2. Test de Welch pour des échantillons indépendants (avec variances différentes).** Le test de Welch est très similaire au test de student; seul le calcul de la valeur de _T_ est différent, pour tenir compte des variances respectives des groupes :

$t = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sqrt{\frac{S^2_A}{n_A}+\frac{S^2_B}{n_B}}}$ et $dl = \frac{ \left( \frac{S^2_{X_A}}{n_A}+\frac{S^2_{X_B}}{n_B} \right)^2} {\frac{S^4_{X_A}}{n^2_A(n_A-1)}+\frac{S^4_{X_B}}{n^2_B(n_B-1)}}$

Dans la syntaxe  ci-dessous, nous avons écrit une fonction dénommée `test_independants` permettant de calculer les deux tests pour des échantillons indépendants. Dans cette fonction, vous pourrez repérer comment sont calculés les moyennes, nombres d'observations et variances pour les deux groupes, le nombre de degrés de liberté, les valeurs de *t* et de *p* pour les deux tests. Puis, nous avons créé aléatoirement deux jeux de données relativement à la vitesse de déplacement de cyclistes utilisant un vélo personnel ou un vélo en libre service (généralement plus lourd et moins utilisé par des cyclistes expérimentés) :

* Au cas 1, 60 cyclistes utilisant un vélo personnel roulant en moyenne à 18 km/h (écart-type de 1,5) et 50 utilisateurs du système de vélo partage avec une vitesse moyenne de 15 km/h (écart-type de 1,5).

* Au cas 2, 60 cyclistes utilisant un vélo personnel roulant en moyenne à 16 km/h (écart-type de 3) et 50 utilisateurs du système de vélo partage avec une vitesse moyenne de 15 km/h (écart-type de 1,5). Ce faible écart des moyennes, combiné à une plus forte variance va réduire la significativité de la différence entre les deux groupes.

D'emblée, l'analyse visuelle des boîtes à moustaches (figure \@ref(fig:figttest1)) signale qu'au cas 1 contrairement au cas 2, les groupes sont plus homogènes (boîtes plus compactes) et les moyennes semblent différentes (les boîtes sont centrées différemment sur l'axe des ordonnées). Cela est confirmé par les résultats des tests.

```{r figttest1, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Boîtes à moustaches sur des échantillons fictifs non appariés', out.width='75%'}
library("ggplot2")
library("ggpubr")
# fonction ------------------
tstudent_independants <- function(A, B){
    x_a <- mean(A)           # Moyenne du groupe A
    x_b <- mean(B)           # Moyenne du groupe B
    var_a <- var(A)          # Variance du groupe A
    var_b <- var(B)          # Variance du groupe B
    sd_a <- sqrt(var_a)      # Écart-type du groupe A
    sd_b <- sqrt(var_b)      # Écart-type du groupe B
    ratio_v <- var_a / var_b # ratio des variances
    n_a <- length(A)         # nombre d'observation du groupe A
    n_b <- length(B)         # nombre d'observation du groupe B
    
    # T-test (variances égales)
    dl_test <- n_a+n_b-2       # degrés de liberté
    PooledVar <- (((n_a-1)*var_a)+((n_b-1)*var_b))/dl_test
    t_test <- (x_a-x_b) / sqrt(((PooledVar/n_a)+(PooledVar/n_b)))
    p_test <-  2*(1-(pt(abs(t_test), dl_test)))     
    # Test Welch-Sattherwaite (variances inégales)
    t_welch <- (x_a-x_b) / sqrt( (var_a/n_a) + (var_b/n_b))
    dl_num = ((var_a/n_a) + (var_b/n_b))^2
    dl_dem = ((var_a/n_a)^2/(n_a-1))  + ((var_b/n_b)^2/(n_b-1))
    dl_welch = dl_num / dl_dem # degrés de liberté
    p_welch <- 2*(1-(pt(abs(t_welch), dl_welch)))     
    
    cat("\n groupe A (n = ", n_a,"), moy = ", round(x_a,1),", 
           variance = ", round(var_a,1),", écart-type = ", round(sd_a,1),
        "\n groupe B (n = ", n_b,"), moy = ", round(x_b,1),", 
          variance = ", round(var_b,1),", écart-type = ", round(sd_b,1),
        "\n ratio variance = ",round(ratio_v,2),
        "\n t-test (variances égales): t(dl = ", dl_test, ") = ",round(t_test,4),
         ", p = ", round(p_test,6),
         "\n t-Welch (variances inégales): t(dl = ", round(dl_welch,3), ") = ",
        round(t_welch,4), ", p = ", round(p_welch,6),  sep="")    
  
    if (ratio_v > 0.5 && ratio_v < 2)  {
      cat("\n Variances similaires. Utilisez le test de Student !")
      p <- p_test
    } else {
      cat("\n Variances dissemblables. Utilisez le test de Welch-Satterwaithe !")
      p <- p_welch
    }
    
    if (p <=.05){
      cat("\n Les moyennes des deux groupes sont significativement différentes.")
    } else {
      cat("\n Les moyennes des deux groupes ne sont pas significativement différentes.")
    }
}
# CAS 1 : données fictives ------------------
# Création du groupe A : 60 observations avec une vitesse moyenne de 18 et un écart-type de 1,5
Velo1A <- rnorm(60,18,1.5)
# Création du groupe B : 50 observations avec une vitesse moyenne de 15 et un écart-type de 1,5
Velo1B <- rnorm(50,15,1.5)
df1 <- data.frame(
  vitesse = c(Velo1A,Velo1B), 
  type = c(rep("Vélo personnel",length(Velo1A)), rep("Vélo partage",length(Velo1B)))
)
boxplot1 <- ggplot(data=df1, mapping=aes(x=type,y=vitesse, colour=type)) +
  geom_boxplot(width=0.2)+
  ggtitle("Données fictives (cas 1)")+
  xlab("Type de vélo")+
  ylab("Vitesse de déplacement (km/h)")+
  theme(legend.position = "none")
# CAS 2 : données fictives ------------------
# Création du groupe A : 60 observations avec une vitesse moyenne de 18 et un écart-type de 3
Velo2A <- rnorm(60,16,3)
# Création du groupe B : 50 observations avec une vitesse moyenne de 15 et un écart-type de 1,5
Velo2B <- rnorm(50,15,1.5)
df2 <- data.frame(
  vitesse = c(Velo2A,Velo2B), 
  type = c(rep("Vélo personnel",length(Velo2A)), rep("Vélo partage",length(Velo2B)))
)
boxplot2 <- ggplot(data=df2, mapping=aes(x=type,y=vitesse, colour=type)) +
  geom_boxplot(width=0.2)+
  ggtitle("Données fictives (cas 2)")+
  xlab("Type de vélo")+
  ylab("Vitesse de déplacement (km/h)")+
  theme(legend.position = "none")
ggarrange(boxplot1, boxplot2, ncol = 2, nrow = 1)
# Appel de la fonction pour le cas 1
tstudent_independants(Velo1A, Velo1B)
# Appel de la fonction pour le cas 2
tstudent_independants(Velo2A, Velo2B)
```


#### Principe de base et formulation pour des échantillons dépendants (appariés) {#sect04311}

Nous disposons de plusieurs individus pour lesquelles nous avons mesuré un phénomène (variable continue) à deux temps différents : généralement avant et après une expérimentation (analyse pré-post). Il s'agit de vérifier si la moyenne des différences des observations avant et après la période est différente de 0. Pour ce faire, on réalise les étapes suivantes :

* On pose l'hypothèse nulle (H<sub>0</sub>), soit que la moyenne des différences entre les deux séries est égale à 0 ($\bar{D} = 0$ avec $d = {x}_{t_1}- {x}_{t_2}$). L'hypothèse alternative (H<sub>1</sub>) est donc $\bar{D} \ne 0$. Notez que l'on peut tester une autre valeur que 0.
* On calcule la valeur de *t* et le nombre de degrés de liberté. La valeur de *t* sera négative quand la moyenne des différences entre ${X}_{t_1}$ et ${X}_{t_2}$ est négative et inversement.
* On compare la valeur absolue de *t* ($\mid T \mid$) avec celle issue de la table des valeurs critiques T avec le bon nombre de degrés de liberté et en choisissant un degré de signification (habituellement, p = 0,05). Si ($\mid t \mid$) est supérieure à la valeur *t* critique, alors les moyennes sont statistiquement différentes au degré de signification retenu.

Pour le test de student avec des échantillons appariées, la valeur de *t* se calcule comme suit :

$$\footnotesize t = \frac{\bar{D}-\mu_0}{\sigma_D / \sqrt{n}}$$
avec $\bar{D}$ étant la moyenne des différences entre les observations appariées de la série A et de la série B, $\sigma_D$ l'écart des différences, *n* le nombre d'observations, et finalement $\mu_0$ la valeur de l'hypothèse nulle que l'on veut tester (habituellement 0). Bien entendu, il est possible fixer une autre valeur pour $\mu_0$ : par exemple, avec $\mu_0 = 10$, on chercherait ainsi à vérifier si la moyenne des différences est significativement différente de 10. Le nombre de degrés de liberté sera égal à $n-1$.

Dans la syntaxe  ci-dessous, nous avons écrit une fonction dénommée `tstudent_dependants` permettant de réaliser le test de student pour des échantillons appariés. Dans cette fonction, vous pourrez repérer comment sont calculés la différence entre les observations pairées, la moyenne et l'écart-type de cette différence, puis le nombre de degrés de liberté, les valeurs de *t* et de *p* pour les deux tests.

Pour illustrer l'utilisation de la fonction, nous avons créé aléatoirement deux jeux de données. Imaginons que ces données décrivent 50 personnes utilisant habituellement l'automobile pour se rendre au travail. Pour ces personnes, nous avons généré des valeurs du risque perçu de l'utilisation du vélo (de 0 à 100), et ce, avant et après une période de 20 jours ouvrables durant lesquels ils devaient impérativement se rendre au travail à vélo.

* Au cas 1, les valeurs de risque ont une moyenne de 70 avant l'expérimentation et de 50 après l'expérimentation, avec des écarts types de 5.
* Au cas 2, les valeurs de risque ont une moyenne de 70 avant et 66 après, avec des écarts types de 5.

D'emblée, l'analyse visuelle des boîtes à moustaches (figure \@ref(fig:figttest2)) pairées montrent que la perception du risque semble avoir nettement diminé après l'expérimentation pour le cas 1 mais pas pour le cas 2. Cela est confirmé par les résultats des tests.


```{r figttest2, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Boites à moustaches sur des échantillons fictifs appariées', out.width='75%'}
library("ggplot2")
library("ggpubr")
tstudent_dependants <- function(A, B, mu=0){
  d <- A-B           # différences entre les observations pairées
  moy <- mean(d)     # Moyenne des différences
  e_t <- sd(d)       # Écart-type des différences
  n   <- length(A)   # nombre d'observations
  dl  <- n-1         # nombre de degrés de liberté (variances égales)
  
  t <- (moy- mu) / (e_t/sqrt(n)) # valeur de t
  p <-  2*(1-(pt(abs(t), dl)))
  
  cat("\n groupe A : moy = ", round(mean(A),1),", var = ", 
         round(var(A),1),", sd = ", round(sqrt(var(A)),1),
      "\n groupe B : moy = ", round(mean(B),1),", var = ", 
         round(var(B),1),", sd = ", round(sqrt(var(B)),1),
      "\n Moyenne des différences = ", round(mean(moy),1),
      "\n Ecart-type des différences = ", round(mean(e_t),1),
      "\n t(dl = ", dl, ") = ",round(t,2),
      ", p = ", round(p,3),  sep="")
  
  if (p <=.05){
    cat("\n La moyenne des différences entre les échantillons est significative")
  }
  else{
    cat("\n La moyenne des différences entre les échantillons n'est pas significative")
  }
}
# CAS 1 : données fictives ------------------
Avant1 <- rnorm(50,70,5)
Apres1 <- rnorm(50,50,5)
df1 <- data.frame(Avant=Avant1, Apres=Apres1)
boxplot1 <- ggpaired(df1, cond1 = "Avant", cond2 = "Apres", fill = "condition", 
                     palette = "jco", title = "Données fictives (cas 1)")
# CAS 2 : données fictives ------------------
Avant2 <- rnorm(50,70,5)
Apres2 <- rnorm(50,66,5)
df2 <- data.frame(Avant=Avant2, Apres=Apres2)
boxplot2 <- ggpaired(df2, cond1 = "Avant", cond2 = "Apres", fill = "condition", 
                     palette = "jco", title = "Données fictives (cas 2)")
ggarrange(boxplot1, boxplot2, ncol = 2, nrow = 1)
# Test t : appel de la fonction tstudent_dependants
tstudent_dependants(Avant1, Apres1, mu=0)
tstudent_dependants(Avant2, Apres2, mu=0)
```


#### Mesurer la taille de l'effet {#sect04312}

Rappelons que la taille de l'effet permet d'évaluer la magnitude (force) de l'effet d'une variable (ici la variable qualitative à deux modalités) sur une autre (ici la variable continue). Dans le cas de comparaisons de moyennes (avec des échantillons pairées ou non), pour mesurer la taille d'effet, on utilise habituellement le *d* de Cohen ou encore le *g* de Hedges; le second étant un ajustement du premier. Notez que nous analyserons la taille de l'effet uniquement si le test student ou de Welch s'est révélé significatif (p<0,05).

**Pourquoi utiliser le *d* de cohen?** Deux propriétés en font une mesure particulièrement intéressante. Premièrement, elle est facile à calculer puisque *d* est le ratio entre la différence de deux moyennes de groupes (A, B) et l'écart-type combiné des deux groupes. Deuxièmement, *d* représente ainsi une mesure standardisée de la taille de l'effet ; elle permet ainsi l'évaluation de la taille d'effet indépendamment de l'unité de mesure de la variable continue. Concrètement, cela signifie que quelle que soit l'unité de mesure de la variable continue *X*, elle est toujours exprimée en unité d'écart-type de *X*. Cette propriété facilite ainsi grandement les comparaisons entre des valeurs de *d* calculées sur différentes combinaisons de variables (au même titre que le coefficient de variation ou le coefficient de corrélation par exemple). Pour des échantillons indépendants de tailles différentes, il s'écrit : 

$$\footnotesize d = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sqrt{\frac{(n_A-1)S^2_A+(n_B-1)S^2_B}{n_A+n_B-2}}}$$
avec $n_A$,$n_B$, $S^2_{X_A}$ et $S^2_{X_B}$ étant respectivement les nombres d'observations et les variances pour les groupes *A* et *B*, $S^2_p$.

Si les échantillons sont de tailles identiques ($n_A=n_B$), alors *d* peut s'écrire :
$$\footnotesize d = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sqrt{(S^2_A+\S^2_A)/2}} = \frac{\bar{X}_{A}-\bar{X}_{B}}{(\sigma_A+\sigma_B)/2}$$
avec $\sigma_A$ et $\sigma_B$ étant les écart-types des deux groupes (rappel : l'écart-type est la racine carrée de la variance !).

Le *g* de Hedge est simplement une correction de *d*, particulièrement importante quand les échantillons sont de taille réduite.
$$\footnotesize g = d- \left(1- \frac{3}{4(n_A+n_B)-9} \right)$$

Moins utilisé en sciences sociales, mais surtout en études cliniques, le delta de Glass est simplement la différence des moyennes des groupes indépendants (numérateur) sur l'écar-type du deuxième groupe (démoninateur). Dans une étude clinique, on a habituellement un groupe qui subit un traitement (groupe de traitement) et un groupe qui a reçu un placebo (groupe de contrôle ou groupe témoin). L'effet de taille est ainsi évalué par rapport au groupe de contrôle : 
$$\footnotesize \Delta = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sigma_B}$$


Finalement, pour des échantillons dépendants (pairés), il s'écrit simplement $d = \bar{D}/{\sigma_D}$ avec $\bar{D}$ et $\sigma_D$ étant la moyenne et l'écart-type des différences entre les observations.

**Commment interpréter le *d* de cohen ?** Un effet sera considéré comme faible avec $\lvert d \rvert$ à 0,2, modéré à 0,50 et fort à 0,80 [@cohen1992]. Notez que ces seuils ne sont que des conventions pour vous guider à interpréter la mesure de Cohen. D'ailleurs, dans son livre intitulé *Statistical power analysis for the behavioral sciences*, il écrit : « all conventions are arbitrary. One can only demand of them that they not be unreasonable » [@cohen2013]. Plus récemment, [@sawilowsky2009] a ajouté d'autres seuils à ceux proposés par Cohen (tableau \@ref(tab:tableconvcohen)).


```{r tableconvcohen, echo=FALSE, message=FALSE, warning=FALSE}
df <- data.frame(
        Sawilowsky = c("0,1 : Très faible", "0,2 : Faible", "0,5 : Moyen","0,8 : Fort", "1,2 : Très fort", "2,0 : Énorme"), 
        Cohen = c("", "0,2 : Faible", "0,5 : Moyen","0,8 : Fort", "", ""))

show_table(df,
           caption = 'Conventions pour l’interprétation du *d* de Cohen'
           )
```


#### Mise en œuvre dans  {#sect04313}

Nous avons écrit précédemment les fonctions `tstudent_independants` et `tstudent_dependants` uniquement pour décomposer les différentes étapes de calcul des tests de Student et de Welch. Il existe des fonctions de base (`t.test` et `var.test`) qui permettent de réaliser l'un ou l'autre de ces deux tests avec une seule ligne de code.

La fonction `t.test` permet ainsi de calculer les test de Student et de Welch :

* `t.test(x ~ y, data=, mu = 0, paired = FALSE, var.equal = FALSE,  conf.level = 0.95)` ou `t.test(x =, y =, mu = 0, paired = FALSE, var.equal = FALSE,  conf.level = 0.95)`. 
* Le paramètre `paired` sera utilisé pour spécifier si les échantillons sont dépendants (`paired=TRUE`) ou indépendants (`paired=FALSE`).
* Le paramètre *var.equal* sera utilisé pour spécifier si les variances sont égales pour le test de Student (`var.equal=TRUE`) ou dissemblables pour le test de Welch (`var.equal=FALSE`).
* `var.test(x, y)` ou `var.test(x ~ y, data=)` pour vérifier au préalable si les variances sont égales ou non et choisir ainsi un t de Student ou un t de Welch.

Les fonctions `cohens_d` et `hedges_g` du *package* **effectsize** renvoient respectivement les mesures de *d* de Cohen et du *g* de Hedge :

* `cohens_d(x ~ y, data = dataframe, paired = FALSE, pooled_sd = TRUE)` ou  `cohens_d(x, y, data = dataframe, paired = FALSE, pooled_sd = TRUE)` ou
* `hedges_g(x ~ y, data = dataframe, paired = FALSE, pooled_sd = TRUE)` ou `hedges_g(x, y, data = dataframe, paired = FALSE, pooled_sd = TRUE)`

* `glass_delta(x ~ y, data = dataframe, paired = FALSE, pooled_sd = TRUE)` ou `glass_delta(x, y, data = dataframe, paired = FALSE, pooled_sd = TRUE)`


Notez que pour toutes ces fonctions deux écritures sont possibles :

* `x ~ y, data=` avec un `dataframe` dans lequel `x` est une variable continue et `y` et un facteur binaire
* `x, y` qui sont tous deux des vecteurs numériques (variable continue).

**Exemple de test pour des échantillons indépendants**

La figure \@ref(fig:figlocataires) représente la cartographie du pourcentage de locataires par secteur de recensement (SR) pour la région métropolitaine de recensement de Montréal (RMR) en 2016, soit une variable continue. L'objectif est de vérifier si la moyenne de ce pourcentage des SR de l'agglomération de Montréal est significativement différente de celles de SR hors de l'agglomération. 

Les résultats de la syntaxe  ci-dessous signalent que le pourcentage de locataires par SR est bien supérieur dans l'agglomération (moyenne = 59,7%; écart-type = 21,4%) qu'en dehors de l'agglomération de Montréal (moyenne = 27,3%; écart-type = 20,1%); cette différence de 32,5 points de pourcentage est d'ailleurs significative et très forte (t = -23,95; p ˂ 0,001, d de Cohen = 1,54).


```{r figlocataires, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Pourcentage de locataires par secteur de recensement, RMR de Montréal, 2016",  out.width='95%'}
knitr::include_graphics('images/bivariee/FigureLocataires.jpg', dpi = NA)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("foreign")
library("effectsize")
library("ggplot2")
library("dplyr")
# Importation du fichier
dfRMR <- read.dbf("data/bivariee/SRRMRMTL2016.dbf")
# Définition d'un facteur binaire                  
dfRMR$Montreal <- factor(dfRMR$Montreal, 
                           levels=  c(0,1), 
                           labels = c("Hors de Montréal","Montréal"))
# Comparaison des moyennes ------------------------
#Boite à moustaches (boxplot)
ggplot(data = dfRMR, mapping=aes(x=Montreal,y=Locataire,colour=Montreal)) +
  geom_boxplot(width=0.2)+
  theme(legend.position="none")+
  xlab("Zone")+ ylab("Pourcentage de locataires")+
  ggtitle("Locataires par secteur de recensement", subtitle="RMR de Montréal, 2016")
# nombre d'observations, moyennes et écart-types pour les deux échantillons
group_by(dfRMR, Montreal) %>%
  summarise(
    n = n(),
    moy = mean(Locataire, na.rm = TRUE),
    ecarttype = sd(Locataire, na.rm = TRUE)
  )
# On vérifie si les variances sont égales avec la fonction var.test
# quand la valeur de P est inférieure à 0,05 alors les variances diffèrent
v <- var.test(Locataire ~ Montreal, alternative='two.sided', conf.level=.95, data=dfRMR)
print(v)
```
Le test indique que nous n'avons aucune raison de rejeter l'hypothèse nulle selon laquelle les variances sont égales. Pour l'île de Montréal, l'écart-type est de 21,4 et de 20,1 hors de l'île, soit une différence négligeable.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Calcul du T de Student ou du T de Welch
p <- v$p.value
if(p >= 0.05){
  cat("\n Les variances ne diffèrent pas !",
     "\n On utilise le test de student avec l'option var.equal=TRUE", sep="")
    t.test(Locataire ~ Montreal,  # variable continue ~ facteur binaire 
           data=dfRMR,            # nom du dataframe
           conf.level=.95,        # intervalle de confiance pour la valeur de t
           paired = FALSE,        # échantillons non pairés (indépendants)
           var.equal=TRUE)        # variances égales
} else {
  cat("\n Les variances diffèrent !",
      "\n On utilise le test de Welch avec l'option var.equal=FALSE", sep="")
  t.test(Locataire ~ Montreal,   # variable continue ~ facteur binaire 
         data=dfRMR,             # nom du dataframe
         conf.level=.95,         # intervalle de confiance pour la valeur de t
         paired = FALSE,         # échantillons non pairés (indépendants)
         var.equal=FALSE)        # variances différentes
}
# Effet de taille à analyser uniquement si le test est significatif
cohens_d(Locataire ~ Montreal, data = dfRMR, paired = FALSE)
hedges_g(Locataire ~ Montreal, data = dfRMR, paired = FALSE)
```
Notez que le *d* de Cohen et le *g* de Hedge sont très proches ici, rappelons que le second est une correction du premier pour des échantillons de taille réduite. Avec 951 observations, nous disposons d'un échantillon suffisamment grand pour que cette correction soit négligeable.

**Exemple de syntaxe  pour un test de Student pour des échantillons dépendants**

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("ggpubr")
library("dplyr")
Pre <- c(79,71,81,83,77,74,76,74,79,70,66,85,69,69,82,69,81,70,83,68,77,76,77,70,68,80,65,65,75,84)
Post <- c(56,47,40,45,49,51,54,47,44,54,42,56,45,45,48,55,59,58,56,41,56,51,45,55,49,49,48,43,60,50)
# Première façon de faire un tableau : avec deux colonnes Avant et Après
df1 <- data.frame(Avant=Pre, Apres=Post)
head(df1)
ggpaired(df1, cond1 = "Avant", cond2 = "Apres", fill = "condition", palette = "jco")
# Nombre d'observations, moyennes et écart-types
cat(nrow(df1), " observations",
    "\nPOST. moy = ", round(mean(df1$Avant),1), ", e.t. = ", round(sd(df1$Avant),1),
    "\nPRE.  moy = ", round(mean(df1$Apres),1), ", e.t. = ", round(sd(df1$Apres),1), sep="")
t.test(Pre, Post, paired = TRUE)
# Deuxième façon de faire un tableau : avec une colonne pour la variable continue et une autre pour la variable qualitative
n <- length(Pre)*2
df2 <- data.frame(
       id=(1:n),
       participant=(1:length(Pre)),
       risque=c(Pre,Post)
       )
df2$periode <- ifelse(df2$id <= length(Pre), "Pré", "Post")
head(df2)
# nombre d'observations, moyennes et écart-types pour les deux échantillons
group_by(df2, periode) %>%
  summarise(
    n = n(),
    moy = mean(risque, na.rm = TRUE),
    et = sd(risque, na.rm = TRUE)
    )
ggpaired(data=df2, x= "periode", y="risque", fill = "periode")
t.test(risque ~ periode, data=df2, paired = TRUE)
```

#### Comparer des moyennes pondérées {#sect04314}

::: {.bloc_objectif data-latex=""}
En études urbaines et en géographie, le recours aux données agrégées (non individuelles) est fréquent, par exemple au niveau des secteurs de recensement (comprenant généralement entre 2500 à 8000 habitants). Dans ce contexte, un secteur de recensement plus peuplé devrait avoir un poids plus important dans l'analyse. Il est possible d'utiliser les versions pondérées des tests présentés précédemment. Prenons deux exemples pour illustrer le tout :

* Pour chaque secteur de recensement des îles de Montréal et de Laval, nous avons calculé la distance au parc le plus proche à travers le réseau de rues avec un Système d'Information Géographique (SIG). On souhaite vérifier si les enfants âgés de moins de 15 ans résidant sur l'île de Montréal bénéficient en moyenne d'une meilleure accessibilité au parc.

* Dans une étude pour sur la concentration de polluants atmosphérique dans l'environnement autour des écoles primaires montréalaises, Carrier *et al.* [-@carrier2014] souhaitaient vérifier si les élèves fréquentant les écoles les plus défavorisés sont plus exposés au dioxyde d'azote (NO<sub>2</sub>) dans leur milieu scolaire. Pour ce faire, ils ont réalisé un test _t_ sur un tableau avec comme observations les écoles primaires et trois variables : la moyenne NO<sub>2</sub> (variable continue), les quintiles extrêmes d'un indice de défavorisation (premier et dernier quintiles, variable qualitative) et le nombres d'élèves inscrits par école (variable pour la pondération).

Pour réaliser un test *t* pondéré, nous pouvons utiliser la fonction `weighted_ttest` du package **sjstats**.

:::

En guise d'exemple appliqué, dans la syntaxe  ci-dessous, nous avons refait le même test *t* que précédemment (`Locataire ~ Montreal`) en pondérant chaque secteur de recensement par le nombre de logements qu'il comprend.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("sjstats")
library("dplyr")
# Calcul des statistiques pondérées
group_by(dfRMR, Montreal) %>%
  summarise(
    n = sum(Logement),
    MoyPond = weighted_mean(Locataire, Logement),
    ecarttypePond = weighted_sd(Locataire, Logement)
  )
# Test t non pondéré
t.test(Locataire ~ Montreal, dfRMR, 
               paired = FALSE, var.equal = TRUE, conf.level=.95)
# Test t pondérée
weighted_ttest(Locataire ~ Montreal + Logement, dfRMR, 
               paired = FALSE, ci.lvl=.95)
               
```

#### Comment rapporter un test de student ou de Welch? {#sect04315}

Pour les différentes versions du test, il est important de rapporter la valeur de *t*, la valeur de _p_ et les moyennes des groupes. Voici quelques exemples :

**Test de Student ou de Welch pour échantillons indépendants**

* Dans la région métropolitaine de Montréal en 2005, le revenu total des femmes  (moyenne = 29117 dollars; écart-type = 258022) est bien inférieur à celui des homme (moyenne = 44463; écart-type = 588081). La différence entre les moyennes des deux sexes (-15345) en faveur des hommes est d’ailleurs significative (t = -27,09; *p* ˂ 0,001).
* Il y un effet significatif selon le sexe (t = -27,09; *p* ˂ 0,001), le revenu total des hommes (moyenne= 44463; écart-type = 588081) étant bien supérieur à celui des femmes (moyenne = 29 117; écart-type = 258 022).
* 50 personnes se rendrent au travail à vélo (moyenne = 33,7, écart-type = 8,5) contre 60 en automobile (moyenne = 34, écart-type = 8,7); il n'y a pas de différence significative entre les moyennes d'âge des deux groupes (t(108) = -0,79, *p* = 0,427).


**Test de Student échantillons dépendants (pairés)**

* On constaste une diminution significative de la perception du risque après l'activité (moyenne = 49,9, écart-type = 5,7) comparativement à avant (moyenne = 74,8, écart-type = 6,1), avec une différence de -24,8 (t(29) = -18,7, *p* < 0,001).
* Les résultats du pré-test (moyenne = 49,9, écart-type = 5,7) et du post-test (moyenne = 74,8, écart-type = 6,1) montrent qu'il y une diminution significative de la perception du risque (t(29) = -18,7, *p* < 0,001).

Pour un texte en anglais, vous pourrez consulter
[https://www.socscistatistics.com/tutorials/ttest/default.aspx](https://www.socscistatistics.com/tutorials/ttest/default.aspx){target="_blank"}.


### Test non paramétrique de Wilcoxon {#sect0432}

::: {.bloc_objectif data-latex=""}
Si la variable continue est fortement anormalement distribuée, il est déconseillé d'utiliser les tests de Student et de Welch. On privilégiera le test des rangs signés de Wilcoxon (*Wilcoxon rank-sum test* en anglais). Attention, il est aussi appelé test U de Mann-Whitney. Ce test permet alors de vérifier si les deux groupes présentent des médianes différentes.

Pour ce faire, on utilise la fonction `wilcox.test` dans laquelle le paramètre `paired` permettra de spécifier si les échantillons sont indépendants ou non (`FALSE` ou `TRUE`).
:::

Dans l'exemple suivant, nous analysons le pourcentage de locataires dans les secteurs de recensements de la région métropolitaine de Montréal. Plus spécifiquement, nous comparons ce pourcentage entre les secteurs présents sur l'île et les secteurs hors de l'île. Il s'agit donc d'un test avec des échantillons indépendants.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("foreign")
library("dplyr")
###############################
# Échantillons indépendants
###############################
dfRMR <- read.dbf("data/bivariee/SRRMRMTL2016.dbf")
# Définition d'un facteur binaire                  
dfRMR$Montreal <- factor(dfRMR$Montreal, 
                           levels=  c(0,1), 
                           labels = c("Hors de Montréal","Montréal"))
# Calul du nombre d'observations, moyennes et écart-types des rangs pour les deux échantillons
group_by(dfRMR, Montreal) %>%
  summarise(
    n = n(),
    moy_rang = mean(rank(Locataire), na.rm = TRUE),
    med_rang = median(rank(Locataire), na.rm = TRUE),
    ecarttype_rang = sd(rank(Locataire), na.rm = TRUE)
  )
# Test des rangs signés de Wilcoxon sur des échantillons indépendants
wilcox.test(Locataire ~ Montreal, dfRMR, paired = FALSE)
```

Nous observons bien ici une différence significative entre le pourcentage de locataires des secteurs de recensement sur l'île (rang médian = 216) et hors de l'île (rang médian = 261).

Pour le second exemple, nous générons deux jeux de données au hasard représentant une mesure d'une variable pré-traitement (*pre*) et post-traitement (*post*) pour un même échantillon.
```{r echo=TRUE, message=FALSE, warning=FALSE}
###############################
# Échantillons dépendants
###############################
pre <- sample(60:80, 50, replace=T)
post <- sample(30:65, 50, replace=T)
df1 <- data.frame(Avant=pre, Apres=post)
# Nombre d'observations, moyennes et écart-types
cat(nrow(df1), " observations",
    "\nPOST. median = ", round(median(df1$Avant),1), 
             ", moy = ", round(mean(df1$Avant),1),
    "\nPRE.  median = ", round(median(df1$Apres),1), 
             ", moy = ", round(mean(df1$Apres),1), sep="")
wilcox.test(df1$Avant, df1$Apres, paired = TRUE)
```

À nouveau, nous obtenons une différence significative entre les deux variables.

**Comment rapporter un test de Wilcoxon?**

Lorsque l'on rapporte les résultats d'un test de Wilcoxon, il est important de signaler la valeur du test (W), le degré de signification (valeur de p) et éventuellement la médiane des rangs ou de la variable originale pour les deux groupes. Voici quelques exemples :

* Les résultats du test des rangs signés de Wilcoxon signalent que les rangs de l'île de Montréal sont significativement plus élevés que ceux de l'île de Laval (W = 1223, p ˂ 0,001).
* Les résultats du test de Wilcoxon signalent que les rangs post-tests sont significativement plus faibles que ceux pré-test (W = 1273,5, p ˂ 0,001).
* Les résultats du test de Wilcoxon signalent que la médiane des rangs pré-tests (médiane = 69) est significativement plus forte que celle du post-test (médiane = 50,5) (W = 1273,5, p ˂ 0,001).


## Relation entre une variable quantitative et une variable qualitative à plus de deux modalités {#sect044}

::: {.bloc_objectif data-latex=""}

**Existe-il une relation entre une variable continue et une variable qualitative comprenant plus de deux modalités?** Pour répondre à cette question, on pourra recourir à deux méthodes : l’analyse de variance – **ANOVA**, _**AN**alysis Of **VA**riance_ en anglais – et le test non paramétrique de Kruskal-Wallis. La première permet de vérifier si les moyennes de plusieurs groupes d'une population donnée sont ou non significativement différentes; la seconde si leurs médianes sont différentes.
:::

### Analyse de variance {#sect0441}

L’analyse de variance (ANOVA) est largement utilisée en psychologie, médecine et pharmacologie. Prenons un exemple classique en pharmacologie pour tester l'efficacité d'un médicament. Quatre groupes de population sont constitués :

* un premier groupe d'individus pour lequel on administre un placebo (un médicament sans substance active), soit le groupe de contrôle ou le groupe témoin;
* un second groupe auquel l'on administre le médicament avec un faible dosage;
* un troisième avec un dosage moyen;
* un quatrième avec un dosage élevé.

La variable continue permettra d'évaluer l'évolution de l'état de santé des individus (par exemple, la variation du taux de globules rouges dans le sang avant et après le traitement). Si le traitement est efficace, on s'attendrait alors à ce que les moyennes des deuxième, troisième et quatrième groupes soient plus élevées que celle du groupe de contrôle. Les différences de moyennes entre les second, troisième et quatrième groupes permettront aussi de repérer quel dosage est le plus efficace. Si nous n'observons aucune différence significative entre les groupes, cela signifie que l'effet du médicamment ne diffère pas de l'effet d'un placébo.

L'ANOVA est aussi très utilisée en études urbaines, principalement pour vérifier si un phénomène urbain varie selon plusieurs groupes d'une population donnée ou régions géographiques. En guise d'exemple, le recours à l'ANOVA permettrait de répondre aux questions suivantes :

* les moyennes des niveaux d'exposition à un polluant atmosphérique (variable continue) varient-elles significativement selon le mode de transport utilisé (automobile, vélo, transport en commun) pour des trajets similaires en heures de pointe?

* pour une métropole donnée, les moyennes des loyers (variable continue) sont-elles différentes entre les logements de la ville centre versus ceux localisés dans la première couronne et ceux de la seconde couronnes?


#### Le calcul des trois variances pour l'ANOVA {#sect04411}

L'ANOVA repose sur le calcul de trois variances :

* **la variance totale** (*VT*) de la variable dépendante continue, soit la somme des carrés des écarts à la moyenne de l'ensemble de la population (équation \@ref(eq:anova1));

* la **variance intergroupe** (*Var~inter~*) ou variance expliquée (*VE*), soit la somme des carrés des écarts entre la moyenne de chaque groupe et la moyenne de l’ensemble du jeu de données multipliées par le nombre d’individus appartenant à chacun des groupes (équation \@ref(eq:anova2));

* la **variance intragroupe**  (*Var~intra~*) ou variance non expliquée (*VNE*), soit la somme des variances des groupes de la variable indépendante (équation \@ref(eq:anova3)).

\begin{equation}\footnotesize 
VT=\sum_{i=1}^n (y_{i}-\overline{y})^2
(\#eq:anova1)
\end{equation}

\begin{equation}\footnotesize 
Var_{inter} \mbox{ ou } VE=\sum_{i\in{g_1}}(\overline{y_{g_1}}-\overline{y})^2 + \sum_{i\in{g_2}}(\overline{y_{g_2}}-\overline{y})^2 + ... + \sum_{i\in{g_n}}(\overline{y_{g_k}}-\overline{y})^2
(\#eq:anova2)
\end{equation}

\begin{equation}\footnotesize 
Var_{intra} \mbox{ ou } VNE=\sum_{i\in{g_1}}(y_{i}-\overline{y_{g_1}})^2 + \sum_{i\in{g_2}}(y_{i}-\overline{y_{g_2}})^2 + ... + \sum_{i\in{g_n}}(y_{i}-\overline{y_{g_k}})^2 
(\#eq:anova3)
\end{equation}

avec $\overline{y}$ est la moyenne de l'ensemble de la population; $\overline{y_{g_1}}$, $\overline{y_{g_1}}$, $\overline{y_{g_k}}$ sont respectivements les moyennes des groupes 1 à _k_ (_k_ étant le nombre de modalités de la variables qualitative) .

La variance totale (*VT*) est égale à la somme de la variance intergroupe (expliquée) et la variance intragroupe (non expliquée) (équation \@ref(eq:anova5)). Le ratio entre la variance intergroupe (expliquée) et la variance totale est dénommé *Eta^2^* (équation \@ref(eq:anova6)). Il varie de 0 à 1 et exprime la proportion de la variance de la variable continue qui est expliquée par les différentes modalités de la variable qualitative.

\begin{equation}\footnotesize 
VT = Var_{inter} + Var_{intra} \mbox{ ou } VT = VNE + VE
(\#eq:anova5)
\end{equation}

\begin{equation}\footnotesize 
\eta^2= \frac{Var_{inter}}{VT} \mbox{ ou }  \eta^2= \frac{VE}{VT}
(\#eq:anova6)
\end{equation}


::: {.bloc_astuce data-latex=""}
**La décomposition de la variance totale** – égale à la somme des variances intragroupe et intergroupe – est fondamentale en statistique. Nous verrons qu'elle est aussi utilisée pour évaluer la qualité d'une partition d'une population dans le chapitre sur les méthodes de classification (chapitre \@ref(chap10)). En ANOVA, on retiendra que :

* plus la variance intragroupe est faible, plus les différents groupes sont homogènes;
* plus la variance intergroupe est forte, plus les moyennes des groupes sont différentes et donc plus les groupes sont dissembables. 

Autrement dit, plus la variance intergroupe (**dissimilarité** des groupes) est maximisée et corollairement plus la variance intragroupe (**homogénéité** de chacun des groupes) est minimisée, plus les groupes sont clairement distincts et plus l'ANOVA sera performante.
:::

Examinons un premier jeu de données fictif sur la vitesse de déplacements de cyclistes (variable continue exprimée en km/h) et une variable qualitative comprenant trois groupes de cyclistes utilisant soit un vélo personnel (*n~A~* = 5), soit en libre service (*n~B~* = 7), soit électrique (*n~C~* = 6) (tableau \@ref(tab:aovfictive1)). D'emblée, on note que les moyennes de vitesse des trois groupes sont différentes : 17,6 km/h pour les cyclistes avec leur vélo personnel, 12,3  km/h les utilisateurs des vélos en libre service et 23,1 km/h pour les cyclistes avec un vélo électrique. 

Pour chaque observation, la troisième colonne du tableau représente les écarts à la moyenne globale mis au carré, tandis que les colonnes suivantes représentent la déviation au carré de chaque observation à la moyenne de son groupe d'appartenance. Ainsi, pour la première observation, on a : $(16,900 - 17,339)^2 = 0,193$ et $(16,900 - 17,580)^2~ = 0,46$. La variance totale (*VT*) est donc égale à la somme de la troisième colonne (424,663), tandis que la variance intragroupe (non expliquée, VNE) est égale à $11,228+21,537+13,993=46,758$. Quant à la variance intergroupe (expliquée, VE), elle est égale à $5\times(17,580-17,339)^2+7\times(12,257-17,339)^2+6\times(23,067-17,339)^2 = 377,904$.

On a donc $VT = Var_{inter} + Var_{intra}$, soit $424,663 = 377,904 + 46,758$ et $\eta_2 = 377,904 / 424,663 = 0,89$. Cela signale que 89% de la variance de la vitesse des cyclistes est expliquée par le type de vélo utilisé.

```{r aovfictive1, echo=FALSE, message=FALSE, warning=FALSE}
VeloA <- c(16.9, 20.4, 16.1, 17.7, 16.8)
VeloB <- c(13.4, 11.3, 14.0, 12.4, 13.7, 8.5, 12.5)
VeloC <- c(22.9, 26.0, 23.6, 21.0, 22.3, 22.6)
moyA <- mean(VeloA)
moyB <- mean(VeloB)
moyC <- mean(VeloC)
grandmoy <- mean(c(VeloA,VeloB,VeloC))
nA <- length(VeloA)
nB <- length(VeloB)
nC <- length(VeloC)
n <- nA + nB + nC
VT <- sum((c(VeloA,VeloB, VeloC)-grandmoy)^2)
VNE_A <- sum((VeloA-moyA)^2)
VNE_B <- sum((VeloB-moyB)^2)
VNE_C <- sum((VeloC-moyC)^2)
VNE <- VNE_A + VNE_B + VNE_C
VE <- nA*(moyA-grandmoy)^2 + nB*(moyB-grandmoy)^2 + nC*(moyC-grandmoy)^2
Eta2 <- round(VE / VT, 4)
  
df <- data.frame(
  velo = c(rep("A. personnel",length(VeloA)), 
           rep("B. libre service",length(VeloB)),
           rep("C. électrique",length(VeloC))),
  kmh = c(VeloA,VeloB, VeloC))
df$VT <- (df$kmh - grandmoy)^2
df$VNE_A <- ifelse(df$velo == "A. personnel", (df$kmh - moyA)^2, NA)
df$VNE_B <- ifelse(df$velo == "B. libre service", (df$kmh - moyB)^2, NA)
df$VNE_C <- ifelse(df$velo == "C. électrique", (df$kmh - moyC)^2, NA)
df_cas1 <- df
tabl <- df
tabl$velo <- as.character(tabl$velo)
tabl[19,1] <- "grande moyenne"
tabl[20,1] <- "moyenne groupe A"
tabl[21,1] <- "moyenne groupe B"
tabl[22,1] <- "moyenne groupe C "
tabl[19,2] <- round(grandmoy,3)
tabl[20,2] <- round(moyA,3)
tabl[21,2] <- round(moyB,3)
tabl[22,2] <- round(moyC,3)
tabl[23,1] <- "Variance totale"
tabl[24,1] <- "Variance intragroupe"
tabl[23,3] <- sum(tabl$VT, na.rm = TRUE)
tabl[24,4] <- sum(tabl$VNE_A, na.rm = TRUE)
tabl[24,5] <- sum(tabl$VNE_B, na.rm = TRUE)
tabl[24,6] <- sum(tabl$VNE_C, na.rm = TRUE)
tabl$VT <- round(tabl$VT,3)
tabl$VNE_A <- round(tabl$VNE_A,3)
tabl$VNE_B <- round(tabl$VNE_B,3)
tabl$VNE_C <- round(tabl$VNE_C,3)
opts <- options(knitr.kable.NA = "--")

show_table(tabl, 
           caption = "Données fictives et calcul des trois variances (cas 1)",
             col.names = c("Type de vélo",
                "km/h",
                "$(y_{i}-\\overline{y})^2$",
                "$(y_{i}-\\overline{y_{A}})^2$",
                "$(y_{i}-\\overline{y_{B}})^2$",
                "$(y_{i}-\\overline{y_{C}})^2$"
                )
)
```

Examinons un deuxième jeu de données fictives pour lequel le type de vélo utilisé n'aurait que peu d'effet sur la vitesse des cyclistes (tableau \@ref(tab:aovfictive2)). D'emblée, les moyennes des trois groupes semblent très similaires (19,3, 17,9 et 18,7). Les valeurs des trois variances sont les suivantes :

* la variance totale est égale à 121,756.
* la variance intragroupe (non expliquée, VNE) est égale à $9,140+50,254+56,275 = 115,669$
* la variance intragroupe (expliquée, VE) est égale à $5\times(19,300-18,528)^2+7\times(17,871-18,528)^2+6\times(18,650-18,528)^2 = 6,087$.

On a donc $VT = Var_{inter} + Var_{intra}$, soit $121,756 = 6,087 + 115,669$ et $\eta_2 = 6,087 / 121,756 = 0,05$. Cela signale que 5% de la variance de la vitesse des cyclistes est uniquement expliquée par le type de vélo utilisé.

```{r aovfictive2, echo=FALSE, message=FALSE, warning=FALSE}
VeloA <- c(17.5, 19.0, 19.7, 18.7, 21.6)
VeloB <- c(13.7, 20.8, 15.1, 18.8, 21.5, 16.5, 18.7)
VeloC <- c(16.6, 16.3, 15.6, 20.0, 24.6, 18.8)
moyA <- mean(VeloA)
moyB <- mean(VeloB)
moyC <- mean(VeloC)
grandmoy <- mean(c(VeloA,VeloB,VeloC))
nA <- length(VeloA)
nB <- length(VeloB)
nC <- length(VeloC)
n <- nA + nB + nC
VT <- sum((c(VeloA,VeloB, VeloC)-grandmoy)^2)
VNE_A <- sum((VeloA-moyA)^2)
VNE_B <- sum((VeloB-moyB)^2)
VNE_C <- sum((VeloC-moyC)^2)
VNE <- VNE_A + VNE_B + VNE_C
VE <- nA*(moyA-grandmoy)^2 + nB*(moyB-grandmoy)^2 + nC*(moyC-grandmoy)^2
Eta2 <- round(VE / VT, 4)
  
df <- data.frame(
  velo = c(rep("A. personnel",length(VeloA)), 
           rep("B. libre service",length(VeloB)),
           rep("C. électrique",length(VeloC))),
  kmh = c(VeloA,VeloB, VeloC))
df$VT <- (df$kmh - grandmoy)^2
df$VNE_A <- ifelse(df$velo == "A. personnel", (df$kmh - moyA)^2, NA)
df$VNE_B <- ifelse(df$velo == "B. libre service", (df$kmh - moyB)^2, NA)
df$VNE_C <- ifelse(df$velo == "C. électrique", (df$kmh - moyC)^2, NA)
df_cas2 <- df
tabl <- df
tabl$velo <- as.character(tabl$velo)
tabl[19,1] <- "grande moyenne"
tabl[20,1] <- "moyenne groupe A"
tabl[21,1] <- "moyenne groupe B"
tabl[22,1] <- "moyenne groupe C "
tabl[19,2] <- round(grandmoy,3)
tabl[20,2] <- round(moyA,3)
tabl[21,2] <- round(moyB,3)
tabl[22,2] <- round(moyC,3)
tabl[23,1] <- "Variance totale"
tabl[24,1] <- "Variance intragroupe"
tabl[23,3] <- sum(tabl$VT, na.rm = TRUE)
tabl[24,4] <- sum(tabl$VNE_A, na.rm = TRUE)
tabl[24,5] <- sum(tabl$VNE_B, na.rm = TRUE)
tabl[24,6] <- sum(tabl$VNE_C, na.rm = TRUE)
tabl$VT <- round(tabl$VT,3)
tabl$VNE_A <- round(tabl$VNE_A,3)
tabl$VNE_B <- round(tabl$VNE_B,3)
tabl$VNE_C <- round(tabl$VNE_C,3)
opts <- options(knitr.kable.NA = "--")

show_table(tabl, 
           caption = "Données fictives et calcul des trois variances (cas 2)",
            col.names = c("Type de vélo",
                "km/h",
                "$(y_{i}-\\overline{y})^2$",
                "$(y_{i}-\\overline{y_{A}})^2$",
                "$(y_{i}-\\overline{y_{B}})^2$",
                "$(y_{i}-\\overline{y_{C}})^2$"
                )
           )
# knitr::kable(
#   head(tabl, nrow(tabl)), booktabs = TRUE, valign = 't', row.names = FALSE,
#    format.args = list(decimal.mark = ",", big.mark = " "),           
#   col.names = c("Type de vélo",
#                 "km/h",
#                 "$(y_{i}-\\overline{y})^2$",
#                 "$(y_{i}-\\overline{y_{A}})^2$",
#                 "$(y_{i}-\\overline{y_{B}})^2$",
#                 "$(y_{i}-\\overline{y_{C}})^2$"
#                 ),
#   caption = "Données fictives et calcul des trois variances (cas 2)")  %>%
#   kableExtra::kable_styling(font_size = font_size_table)
```


#### Le test de Fisher {#sect04412}

Pour vérifier si les moyennes sont satistiquement différentes (autrement dit, si leur différence est significativement différente de 0), on a recours au test *F* de Fisher. Pour ce faire, on pose l'hypothèse nulle (H~0~), soit que les moyennes des groupes sont égales; autrement dit que la variable qualitative n'a pas d'effet sur la variable continue (indépendance entre les deux variables). L'hypothèse alternative (H~1~) est donc que les moyennes sont différentes. Pour nos deux jeux de données fictives ci-dessus comprenant trois groupes, H~0~ signifie que $\overline{y_{A}}=\overline{y_{B}}=\overline{y_{C}}$. La statistique *F* se calcule comme suit :


\begin{equation}\footnotesize 
F = \frac{\frac{Var{inter}}{k-1}}{\frac{Var{intra}}{n-k}}\mbox{ ou } F = \frac{\frac{VE}{k-1}}{\frac{VNE}{n-k}}
(\#eq:anovaF)
\end{equation}

avec $n$ et $k$ étant respectivement les nombres d'observations et de modalités de la variable qualitative. L'hypothèse nulle (les moyennes sont égales) sera rejetée si la valeur du *F* calculé est supérieure à la valeur critique de la table *F* avec les degrés de libertés *(k-1, n-k)* et un seuil 
$\alpha$ (*p*=0,05 habituellement) (voir le tableau des valeurs critiques de *F*, section \@ref(annexe2)). Notez qu'on utilise rarement la table *F* puisqu'avec la fonction `aov` on calcule directement la valeur *F* et celle de *p* qui lui est associée. Concrètement, si le test _F_ est significatif (avec *p*<0,05), plus la valeur de *F* sera élevée, plus la différence entre les moyennes sera élevée.

Appliquons rapidement la démarche du test *F* à nos deux jeux de données fictives qui comprennent 3 modalités pour la variable qualitative et 18 observations. Avec $\alpha$=0,05, 2 degrés de liberté (3-1) au numérateur et 15 au dénominateur (18-3), la valeur critique de F est de 3,68. On en conclut alors que :

* pour le cas A, le F calculé est égal à $F = (377,904 /2) / (46,758 / 15) = 60,62$. Il est  supérieur à la valeur F critique; les moyennes sont donc statistiquement différentes au seuil 0,05. Autrement dit, nous aurions eu moins de 5% de chance d'obtenir un échantillon produisant ces résultats si en réalité la différence entre les moyenne était de 0.
* pour le cas B, le F calculé est égal à $F = (6,087/2)/ (115,669 / 15) = 0,39$. Il est inférieur à la valeur F critique; les moyennes ne sont donc pas statistiquement différentes au seuil 0,05.

#### Conditions d'application de l'ANOVA et solutions alternatives {#sect04414}
Trois conditions d'application doivent être vérifiées avant d'effectuer une analyse de variance sur un jeu de données :

* **Normalité des groupes.** Le test de Fisher repose sur le postulat que les échantillons (groupes) sont normalement distribués. Pour le vérifier, on a recours au test de normalité de Shapiro–Wilk (section \@ref(sect025413)). Notez toutefois que ce test est très restrictif, surtout pour des grands échantillons.

* **Homoscédasticité**. La variance dans les échantillons doit être la même (homogénéité des variances). Pour vérifier cette condition, on utilisera les tests de Levene, de Bartlett ou de Breusch-Pagan.

* **Indépendance des observations (pseudo-réplication).** Chaque individu doit appartenir à un et un seul groupe. En d'autres termes, les observations ne sont pas indépendantes si plusieurs mesures (variable continue) sont faites sur un même individu. Si c'est le cas, on utilisera alors une analyse de variance sur des mesures répétées (voir le bloc à la fin du chapitre).

**Quelles sont les conséquences si les conditions d'application ne sont pas respectées ?** La non vérification des conditions d'application cause deux problèmes distincts : elle affecte la puissance du test (sa capacité à détecter un effet, si celui-ci existe réellement) et le taux d'erreur de type 1 (la probabilité de trouver un résultat significatif alors qu'aucune relation n'existe réellement, soit un faut-positif) [@glass1972consequences; @lix1996consequences].

* Si la distribution est asymétrique plutôt que centrée (comme pour une distribution normale), la puissance et le taux d'erreur de type 1 sont tous les deux peu affectés car le test est non-orienté (la différence de moyennes peut être négative ou positive).
* Si la distribution est leptocurtique (pointue, avec des extrémités de la distribution plus importantes, le taux d'erreur de type 1 est peu affecté, en revanche la puissance du test est réduite. L'inverse s'observe si la distribution est platicurtique (aplatie, c'est-à-dire avec des extrémités de la distribution plus réduites.
* Si les groupes ont des variances différentes, le taux d'erreur de type 1 augmente légèrement.
* Si les observations ne sont pas indépendantes, à la fois le taux d'erreurs de type 1 et la puissance du test sont fortement affectés.
* Si les échantillons sont petits, les effets présentés ci-dessus sont démultipliés.
* Si plusieurs conditions ne sont pas respectées, les conséquences présentées ci-dessus s'additionnent, voir se combinent.

**Que faire quand les conditions d'application relatives à la normalité ou à l'homoscédaticité ne sont vraiment pas respectées ?** Signalons d'emblée que le non respect de ces conditions ne change rien à la décomposition de la variance (VT=V~intra~+V~inter~). Cela signifie que vous pouvez toujours calculer Eta^2^. Par contre, le test de Fisher ne peut pas être utilisé car il sera biaisé comme décrit précédemment. Quatre solutions sont envisageables :

* Lorsque les échantillons sont fortement anormalement distribués, certains auteurs vont simplement transformer leur variable en appliquant une fonction logarithme (le plus souvent) ou racine carré, inverse ou exponentielle, et reporter le test de fisher calculé sur cette transformation. Attention toutefois ! Transformer une variable ne va pas systématiquement la rapprocher d'une distribution normale et complique l'interprétation finale des résultats. Par conséquent, avant de recalculer votre test *F*, il convient de réaliser un test de normalité de Shapiro–Wilk et un test d'homoscédasticité (Levene, Bartlett ou/et Breusch-Pagan) sur la variable continue transformée.

* Détecter les observations qui contribuent le plus à l'anormalité et l'hétéroscédasticité, dites valeurs aberrantes (*outliers* en anglais). Supprimez les et refaites votre ANOVA en vous assurant que les conditions sont désormais respectées. Notez que supprimer des observations peut être une pratique éthiquement questionnable en statistique. Si vos échantillons sont bien constitués et que la mesure collectée n'est pas erronée, pourquoi donc la supprimer? Si vous optez pour cette solution, prenez soin de comparer les résultats avant et après la suppression des valeurs aberrantes. Si les conditions sont respectées après suppression et que les résultats de l'ANOVA (Eta^2^ et test *F* de Fisher) sont très semblables, conservez donc les résultats de l'ANOVA intitiale et signalez que vous avez procédez aux deux tests.

* Lorsque les variances des groupes sont dissemblables, vous pouvez utiliser le test de Welch pour l'ANOVA au lieu du test *F* de Fisher. 

* Dernière solution, lorsque les deux conditions ne sont vraiment pas respectées, utilisez le test non paramétrique de Kruskal-Wallis. Par analogie au *t* de student, il correspond au test des rangs signés de Wilcoxon. Ce test est décrit dans la section suivante.

Vous l'aurez compris, dans de nombreux cas en statistique, les choix méthodologiques dépendent de la subjectivité du chercheur. Il faut s'adapter au jeu de données et à la culture statistique en vigueur dans votre champs d'étude. N'hésitez pas à réaliser plusieurs tests différents pour évaluer la robustesse de vos conclusions et fiez-vous en premier lieu à ceux pour lesquels votre jeu de données est le plus adapté.

### Test non paramétrique de Kruskal-Wallis {#sect0442}

Le test non paramétrique de Kruskal-Wallis est une alternative à l'ANOVA classique lorsque le jeu de données présente de graves problèmes de normalité et d'hétéroscédaticité. Cette méthode représente une ANOVA appliquée à une variable continue transformée préalablement en rangs. Du fait de la transformation en rangs, on ne vérifie plus si les moyennes sont différentes, mais bel et bien si les médianes de la variable continue sont différentes. Pour ce faire, on utilisera la fonction `kruskal.test`.

### Mise en œuvre dans  {#sect0443}

Dans une étude récente, Apparicio *et al.* [-@apparicio2018exposure] ont comparé les expositions au bruit et à la pollution atmosphérique aux heures de pointe à Montréal en fonction du mode de transport utilisé. Pour ce faire, trois équipes de trois personnes ont été constituées : un cycliste, un automobiliste et un utilisateur du transport en commun, équipés de capteurs de pollution, de sonomètres, de vêtements biométriques et d’une montre GPS. Chaque matin, à huit heures précises, les membres de chaque équipe ont réalisé un trajet d'un quartier périphérique de Montréal vers un pôle d'enseignement (université) ou d'emploi localisé au centre-ville. Le trajet inverse était réalisé le soir à 17h. Au total, une centaine de trajets ont ainsi été réalisés. Des analyses de variance ont ainsi permis de comparer entre les trois modes (automobile, vélo et transport en commun) : les temps de déplacement, les niveaux d'exposition au bruit, les niveaux d'exposition au dioxyde d'azote et la dose totale inhalée de dioxyde d'azote. Nous vous proposons ici d'analyser une partie de ces données.


#### Première ANOVA : différences entre les temps de déplacement
Comme première analyse de variance, nous allons vérifier si les moyennes des temps de déplacement sont différentes entre les trois modes de transport. 

Dans un premier temps, nous pouvons calculer les moyennes des différents groupes. On peut alors constater que les moyennes sont très sembables : 37,7 minutes pour l'automobile versus 38,4 et 41,6 pour le vélo et le transport en commun. Aussi, les variances des trois groupes sont relativement similaires.


```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
# chargement des dataframes
load("data/bivariee/dataPollution.RData")
# Statistiques descriptives pour les groupes (moyenne et écart-type)
df_TrajetsDuree %>%                                 # Nom du dataframe
    group_by(Mode) %>%                                # Variable qualitative
    get_summary_stats(DureeMinute, type = "mean_sd")  # Variable continue 
```

Pour visualiser la distribution des données pour les trois groupes, vous pouvez créer des graphiques de densité et en violons (figure \@ref(fig:figAnova1a)). La juxtaposition des trois distributions montre que les distributions des valeurs pour les trois groupes sont globalement similaires. Cela est corroboré par le fait que les boîtes du graphique en violons sont situées à la même hauteur. Autrement dit, à la lecture des deux graphiques, ils ne semblent pas y avoir de différences significatives entre les trois groupes en terme de temps de déplacement.

```{r figAnova1a, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Graphiques de densité et en violons', out.width='75%'}
library("ggplot2")
library("ggpubr")
# Graphique de densité
GraphDens <- ggplot(data = df_TrajetsDuree, 
  mapping=aes(x=DureeMinute,colour=Mode,fill=Mode)) +
  geom_density(alpha=0.55,mapping=aes(y=..scaled..))+
    labs(title="a. graphique de densité",
         x = "Densité",
         y = "Durée du trajets (en minutes)")
# Graphique en violons
GraphViolon <- ggplot(df_TrajetsDuree, aes(x=Mode, y=DureeMinute)) +
  geom_violin(fill="white") +
  geom_boxplot(width=0.1, aes(x=Mode, y=DureeMinute,fill=Mode))+
  labs(title="b. Graphique en violons",
       x = "Mode de transport",
       y = "Durée du trajets (en minutes)")+
  theme(legend.position = "none")
ggarrange(GraphDens, GraphViolon)
``` 

Nous pouvons vérifier si les échantillons sont normalement distribués avec la fonction `shapiro_test` du package **rstatix**. À titre de rappel, l'hypothèse nulle (h<sub>0</sub>) de ce test est que la distribution est normale. Par conséquent, quand la valeur de *P* associée à la statistique de Shapiro est supérieure à 0,05 alors on ne peut rejeter l'hypothèse d'une distribution normale (autrement dit, la distribution est anormale). À la lecture des sorties ci-dessous, seul le groupe des utilisateurs en transport en commun présente une distribution proche de la normalité (p=0,0504). Ce test étant très restrictif, il est fortement conseillé de visualiser le diagramme quantile-quantile pour chaque groupe (graphique QQ plot) (figure \@ref(fig:figQqplot)). Ces graphiques sont utilisés pour déterminer visuellement si une distribution empirique (observées sur des données), s'approche d'une distribution théorique (ici la loi normale). Si effectivement les deux distributions sont proches, les points du diagramme devraient tous tomber sur une ligne droite parfaite. Un intervale de confiance (représenté ici en gris) peut être construit pour obtenir une interprétation plus nuancée. Dans notre cas, seules deux observations pour le vélo et deux autres pour l'automobile s'éloignent vraiment de la ligne droite. On peut considérer que ces trois distributions s'approchent d'une distribution normale.

```{r figQqplot, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='QQ Plot pour les groupes', out.width='75%'}
library("dplyr")
library("ggpubr")
library("rstatix")
# Condition 1 : normalité des échantillons
# Test pour la normalité des échantillons (groupes) : test de Shapiro
 df_TrajetsDuree %>%          # Nom du dataframe
   group_by(Mode) %>%         # Variable qualitative
   shapiro_test(DureeMinute)  # Variable continue 
# Graphiques qqplot pour les groupes
ggqqplot(df_TrajetsDuree, "DureeMinute", facet.by = "Mode")
``` 

Pour vérifier l’hypothèse d’homogénéité des variances, vous pouvez utiliser les tests de Levene, de Bartlett ou de Breusch-Pagan. Les valeurs de *P*, toutes supérieures à 0,05, signalent que la condition d'homogénéité des variances est respectée.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
library("lmtest")
library("car")
# Condition 2 : homogénéité des variances (homocédasticité)
leveneTest(DureeMinute ~ Mode, data = df_TrajetsDuree)
bartlett.test(DureeMinute ~ Mode, data = df_TrajetsDuree)
bptest(DureeMinute ~ Mode, data = df_TrajetsDuree)
```

Deux fonctions peuvent être utilisées pour calculer l'analyse de variance : la fonction de base `aov(variable continue ~ variable qualitative, data = votre dataframe)` ou bien la fonction `anova_test(variable continue ~ variable qualitative, data = votre dataframe)` du package **rstatix**. Comparativement à `aov`, l'avantage de la fonction `anova_test` est qu'elle calcule aussi le Eta^2^.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
library("lmtest")
library("car")
# ANOVA avec la fonction aov
aov1 <- aov(DureeMinute ~ Mode, data = df_TrajetsDuree)
summary(aov1)
# calcul de Eta2 avec la fonction eta_sq du package lmtest
eta_sq(aov1)
# ANOVA avec la fonction anova_test du package rstatix
anova_test(DureeMinute ~ Mode, data = df_TrajetsDuree)
```

La valeur de *P* associée à la statistique *F* (0,444) nous permet de conclure qu'il n'y a pas de différences significatives entre les moyennes des temps de déplacements des trois modes de transport.

#### Deuxième ANOVA : différences entre les niveaux d'exposition au bruit

Dans ce second exercice, nous allons analyser les différences d'exposition au bruit. D'emblée, les statistiques descriptives révèlent que les moyennes sont dissemblables : 66,8 dB(A) pour l’automobile versus 68,8 et 74 pour le vélo et le transport en commun. Aussi, la variance du transport en commun est très différente des autres.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
# chargement des dataframes
load("data/bivariee/dataPollution.RData")
# Statistiques descriptives pour les groupes (moyenne et écart-type)
 df_Bruit %>%                                 # Nom du dataframe
   group_by(Mode) %>%                                # Variable qualitative
   get_summary_stats(laeq, type = "mean_sd")  # Variable continue 
```

À la lecture des graphiques de densité et en violon (figure \@ref(fig:figAnova1b)), il semble clair que les niveaux d'exposition au bruit sont plus faibles pour les automobilistes et plus élevés pour les cyclistes et surtout les utilisateurs du transport en commun. En outre, la distribution des valeurs d'exposition au bruit dans le transport en commun semble bimodale. Cela s'explique par le fait que les niveaux de bruit sont beaucoup élevés dans le métro que dans les autobus.

```{r figAnova1b, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Graphique de densité et en violons', out.width='75%'}
library("ggplot2")
library("ggpubr")
# Graphique en densité
GraphDens <- ggplot(data = df_Bruit, 
  mapping=aes(x=laeq,colour=Mode,fill=Mode)) +
  geom_density(alpha=0.55,mapping=aes(y=..scaled..))+
  labs(title="a. graphique de densité",
       x="Exposition au bruit (dB(A))")
# Graphique en violons
GraphViolon <- ggplot(df_Bruit, aes(x=Mode, y=laeq)) +
  geom_violin(fill="white") +
  geom_boxplot(width=0.1, aes(x=Mode, y=laeq,fill=Mode))+
  labs(title="b. Graphique en violons",
       x = "Mode de transport",
       y="Exposition au bruit (dB(A))")+
  theme(legend.position = "none")
ggarrange(GraphDens, GraphViolon)
``` 


Le test de Shapiro et les graphiques QQ plot (figure \@ref(fig:figQqplot2)) rélèvent que les distributions des trois groupes sont anormales. Ce résultat n'est pas surprenant si l'on tient compte de la nature logarithmique de l'échelle décibel.

```{r figQqplot2, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='QQ Plot pour les groupes', out.width='75%'}
library("dplyr")
library("ggpubr")
library("rstatix")
# Condition 1 : normalité des échantillons
# Test pour la normalité des échantillons (groupes) : test de Shapiro
df_Bruit %>%          # Nom du dataframe
  group_by(Mode) %>%         # Variable qualitative
  shapiro_test(laeq)  # Variable continue 
# Graphiques qqplot pour les groupes
ggqqplot(df_Bruit, "laeq", facet.by = "Mode")
``` 

En outre, selon les valeurs des tests de Levene, de Bartlett ou de Breusch-Pagan, les variances ne sont pas égales.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
library("lmtest")
library("car")
# Condition 2 : homogénéité des variances (homocédasticité)
leveneTest(laeq ~ Mode, data = df_Bruit)
bartlett.test(laeq ~ Mode, data = df_Bruit)
bptest(laeq ~ Mode, data = df_Bruit)
```

Étant donné que les deux conditions (normalité et homogénéité des variances) ne sont pas respectées, il serait préférable d'utiliser un test non paramétrique de Kruskal-Wallis. Calculons toutefois préalablement l'ANOVA classique et l'ANOVA de Welch puisque les variances ne sont pas égales). Les valeurs de *P* des deux tests (Fisher et Welch) signalent que les moyennes d'exposition au bruit sont statistiquement différentes entre les trois modes de transport.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
# ANOVA avec la fonction anova_test du package rstatix
anova_test(laeq ~ Mode, data = df_Bruit)
# ANOVA avec le test de Welch puisque les variances ne sont pas égales
welch_anova_test(laeq ~ Mode, data = df_Bruit)
```

Une fois démontré que les moyennes sont différentes, le test de Tukey est particulièrement intéressant puisqu'il nous permet de repérer les différences de moyennes significatives deux à deux, tout en ajustant les valeurs de *P* obtenues en fonction du nombre de comparaisons effectuées. Ci-dessous, on constate que toutes les paires sont statistiquement différentes et que la différence de moyennes entre les automobilistes et les cyclistes est de 1,9 dB(A) et surtout de 7,1 dB(A) entre les automobilistes et les usagers du transport en commun.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
aov2 <- aov(laeq ~ Mode, data = df_Bruit)
# Test de Tukey pour comparer les moyennes entre elles
TukeyHSD(aov2, conf.level = 0.95)
```

Le calcul de test non paramétrique de Kruskal-Wallis avec la fonction `kruskal.test` démontre aussi que les médianes des groupes sont différentes (*p*< 0,001). De manière comparable au test de Tukey, la fonction `pairwise.wilcox.test` permet aussi de repérer les différences significatives entre les paires de groupes. Pour conclure, tant l'ANOVA que le test non paramétrique de Kruskal-Wallis indiquent que les trois modes de transport sont significativement différents quant à l'exposition au bruit, avec des valeurs plus faibles pour les automobilistes comparativement aux cyclistes et aux usagers du transport en commun.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Test de Kruskal-Wallis
kruskal.test(laeq ~ Mode, data = df_Bruit)
# Calcul de la moyenne des rangs pour les trois groupes
df_Bruit$laeqRank <- rank(df_Bruit$laeq)
df_Bruit %>%
  group_by(Mode) %>%
  get_summary_stats(laeqRank, type = "mean")
# Compiraison des groupes avec la fonction pairwise.wilcox.test
pairwise.wilcox.test(df_Bruit$laeq, df_Bruit$Mode, p.adjust.method = "BH")
```

### Comment rapporter les résultats d'une ANOVA et du test de Kruskal-Wallis {#sect0444}

Plusieurs éléments doivent être reportés pour détailler les résultats d'une ANOVA ou d'un test de Kruskal-Wallis : la valeur de *F*, de *W* (dans le cas d'une ANOVA de Welch) ou du χ2 (Kruskal-Wallis), les valeurs de *P*, les moyennes ou médianes respectives des groupes et éventuellement un tableau détaillant les écarts intergroupes obtenus avec les test de Tuckey ou Wilcoxon par paires.

* Les résultats de l'analyse de variance à un facteur démontrent que le mode de transport utilisé n'a pas d'effet significatif sur le temps de déplacement en heures de pointe à Montréal (*F*(2,96)=0,82, *p*=0,444). En effet, pour des trajets de dix kilomètres entre un quartier périphérique et le centre-ville, les cyclistes (Moy=38,4, ET=15,2) arrivent en moyenne moins d'une minute après les automobilistes (Moy=37,7, ET=12,8) et les usagers du transport en commun moins de quatre minutes (Moy=41,6, ET=11,4). 

* Les résultats de l'analyse de variance à un facteur démontrent que le mode transport utilisé a un impact significatif sur le niveau d'exposition en heures de pointe à Montréal (*F*(2,96)=544, *p*<0,001 et *Welch*(2,96)=446, *p*<0,001). En effet, les usagers du transport en commun (Moy=74,0, ET=6,79) et les cyclistes (Moy=68,8, ET=4,3) sont significativement plus exposés au bruit que les automobilistes (Moy=66,8, ET=4,56). 

* Les résultats du test de Kruskal-Wallis démontre qu'il existe des différences significatives d'exposition au bruit entre les trois modes de transport (χ2(2) = 784,74, p<0,001) avec des moyennes de rangs de 1094 pour l'automobile, 1124 pour le vélo et 1207 pour le transport en commun.

::: {.bloc_aller_loin data-latex=""}
Nous avons vu que l'ANOVA permet de comparer les moyennes d'une variable continue à partir d'une variable qualitative comprenant plusieurs modalités (facteur) pour des observations indépendantes. Il y a un donc une seule variable dépendante (continue) et une seule variable indépendante. Sachez qu'il existe de nombreuses extensions de l'ANOVA classique : 

* **une ANOVA à deux facteurs**, soit avec une variable dépendante continue et deux variables indépendantes qualitatives (_two-way ANOVA_ en anglais). On évalue ainsi les effets des deux variables (*a*, *b*) et de leur interaction (*ab*) sur une variable continue.

* **une ANOVA multifacteur** avec une variable dépendante continue et plus de deux variables indépendantes qualitatives. Par exemple, avec trois variables qualitatives pour expliquer la variable continue, on inclut les effets de chaque variable qualitative (*a*, *b*, *c*) ainsi que de leurs interactions (*ab*, *ac*, *bc*, *abc*).

* **L'analyse de covariance** (**ANCOVA**, **AN**alysis of **COVA**riance en anglais) comprend une variable dépendante continue, une variable indépendante qualitative (facteur) et plusieurs variables indépendantes continues dites covariables. L'objectif est alors de vérifier si les moyennes d'une variable dépendante sont différentes pour plusieurs groupes d'une population donnée, après avoir contrôlé l'effet d'une ou plusieurs variables continues. Par exemple, pour une métropole donnée, on pourra vouloir comparer les moyennes de loyers entre la ville-centre et ceux de la première et de la seconde couronnes (facteur), une fois contrôlée la taille de ces derniers (variable covariée continue). En effet, une partie de la variance des loyers s'explique certainement par la taille des logements.

* **L'analyse de variance multivariée** (**MANOVA**, _**M**ultivariate **AN**alysis of **VA**riance_ en anglais) comprend deux variables dépendantes continues ou plus et une variable indépendante qualitative (facteur). Par exemple, on souheterait comparer les moyennes d'exposition au bruit et à différents polluants (dioxyde d'azote, particules fines, ozone) (variables dépendantes continues) selon le mode de transport utilisé (automobile, vélo, transport en commun, soit le facteur).

* **L'analyse de covariance multivariée** (**MANCOVA**, **M**ultivariate **AN**alysis of **COVA**riance en anglais), soit une analyse qui comprend deux variables dépendantes continues ou plus (comme la MANOVA) et une variable qualitative comme variable indépendante (facteur) et un covariable continue ou plus.

Pour le test *t*, nous avons vu qu'il peut s'appliquer soit à deux échantillons indépendants (non appariés), soit à deux échantillons dépendants (appariés). Notez qu'il existe aussi des extensions de l'ANOVA pour des échantillons pairés. On parle alors d'**analyse de variance sur des mesures répétées**. Par exemple, on pourrait évaluer la perception du sentiment de sécurité relativement à la pratique vélo d'hiver pour un échantillon de cyclistes ayant décidé de l'adopter récemment, et ce, à plusieurs moments : avant leur première saison, à la fin de leur premier hiver, à la fin de leur second hiver. Autre exemple, on pourrait sélectionner un échantillon d'individus (100 par exemple) pour lesquels on évaluerait leurs perceptions de l'environnement sonore dans différents lieux de la ville. Comme pour l'ANOVA classique (échantillons non appariés), il existe des extensions de l'ANOVA sur des mesures répétées permettant d'inclure plusieurs facteurs (groupes de population); on mesure alors une variable continue pour plusieurs groupes d'individus à différents moments ou conditions différentes. Il est aussi possible de réaliser une ANOVA pour des mesures répétées avec une ou plusieurs covariables continues. 

Bref, si l'ANOVA était un roman, elle serait certainement « un monde sans fin » de Ken Follett! Notez toutefois que la SUPERNOVA, la BOSSANOVA et le CASANOVA ne sont pas des variantes de l'ANOVA!
:::

<!--chapter:end:04-bivariee.Rmd-->

# (PART) Modèles de régression {-} 
# La régression linéaire multiple  {#chap05}

Dans ce chapitre, nous présenterons la méthode de régression certainement la plus utilisée en sciences sociales : la régression linéaire multiple. À titre de rappel, dans le chapitre précédent (section \@ref(sect0414)), nous avons vu que la régression linéaire simple, basée sur la méthode des moindres carrées, permet d’expliquer et de prédire une variable continue en fonction d’une autre variable. Toutefois, quel que soit le domaine d’étude, il est rare que le recours à une seule variable explicative (*X*) permette de prédire efficacement une variable continue (*Y*). La régression linéaire multiple est simplement une extension de la régression linéaire simple : elle permet ainsi de prédire et expliquer une variable indépendante (*Y*) en fonction de plusieurs variables indépendantes (explicatives).

Plus spécifiquement, nous aborderons ici les principes et hypothèses de la régression linéaire multiple, comment mesurer la qualité d’ajustement du modèle, introduire des variables explicatives particulières (variable qualitative dichotomique ou polytomique, variable d’interaction, etc.), interpréter les sorties d’un modèle de régression et finalement la mettre en oeuvre dans R.


::: {.bloc_package data-latex=""}
Dans cette section, nous utiliserons principalement les *packages* suivants : 

A MODIFIER A LA FIN

* Pour créer des graphiques :
  - **ggplot2**, le seul, l'unique
  - **ggpubr**, pour combiner des graphiques et réaliser des diagrammes quantiles-quantiles
* Pour manipuler des données : 
  - **dplyr**, avec les fonctions *group_by*, *summarize* et les pipes *%>%*
* Pour les corrélations (section \@ref(sect0411)) : 
  - **correlation**, de l'ensemble de package **easy_stats**, offrant une large gamme de méthodes de corrélations
  - **boot** pour réaliser des corrélations avec *bootstrap* 
  - **Hmisc** pour calculer des corrélations de Pearson et Spearman
  - **ppcor**, notamment pour des corrélations partielles
  - **psych** pour obtenir une matrice de  corrélation (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de p.
  - **stargazer** pour créer des beaux tableaux d’une matrice de corrélation en Html ou en LaTeX ou en ASCII.
  - **corrplot**, pour créer des graphiques de matrices de corrélation
* Pour le tableau de contignence (section \@ref(sect0412)) :
  - **gmodels**, pour construire des tableaux de contingence et calculer les tests *t* et ses différentes variantes (section \@ref(sect0424))
  - **vcd**, pour construire un graphique pour un tableau de contigence ((section \@ref(sect0424)))
* Pour les test *t* : 
  - **sjstats** pour réaliser des test *t* pondérés
  - **effectsize**, pour calculer les tailles d'effet de tests de *t*
* Pour la section sur les ANOVA (section \@ref(sect0441)) : 
  - **car**, pour les ANOVA classiques
  - *lmtest* pour le test de Breusch-Pagan d'homogénéité des variances 
  - **rstatix**, intégrant de nombreux tests classiques (comme le test de Shapiro) avec **tidyverse**
:::

```{r message=FALSE, warning=FALSE, include=FALSE}
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#### Brant test pour vglm (adapte du package brant) ####
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

print.testresult.vglm <- function(model,X2,df.v,by.var) {
  p.values = pchisq(X2,df.v,lower.tail=FALSE) 
  woit <- grepl("Intercept",names(coef(model)),fixed=T) == F
  if(by.var){
    var.names = unlist(strsplit(as.character(formula(model))[3],split=" \\+ "))
  }else{
    var.names = names(coef(model)[woit])
  }
  # longest name
  longest.char = max(nchar(var.names))
  n.tabs = ceiling(longest.char/7)
  n.tabs = ifelse(n.tabs<2,2,n.tabs)
  cat(paste0(rep("-",28+8*n.tabs),collapse = ""),"\n")
  cat(paste0("Test for",paste0(rep("\t",n.tabs-1),collapse = ""),"X2\tdf\tprobability"),"\n")
  cat(paste0(rep("-",28+8*n.tabs),collapse = ""),"\n")
  cat(paste0("Omnibus",paste0(rep("\t",n.tabs),collapse = ""),round(X2[1],digits=2),"\t",df.v[1],"\t",round(p.values[1],digits=2)))
  cat("\n")
  for(i in 1:length(var.names)){
    name = var.names[i]
    tabs.sub = ceiling(nchar(name)/7)-1
    cat(paste0(name,paste0(rep("\t",n.tabs-tabs.sub),collapse = ""),round(X2[i+1],digits=2),"\t",df.v[i+1],"\t",round(p.values[i+1],digits=2),"\n"))
  }
  cat(paste0(rep("-",28+8*n.tabs),collapse = ""),"\n\n")
  cat("H0: Parallel Regression Assumption holds")
  result.matrix = matrix(c(X2, df.v, p.values), ncol = 3)
  rownames(result.matrix) = c("Omnibus", var.names)
  colnames(result.matrix) = c("X2","df","probability")
  result.matrix
}

getCombiCoefs.vglm <- function(model){
  classes = sapply(model@model, class)
  factors = ifelse(classes[2:length(classes)]!="numeric",T,F)
  f = i = var = 1
  woit <- grepl("Intercept",names(coef(model)),fixed=T) == F
  len <- length(coef(model)[woit])
  result = data.frame(i=1:len,var=NA)
  for(factor in factors){
    if(factor){
      n = length(unlist(model@xlevels[f]))
      for(j in 1:(n-1)){
        result[i,"var"] = var
        i = i + 1
      }
      var = var + 1
      f = f + 1
    }else{
      result[i,"var"] = var
      var = var + 1
      i = i + 1
    }
  }
  return(result)
}

brant.vglm <- function(model,by.var=F){
  temp.data = model@model
  y_name = as.character(formula(model))[2]
  x_names = as.character(formula(model))[3]
  y = as.numeric(temp.data[[y_name]])
  temp.data$y = y
  
  x.variables = strsplit(x_names," \\+ ")[[1]]
  x.factors = c()
  for(name in x.variables){
    if(!is.numeric(temp.data[,name])){
      x.factors = c(x.factors,name)
    }
  }
  if(length(x.factors)>0){
    tab = table(data.frame(temp.data[,y_name],temp.data[,x.factors]))
    count0 = sum(tab==0)
  }else{
    count0 = 0
  }
  
  
  J = max(y,na.rm=T)
  
  woit <- grepl("Intercept",names(coef(model)),fixed=T) == F
  
  K = length(coef(model)[woit])
  for(m in 1:(J-1)){
    temp.data[[paste0("z",m)]] = ifelse(y>m,1,0)
  }
  binary.models = list()
  beta.hat = matrix(NA,nrow=J-1,ncol=K+1,byrow=T)
  var.hat = list()
  for(m in 1:(J-1)){
    mod = glm(paste0("z",m," ~ ",x_names),data=temp.data, family="binomial")
    binary.models[[paste0("model",m)]] = mod
    beta.hat[m,] = coef(mod)
    var.hat[[m]] = vcov(mod)
  }
  
  X.temp = model@model[2:length(model@model)]
  X = matrix(1,nrow=length(X.temp[,1]),ncol=1)
  for(var in X.temp){
    if(is.numeric(var)){
      X = cbind(X,var)
    }
    if(is.character(var)){
      var = as.factor(var)
    }
    if(is.factor(var)){
      for(level in levels(var)[2:length(levels(var))]){
        X = cbind(X,ifelse(var==level,1,0))
      }
    }
  }
  zeta <- model@coefficients[woit==F] * -1
  tau = matrix(zeta,nrow=1,ncol=J-1,byrow=T)
  pi.hat = matrix(NA,nrow=length(model@model[,1]),ncol=J-1,byrow=T)
  for(m in 1:(J-1)){
    pi.hat[,m] = binary.models[[m]]$fitted.values
  }
  
  
  varBeta = matrix(NA,nrow = (J-1)*K, ncol = (J-1)*K)
  for(m in 1:(J-2)){
    for(l in (m+1):(J-1)){
      Wml = Matrix::Diagonal(x=pi.hat[,l] - pi.hat[,m]*pi.hat[,l])
      Wm = Matrix::Diagonal(x=pi.hat[,m] - pi.hat[,m]*pi.hat[,m])
      Wl = Matrix::Diagonal(x=pi.hat[,l] - pi.hat[,l]*pi.hat[,l])
      Xt = t(X)
      varBeta[((m-1)*K+1):(m*K),((l-1)*K+1):(l*K)] = as.matrix((solve(Xt %*% Wm %*% X)%*%(Xt %*% Wml %*% X)%*%solve(Xt %*% Wl %*% X))[-1,-1])
      varBeta[((l-1)*K+1):(l*K),((m-1)*K+1):(m*K)] = varBeta[((m-1)*K+1):(m*K),((l-1)*K+1):(l*K)]
    }
  }
  
  betaStar = c()
  for(m in 1:(J-1)){
    betaStar = c(betaStar,beta.hat[m,-1])
  }
  for(m in 1:(J-1)){
    varBeta[((m-1)*K+1):(m*K),((m-1)*K+1):(m*K)] = var.hat[[m]][-1,-1]
  }
  
  I = diag(1,K)
  E0 = diag(0,K)
  for(i in 1:(J-2)){
    for(j in 1:(J-1)){
      if(j == 1){
        temp = I
      }else if(j == i+1){
        temp = cbind(temp,-I)
      }else{
        temp = cbind(temp,E0)
      }
    }
    if(i==1){
      D = temp
    }else{
      D = rbind(D,temp)
    }
  }
  X2 = t(D%*%betaStar) %*% solve(D %*% varBeta %*% t(D)) %*% (D %*% betaStar)
  df.v = (J-2)*K
  
  if(by.var){
    combinations = getCombiCoefs.vglm(model)
    for(v in unique(combinations$var)){
      k = subset(combinations,var==v)$i
      s = c()
      df.v.temp = 0
      for(e in k){
        s = c(s,seq(from=e,to=K*(J-1),by=K))
        df.v.temp = df.v.temp + J-2
      }
      s = sort(s)
      Ds = D[,s]
      Ds = Ds[which(!apply(Ds==0,1,all)),]
      if(!is.null(dim(Ds)))
        X2 = c(X2,t(Ds%*%betaStar[s]) %*% solve(Ds %*% varBeta[s,s] %*% t(Ds)) %*% (Ds %*% betaStar[s]))
      else
        X2 = c(X2,t(Ds%*%betaStar[s]) %*% solve(Ds %*% varBeta[s,s] %*% t(t(Ds))) %*% (Ds %*% betaStar[s]))
      df.v = c(df.v,df.v.temp)
    }
  }else{
    for(k in 1:K){
      s = seq(from=k,to=K*(J-1),by=K)
      Ds = D[,s]
      Ds = Ds[which(!apply(Ds==0,1,all)),]
      if(!is.null(dim(Ds)))
        X2 = c(X2,t(Ds%*%betaStar[s]) %*% solve(Ds %*% varBeta[s,s] %*% t(Ds)) %*% (Ds %*% betaStar[s]))
      else
        X2 = c(X2,t(Ds%*%betaStar[s]) %*% solve(Ds %*% varBeta[s,s] %*% t(t(Ds))) %*% (Ds %*% betaStar[s]))
      df.v = c(df.v,J-2)
    }
  }
  
  result.matrix = print.testresult.vglm(model,X2,df.v,by.var)
  if(count0!=0){
    warning(paste0(count0," combinations in table(dv,ivs) do not occur. Because of that, the test results might be invalid."))
  }
  result.matrix
}


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#### parallel likelihood ratio test pour vglm ordinal ####
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

parallel.likelihoodtest.vglm <- function(model){
  
  ## recuperer les variable de l'equation
  formule <- as.formula(as.character(model@terms))
  xvars <- strsplit(as.character(formule)[[3]]," + ", fixed = T)[[1]]
  
  pb <- txtProgressBar(min = 0, max = length(xvars), style = 3)
  i<-1
  test_results <- lapply(xvars,function(x){
    formule2 <- as.formula(paste(FALSE, "~ 1 +",x))
    model2 <- vglm(formule,
            family = cumulative(link="logitlink",parallel = formule2, reverse = TRUE),
            data = model@model)
    test <- anova.vglm(model2,model, type = "I")
    values <- list(
      "variable non parallele" = x,
      "AIC" = round(AIC(model2)),
      "loglikelihood" = round(logLik(model2)),
      "p.val loglikelihood ratio test" = round(test$`Pr(>Chi)`[[2]],3))
    setTxtProgressBar(pb, i)
    i<<- i+1
    return(values)
  })
  
  tableau_final <- as.data.frame(t(matrix(unlist(test_results), nrow=length(unlist(test_results[1])))))
  names(tableau_final) <- c("variable non parallele", "AIC", "loglikelihood", "p.val loglikelihood ratio test")
  return(tableau_final)
}


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#### Analyse de type 3, modele multinomial ####
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

AnalyseType3<-function(modele, data, fixed_vars = NULL){
  logModeleComplet <- -2*logLik(modele)          # valeur du loglikehood pour le modèle complet
  results <- list()  # liste vide qui comprendra les resultats
  all_vars <- strsplit(as.character(modele@terms), split = " ~ ")
  vardep <- all_vars[[1]][[1]]
  varsindep <- strsplit(all_vars[[1]][[2]], split = " + ", fixed = T)[[1]]
  if(is.null(fixed_vars) == F){
    testing_vars <- varsindep[varsindep %in% fixed_vars == F]
  }else{
    testing_vars <- varsindep
  }
  i <- 1
  pb <- txtProgressBar(min = 0, max = length(testing_vars), style = 3)
  for(x1 in testing_vars){
    # Récupération de la liste des variables indépendantes moins chaque variable dépendante
    listvarindep = ""
    for(x2 in varsindep){
      if(x1 != x2){
        listvarindep <- paste(listvarindep, "+", x2)
      }
    }
    listvarindep <-  substr(listvarindep,3,nchar(listvarindep))
    
    # Construction du modèle',
    formule <- as.formula(paste(vardep, " ~ ", listvarindep))
    model2 <- vglm(formule,
                   family = multinomial(parallel = FALSE),
                   data = LCGM)
    logLikVI[i] <- -2*logLik(model2)
    test <- anova(model2,modele, type = "I")
    values <- list(
      "removed" = x1,
      "AIC" = round(AIC(model2)), 
      "loglike" = round(-2* logLik(model2)),
      "sign" = round(test$`Pr(>Chi)`[[2]],4))
    results[[length(results) + 1]] <- values
    setTxtProgressBar(pb, i)
    i <- i + 1;
  }
  
  cat("*************************************", "\n")
  cat("Type 3 Analysis of Effects", "\n")
  cat("*************************************", "\n")
  cat("AIC model complet : ", round(AIC(modele)), "\n")
  cat("loglikelihood model complet : ", round(logModeleComplet), "\n")
  tableau_final <- as.data.frame(t(matrix(unlist(results), nrow=length(unlist(results[1])))))
  names(tableau_final) <- c("variable retiree", 
                            "AIC", "loglikelihood", "p.val"
  )
  print(tableau_final)
  return(tableau_final)
  
}

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#### pseudo R2 ####
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
rsqs <- function(loglike.full, loglike.null,full.deviance, null.deviance, nb.params, n){
  
  explained_dev <- 1-(full.deviance / null.deviance)
  
  K <- nb.params
  
  r2_faddenadj <- 1- (loglike.full - K) / loglike.null
  
  Lm <- loglike.full
  Ln <- loglike.null
  Rcs <- 1 - exp((-2/n) * (Lm-Ln))
  Rn <- Rcs / (1-exp(2*Ln/n))
  return(
    list("deviance expliquee" = explained_dev,
         "MacFadden ajuste" = r2_faddenadj,
         "Cox and Snell" = Rcs,
         "Nagelkerke" = Rn
    )
  )
}


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#### Afficher une belle matrice de confusion ####
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

nice_confusion_matrix <- function(yreal, ypred){
  library(caret)
  ## generation de la matrice avec caret
  info <- confusionMatrix(as.factor(yreal), as.factor(ypred))
  
  ##pimper le tout
  mat <- info[[2]]
  rs <- rowSums(mat)
  rp <- round(rowSums(mat) / sum(mat) * 100,1)
  cs <- colSums(mat)
  cp <- round(colSums(mat) / sum(mat) * 100,1)
  mat2 <- cbind(mat,rs,rp)
  mat3 <- rbind(mat2,c(cs,sum(mat),NA),c(cp,NA,NA))
  
  rowsnames <- c(paste(colnames(mat),"(predit)"),"Total", "%")
  colsnames <- c("",paste(colnames(mat),"(reel)"),"Total", "%")
  
  mat4 <- cbind(rowsnames, mat3)
  mat5 <- rbind(colsnames, mat4)
  print(kable(mat5,
        row.names = F
  ))
  
  ## calcule des indicateurs pour chaque categorie
  precision <- diag(mat) / rowSums(mat)
  rappel <- diag(mat) / colSums(mat)
  F1 <- 2*((precision*rappel)/(precision + rappel))
  
  macro_scores <- c(weighted.mean(precision,colSums(mat)),
                    weighted.mean(rappel,colSums(mat)),
                    weighted.mean(F1,colSums(mat)))
  
  final_table <- rbind(cbind(precision,rappel,F1),macro_scores)
  final_table <- rbind(final_table, c(info[[3]][[2]],NA,NA), c(info[[3]][[6]],NA,NA))
  rnames <- c(rownames(mat),"macro","Kappa","Valeur de p  (prÃ©cision > NIR)")
  final_table <- cbind(rnames,round(final_table,2))
  
  print(kable(final_table, row.names =F))
  return(list("confusion_matrix" = mat5,
         "indicators" = final_table))
  
}


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#### Afficher les coeff de models (glm, vglm) ####
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

## fonction pour cleaner les colonnes

clean_columns <- function(tableau, digits){
  for(i in 1:length(digits)){
    d <- digits[[i]]
    if(is.na(d)==F){
      col <- tableau[,i]
      #nettoyer les 0
      test1 <- col == "0"
      col[test1] <- paste("<0.",paste(rep("0",(d-1)),collapse=""),"1",sep="")
      #nettoyer les arrondis trop intenses
      col <- sapply(col,function(t){
        if (grepl(".",t,fixed = T)){
          dig <- strsplit(t,".",fixed=T)[[1]][[2]]
          if(nchar(dig) == d){
            return(t)
          }else{
            dif <- d - nchar(dig)
            newt <- paste(t,paste(rep("0",dif),collapse=""),sep="")
          }
          return(newt)
        }else if (t == "1"){
          newt <- paste("1.",paste(rep("0",d),collapse=""),sep="")
        }else{
          return(t)
        }
      })
      tableau[,i] <- col
    }
  }
  return(tableau)
}

sign_col <- function(tableau){
  i<-match("val .p", colnames(tableau))
  col <- tableau[,i]
  sign <- sapply(col, function(j){
    if(j == "--"){
      return("--")
    }else if (j == ""){
      return("")
    }else{
      num <- as.numeric(gsub("<","",j, fixed=T))
      if(num<=0.001){
        return("***")
      }else if (num <= 0.01){
        return("**")
      }else if (num <= 0.05){
        return("*")
      }else if (num <= 0.1){
        return(".")
      }else {
        return("")
      }
    }
  })
  return(sign)
}

## fonction generale
build_table <- function(model, confid = T, sign = T, coef_digits = 2, std_digits = 2, z_digits = 2, p_digits = 3, OR_digits = 3){
  
  if (class(model)[[1]]=="vglm"){
    tableau <- build_table.vglm(model, confid = confid, coef_digits = coef_digits, std_digits = std_digits, z_digits = z_digits, p_digits = p_digits, OR_digits = p_digits)
  }else if(class(model)[[1]] %in% c("glm","lm")){
    tableau <- build_table.glm(model, confid = confid, coef_digits = coef_digits, std_digits = std_digits, z_digits = z_digits, p_digits = p_digits, OR_digits = p_digits)
  }else if(class(model)[[1]] == "gam"){
    tableau <- build_table.gam(model, confid = confid, coef_digits = coef_digits, std_digits = std_digits, z_digits = z_digits, p_digits = p_digits, OR_digits = p_digits)
  }
  
  if(sign){
    if (class(tableau) == "matrix"){
      newcol <- sign_col(tableau)
      new_names <- c(colnames(tableau),"Signif. codes")
      tableau <- cbind(tableau, newcol)
      colnames(tableau) <- new_names
      rownames(tableau) <- NULL
    }else {
      tableau <- lapply(tableau, function(t1){
        newcol <- sign_col(t1)
        new_names <- c(colnames(t1),"Signif. codes")
        t1 <- cbind(t1, newcol)
        colnames(t1) <- new_names
        rownames(t1) <- NULL
        return(t1)
      })
    }
  }
  
  return(tableau)
  
}

## pour un glm
build_table.glm <- function(model, confid = T, coef_digits = 2, std_digits = 2, z_digits = 2, p_digits = 3, OR_digits = 3){
  ## extraction des elements principaux
  base_table <- summary(model)$coefficients
  
  ## calcule des intervale de confiance sur les coeffs
  if(confid){
    base_table <- cbind(base_table, round(confint(model),coef_digits))
  }
  
  ## si fonction de lien = logit : OR
  if(class(model)=='glm'){
    if (model$family$link == "logit"){
      base_table <- cbind(base_table[,1], round(exp(base_table[,1]),OR_digits) , base_table[,2:ncol(base_table)])
      if(confid){
        n <- ncol(base_table)
        base_table <- cbind(base_table, round(exp(base_table[,c(n-1,n)]), OR_digits))
      }
    }else if (model$family$link == "log"){
      base_table <- cbind(base_table[,1], round(exp(base_table[,1]),coef_digits),base_table[,2:ncol(base_table)])
      if (confid){
        n <- ncol(base_table)
        base_table <- cbind(base_table, round(exp(base_table[,c(n-1,n)]), coef_digits))
      }
    }
  }
  
  
  ## gerer les arrondis
  base_table[,1] <- round(base_table[,1], coef_digits)
  i<-match("Std. Error", colnames(base_table))
  base_table[,i] <- round(base_table[,i], std_digits)
  
  testt <- i<-match("t value", colnames(base_table))
  
  if(is.na(testt)){
    i<-match("z value", colnames(base_table))
    base_table[,i] <- round(base_table[,i], z_digits)
    i<-match("Pr(>|z|)", colnames(base_table))
    base_table[,i] <- round(base_table[,i], p_digits)
  }else{
    i<-match("t value", colnames(base_table))
    base_table[,i] <- round(base_table[,i], z_digits)
    i<-match("Pr(>|t|)", colnames(base_table))
    base_table[,i] <- round(base_table[,i], p_digits)
  }
  
  params_names <- as.character(model$terms)
  params_names <- params_names[3:length(params_names)]
  params_names <- strsplit(params_names," + ", fixed = T)[[1]]
  params_types <- sapply(params_names, function(i){
    if(class(model)=="lm"){
      return(class(model$model[[i]]))
    }else{
      return(class(model$data[[i]]))
    }
    
  })
  
  params_names <- c("Intercept",params_names)
  params_types <- c("numeric", params_types)
  
  ## creation d'un beau tableau
  allrows <- lapply(1:length(params_names), function(i){
    pname <- params_names[[i]]
    ptype <- params_types[[i]]
    rn <- rownames(base_table)
    if(ptype %in% c("character","factor")){
      rows <- base_table[grepl(pname,x = rn,fixed = T),]
      uvalues <- unique(as.character(model$data[[pname]]))
      if(length(uvalues)>2){
        new_names <- gsub(pattern = pname, x = row.names(rows), replacement = "", fixed = T)
      }else{
        oldname <- rn[grepl(pname,x = rn,fixed = T)]
        new_names <- gsub(pattern = pname, x = oldname, replacement = "", fixed = T)
      }
      ref <- unique(uvalues[! uvalues %in% new_names])
      new_table <- rbind("--", rows)
      new_table <- cbind(c(paste("ref : ",ref,sep=""),new_names),new_table)
      row1 <- c(paste("*",pname,"*",sep=""),rep("",ncol(new_table)-1))
      new_table <- rbind(row1,new_table)
      return(new_table)
    }
    if(ptype %in% c("integer", "double", "numeric")){
      if(pname == "Intercept"){
        row <- base_table[rn=="(Intercept)",]
      }else{
        row <- base_table[rn==pname,]
      }
      row <- c(pname, row)
      return(row)
    }
  })
  
  if(is.na(testt)){
    letter <- "z"
  }else{
    letter <- "t"
  }
  
  final_table <- do.call(rbind,allrows)
  if(class(model)!="lm"){
    if(model$family$link == "logit" & confid){
      colnames(final_table) <- c("variable", "coefficient", "OR",
                                 "err. std",paste("val.",letter), "val .p",
                                 "coeff 2.5%", "coeff 97.5%",
                                 "OR 2.5%", "oR 97.5%")
      final_table <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                  z_digits,p_digits,coef_digits,coef_digits,
                                                  OR_digits,OR_digits))
      
    }else if (model$family$link == "logit" & confid==F){
      colnames(final_table) <- c("variable", "coefficient", "OR",
                                 "err. std",paste("val.",letter), "val .p")
      final_table <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                  z_digits,p_digits))
      
    }else if(model$family$link == "log" & confid){
      colnames(final_table) <- c("variable", "coefficient", "exp(coeff)",
                                 "err. std",paste("val.",letter), "val .p",
                                 "coeff 2.5%", "coeff 97.5%",
                                 "exp(coeff 2.5%)", "exp(coeff 97.5%)")
      final_table <- clean_columns(final_table, c(NA, coef_digits, coef_digits, std_digits,
                                                  z_digits,p_digits,coef_digits,coef_digits,
                                                  coef_digits,coef_digits))
      
    }else if (model$family$link == "logit" & confid==F){
      colnames(final_table) <- c("variable", "coefficient", "exp(coeff)",
                                 "err. std",paste("val.",letter), "val .p")
      final_table <- clean_columns(final_table, c(NA, coef_digits, coef_digits, std_digits,
                                                  z_digits,p_digits))
      
    }else if(confid){
      colnames(final_table) <- c("variable", "coefficient",
                                 "err. std",paste("val.",letter), "val .p",
                                 "coeff 2.5%", "coeff 97.5%")
      final_table <- clean_columns(final_table, c(NA, coef_digits, std_digits,
                                                  z_digits,p_digits, coef_digits, coef_digits))
      
    }else if(confid==F){
      colnames(final_table) <- c("variable", "coefficient",
                                 "err. std",paste("val.",letter), "val .p")
      final_table <- clean_columns(final_table, c(NA, coef_digits, std_digits,
                                                  z_digits,p_digits))
    }
    
  }else{
    if(confid){
      colnames(final_table) <- c("variable", "coefficient",
                                 "err. std",paste("val.",letter), "val .p",
                                 "coeff 2.5%", "coeff 97.5%")
      final_table <- clean_columns(final_table, c(NA, coef_digits, std_digits,
                                                  z_digits,p_digits, coef_digits, coef_digits))
    }else if(confid==F){
      colnames(final_table) <- c("variable", "coefficient",
                                 "err. std",paste("val.",letter), "val .p")
      final_table <- clean_columns(final_table, c(NA, coef_digits, std_digits,
                                                  z_digits,p_digits))
    }
    
  }
  
  rownames(final_table) <- NULL
  return(final_table)
}

## pour un gam
build_table.gam <- function(model, confid = T, coef_digits = 2, std_digits = 2, z_digits = 2, p_digits = 3, OR_digits = 3){
  ## extraction des elements principaux
  base_table <- summary(model)
  coeffs <- model$coefficients
  test <- grepl("s(",names(coeffs),fixed=T) == F
  base_table <- cbind(coeffs[test],
                      base_table$se[test],
                      base_table$p.t[test],
                      base_table$p.pv[test]
                      )
  colnames(base_table) <- c("Estimate","Std. Error", "z value","Pr(>|z|)")
  
  ## calcule des intervale de confiance sur les coeffs
  if(confid){
    base_table <- cbind(base_table, round(hand_contint.gam(model),coef_digits))
  }
  
  ## si fonction de lien = logit : OR
  if (model$family$link == "logit"){
    base_table <- cbind(base_table[,1], round(exp(base_table[,1]),OR_digits) , base_table[,2:ncol(base_table)])
    if(confid){
      n <- ncol(base_table)
      base_table <- cbind(base_table, round(exp(base_table[,c(n-1,n)]), OR_digits))
    }
  }else if (model$family$link == "log"){
    base_table <- cbind(base_table[,1], round(exp(base_table[,1]),coef_digits) , base_table[,2:ncol(base_table)])
    if(confid){
      n <- ncol(base_table)
      base_table <- cbind(base_table, round(exp(base_table[,c(n-1,n)]), coef_digits))
    }
  }else if (as.character(model$family)[[1]] == "ziplss"){
    base_table <- cbind(base_table[,1], round(exp(base_table[,1]),coef_digits) , base_table[,2:ncol(base_table)])
    if(confid){
      n <- ncol(base_table)
      base_table <- cbind(base_table, round(exp(base_table[,c(n-1,n)]), coef_digits))
    }
  }
  
  ## gerer les arrondis
  base_table[,1] <- round(base_table[,1], coef_digits)
  i<-match("Std. Error", colnames(base_table))
  base_table[,i] <- round(base_table[,i], std_digits)
  i<-match("z value", colnames(base_table))
  base_table[,i] <- round(base_table[,i], z_digits)
  i<-match("Pr(>|z|)", colnames(base_table))
  base_table[,i] <- round(base_table[,i], p_digits)
  
  if(as.character(model$family)[[1]] %in% c("ziplss")){
    params_names <- strsplit(as.character(model$formula)[[1]]," ~ ", fixed = T)[[1]][[2]]
    params_names2 <- as.character(model$formula)[[2]]
    params_names <- strsplit(params_names," + ", fixed = T)[[1]]
    params_names2 <- strsplit(params_names2," + ", fixed = T)[[1]]
    params_names2 <- params_names2[2:length(params_names2)]
    params_names <- params_names[grepl("s(",params_names, fixed=T)==F]
    params_names2 <- params_names2[grepl("s(",params_names2, fixed=T)==F]
    params_types <- sapply(params_names, function(i){
      class(model$model[[i]])
    })
    params_types2 <- sapply(params_names2, function(i){
      class(model$model[[i]])
    })
    params_names <- c("Intercept",params_names)
    params_types <- c("numeric", params_types)
    params_names2 <- c("Intercept",params_names2)
    params_types2 <- c("numeric", params_types2)
    
  }else{
    params_names <- as.character(model$formula)
    params_names <- params_names[3:length(params_names)]
    params_names <- strsplit(params_names," + ", fixed = T)[[1]]
    params_names <- params_names[grepl("s(",params_names, fixed=T)==F]
    params_types <- sapply(params_names, function(i){
      class(model$model[[i]])
    })
    params_names2 <- NULL
    params_names <- c("Intercept",params_names)
    params_types <- c("numeric", params_types)
  }
  
  
  ## creation d'un beau tableau
  allrows <- lapply(1:length(params_names), function(i){
    pname <- params_names[[i]]
    ptype <- params_types[[i]]
    rn <- rownames(base_table)
    if(ptype %in% c("character","factor")){
      rows <- base_table[grepl(pname,x = rn,fixed = T) & grepl(".1",x=rn,fixed=F)==F,]
      uvalues <- unique(as.character(model$data[[pname]]))
      if(length(uvalues)>2){
        new_names <- gsub(pattern = pname, x = row.names(rows), replacement = "", fixed = T)
      }else{
        oldname <- rn[grepl(pname,x = rn,fixed = T)]
        new_names <- gsub(pattern = pname, x = oldname, replacement = "", fixed = T)
      }
      ref <- unique(uvalues[! uvalues %in% new_names])
      new_table <- rbind("--", rows)
      new_table <- cbind(c(paste("ref : ",ref,sep=""),new_names),new_table)
      row1 <- c(paste("*",pname,"*",sep=""),rep("",ncol(new_table)-1))
      new_table <- rbind(row1,new_table)
      return(new_table)
    }
    if(ptype %in% c("integer", "double", "numeric")){
      if(pname == "Intercept"){
        row <- base_table[rn=="(Intercept)",]
      }else{
        row <- base_table[rn==pname,]
      }
      row <- c(pname, row)
      return(row)
    }
  })
  
  final_table <- do.call(rbind,allrows)
  
  if(is.null(params_names2)==F){
    allrows2 <- lapply(1:length(params_names2), function(i){
      pname <- params_names2[[i]]
      ptype <- params_types2[[i]]
      rn <- rownames(base_table)
      if(ptype %in% c("character","factor")){
        rows <- base_table[grepl(pname,x = rn,fixed = T) & grepl(".1",x=rn,fixed=F)==T,]
        uvalues <- unique(as.character(model$data[[pname]]))
        if(length(uvalues)>2){
          new_names <- gsub(pattern = pname, x = row.names(rows), replacement = "", fixed = T)
        }else{
          oldname <- rn[grepl(pname,x = rn,fixed = T)]
          new_names <- gsub(pattern = pname, x = oldname, replacement = "", fixed = T)
        }
        ref <- unique(uvalues[! uvalues %in% new_names])
        new_table <- rbind("--", rows)
        new_table <- cbind(c(paste("ref : ",ref,sep=""),new_names),new_table)
        row1 <- c(paste("*",pname,"*",sep=""),rep("",ncol(new_table)-1))
        new_table <- rbind(row1,new_table)
        return(new_table)
      }
      if(ptype %in% c("integer", "double", "numeric")){
        if(pname == "Intercept"){
          row <- base_table[rn=="(Intercept).1",]
        }else{
          row <- base_table[rn==paste(pname,".1",sep=""),]
        }
        row <- c(pname, row)
        return(row)
      }
    })
    
    final_table2 <- do.call(rbind,allrows2)
    if(confid){
      colnames(final_table2) <- c("variable", "coefficient", "OR",
                                  "err. std","val. z", "val .p",
                                  "coeff 2.5%", "coeff 97.5%",
                                  "OR 2.5%", "OR 97.5%")
      final_table <- final_table[,c(1,2,3,4,5,6,7,8,9,10)]
      
      final_table <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                  z_digits,p_digits, coef_digits, coef_digits, 
                                                  OR_digits, OR_digits))
      
      colnames(final_table) <- c("variable", "coefficient", "exp(coeff)",
                                 "err. std","val. z", "val .p",
                                 "coeff 2.5%", "coeff 97.5%",
                                 "exp(coeff) 2.5%", "exp(coeff) 97.5%")
      final_table2 <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                  z_digits,p_digits, coef_digits, coef_digits, 
                                                  OR_digits, OR_digits))
    }else{
      colnames(final_table2) <- c("variable", "coefficient", "OR",
                                  "err. std","val. z", "val .p")
      final_table <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                  z_digits,p_digits))
      final_table <- final_table[,c(1,2,3,4,5,6)]
      colnames(final_table) <- c("variable", "coefficient", "exp(coeff)",
                                 "err. std","val. z", "val .p")
      final_table2 <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                   z_digits,p_digits))
      
    }
    rownames(final_table2) <- NULL
    rownames(final_table) <- NULL
    return(list(final_table, final_table2))
  }
  
  
  if(model$family$link == "logit" & confid){
    colnames(final_table) <- c("variable", "coefficient", "OR",
                               "err. std","val. z", "val .p",
                               "coeff 2.5%", "coeff 97.5%",
                               "OR 2.5%", "oR 97.5%")
    final_table <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                z_digits,p_digits, coef_digits, coef_digits, 
                                                OR_digits, OR_digits))
    
  }else if (model$family$link == "logit" & confid==F){
    colnames(final_table) <- c("variable", "coefficient", "OR",
                               "err. std","val. z", "val .p")
    final_table <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                z_digits,p_digits))
    
  }else if(model$family$link == "log" & confid){
    colnames(final_table) <- c("variable", "coefficient", "exp(coeff)",
                               "err. std","val. z", "val .p",
                               "coeff 2.5%", "coeff 97.5%",
                               "exp(coeff) 2.5%", "exp(coeff) 97.5%")
    final_table <- clean_columns(final_table, c(NA, coef_digits, coef_digits, std_digits,
                                                z_digits,p_digits, coef_digits, coef_digits, 
                                                coef_digits, coef_digits))
    
  }else if (model$family$link == "log" & confid==F){
    colnames(final_table) <- c("variable", "coefficient", "exp(coeff)",
                               "err. std","val. z", "val .p")
    final_table <- clean_columns(final_table, c(NA, coef_digits, coef_digits, std_digits,
                                                z_digits,p_digits))
    
  }else if(confid){
    colnames(final_table) <- c("variable", "coefficient",
                               "err. std","val. z", "val .p",
                               "coeff 2.5%", "coeff 97.5%")
    final_table <- clean_columns(final_table, c(NA, coef_digits, std_digits,
                                                z_digits,p_digits,coef_digits,coef_digits))
    
  }else if(confid==F){
    colnames(final_table) <- c("variable", "coefficient",
                               "err. std","val. z", "val .p")
    final_table <- clean_columns(final_table, c(NA, coef_digits, std_digits,
                                                z_digits,p_digits))
  }
  rownames(final_table) <- NULL

  return(final_table)
  
}

## pour un vglm
build_table.vglm <- function(model, confid = T, coef_digits = 2, std_digits = 2, z_digits = 2, p_digits = 3, OR_digits = 3){
  ## extraction des elements principaux
  base_table <- summary(model)@coef3
  
  ## calcule des intervale de confiance sur les coeffs
  if(confid){
    base_table <- cbind(base_table, round(confint(model),coef_digits))
  }
  
  ## si fonction de lien = logit : OR
  if (model@family@vfamily[[1]] %in% c("cumulative", "binomial")){
    base_table <- cbind(base_table[,1], round(exp(base_table[,1]),OR_digits) , base_table[,2:ncol(base_table)])
    if(confid){
      n <- ncol(base_table)
      base_table <- cbind(base_table, round(exp(base_table[,c(n-1,n)]), OR_digits))
    }
  }else if (model@family@vfamily[[1]] %in% c("gamma2")){
    base_table <- cbind(base_table[,1],
                        round(exp(base_table[,1]),coef_digits),
                        base_table[,2:6]
                        )
  }
  
  ## gerer les arrondis
  base_table[,1] <- round(base_table[,1], coef_digits)
  i<-match("Std. Error", colnames(base_table))
  base_table[,i] <- round(base_table[,i], std_digits)
  i<-match("z value", colnames(base_table))
  base_table[,i] <- round(base_table[,i], z_digits)
  i<-match("Pr(>|z|)", colnames(base_table))
  base_table[,i] <- round(base_table[,i], p_digits)
  
  params_names <- strsplit(as.character(model@terms)," ~ ")[[1]][[2]]
  params_names <- strsplit(params_names," + ", fixed = T)[[1]]
  params_types <- sapply(params_names, function(i){
    class(model@model[[i]])
  })
  
  
  if(model@family@vfamily[[1]] == "cumulative"){
    inter_names <- rownames(base_table)[grepl("(Intercept",rownames(base_table),fixed = T)]
    params_names <- c(inter_names, params_names)
    params_types <- c(rep("numeric", length(inter_names)),params_types)
    
    ##NB : dealing with not parallel elements
    elements <- as.character(model@family@infos()$parallel[[3]])
    if ("+" %in% elements){
      not_paralelle <- elements[2:length(elements)]
    }else{
      not_paralelle <- elements
    }
    
    test <- (params_names %in%  not_paralelle) == F
    params_names <- params_names[test]
    params_types <- params_types[test]
    
  }else{
    params_names <- c("Intercept",params_names)
    params_types <- c("numeric", params_types)
  }
    
  ## creation d'un beau tableau
  if(model@family@vfamily[[1]] == "gamma2"){
    rownames(base_table)[1] <- "Intercept"
    rownames(base_table)[2] <- "shape"
  }
  
  rn <- rownames(base_table)
  allrows <- lapply(1:length(params_names), function(i){
    pname <- params_names[[i]]
    ptype <- params_types[[i]]
    if(ptype %in% c("character","factor")){
      rows <- base_table[grepl(pname,x = rn,fixed = T),]
      uvalues <- unique(as.character(model@model[[pname]]))
      if(length(uvalues)>2){
        new_names <- gsub(pattern = pname, x = row.names(rows), replacement = "", fixed = T)
      }else{
        oldname <- rn[grepl(pname,x = rn,fixed = T)]
        new_names <- gsub(pattern = pname, x = oldname, replacement = "", fixed = T)
      }
      ref <- unique(uvalues[! uvalues %in% new_names])
      new_table <- rbind("--", rows)
      new_table <- cbind(c(paste("ref : ",ref,sep=""),new_names),new_table)
      row1 <- c(paste("*",pname,"*",sep=""),rep("",ncol(new_table)-1))
      new_table <- rbind(row1,new_table)
      return(new_table)
    }
    if(ptype %in% c("integer", "double", "numeric")){
      if(pname == "Intercept"){
        row <- base_table[rn=="(Intercept)",]
      }else{
        row <- base_table[rn==pname,]
      }
      row <- c(pname, row)
      return(row)
    }
  })
  
  ## ajouter les lignes pour la fonction cumulative
  if(model@family@vfamily[[1]] == "cumulative"){
    k <- 1
    more_rows <- lapply(not_paralelle,function(i){
      type_param <- class(model@model[[i]])
      if(type_param %in% c("character","factor")){
        rows <- base_table[grepl(i,x = rn,fixed = T),]
        uvalues <- unique(as.character(model@model[[i]]))
        new_names <- gsub(pattern = i, x = row.names(rows), replacement = "", fixed = T)
        new_names2 <- unique(sapply(new_names, function(j){strsplit(j,":",fixed=T)[[1]][[1]]}))
        ref <- unique(uvalues[! uvalues %in% new_names2])
        new_table <- rbind("--", rows)
        new_table <- cbind(c(paste("ref : ",ref,sep=""),new_names),new_table)
        row1 <- c(paste("*",i,"*",sep=""),rep("",ncol(new_table)-1))
        new_table <- rbind(row1,new_table)
      }
      
      if(type_param %in% c("integer", "double", "numeric")){
        rows <- base_table[grepl(i,x = rn,fixed = T),]
        rnames <- paste(paste(rep(i),":",sep=""),1:(ncol(model@y)-1), sep="")
        new_table <- cbind(rnames, rows)
      }
      if(k == 1){
        new_table <- rbind(rep("",ncol(new_table)),c("**Effets par niveau**", rep("",ncol(new_table)-1)), new_table)
      }
      
      k <<- k + 1
      return(new_table)
    })
    
    allrows <- append(allrows,more_rows)
  }
  
  ## ajouter les lignes pour le modele de gamma2
  if(model@family@vfamily[[1]] == "gamma2"){
    row2 <- base_table[grepl("shape",x = rn,fixed = T),]
    row2 <- c("shape",row2)
    row1 <- rep("",length(row2))
    more_rows <- list(rbind(row1,row2))
    allrows <- append(allrows,more_rows)
  }
  
  
  final_table <- do.call(rbind,allrows)
  
  if(model@family@vfamily[[1]] %in% c("cumulative", "binomial") & confid){
    colnames(final_table) <- c("variable", "coefficient", "OR",
                               "err. std","val. z", "val .p",
                               "coeff 2.5%", "coeff 97.5%",
                               "OR 2.5%", "oR 97.5%")
    
    final_table <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                z_digits,p_digits, coef_digits, coef_digits, 
                                                OR_digits, OR_digits))
    
  }else if (model@family@vfamily[[1]] %in% c("cumulative", "binomial") & confid==F){
    colnames(final_table) <- c("variable", "coefficient", "OR",
                               "err. std","val. z", "val .p")
    
    final_table <- clean_columns(final_table, c(NA, coef_digits, OR_digits, std_digits,
                                                z_digits,p_digits))
    
  }else if (model@family@vfamily[[1]] %in% c("gamma2") & confid==F){
    colnames(final_table) <- c("variable", "coefficient", "exp(coefficient)",
                               "err. std","val. z", "val .p")
    
    final_table <- clean_columns(final_table, c(NA, coef_digits, coef_digits, std_digits,
                                                z_digits,p_digits))
    
  }else if (model@family@vfamily[[1]] %in% c("gamma2") & confid==T){
    colnames(final_table) <- c("variable", "coefficient", "exp(coefficient)",
                               "err. std","val. z", "val .p","coeff 2.5%", "coeff 97.5%")
    final_table <- clean_columns(final_table, c(NA, coef_digits, coef_digits, std_digits,
                                                z_digits,p_digits,coef_digits,coef_digits))
    
  }else if(confid){
    colnames(final_table) <- c("variable", "coefficient",
                               "err. std","val. z", "val .p",
                               "coeff 2.5%", "coeff 97.5%")
    final_table <- clean_columns(final_table, c(NA, coef_digits, std_digits,
                                                z_digits,p_digits,coef_digits,coef_digits))
  }else if(confid==F){
    colnames(final_table) <- c("variable", "coefficient",
                               "err. std","val. z", "val .p")
    final_table <- clean_columns(final_table, c(NA, coef_digits, std_digits,
                                                z_digits,p_digits))
  }
  rownames(final_table) <- NULL
  return(final_table)
}


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#### calculer des intervals de confiance ####
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

hand_contint.gam <- function(model){
  base_table <- summary(model)
  coeffs <- model$coefficients
  test <- grepl("s(",names(coeffs),fixed=T) == F
  ok_coeff <- coeffs[test]
  ok_se <- base_table$se[test]
  freed <- length(model$y) -model$rank
  mat <- cbind(
    (ok_coeff - qt(0.975, df = freed) * ok_se),
    (ok_coeff + qt(0.975, df = freed) * ok_se)
  )
  colnames(mat) <- c("2.5%","97.5%")
  return(mat)
  
}


```

## Objectifs de la régression linéaire multiple et construction d'un modèle de régression {#sect051}

Selon Tabachnich et Fidell [-@tabachnick2007], un modèle de régression permet de répondre à deux objectifs principaux relevant chacun d'une approche de modélisation particulière.

La première approche a pour objectif d’identifier les relations entre une variable dépendante (VD) et plusieurs variables indépendantes (VI). Il s’agit alors de déterminer si ces relations sont positives ou négatives, significatives ou non et d'évaluer leur ampleur. La construction du modèle de régression repose alors sur un cadre théorique et la formulation d’hypothèses sur les relations entre chacune des VI et la VD.

La seconde approche est exploratoire et très utilisée en *data mining* (forage ou fouille de données). Parmi un grand ensemble de variables disponibles dans un jeu de données, elle vise à identifier la ou les variables permettant de prédire le plus efficacement (précisément) une variable dépendante. Parfois, ce type de démarche ne repose ni sur un cadre théorique, ni sur la formulation d’hypothèses entre les VI et la VD. Dans des cas extrêmes, on s’intéresse uniquement à la capacité de prédiction du modèle, et ce, sans analyser les associations entre les VI et la VD. L’objectif étant d’obtenir le modèle le plus efficacement possible afin de prédire, à l’avenir, la valeur de la variable dépendante pour des observations pour lesquelles elle est inconnue. Pour ce faire, on a recours à des régressions séquentielles (*stepwise regressions*) dans lesquelles les variables peuvent être ajoutées (ou retirées) une à une au modèle; on conservera dans le modèle de régression final uniquement celles qui ont un apport explicatif significatif. Signalons d’emblée, que dans le reste du chapitre, comme du livre, nous ne nous étendrons pas plus sur cette approche de modélisation, et ce, pour deux raisons. D’une part, cette approche met souvent en évidence des relations significatives entre des variables sans qu’il y ait une relation de causalité entre elles. D'autre part, en sciences sociales, un modèle de régression doit être basé sur un cadre théorique et conceptuel élaboré suite à une revue de littérature rigoureuse.

::: {.bloc_attention data-latex=""}
**Cadre conceptuel et élaboration d'un modèle de régression**

Pour bien construire un modèle de régression, il convient de définir un cadre conceptuel élaboré suite à une revue de littérature sur votre sujet de recherche. Ce cadre conceptuel permettra d’identifier les dimensions et concepts clefs permettant d’expliquer le phénomène à l’étude. Par la suite, pour chacun de ces concepts ou dimensions, il sera alors possible 1) d’identifier les différentes variables indépendantes qui seront introduites dans le modèle et 2) de formuler pour chacune d’elles une hypothèse. Par exemple, pour telle ou telle variable explicative, on s’attendra à ce qu’elle fasse augmenter ou diminuer significativement la variable dépendante. De nouveau, la formulation de cette hypothèse doit s'appuyer sur une interprétation théorique de la relation entre la VI et la VD.

Prenons en guise d'exemple, une étude récente portant sur la multiexposition des cyclistes au bruit et à la pollution atmosphérique [@gelb2020modelling]. Dans cet article, les auteurs s’intéressent aux caractéristiques de l’environnement urbain qui contribuent à augmenter ou réduire l’exposition des cyclistes à la pollution de l’air et au bruit routier. Pour ce faire, une collecte de données primaires a été réalisée avec trois cyclistes dans les rues de Paris du 4 au 7 septembre 2017. Au total, 64 heures et 964 kilomètres ont ainsi été parcourus à vélo afin de maximiser la couverture de la Ville de Paris et les types d'environnements urbains traversés.

Leur cadre conceptuel est schématisé à la figure ci-dessous. Les deux variables indépendantes (à expliquer) sont l'exposition au dioxyde d'azote (NO<sub>2</sub>) et l'exposition au bruit (mesurée en décibel dB(A)). Avant d'identifier les caractéristiques de l'environnement urbain affectant ces deux expositions, plusieurs facteurs, dits **variables de contrôle**, sont considérées. Par exemple, la concentration de NO<sub>2</sub> varie en fonction des conditions météorologiques (vent, température, humidité) et de la pollution d'arrière-plan (variant selon le moment de la journée, le jour de la semaine et la localisation géographique au sein de la ville). Ces dimensions ne sont pas le centre d'intérêt direct de l'étude. En effet, les auteurs s'intéressent aux impacts des caractéristiques locales de l'environnement urbain. Pour pouvoir les identifier sans biais, il est nécessaire de contrôler (filtrer) l'ensemble de ces autres facteurs.

Dans leur cadre conceptuel, les auteurs regroupent les caractéristique locale de l'environnment urbain  en trois grandes dimensions : les caractéristiques du segment (types de rues ou de voies cyclables empruntés, intersections traversées, pente et vitesse), celles de la forme urbaine (densité résidentielle, végétation, ouverture de la rue, occupations du sol) et celles du trafic (nombre et types de véhicules croisés, congestion, zones 30 km/h). Une fois ce cadre conceptuel construit, il reste alors à identifier les variables qui permettent d'opérationnaliser chacuns de concepts retenus.

```{r figcadreconcept, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Exemple de cadre conceptuel",  out.width='100%', fig.pos="H"}
knitr::include_graphics('images/lm/CadreTheorique.jpg', dpi = NA)
```

**Notion de variables de contrôle *versus* variables explicatives**

Dans un modèle de régression, on distingue habituellement trois types de variables : la variable dépendante (*Y*) que l'on souhaite prédire ou expliquer et les variables indépendantes (*X*) qui peuvent être, soit des variables de contrôle (*covariates* en anglais), soit des variables explicatives. Les premières sont des facteurs qu'il faut prendre en compte (contrôler) avant d'évaluer nos variables d'intérêt (explicatives). 

Dans l'exemple précédent, les chercheurs voulaient évaluer l'impact des caractéristiques de l'environnement urbain (variables explicatives) traversés par les cyclistes sur leurs expositions au dioxyde d'azote et au bruit, une fois contrôlés les effets de facteurs reconnus comme ayant un impact significatif sur la concentration de polluants comme les conditions météorologiques et la pollution d’arrière-plan. Autrement dit, si les variables de contrôle n'avaient pas été prise en compte, l'étude des variables d'intérêt serait biaisée par les effets de ces facteurs qui n'auraient pas été contrôlés. À titre d'exemple, il serait possible que les zones de circulation limitées à 30 km/h soient concentrées dans les quartiers centraux et denses de Paris. Dans ces quartiers, la pollution d'arrière plan a tendance à être supérieure. Si l'on ne tient pas compte de cette pollution d'arrière plan, on pourrait arriver à la conclusion que les zones 30 sont des milieux dans lesquels les cyclistes sont plus exposés à la pollution atmosphérique.

**Construction de modèles de régression imbriqués, incrémentiels**

En lien avec le cadre conceptuel du modèle, il est fréquent de construire plusieurs modèles emboîtés. Par exemple, à partir du cadre conceptuel (figure \@ref(fig:figcadreconcept)), les auteurs auraient très bien pu construire quatre modèles :

* un premier avec uniquement les variables de contrôle (modèle A)
* un second incluant les variables de contrôle et les variables explicatives de la dimension des caractéristiques du segment (modèle B)
* un troisième reprenant les variables du modèle B dans lequel sont introduites les variables explicatives relatives à la forme urbaine (modèle C)
* un dernier modèle dans lequel sont ajoutées les variables explicatives relatives aux conditions du trafic (modèle D).

L’intérêt d’une telle approche est qu'elle permet d’évaluer successivement l’apport explicatif de chacune des dimensions du modèle; nous y reviendrons dans la section \@ref(sect053).

On dit donc de deux modèles qu'ils sont imbriqués si le modèle avec le plus de variables comprends également **toutes** les variables du modèle avec le moins de variables.
:::


## Principes de base de la régression linéaire multiple {#sect052}

### Un peu d'équations... {#sect0521}

La régression linéaire multiple vise à déterminer une équation qui résume le mieux les relations linéaires entre une variable dépendante (*Y*) et un ensemble de variables indépendantes (*X*). L'équation de régression s'écrit alors :

\footnotesize
\begin{equation} 
y_i = \beta_{0} + \beta_{1}x_{1i} + \beta_{2}x_{2i} +\ldots+ \beta_{k}x_{ki} + \epsilon_{i}
(\#eq:regmultiple1)
\end{equation}
\normalsize

avec :

* $y_i$ la valeur de la variable dépendante *Y* pour l'observation *i*.
* $\beta_{0}$ la constante, soit la valeur prédite pour *Y* quand toutes variables indépendantes sont égales à 0.
* $k$ le nombre de variables indépendantes.
* $\beta_{1}$ à  $\beta_{k}$ les coefficients de régression pour les variables indépendantes de 1 à *k* ($X_1$ à $X_k$). 
* $\epsilon_{i}$ le résidu pour l'observation de *i*, soit la partie de la valeur de $y_i$ qui n'est pas expliqué par le modèle de régression.

Notez qu'il existe plusieurs écritures simplifiées de cette équation. D'une part, il est possible de ne pas indiquer l'observation *i* et de remplacer les lettres grecques *bêta* et *epsilon* ($\beta$ et $\epsilon$) par les lettres *b* et *e* :

\footnotesize
\begin{equation}
Y = b_{0} + b_{1}X_{1} + b_{2}X_{2} +\ldots+ b_{k}X_{k} + e
(\#eq:regmultiple2)
\end{equation}
\normalsize

D'autre part, cette équation peut être présentée sous forme matricielle. Rappelez-vous que pour chacune des *n* observations de l'échantillon, une équation est formulée :

\footnotesize 
\begin{equation}
\left\{\begin{array}{l}
y_{1}=\beta_{0}+\beta_{1} x_{1,1}+\ldots+\beta_{p} x_{1, k}+\varepsilon_{1} \\
y_{2}=\beta_{0}+\beta_{1} x_{2,1}+\ldots+\beta_{p} x_{2, k}+\varepsilon_{2} \\
\cdots \\
y_{n}=\beta_{0}+a_{1} x_{n, 1}+\ldots+\beta_{p} x_{n, k}+\varepsilon_{n}
\end{array}\right.
(\#eq:regmultiple3)
\end{equation}
\normalsize

Par conséquent, sous forme matricielle, l'équation s'écrit :

\footnotesize
\begin{equation}
\left(\begin{array}{c}
y_{1} \\
\vdots \\
y_{n}
\end{array}\right)=\left(\begin{array}{cccc}
1 & x_{1,1} & \cdots & x_{1, k} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{n, 1} & \cdots & x_{n, k}
\end{array}\right)\left(\begin{array}{c}
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{k}
\end{array}\right)+\left(\begin{array}{c}
\varepsilon_{1} \\
\vdots \\
\varepsilon_{n}
\end{array}\right)
(\#eq:regmultiple4)
\end{equation}
\normalsize

ou tout simplement :

\footnotesize
\begin{equation}
Y = X\beta + \epsilon
(\#eq:regmultiple5)
\end{equation}
\normalsize

avec :

* $Y$ un vecteur de dimension $n \times 1$ pour la variable dépendante, soit une colonne avec *n* observations.
* $X$ une matrice de dimension $n \times (k + 1)$ pour les *k* variables indépendantes, incluant une autre colonne (avec la valeur de 1 pour les *n* observations) pour la constante d'où $k + 1$.
* $\beta$ un vecteur de dimension $k + 1$, soit les coefficients de régression pour les *k* variables et la constante. 
* $\epsilon$ un vecteur de dimension $n \times 1$ pour les résidus.
 

::: {.bloc_attention data-latex=""}
Vous aurez compris que comme pour la régression linéaire simple (section \@ref(sect04143)), l'équation de la régression linéaire multiple comprend aussi une partie expliquée et une autre non expliquée (stochastique) par le modèle :

\footnotesize
\begin{equation}
Y  = \underbrace{\beta_{0} + \beta_{1}X_{i} + \beta_{2}X_{i} +\ldots+ \beta_{k}X_{k}}_{\mbox{partie expliquée par le modèle}}+ \underbrace{\epsilon}_{\mbox{partie non expliquée (stochastique)}}
(\#eq:regmultiple6)
\end{equation}
\normalsize

\footnotesize
\begin{equation}
Y  = \underbrace{X\beta}_{\mbox{partie expliquée par le modèle}}+ \underbrace{\epsilon}_{\mbox{partie non expliquée (stochastique)}}
(\#eq:regmultiple8)
\end{equation}
\normalsize
:::

### Les hypothèses de la régression linéaire multiples {#sect0522}


## Mesurer la qualité d’ajustement du modèle {#sect053}

Pour illustrer la régression linéaire multiple, nous utilisons un jeu de données tiré d'un article portant sur la distribution spatiale de la végétation sur l'île de Montréal abordée sous l'angle de l'équité environnementale [@apparicio2016spatial]. Dans cette étude, les auteurs veulent vérifier si certains groupes de population (personnes à faible revenu, minorités visibles, personnes âgées et enfants de moins de 15 ans) ont ou non une accessibilité plus limitée à la végétation urbaine. En d’autres termes, cet article tente de répondre à la question suivante : une fois contrôlées les caractéristiques de la forme urbaine (densité de population et âge du bâti), est-ce que les quatre groupes de population résident dans des îlots urbains avec proportionnellement moins ou plus de végétation ?

Dans le tableau \@ref(tab:datareg), sont reportées les variables utilisées (calculées au niveau des îlots de l'île de Montréal) introduite dans le modèle de régression : 

* le pourcentage de la superficie de l'îlot couverte par de la végétation, soit la variable indépendante  (VI);
* deux variables indépendantes de contrôle (VC) relatives à la forme urbaine;
* les pourcentages des quatre groupes de population comme variables indépendantes explicatives (VE).

L'équation de départ du premier modèle de régression est donc :
`VegPct ~ HABHA + AgeMedian + Pct_014 + Pct_65P + Pct_MV + Pct_FR`

**Notez que ce jeu de données est utilisé tout au long du chapitre.**



```{r datareg, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
library("foreign")
load("data/lm/DataVegetation.RData")

vars <- c("VegPct","HABHA","AgeMedian","Pct_014","Pct_65P","Pct_MV","Pct_FR")
intitule <- c("Végétation (%)","Habitants au km2","Âge médian des bâtiments","Moins de 15 ans (%)","65 et plus ans (%)","Minorités visible (%)","Personnes à faible revenu (%)")
type <- c("VD","VC","VC","VE","VE","VE","VE")

moy <- round(sapply(DataFinal[vars], mean),1)
et <- round(sapply(DataFinal[vars], sd),1)
q1 <- round(sapply(DataFinal[vars], quantile)[2,],1)
q2 <- round(sapply(DataFinal[vars], median),1)
q3 <- round(sapply(DataFinal[vars], quantile)[4,],1)

stats <- data.frame(cbind(moy, et, q1, q2, q3))
stats <- cbind(vars, intitule, type, stats)

show_table(stats,
           digits = 1,
            caption = 'Statistiques descriptives pour les variables du modèle',
           col.names=c("Nom","Intitulé","Type", "Moy.", "E.-T.", "Q1", "Q2", "Q3"),
           align= c("l","l","c", "r", "r", "r", "r", "r")
           )
```


### Mesurer la qualité d'un modèle {#sect0531}

Comme pour la régression linéaire simple (section \@ref(sect04143)), les trois mesures les plus couramment utilisées pour évaluer la qualité d'un modèle sont :

* Le **coefficient de détermination** (R^2^) qui indique la proportion de la variance de la variable dépendante expliquée par les variables indépendantes du modèle (équation \@ref(eq:regmR2)). Il varie ainsi de 0 à 1.

* La **statistique de Fisher** qui permet d'évaluer la significativité globale du modèle (équation \@ref(eq:regmFFisher)). Dans le cas d'une régression linéaire multiple, l'hypothèse nulle du test *F* est que toutes les valeurs des coefficients de régression des variables indépendantes sont égales à 0; autrement dit, qu'aucune des variables indépendantes n'a d'effet sur la variable dépendante. Tel que décrit à la section \@ref(sect04143), il est possible d'obtenir une valeur *P* rattachée avec la statistique F avec *k* degrés de liberté au dénominateur et *n-k-1* degrés de liberté au numérateur (*k* et *n* étant respectivement le nombre de variables indépendantes et le nombre d'observations). Lorsque la valeur de P est inférieure à 0,05, on pourra en conclure que le modèle est globalement significatif, c'est-à-dire qu'au moins un coefficients de régression est significativement différent de zéro. Notez qu'il est plutôt rare qu'un modèle de régression, comprenant plusieurs variables indépendantes, soit globalement non significatif (P>0,05), et ce, surtout s'il est basé sur un cadre conceptuel et théorique solide. Le test de la statistique de Fisher est donc facile à passer et ne constitue pas une preuve absolue de la pertinence du modèle.

* **L'erreur quadratique moyenne (RMSE)** qui indique l'erreur absolue moyenne du modèle exprimée dans l'unité de mesure de la variable dépendante ou autrement dit, l'écart absolu moyen est les valeurs observées et prédites du modèle (équation \@ref(eq:regmRMSE)). Une valeur élevée indique que le modèle se trompe largement en moyenne et inversement.

::: {.bloc_attention data-latex=""}
**Rappel sur la décomposition de la variance et calcul du $R^2$, de la statistique F et du RMSE**

Rappelez-vous que la variance totale (SCT) est égale à la somme de la variance expliquée (SCE) par le modèle et de la variance non expliquée (SCR) par le modèle.

\footnotesize 
\begin{equation}
\underbrace{\sum_{i=1}^n (y_{i}-\bar{y})^2}_{\mbox{variance de Y}} = \underbrace{\sum_{i=1}^n (\widehat{y}_i-\bar{y})^2}_{\mbox{var. expliquée}} + \underbrace{\sum_{i=1}^n (y_{i}-\widehat{y})^2}_{\mbox{var. non expliquée}} \Rightarrow 
SCT = SCE + SCR
(\#eq:regmVariances)
\end{equation}
\normalsize

avec :

* $y_{i}$ est la valeur observée de la variable dépendante pour *i*;
* $\bar{y}$ est la valeur moyenne de la variable dépendante;
* $\widehat{y}_i$ est la valeur prédite de la variable dépendante pour *i*.

À partir des variance totale, expliquée et non expliquée, il est alors possible de calculer les trois mesures de la qualité d'ajustement du modèle.

\footnotesize 
\begin{equation}
R^2 = \frac{\sum_{i=1}^n (\widehat{y}_i-\bar{y})^2}{\sum_{i=1}^n (y_{i}-\bar{y})^2} = \frac{SCE}{SCT} \mbox{ avec } R^2 \in \left[0,1\right]
(\#eq:regmR2)
\end{equation}
\normalsize

\footnotesize
\begin{equation} 
F = \frac{\frac{\sum_{i=1}^n (\widehat{y}_i-\bar{y})^2}{k}}{\frac{\sum_{i=1}^n (y_{i}-\widehat{y})^2}{n-k-1}} = \frac{\frac{SCE}{k}}{\frac{SCR}{n-k-1}} = \frac{\frac{R^2}{k}} {\frac{1-R^2}{n-k-1}} = \frac{(n-k-1)R^2}{k(1-R^2)}
(\#eq:regmFFisher)
\end{equation}
\normalsize

\footnotesize 
\begin{equation}
RMSE = \sqrt{\frac{\sum_{i=1}^n (y_{i}-\widehat{y})^2}{n}} = \sqrt{\frac{SCR}{n}}
(\#eq:regmRMSE)
\end{equation}
\normalsize
:::

Globalement, plus un modèle de régression est efficace, plus les valeurs du R^2^ et de la statistique F sont élevées et inversement, plus celle de RMSE sera faible. En effet, remarquez qu'à l'équation \@ref(eq:regmFFisher), la statistique F peut être obtenue à partir du R^2^; par conséquent, plus la valeur du R^2^ est forte (proche de 1), plus celle de F est aussi élevée. Notez aussi que plus un modèle est performant, plus la partie expliquée par le modèle (SCE) est importante et plus celle non expliquée (SCR) est faible; ce qui signifie que plus le R^2^ est proche de 1 (équation \@ref(eq:regmR2)), plus le RMSE – calculé à partir du SCR – est faible (équation \@ref(eq:regmRMSE)).

La syntaxe R ci-dessous illustre comment calculer les différentes variances (SCT, SCE et SCR) à partir des valeurs observées et prédites par le modèle, puis les valeurs du R^2^, de F et du RMSE. Nous verrons par la suite qu'il est possible d'obtenir directement ces valeurs à partir de la fonction `summary(VotreModele)`.

```{r dataregcalculs, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}

# Construction du modèle de régression
Modele1 <- lm(VegPct ~ HABHA+AgeMedian+Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)

# Nombre d'observations 
n <- nrow(DataFinal)

# Nombre de variables indépendantes (coefficients moins la constante)
k <- length(Modele1$coefficients)-1

# Vecteur pour les valeurs observées
Yobs <- DataFinal$VegPct

# Vecteur pour les valeurs prédites
Ypredit <- Modele1$fitted.values

# Variance totale
SCT <- sum((Yobs-mean(Yobs))^2)

# Variance expliquée
SCE <- sum((Ypredit-mean(Yobs))^2)

# Variance expliquée
SCR <- sum((Yobs-Ypredit)^2)

#  Calcul du coefficient de détermination (R2)
R2 <- SCE / SCT

#  Calcul la valeur de F
valeurF <- (R2 / k) /((1-R2)/(n-k-1))

cat("R2 =", round(SCE / SCT,4),
    "\nF de Fisher = ", round(valeurF,0),
    "\nRMSE =", round(sqrt(SCR/ n),4)
    )
```


### Comparer des modèles incrémentiels {#sect0532}

Tel que signalé plus haut, il est fréquent de construire plusieurs modèles de régression imbriqués. Cette démarche est très utile pour évaluer l'apport de l'introduction d'une nouveau bloc de variables dans un modèle. De manière exploratoire, cela permet également de vérifier si l'introduction d'une variable indépendante supplémentaire dans un modèle a ou non un apport significatif et ainsi décider de la conserver ou non dans le modèle final selon le principe de parcinomie. 

::: {.bloc_notes  data-latex=""}
**Le principe de parcimonie** 

Le principe de parcimonie appliquée aux régressions correspond à l'idée qu'il est préférable de disposer d'un **modèle plus simple** pour expliquer un phénomène qu'un **modèle compliqué** si la qualité de leurs prédictions – qualité d’ajustement des deux modèles – est équivalente.

Une première justification de ce principe trouve son origine dans la philosophie des sciences avec le **rasoir d'Ockham**. Il s'agit d'un principe selon lequel il est préférable de privilégier des théories faisant appel à un plus petit nombre d'hypothèses. L'idée centrale étant d'éviter d'apporter des réponses à une question qui soulèverait davantage de nouvelles questions. Dans le cas d'une régression, on pourrait être tenté d'ajouter de nombreux prédicteurs pour améliorer la capacité de prédiction du modèle. Cette stratégie conduit généralement à observer des relations contraires à nos connaissances entre les variables du modèle, ce qui soulève de nouvelles questions de recherche (pas toujours judicieuses…). Dans notre quotidien, si une casserole tombe de son support, il est plus raisonnable d'imaginer que nous l'avions mal fixée que d'émettre l'hypothèse qu'un fantôme l'a volontairement fait tomber. Cette seconde hypothèse soulève d’autres questions (pas toujours judicieuses…) sur la nature d'un fantôme, son identité, la raison le poussant à agir, etc.

Une seconde justification de ce principe s'observe dans la pratique statistique : des modèles plus complexes ont souvent une plus faible capacité de généralisation. En effet, un modèle complexe et trop bien ajusté aux données observées est souvent incapable d'effectuer des prédictions justes pour de nouvelles données. Ce phénomène est appelé surajustement ou surinterprétation (*overfitting* en anglais). 
Le surajustement impliqué par des modèles trop complexes entre en conflit direct avec l'enjeu principal de l'inférence en statistique : pouvoir généraliser des observations faites sur un échantillon au reste d'une population.

Notez que ce principe de parcimonie ne signifie pas que vous devez systématique retirer toutes les variables non significatives de votre analyse. En effet, il peut y avoir un intérêt théorique à démontrer l'absence de relation entre des variables. Il s'agit plutôt d'une ligne de conduite à garder à l'esprit lors de l'élaboration du cadre théorique et de l'interprétation des résultats.
:::

Mathématiquement, plus on ajoute de variables supplémentaires dans un modèle, plus le R^2^ augmente. On ne peut donc pas utiliser directement le R^2^ pour comparer deux modèles de régression ne comprenant pas le même nombre de variables indépendantes. On préviligera alors l'utilisation du R^2^ ajusté qui, tel qu'illustré dans l'équation \@ref(eq:R2ajuste), tient compte à la fois du nombre d'observations et de variables indépendantes utilisés pour construire le modèle.

\footnotesize
\begin{equation} 
R^2_{ajusté}= 1 - \frac{(1-R^2)(n-1)}{n-k-1} \mbox{ avec } R^2{ajusté} \in \left[0,1\right]
(\#eq:R2ajuste)
\end{equation}
\normalsize

Si le R^2^ ajusté du second modèle est supérieur au premier modèle, cela signifie qu'il y a un gain de la variance expliquée entre le premier et le second modèle. Ce gain est-il pour autant significatif ? Pour y répondre, il convient de comparer les valeurs des statistiques F des deux modèles. Pour ce faire, on calcule le *F* incrémentiel et la valeur de P qui lui est associée avec comme degrés de liberté le nombre de prédicteurs ajouté ($k_2-k_1$) et $n-k_2-1$. Si la valeur de P<0,05, on pourra conclure que le gain de variance expliquée par le second modèle est significatif comparativement au premier modèle (au seuil de 5%). 

\footnotesize 
\begin{equation}
F_{incrémentiel}= \frac{\frac{R^2_2-R^2_1}{k_2-k_1}} {\frac{1-R^2_2}{n-k_2-1}}
(\#eq:Fincrementiel)
\end{equation}
\normalsize

avec $R^2_1$ et $R^2_2$ étant les coefficients de détermination des modèles 1 et 2 et $k_1$ et $k_2$ étant les nombres de variables indépendantes qu'ils comprennent ($k_2 > k_1$).

Illustrons le tout avec deux modèles. Dans la syntaxe R ci-dessous, nous avons construit un premier modèle avec uniquement les variables de contrôle (`modele1`), comprenant uniquement deux variables indépendantes (`HABHA` et `AgeMedian`). Puis, dans un second modèle (`modele2`), nous ajoutons comme variables indépendantes les pourcentages des quatre groupes de population (`Pct_014`, `Pct_65P`, `Pct_MV`, `Pct_FR`). Repérez comment sont calculés les R^2^ ajustés pour les modèles et le F incrémentiel.

Le R^2^ ajusté passe de 0,269 à 0,418 des modèles 1 à 2 signalant que l'ajout des quatre variables indépendantes augmente considérablement la variance expliquée. Autrement dit, le second modèle est bien plus performant. Le F incrémentiel s'élève à 653,8 et est significatif (P < 0,001). Notez que la syntaxe ci-dessous illustre comment calculer les valeurs du R^2^ ajusté et du F incrémentiel à partir des équations \@ref(eq:R2ajuste) et \@ref(eq:Fincrementiel). Sachez toutefois qu'il est possible d'obtenir directement le R^2^ ajusté avec la fonction `summary(VotreModele)` et le F incrémentiel avec la fonction `anova(modele1, modele2)`. 

```{r dataregcalculs2, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}
modele1 <- lm(VegPct ~ HABHA+AgeMedian, data = DataFinal)
modele2 <- lm(VegPct ~ HABHA+AgeMedian+Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)

# nombre d'observations pour les deux modèles
n1 <- length(modele1$fitted.values)
n2 <- length(modele2$fitted.values)

# nombre de variables indépendantes
k1 <- length(modele1$coefficients)-1
k2 <- length(modele2$coefficients)-1

# coefficient de détermination
R2m1 <- summary(modele1)$r.squared
R2m2 <- summary(modele2)$r.squared

# coefficient de détermination ajusté
R2ajustm1 <- 1-(((n1-1)*(1-R2m1)) / (n1-k1-1))
R2ajustm2 <- 1-(((n2-1)*(1-R2m2)) / (n2-k2-1))

# Statistique F
Fm1 <- summary(modele1)$fstatistic[1]
Fm2 <- summary(modele2)$fstatistic[1]

# F incrementiel
Fincrementiel <- ((R2m2-R2m1) / (k2 - k1)) / ( (1-R2m2)/(n2-k2-1))
pFinc <- pf(Fincrementiel, k2-k1, n2-k2-1, lower.tail = FALSE)

cat("\nR2 (modèle 1) =", round(R2m1,4), 
    "; R2 ajusté = ", round(R2ajustm1,4), 
    "; F =", round(Fm1, 1),
    "\nR2 (modèle 2) =", round(R2m2,4), 
    "; R2 ajusté = ", round(R2ajustm2,4), 
    "; F =", round(Fm2, 1),
    "\nF incrémentiel =", round(Fincrementiel,1), 
    "; P = ", round(pFinc,3)
)

# F incrémentiel avec la fonction anova
anova(modele1, modele2)
```


## Les différentes mesures pour les coefficients de régression {#sect054}

La fonction `summary(nom du modèle)` permet d'obtenir les sorties du modèle de régression. D'emblée, signalons que le modèle est globalement significatif (F(6,10203)=1123, p=0,000) avec un R^2^ de 0,4182 indiquant que les prédicteurs du modèle expliquent 41,82% de la variance du pourcentage de végétation dans les îlots de l'île de Montréal.


```{r dataregmodel1, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}
modelereg <- lm(VegPct ~ HABHA+AgeMedian+Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)
summary(modelereg)
```

### Les coefficients de régression : évaluer l'effet des variables indépendantes {#sect0541}

Les différentes sorties pour les coefficients sont reportées au tableau \@ref(tab:dataregmodel2). 

**La constante** ($\beta_0$) est la valeur attendue de la variable dépendante (*Y*) quand les valeurs de toutes les variables indépendantes sont égales à 0. Pour ce modèle, quand les variables indépendantes sont égales à 0, plus du quart de la superficie des îlots serait en moyenne couverte par de la végétation ($\beta_0$=26,36). Notez que la constante n'a pas toujours une interprétation pratique. Il est par exemple très invraissemblable de trouver un îlot avec de la population dans lequel il n'y aurait aucune personne à faible revenu, aucune personne déclarant appartenir d'une minorité visible, aucun enfant de moins de 14 ans et aucune personne âgée 65 ans et plus. La constante a donc avant tout un rôle mathématique dans le modèle.

**Le coefficient de régression** ($\beta_1$ à $\beta_k$) indique le changement de la variable dépendante (*Y*) lorsque la variable indépendante augmente d'une unité, toutes choses étant égales par ailleurs. Il permet ainsi d'évaluer l'effet d'une augmentation d'une unité dans laquelle est mesurée sur la VI sur la VD.

::: {.bloc_attention data-latex=""}
**Que signifie l'expression *toutes choses étant égales par ailleurs* pour un coefficient de régression ?**

Cela signifie que l'on estime l'effet de la variable indépendante sur la variable dépendante, si toutes les autres variables indépendantes restaient constantes ou autrement dit, une fois contrôlés tous les autres prédicteurs. 

Avec les nombreuses équations intégrées dans le livre, vous apprenez peu à peu l'alphabet grec (alpha, bêta, gamma, phi, epsilon, etc.). Passons désormais au latin ! En fait, *toutes choses étant égales par ailleurs* est la traduction de l'expression latine *ceteris paribus* que certains auteurs emploient encore; il est donc possible que vous la retrouviez dans un article scientifique... Ne pas confondre *ceteris paribus* avec *c'est terrible Paris en bus* (petite blague formulée par un étudiant ayant suivi notre cours *méthodes quantitatives appliquées en études urbaines* il y a quelques années) !
:::

```{r dataregmodel2, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
tabreg <- build_table(modelereg, confid = T, std_digits = 3, coef_digits = 3, z_digits = 2, p_digits = 3)

show_table(data.frame(tabreg),
           caption = 'Les différentes mesures pour les coefficients',
           col.names=c("Variable","Coef.","Erreur type", "Valeur de T", "P", "coef. 2,5%", "coef. 97,5%", ""),
           align=c("l","r","r", "r", "r", "r", "r", "r")
           )
```


À partir des coefficients du tableau ci-dessus, l'équation du modèle de régression s'écrit alors comme suit :

`VegPct = 26,356 * HABHA − 0,070 * AgeMedian + 0,011 * AgeMedian + 1,084 * Pct_014 + 0,401 * Pct_65P − 0,031 * Pct_MV − 0,348 * Pct_FR + e`


**Comment interpréter un coefficient de régression pour une variable indépendante ?**

Le signe du coefficient de régression indique si la variable indépendante est associée positivement ou négativement avec la variable dépendante. Par exemple, plus la densité de population est importante à travers les îlots de l'île de Montréal, plus la couverture végétale diminue.

Quand à la valeur absolue du coefficient, elle indique la taille de l'effet du prédicteur. Par exemple, 1,084 signifie que si toutes les autres variables indépendantes restaient constantes, alors le pourcentage de végétation dans l'îlot augmenterait de 1,084 points de pourcentage pour chaque différence d’un point de pourcentage d'enfants de moins de 15 ans. Toutes choses étant égales par ailleurs, une augmentation de 10% de pourcentage d'enfants dans un îlot entraînerait alors une hausse de 10,8% de la couverture végétale dans l'îlot.

L'analyse des coefficients montre ainsi qu'une fois contrôlées les deux caractéristiques relatives à la forme urbaine (densité de population et âge médian des bâtiments), plus les pourcentages d'enfants et de personnes âgées sont fortent, plus la couverture végétale de l'îlot est importante (B=1,084 et 0,401), toutes choses étant égales par ailleurs. À l'inverse, de plus grands pourcentages de personnes à faible revenu et de minorités sont associés à une plus faible couverture végétale (B=−0,348 et −0,031).


**L'erreur type du coefficient de régression**

L'erreur type d'un coefficient permet d'évaluer son niveau de précision, soit le degré d'incertitude vis à vis du coefficient. Succinctement, elle correspond à l'écart-type de l'estimation (coefficient); elle est ainsi toujours positive. Plus la valeur de l'erreur type est faible, plus l'estimation du coefficient est précise. Notez toutefois qu'il n'est pas judicieux de comparer les erreurs type des coefficients pour des variables exprimées dans des unités de mesure différents. 

Comme nous le verrons plus loin, l'utilité principale de l'erreur type est qu'elle permet de calculer la valeur de *T* et l'intervalle de confiance du coefficient de régression.


### Coefficients de régression standardisés : repérer les variables les plus importantes du modèle {#sect0542}

Un coefficient de régression est exprimé dans les unités de mesure des variables indépendante (VI) et dépendante (VD) : une augmentation d’une unité de la VI a un effet de $\beta$ (valeur de coefficient) unité de mesure sur la VD, toutes choses étant égales par ailleurs. Prenons l’exemple d’un modèle fictif dans lequel une variable indépendante mesurée en mètres obtiendrait un coefficient de régression de 0,000502. Si cette variable était exprimée en kilomètres et non en mètres, son coefficient serait de alors 0,502 ($0,000502 \times 1000 = 0,502$). Cela explique que pour certaines variables, il est souvent préférable de modifier son unité de mesure, particulièrement pour les variables de distance ou de revenu. Par exemple, dans un modèle de régression, on introduit habituellement une variable de revenu par tranche de 1000 dollars ou le loyer mensuel par tranche de 100 dollars puisque les coefficients du revenu ou de loyer exprimé en dollars risquent d'être faibles. Concrètement, cela signifie que l'on divisera la variable *revenu* par 1000 et celle du *loyer* par 100 avant de l'introduire dans le modèle.

Du fait de leur unités de mesures souvent différentes, vous aurez compris qu'on ne peut pas comparer directement les coefficients de régression afin de repérer la ou les variables indépendantes (*X*) qui ont les effets (impacts) les plus importants sur la variable dépendante (*Y*). Pour rémédier à ce problème, on utilise les **coefficients de régression standardisés**. Ces coefficients standardisés sont simplement les valeurs de coefficients de régression qui seraient obtenus si toutes les variables du modèle (VD et VI) étaient préalablement centrées et réduites (soit avec une moyenne égale à 0 et un écart-type égal à 1; voir la section \@ref(02552) pour un rappel). Puisque toutes les variables du modèle sont exprimées en écart-types, les coefficients standardisés permettent ainsi d'évaluer **l'effet relatif** des VI sur VD. Cela permet ainsi de repérer la ou les variables les plus « importantes » du modèle.

**L'interprétation d'un coefficient de régression standardisé est donc la suivante : il indique le changement en termes d'unités d'écart-type de la variable dépendante (Y) à chaque ajout d'un écart-type de la variable indépendante, toutes choses étant égales par ailleurs**. 


Le coefficient de régression standardisé peut être aussi facilement calculé en utilisant les écarts-types des deux variables VI et VD :

\footnotesize 
\begin{equation}
\beta_{standardisé}= \beta \frac{s_x}{s_y}
(\#eq:CoefStand)
\end{equation}
\normalsize

La syntaxe R ci-dessous illustre trois façons d'obtenir les coefficients standardisés :

* en centrant et réduisant préalablement les variables avec la fonction `scale` avant de construire le modèle avec la fonction `lm`
* en calculant les écarts-types de VD et VI et en appliquant l'équation ci-dessous
* avec la fonction `lm.beta` du package **QuantPsyc**. Cette dernière méthode est moins « verbeuse » (deux lignes de codes uniquement), mais nécessite de charger un package supplémentaire.


```{r CoefStand1, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}

# Modèle de régression
Modele1 <- lm(VegPct ~ HABHA+AgeMedian+Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)

#  Méthode 1 : lm sur des variables centrées-réduites
ModeleZ <- lm(scale(VegPct) ~ scale(HABHA)+scale(AgeMedian)+
                           scale(Pct_014)+scale(Pct_65P)+
                           scale(Pct_MV)+scale(Pct_FR), data = DataFinal)
coefs <- ModeleZ$coefficients
coefs[1:length(coefs)]

#  Méthode 2 : à partir de l'équation
# Écart-type de la variable dépendante
VDet <- sd(DataFinal$VegPct)
cat("Écart-type de Y =", round(VDet,3))
# Écarts-types des variables indépendantes
VI <- c("HABHA","AgeMedian","Pct_014","Pct_65P","Pct_MV","Pct_FR")
VIet <- sapply(DataFinal[VI], sd)
# Coefficients de régression du modèle sans la constante
coefs <- Modele1$coefficients[1:length(VIet)+1]
# Coefficients de régression du modèle
coefstand <- coefs * (VIet / VDet)
coefstand

#  Méthode 3 : avec la fonction lm.beta du package QuantPsyc
library(QuantPsyc)
lm.beta(lm(VegPct ~ HABHA+AgeMedian+Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal))

```

```{r CoefStand2, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}

# Construction d'un tableau pour les coefficients
TabCoef <- data.frame(cbind(VI, round(VIet,3), round(coefs,3), round(coefstand,3)))

show_table(TabCoef,
           digits = 3,
           caption = 'Calcul des coefficients standardisés',
           col.names=c("Variable dépendante", "Écart-type","Coef.","Coef. standardisé"),
           align=c("l","r","r","r")
           )
```

Par exemple, pour la variable `Pct_014`, le coefficient de régression standardisé est égal à : 

\footnotesize 
\begin{equation}
\beta_{standardisé}= 1,084 \times \frac{5,295}{18,562}=0,309
(\#eq:CoefStand2)
\end{equation}
\normalsize

avec 1,084 étant le coefficient de régression de `Pct_014`, 5,295 et 18,562 étant respectivement les écart-types de `Pct_014` (variable indépendante) et de `VegPct` (variable dépendante).

Au tableau \@ref(tab:CoefStand2), on constate que la valeur absolue du coefficient de régression pour `HABHA` est inférieure à celle de `Pct_65P` (−0,070 *versus* 0,401), ce qui n'est pas le cas pour leurs coefficients standardisés (−0,281 *versus* 0,179). Rappelez-vous aussi qu'on ne peut pas directement comparer les effets de ces deux variables à partir des coefficients de régression puisqu'elles sont exprimées dans des unités de mesure différentes : `HABHA` est exprimée en habitants par hectare et `Pct_65P` en pourcentage. À la lecture des coefficients standardisés, on peut en conclure que la variable `HABHA` a un effet relatif plus important que `Pct_65P` (−0,281 *versus* 0,179).

### Significativité des coefficients de régression : valeurs de T et de P {#sect0543}

Une fois les coefficients de régression obtenus, il convient de vérifier s'ils sont ou non significativement différents de 0. Si le coefficient de régression d'une variable indépendante est significativement différent de 0, on en conclut que la variable a un effet significatif sur la variable dépendante, toutes choses étant égales par ailleurs. Pour ce faire, il suffit de calculer la valeur de T qui est simplement le coefficient de régression divisé par son erreur type.

\footnotesize
\begin{equation} 
T=\frac{\beta_k - 0}{s(\beta_k)}  
(\#eq:ValeurT)
\end{equation}
\normalsize

avec $s(\beta_k)$ étant l'erreur type du coefficient de régression. Notez que dans l'équation ci-dessous, on indique habituellement $-0$ pour signaler que l'on veut tester si le coefficient est différent de 0.

En guise d'exemple, au tableau \@ref(tab:dataregmodel2), la valeur de T du la variable `HABHA` est bien égale à :

`−0.070401 / 0.002202 = −31.975`.

::: {.bloc_attention data-latex=""}

**Démarche pour vérifier si un coefficient est significativement différent de 0 avec un seuil de confiance**

1. Poser l'hypothèse nulle stipulant que le coefficient est égal à 0, soit $H_0 : \beta_k = 0$. L'hypothèse alternative est que le coefficient est différent de 0, soit $H_1 : B_k \neq 0$.
2. Calculer la valeur de T, soit le coefficient de régression divisé par son erreur type (équation \@ref(eq:ValeurT)).
3. Calculer le nombre de degrés de liberté, soit $dl = n − k - 1$, *n* et *k* étant respectivement les nombres d'observations et de variables indépendantes.
4. Choisir un seuil de signification alpha (5%, 1% ou 0,1% de chances de se trouver, soit *p* = 0,05, 0,01 ou 0,01).
5. Trouver la valeur critique de T dans la table T de Student (section \@ref(sect123)) avec *p* et le nombre de degrés de liberté (dl).
6. Valider ou réfuter l'hypothèse nulle (H<sub>0</sub>) :
  - si la valeur de T est inférieure à la valeur critique de T avec *dl* et le seuil choisi, on valide *h<sub>0</sub>* : le coefficient n'est pas significativement différent de 0.
  - si la valeur de T est supérieure à la valeur critique de T avec *dl* et le seuil choisi, on peut réfuter l'hypothèse nulle (*h<sub>0</sub>*), et choisir l'hypothèse *H<sub>1</sub>* stipulant que le coefficient est statistiquement différent de 0.


**Valeurs critiques de la valeur de T à retenir !**

Lorsque le nombre de degrés de liberté (*n − k - 1*) est très important (supérieur à 2500), et donc le nombre d'observations de votre jeu de données, on retient habituellement les valeurs critiques suivantes : **1,65 (p=0,10), 1,96 (p=0,05)**, **2,58 (p=0,01)** et **3,29 (p=0,001)**. Concrètement, cela signifie que :

* une valeur de T supérieure à 1,96 ou inférieure à -1,96 nous informe que la relation entre la variable indépendante et la variable dépendante est significative positivement ou négativement au seuil de 5%. Autrement dit, vous avez moins de 5% de chances de vous tromper en affirmant que le coefficient de régression est bien significativement différent de 0.

* une valeur de T supérieure à 2,58 ou inférieure à -2,58 nous informe que la relation entre la variable indépendante et la variable dépendante est significative positivement ou négativement au seuil de 5%. Autrement dit, vous avez moins de 1% de chances de vous tromper en affirmant que le coefficient de régression est bien significativement différent de 0.

* une valeur de T supérieure à 3,29 ou inférieure à -3,29 nous informe que la relation entre la variable indépendante et la variable dépendante est significative positivement ou négativement au seuil de 5%. Autrement dit, vous avez moins de 0,1% de chances de vous tromper en affirmant que le coefficient de régression est bien significativement différent de 0.

Concrètement, retenez et utilisez les seuils de $\pm1,96$, $\pm2,58$ et $\pm3,29$ pour repérer les variables significatives positivement ou négatifs aux seuils respectifs de 0,5, 0,1 et 0,001.

**Que signifie les seuils 0,5, 0,1 et 0,001**

L'interprétation exacte des seuils de significativité des coefficients d'une régression est quelque peu alambiquée, mais mérite que l'on s'y attarde. En effet, indiquer qu'un coefficient est significatif est souvent perçu comme un argument fort pour une théorie, il est donc nécessaire d'avoir du recul et de bien comprendre ce que l'on entend par **significatif**. 


Si un coefficient est significatif au seuil 5% dans notre modèle, cela signifie que si la valeur du coefficient est de 0 en réalité pour l’ensemble d’une population, alors nous avions moins de 5% de chances de collecter un échantillon (pour cette population) ayant produit un coefficient aussi fort que celui que nous observons dans notre propre échantillon. 

Par conséquent, il serait très invraisemblable que le coefficient soit 0 puisque nous avons effectivement collecté un tel échantillon. Il s'agit d'une forme d'argumentation par l'absurde propre à la statistique fréquentiste. 

Notez que si 100 études étaient conduites sur le même sujet et dans les mêmes conditions, on s'attendrait à ce que 5 d'entre elles trouvent un coefficient significatif, du fait de la variation des échantillons. Ce constat souligne le fait que la recherche est un effort collectif et qu'une seule étude n'est pas suffisante pour trancher sur un sujet. Les revues systématiques de la littérature sont donc des travaux particulièrement importants pour la construction du consensus scientifique.
:::

Prenons deux variables indépendantes au tableau \@ref(tab:dataregmodel2) – `HABHA` et `AgeMedian` – et vérifions si leurs coefficients de régression respectifs (−0,070 et 0,011) sont significatifs. Appliquons la démarche décrite dans l'encadré ci-dessus :

1. On l'hypothèse pose nulle stipulant les valeurs de ces deux coefficients sont égales à 0, soit $H_0 : \beta_k = 0$.
2. Leurs valeurs de T sont égales à `−0,070401 / 0,002202 = −31,97139` pour `HABHA` et à `0,010790 / 0,006369 = 1,694144` pour `AgeMedian`.
3. Le nombre de degrés de liberté est égale à *dl = n-k-1 = 10210 − 6 - 1 = 10203*.
4. On choisit respectivement les seuil alpha de 0,10, 0,05, 0,01 ou 0,01.
5. Avec 10210 degrés de liberté, les valeurs critiques de la table T de Student (section \@ref(sect123)) sont de 1,65 (p=0,10), 1,96 (p=0,05), 2,58 (p=0,01), 3,29 (p=0,001).
6. Valider ou réfuter l'hypothèse nulle (H<sub>0</sub>) :
  - pour `HABHA`, la valeur absolue de T (−31,975) est supérieure la valeur critique de 3,29. Son coefficient de régression est donc significativement différent de 0. Autrement dit, ce prédicteur a un effet significatif et négatif sur la variable dépendante.

  - pour `AgeMedian`, la valeur absolue de T (1,694) est supérieure à 1,65 (p=0,10), mais inférieure à 1,96 (p=0,05), 2,58 (p=0,01), 3,29 (p=0,001). Par conséquent, ce coefficient est différent de 0 uniquement au seuil de p=0,10, et non au seuil de p=0,05. Cela signifie qu'on a un peu moins de 10% de chances de se tromper en affirmant que cette variable a un effet significatif sur la variable dépendante.


::: {.bloc_astuce data-latex=""}

**Calculer et obtenir des valeurs de P dans R**

Il est très rare que l'on utilise directement la table T de Student pour obtenir un seuil de signification. 

D'une part, il est possible de calculer directement la valeur de P à partir de de la valeur de T et du nombre de degrés de liberté avec la fonction `pt` avec les paramètres suivants :

`pt(q= abs(valeur de T), df= nombre de degrés de libertés, lower.tail = F) *2`

```{r ValeurT, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}

# Degrés de liberté
dl <- nrow(DataFinal) - (length(Modele1$coefficients) - 1) + 1

# Valeurs de T
ValeurT <- summary(Modele1)$coefficients[,3]

# Calcul des valeurs de P
ValeurP <- pt(q= abs(ValeurT), df= dl, lower.tail = F) *2

df_tp <- data.frame(
                ValeurT = round(ValeurT,3), 
                ValeurP = round(ValeurP,3)
)
print(df_tp)
```

D'autre part, la fonction `summary` renvoie d'emblée les valeurs de T et de P. Par convention, R comme la plupart des logiciels utilisent aussi des symboles pour indiquer le seuil de signification du coefficient (voir tableau \@ref(tab:CoefStand2)) : 

 '***' p<=0,001
 
 '**'  p<=0,01 
 
 '*'  p<=0,05 
 
 '.'   p<=0,10
 
:::

### Intervalle de confiance des coefficients {#sect0544}

Finalement, il est possible de calculer l'intervalle de confiance d'un coefficient à partir d'un niveau de signification (habituellement 0,95 ou encore 0,99). Pour ce faire, la fonction `confint(nom du modèle, level=.95)` est très utile. L'intérêt de ces intervalles de confiance pour les coefficients de régression est double :

* il permet de vérifier si le coefficient est ou non significatif au seuil retenu. Pour cela la borne inférieure et la borne supérieure du coefficient doivent être toutes deux négatives ou positives. À l'inverse, un intervalle à cheval sur 0, soit avec une borne inférieure négative et une borne supérieure positive, n'est pas significatif.
* il permet d'estimer la précision de l'estimation; plus l'intervalle du coefficient est réduit, plus l'estimation de l'effet de la variable indépendante est précise. Inversement, un intervalle large signale que le coefficient est incertain.

Cela explique que de nombreux auteurs reportent les intervalles de confiance dans les articles scientifiques (habituellement à 95%). Dans le modèle ici présenté, il serait alors possible d'écrire : toutes choses étant égales par ailleurs, le pourcentage d'enfants de moins de 15 ans est positivement et significativement associé avec le pourcentage de la couverture végétale dans l'îlot (B=1,084; IC 95% = [1,021 - 1,148], p <0.001).

En guise d'exemple, à la lecture de la sortie R ci-dessous, l'estimation de l'effet de la variable indépendante `AgeMedian` sur la variable `VegPct` se situe dans l'intervalle -0,002 à 0,023 qui est à cheval sur 0. Contrairement aux autres variables, on ne peut donc pas en conclure que cet effet est significatif avec p=0,05.

```{r confintReg2, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}

# Intervalle de confiance à 95% des coefficients
round(confint(Modele1, level=.95),3)
```

::: {.bloc_astuce data-latex=""}
**Comment est calculé un intervalle de confiance ?**

L'intervalle du coefficient est obtenu à partir de : 

1.la valeur du coefficient ($\beta_k$),
2. la valeur de son erreur standard $s(\beta_k)$ et 
3. la valeur critique de T ($t_{\alpha/2}$) obtenu avec $n-k-1$ degrés de liberté et le niveau de significatité retenu (95%, 99% ou 99,9%) .

\footnotesize
\begin{equation}
IC_{\beta_k}=  \left[ \beta_k - t_{\alpha/2} \times s(\beta_k) ; \beta_k + t_{\alpha/2} \times s(\beta_k)  \right]
(\#eq:ICcoef)
\end{equation}
\normalsize

Autrement dit, lorsque vous disposez d'un nombre très important d'observations, les intervalles de confiance s'écrivent simplement avec les fameuses valeurs de critiques de T de 1,96, 2,58, 3,29 :

\footnotesize
\begin{equation}
\text{Intervalle à 95\%\: } IC_{\beta_k}=  \left[ \beta_k - 1,96 \times s(\beta_k) ; \beta_k + 1,96 \times s(\beta_k)  \right]
(\#eq:IC95)
\end{equation}
\normalsize

\footnotesize
\begin{equation}
\text{Intervalle à 99\%\: } IC_{\beta_k}=  \left[ \beta_k - 2,58 \times s(\beta_k) ; \beta_k + 2,58 \times s(\beta_k)  \right]
(\#eq:IC99)
\end{equation}
\normalsize

\footnotesize
\begin{equation}
\text{Intervalle à 99,9\%\: } IC_{\beta_k}=  \left[ \beta_k - 3,29 \times s(\beta_k) ; \beta_k + 3,29 \times s(\beta_k)  \right]
(\#eq:IC999)
\end{equation}
\normalsize

La syntaxe R ci-dessous illustre comment calculer les intervalles de confiance à 95% à partir de l'équation \@ref(eq:ICcoef). Rappelez toutefois qu'il est bien simple d'utiliser la fonction `confint`:

* `round(confint(Modele1, level=.95),3) `
* `round(confint(Modele1, level=.99),3)`
* `round(confint(Modele1, level=.999),3)`

```{r T, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}
# Coeffients de régression
coefs <- Modele1$coefficients

# Erreur type des coef.
coefs_se <- summary(Modele1)$coefficients[,2]

# Nombre de degrés de libertés
n <- length(Modele1$fitted.values)
k <- length(Modele1$coefficients)-1
dl <- n-k-1

# valeurs critique de T
t95 <-  qt(p=1 - (0.05/2),  df=dl)
t99 <-  qt(p=1 - (0.01/2),  df=dl)
t99.9 <-  qt(p=1 - (0.001/2),  df=dl)
cat("Valeurs critiques de T en fonction du niveau de confiance",
    "\n et du nombre de degrés de liberté",
    "\n95% : ", t95,
    "\n99% : ", t99,
    "\n99.9% : ", t99.9
    )

# Intervalle de confiance à 95

data.frame(
  IC2.5  = round(coefs-t95*coefs_se,3),
  IC97.5 = round(coefs+t95*coefs_se,3)
  )

# Intervalle de confiance à 99

data.frame(
  IC0.5 = round(coefs-t99*coefs_se,3),
  IC99.5 = round(coefs+t99*coefs_se,3)
)

# Intervalle de confiance à 99.9
data.frame(
  IC0.05 = round(coefs-t99.9*coefs_se,3),
  IC99.95 = round(coefs+t99.9*coefs_se,3)
  )
```
:::

## Introduction de variables explicatives particulières {#sect055}

### Variable indépendante avec une fontion polynomiale {#sect0551}

Dans la section \@ref(sect0411), nous avons vu que la relation entre deux variables continues n'est pas toujours linéaire; elle peut être aussi curvilinéaire. Pour explorer les relations curvilinéaires, on introduit la variable indépendante sous la forme polynomiale d'ordre 2. L'équation de régression s'écrit alors :

\footnotesize 
\begin{equation}
Y = b_{0} + b_{1}X_{1} + b_{11}X_{1}^2 + b_{2}X_{2} +\ldots+ b_{k}X_{k} + e
(\#eq:regpolyordre2)
\end{equation}
\normalsize

Dans l'équation ci-dessus, la première variable indépendante est introduite dans le modèle de régression à la fois dans sa forme originelle et mise au carré : $b_{1}X_{1} + b_{1}X_{1}^2$. 

La démographie est certainement une des disciplines des sciences sociales qui a le plus recours aux régressions polynomiales. En effet, la variable `âge` est souvent introduite comme variable explicative dans sa forme originale et mise au carré. L'objectif est de vérifier si l'âge partage ou non une relation curvilinéaire avec un phénomène donné : par exemple, il pourrait y être associé positivement jusqu'à un certain seuil (45 ans par exemple), puis négativement à partir de ce seuil.


::: {.bloc_aller_loin data-latex=""}
**Régression polynomiale et nombre d'ordres**.

Sachez qu'il est aussi possible de construire des régressions polynomiales avec plus deux ordres. Par exemple, une régression polynomiale d'ordre 3, comprend une variable dans sa forme originelle, puis mise au carré et au cube. Cela a l'inconvénient d'augmenter corrolairement le nombre de coefficients. Nous verrons au chapitre \@ref(chap06) qui existe une solution plus élégante et efficace : le recours aux modèles de régressions linéaires généralisées avec des *splines*. Dans le cadre de cette section, nous limiterons à des régression polynomiale d'ordre 2.

\footnotesize 
\begin{equation}
\mbox{Ordre 2 : } Y = b_{0} + b_{1}X_{1} + b_{1}X_{11}^2 + b_{2}X_{2} +\ldots+ b_{k}X_{k} + e
(\#eq:regpolyordre2b)
\end{equation}
\normalsize

\footnotesize 
\begin{equation}
\mbox{Ordre 3 : } Y = b_{0} + b_{1}X_{1} + b_{1}X_{11}^2 + b_{1}X_{111}^3 + b_{2}X_{2} +\ldots+ b_{k}X_{k} + e
(\#eq:regpolyordre3)
\end{equation}
\normalsize

\footnotesize 
\begin{equation}
\mbox{Ordre 4 : } Y = b_{0} + b_{1}X_{1} + b_{1}X_{11}^2 + b_{1}X_{111}^3 + b_{1}X_{1111}^4 + b_{2}X_{2} +\ldots+ b_{k}X_{k} + e
(\#eq:regpolyordre4)
\end{equation}
\normalsize

:::

Pour construire une régression polynomiale dans R, il est possible d'utiliser deux fonctions de R :

* `I(VI^2)` avec `VI` est la variable indépendante sur laquelle est appliquée la mise au carré.
* `poly(VI,2)` qui utilise une forme polynomiale orthogonale pour éviter les problèmes de corrélation entre les deux termes, c'est-à-dire entre **VI** et **VI^2^**.

Ces deux méthodes produiront les mêmes résultats pour les autres variables dépendantes et pour la qualité d'ajustement du modèle (R^2^, F, etc.). On privilégie la seconde fonction pour éviter ainsi les problèmes de multicolinéarité excessive.

Appliquons cette démarche à la variable `AgeMedian` (âge médian des bâtiments) afin de vérifier si elle partage ou non une relation curvilinéaire avec la couverture végétale de l'îlot. À la lecture des sorties des sorties pour les deux modèles, les constats suivants peuvent être avancées :

* Le R^2^ ajusté passe de 0,4179 à 0,4378 du modèle 1 au modèle 2, ce qui signale un gain de variance expliquée.
* Le F incrémentiel entre les deux modèles s'élève à 362,64 et est significatif (P<0,001). On peut donc en conclure que le second modèle est plus performant que le premier, ce qui signale que la forme curvilinéaire pour `AgeMedian` (modèle 2) est plus efficace que la forme linéaire (modèle 1).
* Dans le premier modèle, le coefficient de régression pour `AgeMedian` n'est pas significatif. L'âge médian des bâtiments n'est donc pas associé linéairement avec la variable dépendante.
* Dans le second modèle, la valeur du coefficient de `poly(AgeMedian, 2)1` est positive et celle de `poly(AgeMedian, 2)2` est négative et significative. Cela indique qu'il existe une relation linéaire en forme de U inversé. Si le premier coefficient avait été négatif et le second positif, on aurait alors conclu que la forme curvilinéaire prend la forme d'un U.


```{r calculRegPoly1, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}

# Régression linéaire
modele1 <- lm(VegPct ~ HABHA+AgeMedian+Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)

# Régression polynomiale
modele2 <- lm(VegPct ~ HABHA+poly(AgeMedian,2)+Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)

summary(modele1)
summary(modele2)
anova(modele1, modele2)
```

**Construction d'un graphique des effets marginaux**

Pour visualiser la relation linéaire et curvilinéaire, nous vous proposons de réaliser un graphique des effets marginaux à partir de la syntaxe ci-dessous.

Les graphiques des effets marginaux permettent de visualiser l'impact d'une variable indépendante sur la variable dépendante d'une régression. On se base pour cela sur les prédictions effectuées par le modèle. Admettons que nous nous intéressons à l'effet de la variable *X<sub>1</sub>* sur la variable *Y*. Il est possible de créer de nouvelles données fictives pour lesquelles l'ensemble des autres variables *X* sont fixées à leurs moyennes respectives, seule X<sub>1</sub> est autorisée à varier. En utilisant l'équation de régression du modèle sur ces données fictives, on peut observer l'évolution de la valeur prédite de Y quand *X<sub>1</sub>* augmente ou diminue, et ce, toute choses étant égales par ailleurs. Cette approche est particulièrement intéressante pour décrire des effets non-linéaires obtenus avec des polynomiales. Elle est également utilisée dans les modèles linéaires généralisés (GLM) et additifs (GAM) (chapitres \@ref(chap06) et \@ref(chap08)).


```{r calculRegPoly2, echo=TRUE, fig.align='center', fig.cap="Relations linéaire et curvilinéaire", message=FALSE, warning=FALSE, out.width='75%'}
library(ggplot2)
# Statistique sur la variable AgeMedian qui varie de 0 à 226 ans
summary(DataFinal$AgeMedian)

# Création d'un dataframe temporaire
# remarquez que les autres variables indépendantes sont constantes :
# nous leur avons attribué leur moyenne correspondante
df <- data.frame(
  HABHA = mean(DataFinal$HABHA),
  AgeMedian= seq(0,200, by = 2),
  AgeMedian2 = seq(0,200, by = 2)**2,
  Pct_014= mean(DataFinal$Pct_014),
  Pct_65P= mean(DataFinal$Pct_65P),
  Pct_MV= mean(DataFinal$Pct_MV),
  Pct_FR= mean(DataFinal$Pct_FR)
)

# calcul de la valeur de t pour un intervalle à 95%
n <- length(modele1$fitted.values) 
k <- length(modele1$coefficients)-1
t95 <- qt(p=1 - (0.05/2),  df=n-k-1)

# Calcul des valeurs prédites pour le 1er modèle
# avec l'intervalle de confiance à 95%
predsM1 <- predict(modele1,se = T, newdata = df)
df$predM1 <- predsM1$fit
df$lowerM1 <- predsM1$fit - t95*predsM1$se.fit
df$upperM1 <- predsM1$fit + t95*predsM1$se.fit

# Calcul des valeurs prédites pour le 2e modèle
# avec l'intervalle de confiance à 95%
predsM2 <- predict(modele2,se = T, newdata = df)
df$predM2 <- predsM2$fit
df$lowerM2 <- predsM2$fit - t95*predsM2$se.fit
df$upperM2 <- predsM2$fit + t95*predsM2$se.fit

# Graphique
ggplot(data = df) + 
  geom_ribbon(aes(x = AgeMedian, ymin = lowerM1, ymax = upperM1), 
             fill = rgb(0.1,0.1,0.1,0.4)) + 
  geom_path(aes(x = AgeMedian, y = predM1), color = 'blue', size = 1)+
  
  geom_ribbon(aes(x = AgeMedian, ymin = lowerM2, ymax = upperM2), 
              fill = rgb(0.1,0.1,0.1,0.4)) + 
  geom_path(aes(x = AgeMedian, y = predM2), color = 'red', size = 1)+
  
  labs(title="Effet marginal de l'âge médian des bâtiments sur la",
       subtitle = "des îlots de l'île de Montréal",
       capation = "bleu : relation linéaire; rouge : curvilinéaire",
       x = "Âge médian des bâtiments",
       y = "Couverture végétale (%)")
```

La figure \@ref(fig:calculRegPoly2) démontre bien que la relation linéaire n'est pas significative : la pente est extrêmement faible, ce qui signale que l'effet de l'âge médian est presque nul (B=0,0108, P=0,0902). Par contre, la relation linéaire est plus intéressante : la couverture végétale croît quand l'âge médian des bâtiments dans l'îlot augmente de 0 à 60 ans environ, puis, elle décroît.

### Variable indépendante qualitative dichtonomique {#sect0552}

Il est très fréquent d'introduire une variable qualitative dichtonomique comme variable explicative ou de contrôle dans un modèle. À titre de rappel, une variable dichotomique comprend deux modalités (section \@ref(sect0212)). 

Dans le modèle ci-dessous, nous voulons vérifier si un îlot situé sur le territoire de la Ville de Montréal a proportionnellement moins de végétation qu'un îlot situé dans une autre municipalité de l'île de Montréal, toutes choses étant égales à ailleurs. Pour ce faire, nous créons une variable binaire dénommée `VilleMtl` qui prend la valeur de 1 pour la Ville de Montréal et 0 pour une autre municipalité.

Nous obtenons ainsi un coefficient de régression pour `VilleMtl` de -9,085. Cela signifie que si toutes les autres variables indépendantes du modèle étaient constantes, alors en moyenne, un îlot de la Ville de Montréal a une valeur de -9,085% de moins de végétation comparativement à un îlot situé dans une autre municipalité.

```{r regmodeleDich, echo=TRUE, message=FALSE, warning=FALSE}

# Création d'une variable muette pour Montréal
DataFinal$VilleMtl <- ifelse(DataFinal$SDRNOM == "Montréal", 1, 0)

# Modèle avec la variable dichtomique
modele3 <- lm(VegPct ~ VilleMtl+HABHA+poly(AgeMedian,2)+
              Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)
```


```{r regmodeleDich2, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}

tabreg <- build_table(modele3, confid = T, std_digits = 3, coef_digits = 3, z_digits = 2, p_digits = 3)
 
 show_table(data.frame(tabreg),
            caption = 'Modèle avec une variable dichotomique',
            col.names=c("Variable","Coef.","Erreur type", "Valeur de T",                           "P", "coef. 2,5%", "coef. 97,5%", ""),
            align=c("l","r","r", "r", "r", "r", "r", "r", "r", "r")
             )
```

 
::: {.bloc_astuce data-latex=""}
**Bien interpréter un coefficient d'une variable dichotomique**

Nous avons vu que le coefficient de régression ($\beta_k$) indique le changement de la variable dépendante (*Y*), lorsque la variable indépendante augmente d’une unité, toutes choses étant égales par ailleurs.

Pour une variable dichotomique, le coefficient indique le changement de *Y* quand les observations appartiennent à la modalité qui a la valeur de 1 (ici la ville de Montréal), comparativement à celles qui a la valeur de 0 (autres municipalités de l'île de Montréal), toutes choses étant égales par ailleurs.

La modalité qui a la valeur de 0 est alors appelée la **modalité ou catégorie de référence**.

Autrement dit, si la variable aurait été codée : 0 pour la Ville et de Montréal et 1 pour les autres municipalités alors le coefficient aurait été de 9,085.

Pour éviter d'oublier quelle est la modalité de référence (valeur de 0), nous verrons plus tard (dans la section mise en œuvre des modèles de régression dans R (section \@ref(sect0509)) qu'il est préférable de définir un facteur avec la fonction `as.factor` et d'indiquer la catégorie de référence avec la fonction `relevel(x, ref)`.
:::

Comme pour une variable indépendante introduite avec une fonction polynomiale, il est peut-être très intéressant d'illuster l'effet marginal de la variable dichotomique avec un graphique qui montre l'écart entre les moyennes des deux modalités, une fois contrôlées les autres variables indépendantes (figure \@ref(fig:EffetMarginalDich)).

```{r EffetMarginalDich, echo=TRUE, fig.align='center', fig.cap="Effet marginal d'une variable dichtomique", message=FALSE, warning=FALSE, out.width='70%'}
library(ggplot2)
library(ggeffects)

# Valeurs prédites selon le modèle avec la variable dichtomique
dfPreds <- ggpredict(modele3, terms = "VilleMtl")

# Graphique
ggplot(dfPreds, aes(x=x, y=predicted)) +
  geom_bar(stat = "identity", position = position_dodge(), fill="wheat") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), alpha = .9,position = position_dodge())+
  labs(title="Effet marginal de la Ville de Montréal sur la végétation",
       x="Municipalités de la région de Montréal",
       y="Couverture végétation de l'îlot (%)")+
  scale_x_continuous(breaks=c(0,1),
                     labels = c("Autre municipalité", "Ville de Montréal"))
```                                
                                
### Variable indépendante qualitative polytomique {#sect0553}

Il peut être aussi pertinent d'introduire une variable qualitative polytomique comme variable explicative ou de contrôle dans un modèle. À titre de rappel, une variable polytomique comprend plus de deux modalités, qu'elle soit nominale ou ordinale (section \@ref(sect0212)). 

En guise d'exemple, une variable qualitative pourrait être : différents groupes de population (groupes d'âge, minorités visibles, catégories socioprofessionnelles, etc.), différents territoires ou régions (ville centrale, première couronne, deuxième couronne, etc.), une variable semi-quantitative (par exemple une variable continue transformée en quatre ou cinq catégories ordinales selon les quartiles ou les quintiles).

#### Comment est construit une modèle de régression avec variable explicative qualitative polytomique ? {#sect05531}

Prenons l'exemple d'un modèle de régression comprenant deux variables indépendantes : l'une continue (`X1`), l'autre qualitative (`X2`) comprenant quatre modalités (A, B, C et D). L'introduction de la variable qualitative dans le modèle revient à :

* Transformer chaque modalité en variable muette (binaire). On a ainsi quatre nouvelles variables binaires : `X2A`, `X2B`, `X2C` et `X2D`. Par exemple, les observations de la modalité *A* se verront affectées la valeur de 1 tandis que les autres observations la valeur de 0 pour `X2A`. La même démarche s'applique à `X2B`, `X2C` et `X2D` (voir tableau \@ref(tab:transfVarQMuettes)).

```{r transfVarQMuettes, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
df <- data.frame(
          obs = c(1:10),
          Y= round(rnorm(10, mean=50, sd=10),2),
          X1 = round(rnorm(10, mean=20, sd=5),2),
          X2 = c("A","A","A","B","B","B","C","C","D","D")
  )

df$X2A <- ifelse(df$X2 == "A", 1, 0)
df$X2B <- ifelse(df$X2 == "B", 1, 0)
df$X2C <- ifelse(df$X2 == "C", 1, 0)
df$X2D <- ifelse(df$X2 == "D", 1, 0)

show_table(df,
            caption = "Transformation d'une variable qualitative en variables muettes pour chaque modalité",
           align= c("r","r","r","c", "c", "c", "c", "c")
           )
```

* Toutes les modalités transformées en variables muettes sont introduites dans le modèle comme variables indépendantes **sauf une servant de catégorie de référence**. Pourquoi sauf une? Si on mettait toutes les modalités en variable muette alors chaque observation serait repérée par une valeur de 1, « il y aurait alors une parfaite multicolinéarité et aucune solution unique pour les coefficients de régression ne pourrait être trouvée » [@bressoux2010, p. 128]. 

* Par exemple, si l'on choisit la modalité *A* comme catégorie de référence, l'équation de régression s'écrit alors :


\footnotesize 
\begin{equation}
Y = b_{0} + b_{1}X_{1} + b_{2B}X_{2B} + b_{2C}X_{2C} + b_{2D}X_{2D}+ e
(\#eq:regvarpoly1)
\end{equation}
\normalsize

* Vous aurez compris que choisir la modalité *D* comme catégorie de référence revient à écrire l'équation suivante :

\footnotesize 
\begin{equation}
Y = b_{0} + b_{1}X_{1} + + b_{2A}X_{2A} + b_{2B}X_{2B} + b_{2C}X_{2C} + e
(\#eq:regvarpoly2)
\end{equation}
\normalsize

#### Comment interpréter les coefficients des modalités d'une variable explicative qualitative polytomique  {#sect05532}

Les coefficients des différentes modalités s'interprètent en fonction de la catégorie de référence. Dans l'exemple ci-dessous, nous avons inclus la Ville de Montréal comme catégorie de référence. On peut alors constater que toutes choses étant égales par ailleurs :

* en moyenne, les îlots résidentiels de Senneville et Baie-D’Urfé ont respectivement 33,136% et 26,759% plus de végétation que ceux de la Ville de Montréal.
* la seule municipalité comprenant en moyenne moins de végétation dans ces îlots résidentiels est celle Montréal-Nord (-2,368%) 
* on remarque aussi que les municipalités de Sainte-Anne-de-Bellevue, Montréal-Ouest et Côte-Saint-Luc ne présentent pas significativement moins ou plus de végétation que ceux de la Ville de Montréal (leurs valeurs de P sont supérieures à 0,05).

Par conséquent, les valeurs de T et de P pour une modalité permettent de vérifier si elle est ou non significativement différente de la catégorie de référence.

```{r ModeleVarPoly1, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
DataFinal$Municipalité <- relevel(DataFinal$SDRNOM, ref="Montréal")
modele4a <- lm(VegPct ~ HABHA+poly(AgeMedian,2)+Pct_014+Pct_65P+Pct_MV+Pct_FR+Municipalité, data = DataFinal)

tabreg <- build_table(modele4a, confid = F, std_digits = 3, coef_digits = 3, z_digits = 2, p_digits = 3)
 
 show_table(data.frame(tabreg),
            caption = 'Modèle avec une variable polytomique (Ville Montréal en catégorie de référence)',
            col.names=c("Variable","Coef.","Erreur type", "Valeur de T","P", ""),
            align=c("l","r","r", "r", "r", "r", "r", "r")
             )
```

Mettons désormais la municipalité qui avait le coefficient le plus fort dans le modèle précédent, soit Senneville. Bien entendu, les coefficients des variables continues et de la constante ne changent pas. Par contre, les coefficients de toutes les municipalités sont négatifs puisque la Ville de Senneville est celle qui a proportionnellement le plus de végétation dans ces îlots, toutes choses étant égales par ailleurs.

```{r ModeleVarPoly2, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
DataFinal$Municipalité <- relevel(DataFinal$SDRNOM, ref="Senneville")
modele4b <- lm(VegPct ~ HABHA+poly(AgeMedian,2)+Pct_014+Pct_65P+Pct_MV+Pct_FR+Municipalité, data = DataFinal)

tabreg <- build_table(modele4b, confid = F, std_digits = 3, coef_digits = 3, z_digits = 2, p_digits = 3)
 
 show_table(data.frame(tabreg),
            caption = 'Modèle avec une variable polytomique (Senneville en catégorie de référence)',
            col.names=c("Variable","Coef.","Erreur type", "Valeur de T","P", ""),
            align=c("l","r","r", "r", "r", "r", "r", "r")
             )
```

À l'inverse, si Montréal-Est, soit la municipalité avec le coefficient le plus faible dans le premier modèle, tous les coefficients deviendront positifs.


```{r ModeleVarPoly3, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
DataFinal$Municipalité <- relevel(DataFinal$SDRNOM, ref="Montréal-Est")
modele4c <- lm(VegPct ~ HABHA+poly(AgeMedian,2)+Pct_014+Pct_65P+Pct_MV+Pct_FR+Municipalité, data = DataFinal)

tabreg <- build_table(modele4c, confid = F, std_digits = 3, coef_digits = 3, z_digits = 2, p_digits = 3)
 
 show_table(data.frame(tabreg),
            caption = 'Modèle avec une variable polytomique (Montréal-Est en catégorie de référence)',
            col.names=c("Variable","Coef.","Erreur type", "Valeur de T","P", ""),
            align=c("l","r","r", "r", "r", "r", "r", "r")
             )
```

::: {.bloc_astuce data-latex=""}
**Mais, mais alors Jamy comment choisir la catégorie de référence ?**

Plusieurs options sont possibles, choisir :

* la modalité comprenant le plus d'observations
* la modalité avec la plus forte valeur pour la variable dépendante
* la modalité celle avec la plus faible valeur pour la variable dépendante
* la modalité qui fait le plus de sens avec votre cadre théorique. Prenons l'exemple d'une variable qualitative comprenant plusieurs groupes d'âge (15-29 ans, 30-39 ans, 40-49 ans, 50-54 ans, 65 ans et plus). Si votre étude porte sur les jeunes et que vous souhaitez comparer leur situation comparativement aux autres groupes d'âge, toutes choses étant égales par ailleurs, sélectionnez bien évidemment la modalité des 15 à 29 ans comme catégorie de référence.

Mais surtout, éviter de choisir une catégorie comprenant très peu d'observations.
::: 

#### L'effet marginal d'une variable explicative qualitative polytomique  {#sect05533}

Comme pour une variable dichotomique, il est possible d'illustrer l'effet marginal de variable qualitative dichotomique avec un graphique. Quel que soit la catégorie de référence choisi, le graphique sera le même. La figure \@ref(fig:EffetMarginalPoly) illustre ainsi la valeur moyenne avec son intervalle de confiance (à 95%) de la végétation dans les îlots résidentiels de chacune des municipalités de la région de Montréal, *ceteris paribus*.

```{r EffetMarginalPoly, echo=TRUE, fig.align='center', fig.cap="Effet marginal d'une variable polytomique", message=FALSE, warning=FALSE, out.width='90%'}
library(ggplot2)
library(ggeffects)

# Valeurs prédites selon le modèle avec la variable polytomique
dfPreds <- ggpredict(modele4a, terms = "Municipalité")

# Graphique
Graphique <-  ggplot(dfPreds, aes(x=x, y=predicted)) +
  geom_bar(stat = "identity", position = position_dodge(), fill="wheat") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), alpha = .9,position = position_dodge())+
  labs(title="Effet marginal des différentes municipalités",
       x="Municipalités de la région de Montréal",
       y="Couverture végétation de l'îlot (%)")

Graphique + coord_flip()

```


### Variable d'interaction {#sect0554}

https://www.econometrics-with-r.org/8-3-interactions-between-independent-variables.html


## Diagnostics de la régression {#sect056}


## Comment rapporter une régression  {#sect057}

## Régression linéaire multiple robuste {#sect058}

## Mise en œuvre dans R {#sect059}


Le tableau de sortie pour les coefficients comprend plusieurs colonnes, à savoir les coefficients de régression (`Estimate`), l'erreur type du coefficient (`Std. Error`), la valeur de T (`t value`) et la probabilité associée à la valeur de T (`Pr(>|t|)`). La première ligne de ce tableau (`Estimate`) est pour la constante (*Intercept* en anglais) et celles qui suivent sont pour les variables indépendantes (`HABHA`, `AgeMedian`, `Pct_014`, `Pct_65P`, `Pct_MV`, `Pct_FR`).





<!--chapter:end:05-regressionlineaire.Rmd-->

# Régressions linéaires généralisées (GLM)  {#chap06}

## Qu'est qu'un modèle GLM? {#sect061}

### Principes de base des GLM {#sect0611}

### Reformulation d'un modèle OLS sous forme GLM {#sect0612}

## Les principaux modèles GLM utilisées {#sect062}

### Les régressions logistiques {#sect0621}

#### Régression logit et probit simple {#sect06211}

#### Régression logistique multinomiale {#sect06212}

#### Régression logistique ordinale {#sect06213}

### Les régressions de type Poisson {#sect0623}


## D'autres modèles GLM  {#sect063}
Student, Beta, GAMMA, Exponentiel




<!--chapter:end:06-GLM.Rmd-->

# Régressions à effets mixtes (GLMM) et régression multiniveaux  {#chap07}

## Principes de base des GLMM {#sect071}

Notion de pseudo-réplications

Effets aléatoire *versus* effets fixes

## Application à une variable continue {#sect072}

## Régressions multiniveaux {#sect073}

<!--chapter:end:07-GLMM.Rmd-->

# Modèles généralisés additifs  {#chap08}

## Principes de base des GAM {#sect081}

## Extensions des GAM {#sect082}

### GAMM {#sect0821}
### GAMMAR {#sect0822}

<!--chapter:end:08-GAM.Rmd-->


# (PART) Analyses exploratoires multivariées {-} 

# Méthodes factorielles {#chap09}

## Un petit historique {#sect091}

## Analyses de composantes principales (ACP) {#sect092}

## Analyses factorielles de correspondances (AFC)  {#sect093}

## Analyses factorielles de correspondances multiples (AFM)  {#sect094}

## Analyses factorielles de correspondances mixte  {#sect095}

<!--chapter:end:09-AnalysesFactorielles.Rmd-->

# Méthodes de classification non-supervisées {#chap10}

## Un aperçu sur la multitude des méthodes de classications {#sect101}

## Combinaisons méthodes factorielles et méthodes de classications {#sect102}

## Classification ascendantes hiérachiques {#sect103}

## Nuées dynamiques  {#sect104}

### k-means  {#sect1041}
### k-median  {#sect1042}
### Les extensions en logique floues : c-means, c-median  {#sect1043}

<!--chapter:end:10-MethodeClassification.Rmd-->

# Conclusion {#conclusion}

## Synthèse des méthodes abordées  {#sect111}
*Road map method*

## Autres méthodes non abordées {#sect112}

### Régressions par quantile {#sect1121}
### Régressions par panel {#sect1122}
### Régressions par Tobit {#sect1123}
### Analyses de survie {#sect1124}
### Équations structurelles {#sect1125}


## Deux écoles de statistique inférielle : fréquentiste et bayésienne  {#sect113}


## Éthique en méthodes quantitatives {#sect114}

<!--chapter:end:11-Conclusion.Rmd-->

# Annexes {#annexes}

## Tableau des valeurs critiques de khi^2^ {#annexe1}

La courte syntaxe ![](Images/Rlogo.png) ci-dessous permet de générer un tableau avec les valeurs critiques du khi^2^ pour différents degrés de signification (valeurs de p).

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(stargazer)

# vecteur pour les degrés de liberté de 1 à 30, puis 40 et 50
dl <- c(1:30, 40, 50, 100, 250, 500) 
# la fonction qchisq permet d'obtenir la valeur théorique en fonction 
# d'une valeur de p et d'un nombre de degrés de liberté
tableChi2 <- cbind(dl,
                p0.05 =  round(qchisq(p=0.95,  df=dl, lower.tail = FALSE),3),
                p0.01 =  round(qchisq(p=0.99,  df=dl, lower.tail = FALSE),3),
                p0.001 = round(qchisq(p=0.999, df=dl, lower.tail = FALSE),3))
# Impression du tableau avec la library stargazer
stargazer(tableChi2, type="text", summary=FALSE, rownames=FALSE, align = TRUE, digits = 3,
          title="Distribution des valeurs critiques du Khi2")
```


```{r tableCritiqueChi2, echo=FALSE, message=FALSE, warning=FALSE}
dl <- c(1:30, 40, 50, 100, 250, 500)
tableChi2 <- cbind(dl,
                p0.05 =  round(qchisq(p=0.95,  df=dl, lower.tail = FALSE),3),
                p0.01 =  round(qchisq(p=0.99,  df=dl, lower.tail = FALSE),3),
                p0.001 = round(qchisq(p=0.999, df=dl, lower.tail = FALSE),3))

# knitr::kable(
#   head(tableChi2, n=nrow(tableChi2)), booktabs = TRUE,
#   col.names=c("dl", "p=0,05", "p=0,01", "p=0,001"),
#   caption = 'Distribution des valeurs critiques du khi2'
#   )

show_table(tableChi2, 
           caption = "Distribution des valeurs critiques du khi2",
           col.names=c("dl", "p=0,05", "p=0,01", "p=0,001")
)
```

## Tableau des valeurs critiques de F {#annexe2}

La courte syntaxe ![](Images/Rlogo.png) ci-dessous permet de générer un tableau avec les valeurs critiques de F avec _p_=0,05.

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(stargazer)

dl1 <- c(1:10, 15, 20, 25, 30, 50, 100, 500)
dl2 <- c(1:30, 40, 50, 100)
matrice <- matrix(ncol=length(dl1), nrow=length(dl2), byrow = TRUE)
for(r in 1:length(dl1)){
  for(c in 1:length(dl2)){
    matrice[c,r] <- round(qf(p=0.05, dl1[r], dl2[c], lower.tail = FALSE),2)
  }
}

tableF_p0.05 <- as.data.frame(matrice, row.names = paste0("dl2=",dl2, sep=""))
colnames(tableF_p0.05) <- paste0("dl1=",dl1, sep="")

stargazer(tableF_p0.05, type="text", summary=FALSE, rownames=FALSE, align = TRUE, digits = 3,
          title="Distribution des valeurs critiques de F avec p=0,05")
```


```{r tableCritiqueF, echo=FALSE, message=FALSE, warning=FALSE}
dl1 <- c(1:10, 15, 20, 25, 30, 50, 100, 500)
dl2 <- c(1:30, 40, 50, 100)
matrice <- matrix(ncol=length(dl1), nrow=length(dl2), byrow = TRUE)
for(r in 1:length(dl1)){
  for(c in 1:length(dl2)){
    matrice[c,r] <- round(qf(p=0.05, dl1[r], dl2[c], lower.tail = FALSE),2)
  }
}

tableF_p0.05 <- as.data.frame(matrice, row.names = paste0("dl2=",dl2, sep=""))
colnames(tableF_p0.05) <- paste0("dl1=",dl1, sep="")

# knitr::kable(
#   head(tableF_p0.05, n=nrow(tableF_p0.05)), booktabs = TRUE,
#   row.names = TRUE,
#   caption = 'Distribution des valeurs critiques de F avec p=0,05'
# )

show_table(tableF_p0.05, 
           caption = "Distribution des valeurs critiques de F avec p=0,05")
```


## Tableau des valeurs critiques de *t* {#annexe3}

La courte syntaxe ![](Images/Rlogo.png) ci-dessous permet de générer un tableau avec les valeurs critiques de T avec _p_=0,05, 0,01 et 0,01.


```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(stargazer)

# vecteur pour les degrés de liberté de 1 à 30, puis 40 et 50
dl <- c(1:30, 40, 50, 60, 70, 80, 90, 100, 250, 500, 1000, 2500) 
# la fonction qchisq permet d'obtenir la valeur théorique en fonction 
# d'une valeur de p et d'un nombre de degrés de liberté
tableT <- cbind(dl,
                p0.10 =  round(qt(p=1 - (0.10/2),  df=dl),2),
                p0.05 =  round(qt(p=1 - (0.05/2),  df=dl),2),
                p0.01 =  round(qt(p=1 - (0.01/2),  df=dl),2),
                p0.001 = round(qt(p=1 - (0.001/2), df=dl),2))
# Impression du tableau avec la library stargazer
stargazer(tableT, type="text", summary=FALSE, rownames=FALSE, align = TRUE, digits = 2,
          title="Distribution des valeurs critiques de t")
```


```{r tableCritiqueT, echo=FALSE, message=FALSE, warning=FALSE}
# vecteur pour les degrés de liberté de 1 à 30, puis 40 et 50
dl <- c(1:30, 40, 50, 60, 70, 80, 90, 100, 250, 500, 1000, 2500) 
# la fonction qchisq permet d'obtenir la valeur théorique en fonction 
# d'une valeur de p et d'un nombre de degrés de liberté
tableT <- cbind(dl,
                p0.10 =  round(qt(p=1 - (0.10/2),  df=dl),2),
                p0.05 =  round(qt(p=1 - (0.05/2),  df=dl),2),
                p0.01 =  round(qt(p=1 - (0.01/2),  df=dl),2),
                p0.001 = round(qt(p=1 - (0.001/2), df=dl),2))

# Impression du tableau avec la library stargazer
# stargazer(tableT, type="text", summary=FALSE, rownames=FALSE, align = TRUE, digits = 3,
          # title="Distribution des valeurs critiques du t")

show_table(tableT, 
           caption = "Distribution des valeurs critiques de t")
```

<!--chapter:end:19-Annexes.Rmd-->

