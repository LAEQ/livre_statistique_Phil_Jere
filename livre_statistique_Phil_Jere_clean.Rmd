--- 
title: "Introduction aux méthodes quantitatives en sciences sociales avec R"
author: "Philippe Apparicio et Jérémy Gelb"
date: "`r Sys.Date()`"
colorlinks: yes
cover-image: images/cover.png
bibliography: book.bib
description: Ce livre propose une introduction aux méthodes quantitatives en sciences
  sociales basée sur le logiciel ouvert R. Le contenu est pensé pour être accessible
  même à ceux et celles n'ayant presque aucune base en statistique ou en programmation. Les
  personnes plus expérimentées y découvriront également des sections sur des méthodes
  poussées comme les modèles généralisés additifs à effets mixtes ou les méthodes
  factorielles mixtes. Celles cherchant à passer à R et délaisser progressivement SPSS, SAS ou STATA trouveront dans cet ouvrage les éléments pour une transition en douceur. La philosophie
  de se livre est de donner toutes les clefs de compréhension et de mise en œuvre
  des méthodes abordées afin de faciliter l'assimilation par les lecteur·trices. La présentation
  des méthodes est basée sur une approche compréhensive et intuitive plutôt que mathématique
  sans pour autant que la rigueur statistique ne soit négligée. Servez-vous votre
  boisson chaude ou froide favorite, installez-vous dans votre meilleur fauteuil et
  bonne lecture !
documentclass: book
fontsize: 11pt
github-repo: LAEQ/livre_statistique_Phil_Jere
graphics: yes
link-citations: yes
lof: yes
lot: yes
mainfont: Palatino Linotype
monofont: "Source Code Pro"
monofontoptions: "Scale=0.8"
mathfontoptions: "Scale=0.1"
site: bookdown::bookdown_site
biblio-style: apalike
url: https\://bookdown.org/yihui/rmarkdown/
geometry: "left=2cm,right=2cm,top=2cm,bottom=2cm"
toc-title: "Table des matières"
lang: fr
---


# Préface {-}

```{asis, echo=identical(knitr:::pandoc_to(), 'html')}

Ce livre propose une introduction aux méthodes quantitatives en sciences sociales basée sur le logiciel ouvert R. Il a d'ailleurs été écrit intégralement dans R avec **rmarkdown**. Le contenu est pensé pour être accessible même à ceux n'ayant presque aucune base en statistique ou en programmation. Les personnes plus expérimentées y découvriront également des sections sur des méthodes plus avancées comme les modèles généralisés additifs à effets mixtes ou les méthodes factorielles. Ceux cherchant à migrer progressivement d'un autre logiciel statistique vers R trouveront dans cet ouvrage les éléments pour une transition en douceur. La philosophie de ce livre est de donner toutes les clefs de compréhension et de mise en œuvre des méthodes abordées afin de facilité l'assimilation par le lecteur. La présentation des méthodes est basée sur une approche compréhensive et intuitive plutôt que mathématique sans pour autant que la rigueur statistique ne soit négligée. Servez-vous votre boisson chaude ou froide favorite, installez-vous dans votre meilleur fauteuil et bonne lecture !


::: {.bloc_notes data-latex=""}

**Ce livre est un projet en cours d'écriture !**

Il sert de matériel pour le cours **Méthodes quantitatives appliquées aux études urbaines** (EUR8219), offert au Centre Urbanisation Culture Société de l'INRS. Son contenu est amené à changer et des erreurs peuvent encore être présentes. À terme, nous espérons le publier. Par conséquent, **son contenu ne peut donc en aucun cas être partagé en dehors du cours**. Vos commentaires et suggestions sur le contenu et la forme sont bienvenus ! Si certains passages vous semblent peu clairs, n'hésitez pas à nous en faire part.

:::

```

## Comment lire ce livre {-}

Si vous googlez l'expression « comment lire un livre ? », vous trouverez une multitude de conseils et astuces. Pour ce livre, nous conseillons de le lire de gauche à droite et page par page. Plus sérieusement, il comprend plusieurs types de blocs de texte qui, on l'espère, faciliteront la lecture.


::: {.bloc_package data-latex=""}
**Bloc packages**: habituellement localisé en début du chapitre, il comprend la liste des *packages* R utilisés pour un chapitre.
:::

::: {.bloc_objectif data-latex=""}
**Bloc objectif**: comprend une description des objectifs d'une section.
:::

::: {.bloc_notes  data-latex=""}
**Bloc notes**: comprend une information secondaire sur une notion, un élément, une idée abordée dans une section.
:::

::: {.bloc_aller_loin data-latex=""}
**Bloc pour aller plus loin** : peut comprendre des références ou des extensions d'une méthode statistique abordée dans une section.
:::

::: {.bloc_astuce data-latex=""}
**Bloc astuce**: décrit un élément qui vous facilera le vie : une propriété statistique, un *package*, une fonction, une syntaxe R.
:::

::: {.bloc_attention data-latex=""}
**Bloc attention**:  comprend une notion ou un élément important à bien maîtriser.
:::

## Structure du livre {-}

Le livre est organisé autour de cinq grandes parties.


**Partie 1. La découverte de R.** Dans cette première partie, nous discuterons brièvement de l’histoire et de la philosophie de R. Nous verrons ensuite comment installer R et RStudio. Les bases du langage R (particulièrement les principaux objets que sont le vecteur, la matrice, la liste et le *dataframe*) ainsi la manipulation des données avec R sont aussi largement abordés dans le chapitre \@ref(chap01).

**Partie 2. Analyses univariées et représentations graphiques**.

**Partie 3. Analyses bivariées.** Cette troisième partie comprend trois chapitres dans lesquelles sont présentées les principales méthodes exploratoires et confirmatoires bivariées permettant d'évaluer la relation entre deux variables (figure \@ref(fig:fig1)). Plus spécifiquement, nous présenterons puis mettrons en œuvre dans R les méthodes permettant d'explorer les relations entre deux variables quantitatives (covariance, corrélation et régression linéaire simple dans le chapitre \@ref(chap03)), deux variables qualitatives (tableau de contingence et test du khi^2^ dans le chapitre \@ref(chap04)) et une variable qualitative avec deux modalités (tests de student, de Welch et de Wilcoxon) ou avec plus de deux modalités (ANOVA et test de Kruskal-Wallis) (chapitre \@ref(chap05)).


```{r fig1, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Les principales méthodes bivariées",  out.width='65%'}
knitr::include_graphics('images/bivariee/figure1.jpg', dpi = NA)
```

**Partie 4. Modèles de régression**. Dans cette partie, sont présentées les principales méthodes de statistique inférentielle utilisées en sciences sociales : la **régression linéaire multiple** (chapitre \@ref(chap06)), les régressions linéaires généralisées (chapitre \@ref(chap07)), les régressions à effets mixtes (chapitre \@ref(chap08)), les régressions à effets mixtes (chapitre \@ref(chap09)), les régressions multiniveaux (chapitre \@ref(chap10)) et les modèles généralisés additifs (chapitre \@ref(chap11)).

**Partie 5. Analyses exploratoires multidimensionnelles**. Dans cette cinquième partie, seront abordées les méthodes de statistique exploratoire et descritive permettant de décrire des tableaux de données comprenant plusieurs variables. Nous décrirons d'abord les méthodes de réduction de données — les méthodes factorielles  (analyses de composantes principales, analyses factorielles de correspondances, analyses factorielles de correspondances multiples et analyses factorielles de correspondances mixte) dans le chapitre \@ref(chap12); puis, les méthodes de classification dans le chapitre \@ref(chap13) classification ascendantes hiérachiques, k-means, k-median et leurs extensions en logique floue comme le c-means et c-median).


## Pourquoi faut-il programmer en sciences sociales ? {-}

Vous contrasterez rapidement que R est un véritable langage de programmation. Il est donc légitime de se questionner à savoir si l'apprentissage d'un tel langage est pertinent pour un·e étudiant·e ou un·e chercheur·e en sciences sociales. Il est vrai que la programmation n'est pas la compétence qui vient tout de suite à l'esprit lorsque l'on pense aux sciences sociales. Pourtant, cette compétence est de plus en plus importante, et ce, pour plusieurs raisons :

* Une part toujours plus grande des phénomènes sociaux se produisent ou peuvent s'observer au travers d'environnements numériques. Être capable d'exploiter efficacement ces outils permet d'extraire des données riches sur des phénomènes complexes, tels qu’en témoigne des études récentes sur la propagation de la désinformation sur les réseaux sociaux [@allcott2017social], la migration des personnes [@spyratos2019quantifying], la propagation et les risques de contamination de la COVID19 [@boulos2020geographical], etc. Le plus souvent, les interfaces (API par exemple) permettant d'accéder à ces données nécessitent une base en programmation.
* La quantité de données numériques ouvertes et accessibles en ligne croit chaque année sur des sujets très divers. La plupart des villes et des gouvernements ont maintenant leurs portails de données ouvertes auxquels s'ajoutent les données produites par des projets collaboratifs comme [OpenStreetMap](https://www.openstreetmap.org) ou [NoisePlanet](https://noise-planet.org/map_noisecapture/index.html). Récupérer ces données et les structurer pour les utiliser à des fins de recherche nécessite le plus souvent des compétences en programmation.
* Les méthodes d'analyse quantitative connaissent également un développement très important. Les logiciels propriétaires peinent à suivre la cadence de ce développement contrairement aux logiciels à code source ouvert qui permettent d'avoir accès aux dernières méthodes. Il est souvent long et coûteux de développer une interface graphique pour un logiciel, ce qui explique que la plupart de ces programmes en sont dépourvus et nécessitent alors de savoir programmer pour les utiliser.
* Savoir programmer donne une liberté considérable en recherche. Cette compétence permet notamment de ne plus être limité aux fonctionnalités proposées par des logiciels spécifiques. Il devient possible d'innover tant en matière de structuration, d'analyses que de représentations des résultats en écrivant vos propres fonctions. Cette flexibilité contribue directement à la production d'une recherche de meilleure qualité et plus diversifiée.
* Programmer permet également d'automatiser des tâches qui autrement seraient extrêmement répétitives. Déplacer et renommer une centaine de fichiers ? Retirer les lignes inutiles dans un ensemble de fichiers CSV et les compiler dans une seule base de données ? Tester parmi des milliers d'adresses lesquelles sont valides ? Récupérer chaque jour les messages postés sur un forum ? Autant de tâches faciles à automatiser si l'on sait programmer.
* Dans un logiciel avec une interface graphique, il est compliqué de conserver un historique des opérations effectuées. Programmer permet au contraire de garder une trace de l'ensemble des actions effectuées au cours d'un projet de recherche. En effet, le code utilisé reste disponible et permet de reproduire la méthode et les résultats obtenus ce qui est essentiel dans le monde de la recherche. À cela s'ajoute le fait que chaque ligne de code que vous écrivez vient s'ajouter à un capital de code que vous possédez, car elles pourront être réutilisées dans d'autres projets !


## Remerciements {-}

Ce livre est dédié au beau Cargo (chien Mira) qui nous a tant supporté dans l'écriture du livre !

Nous tenons aussi à remercier sincérement les étudiant·e·s du cours **Méthodes quantitatives appliquées aux études urbaines (EUR8219)** du programme de maîtrise en études urbaines de l'INRS. À compléter plus tard.

```{r cargo, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Cargo, le plus beau",  out.width='50%'}
knitr::include_graphics('images/Cargo.jpg', dpi = NA)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
## quelques parametres generaux en fonction du type de document
library(tidyverse)
library(ggpubr)

if (knitr::is_latex_output()){
  font_size_table <- 8
}else{
  font_size_table <- 14
}

# note : il ne faut changer que les caracteres en dehors des equations
clean_text <- function(string){
  if (is.na(string)){
    return (NA)
  }
  if(string == ""){
    return("")
  }
  changes <- list(
     c("`", "", F),
      c("\\^([^\"]*?)\\^","\\\\textsuperscript{\\1}",F),
      c("\\*{2}([^\"]*)\\*{2}","\\\\textbf{\\1}", F),
      c("\\*{2}([^\"]*)\\*{2}","\\\\textbf{\\1}", F),
      c("\\%", "\\\\%", F),
      c("&", "\\\\&", F),
      #c("\\^(?=(?:[^$]*$[^$]*$)*[^$]*$)", "\\\\textasciicircum{}", F),
      c("\\*{2}", "doublestars", F),
      c("\\*([^\"]*?)\\*", "\\\\textit{\\1}", F),
      c("doublestars", "**", F),
      c("\\\\@ref\\(([^\"]*?)\\)", "\\\\ref{\\1}", F),
      c("\\(\\%\\)", "(\\\\%)", F),
      c("_", "\\_", TRUE),
      c("beta\\_", "beta_", TRUE),
      c("alpha\\_", "alpha_", TRUE)
      )
  parts <- strsplit(string, "$", fixed = T)[[1]]
  new_strings <- sapply(1:length(parts), function(i){
    thisstring <- parts[[i]]
    if (thisstring != ""){
      parts2 <- strsplit(thisstring, "`", fixed = T)[[1]]
      new_strings2 <- sapply(1:length(parts2), function(j){
        thisstring2 <- parts2[[j]]
        if (j %% 2 ==0){
          thisstring2 <- gsub("^","\\textasciicircum{}",thisstring2, fixed = T)
          return(thisstring2)
        }else{
          return(thisstring2)
        }
      })
      thisstring <- paste(new_strings2, collapse="")
    }
    
    
    if (i %% 2 ==0){
      return(paste("$",thisstring,"$",sep=""))
    }else{
      for (ch in changes){
          if(ch[[3]]){
            thisstring <- gsub(ch[[1]],ch[[2]],thisstring, fixed = T)
          }else{
            thisstring <- gsub(ch[[1]],ch[[2]],thisstring, perl = T)
          }
          
      }
      return(thisstring)
    }
  })
  fullstring <- paste(new_strings, collapse = "")
  return (fullstring)
  
}

clean_df_latex <- function(df){
  if(inherits(df, "list") == F){
    df <- list(df)
  }
  df2 <- lapply(df, function(i){
    i <- data.frame(i)
    for(col in names(i)){
      if(is.character(i[[col]])){
       i[[col]] <- sapply(i[[col]], clean_text)
      }
    }
    return(i)
  })
  return(df2)
}

show_table <- function(df, col.names = NA, caption = NULL, col.to.resize = NULL, 
                       col.width = NULL, digits = getOption("digits"),
                       align = NULL){
  options(knitr.kable.NA = '')
  ## dans le cas d'un tableau LATEX
  if (knitr::is_latex_output()){
    #etape1 : supprimer les caracteres speciaux, echapper les gras et italic
    df2 <- clean_df_latex(df)
    if (is.null(col.names) == F){
      if(is.na(col.names)==F){
        #col.names <- gsub("\\(\\%\\)","(\\\\%)",col.names)
        col.names <- gsub("\\%","\\\\%",col.names)
        col.names <- gsub("\\^([^\"]*?)\\^","\\\\textasciicircum{\\1}",col.names,fixed = F)
      }
    }

    if(is.null(caption)==F){
      caption <- gsub("\\*{2}([^\"]*)\\*{2}","\\\\textbf{\\1}", caption)
      caption <- gsub("\\*([^\"]*?)\\*","\\\\textit{\\1}", caption)
    }
    if(length(df2) == 1){
      df2 <- df2[[1]]
    }
    table1 <- knitr::kable(
      df2, booktabs = TRUE,
      format = "latex",
      digits = digits,
      format.args = list(decimal.mark = ",", big.mark = " "),
      valign = 't', row.names = FALSE,
      align = align,
      col.names = col.names,
      caption = caption,
      escape = FALSE) %>% 
      kableExtra::kable_styling(font_size = 8,protect_latex = T)
    if (is.null(col.to.resize) == FALSE){
      table1 <- table1 %>% 
        kableExtra::column_spec(col.to.resize, width=col.width)
    }
  ## dans le cas d'un tableau HTLM
  }else{
    
    # gerer les newline
    if(class(df) != "list"){
      df <- data.frame(df)
      for(col in names(df)){
        if(is.character(df[[col]])){
          df[[col]] <- gsub("\\newline","<br/>",df[[col]], fixed = T)
        }
      }
    }else {
      df <- lapply(df, function(i){
        i <- data.frame(i)
        i <- i %>% mutate_if(~is.character(.), funs(str_replace_all(., "\\newline", "<br/>")))
        return(i)
      })
    }
    table1 <- knitr::kable(
      df, booktabs = TRUE,
      format.args = list(decimal.mark = ",", big.mark = " "),
      col.names = col.names,
      align = align,
      valign = 't', row.names = FALSE,
      caption = caption,
      escape = FALSE)
  }
  return(table1)
}

tofr <- function(float){
  return(gsub(".",",",as.character(float),fixed = T))
}

```


```{r message=FALSE, warning=FALSE, include=FALSE}
source("code_complementaire/JG_helper.R")
```

<!--chapter:end:index.Rmd-->

# À propos des auteurs {#auteurs .unnumbered}

**Philippe Apparicio** (<http://www.ucs.inrs.ca/philippe-apparicio>) est professeur titulaire au Centre Urbanisation Culture Société de l'INRS (<http://www.ucs.inrs.ca/>). Il enseigne au programme de maîtrise en études urbaines (<http://www.ucs.inrs.ca/ucs/etudier/programmes/etudes-urbaines>) les cours *méthodes quantitatives appliquées aux études urbaines* et _analyses spatiales appliquées aux études urbaines_. Il a aussi créé et enseigné, il y a plusieurs années, le cours _systèmes d'information géographique appliqués aux études urbaines_. Durant les dernières années, il a offert plusieurs formations aux Écoles d'été du Centre interuniversitaire québécois de statistiques sociales (CIQSS, <https://www.ciqss.org/>). Titulaire de la Chaire de recherche du Canada (niveau 2) sur l'équité environnementale et la ville, il est le directeur du **laboratoire d'équité environnementale** (<http://laeq.ucs.inrs.ca>). Géographe de formation, ses intérêts de recherche actuels incluent la justice et l'équité environnementale, la pollution atmosphérique, le bruit et le vélo en ville. Il a publié une centaine d'articles scientifiques dans différents domaines des études urbaines et de la géographie.

**Jérémy Gelb** est candidat au doctorat en études urbaines à l’INRS (sous la supervision de Philippe Apparicio) et membre du **laboratoire d'équité environnementale** (<http://laeq.ucs.inrs.ca>). Son sujet de thèse porte sur l’exposition des cyclistes aux pollutions atmosphériques et sonores en milieu urbain. Il utilise quotidiennement des systèmes d’information géographique (SIG) et est tombé dans la marmite de l'*open source* avec le triptyque QGIS, R et Python au début de sa maîtrise. Il a récemment développé deux packages R : **geocmeans** et **spNetwork**, permettant respectivement d’effectuer des analyses de classification floue non-supervisée pondérée spatialement et des estimations de densité par kernel sur réseau.

Philippe et Jérémy travaillent étroitement ensemble depuis déjà plusieurs années. Avec d’autres collègues, ils ont copublié plusieurs articles [@2021_1; @2021_2; @2021_3; ; @2021_4;@2020_1; @2020_2; @2020_3; @2019_1; @2019_2; @2019_3; @2020_1; @2020_2; @2018_1; @2017_1; @2016_1]. Tous deux s’intéressent à l’exposition des cyclistes à la pollution atmosphérique et sonore dans plusieurs villes à travers le monde : Philippe ayant une préférence pour les collectes dans les villes des Suds (notamment indiennes, africaines et latino-américaines) et Jérémy dans les villes du Nord (européennes et nord-américaines).


```{r JerPhil, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Philippe Apparicio et Jérémy Gelb lors d’une collecte de données à vélo à Delhi",  out.width='33%'}
knitr::include_graphics('images/01_about_figure01.jpg', dpi = NA)
```

<!--chapter:end:00-auteurs.Rmd-->

# (PART) Découverte de R {-} 

# Prise en main de R {#chap01}

Dans ce chapitre, nous reviendrons brièvement sur l’histoire de R et la philosophie qui entoure le logiciel. Nous donnerons quelques conseils pour son installation et la mise en place d’un environnement de développement. Nous présenterons les principaux objets qui sous-tendent le travail effectué avec R (*dataframe*, vecteur, matrice, etc.) et comment les manipuler avec des exemples appliqués. Si vous maîtrisez déjà R, nullement besoin de lire ce chapitre !

::: {.bloc_package data-latex=""}
Dans ce chapitre, nous utiliserons principalement les *packages* suivants : 

* Pour importer des fichiers externes :
  - **foreign** pour entre autres les fichiers *dbase* et ceux des logiciels. SPSS et Stata
  - **sas7bdat** pour les fichiers du logiciel SAS.
  - **xlsx** pour les fichiers Excel.
* Pour manipuler des chaînes de caractères et des dates : 
  - **stringr** pour les chaînes de caractères.
  - **lubridate** pour les dates.
* Pour manipuler des données :
  - **dplyr ** du **tidyverse** propose une grammaire pour manipuler et structurer des données.
:::

## Histoire et philosophie de R{#sect011}

R est à la fois un langage de programmation et un logiciel libre (sous la licence publique générale GNU) dédié à l'analyse statistique et soutenu par une fondation : _R foundation for Statistical computing_. Il est principalement écrit en C et Fortran.


R a été créé par Ross Ihaka et Robert Gentleman à l'Université d'Auckland en Nouvelle-Zélande. Si vous avez un jour l'occasion de passer dans le coin, une plaque est affichée dans le département de statistique de l'université, ça mérite le détour (figure \@ref(fig:fig01)). Une version expérimentale a été publiée en 1996, mais la première version stable ne date que de 2000, il s'agit donc d'un logiciel relativement récent si on le compare à ses concurrents SPSS (1968), SAS (1976) et Stata (1984).

```{r fig01, echo=FALSE, fig.align='center', fig.cap="Lieu de pélerinage de R", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='40%'}
library(dplyr)
knitr::include_graphics('images/introduction/plaque.jpg', dpi = NA)
```

R a cependant réussi à s'imposer tant dans la milieu de la recherche que dans le secteur privé. Pour s'en convaincre, il suffit de lire l'excellent article concernant la popularité des logiciels d'analyse de données tiré du site [r4stats.com](http://r4stats.com/articles/popularity){target="_blank"} (figure \@ref(fig:fig02)). 

```{r fig02, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Nombre d'articles trouvés sur Google Scholar (Source : Robert A. Muenchen)",  out.width='50%'}
knitr::include_graphics('images/introduction/r_citations.jpg', dpi = NA)
```

Les nombreux atouts de R justifient largement sa popularité sans cesse croissante : 

* R est un logiciel à code source ouvert (*open source*) et ainsi accessible à tous gratuitement.
* Le développement du langage R est centralisé, mais la communauté peut créer et partager facilement des *packages*. Les nouvelles méthodes sont ainsi rapidement implémentées comparativement aux logiciels propriétaires.
* R est un logiciel multi-plateforme, fonctionnant sur Linux, Unix, Windows et Mac.
* Comparativement à ses concurrents, R dispose d'excellentes solutions pour manipuler des données et réaliser des graphiques.
* R dispose de nombreuses interfaces lui permettant de communiquer, notamment avec des systèmes de bases de données SQL et non SQL (MySQL, PostgresSQL, MongoDB, etc.), avec des systèmes de *big data* (Spark, Hadoop), avec des systèmes d'information géographique (QGIS, ArcGIS) et même avec des services en ligne comme Microsoft Azure ou Amazon AWS.
* R est un langage de programmation à part entière, ce qui lui donne plus de flexibilité que ses concurrents commerciaux (SPSS, SAS, STATA). Avec R, vous pouvez accomplir des tâches aussi variées que : monter un site web, créer un robot collectant des données en ligne, combiner des fichiers PDF, composer des diapositives pour une présentation ou même éditer un livre (comme celui-ci), mais aussi et surtout réaliser des analyses statistiques.

Un des principaux attrait de R est la quantité astronomique de *packages* actuellement disponibles. **Un *package* est un ensemble de nouvelles fonctionnalités développées par un·e ou plusieurs utilisateurs·trices de R et mises à disposition de l'ensemble de la communauté**. Par exemple, le *package* **ggplot2** est dédié à la réalisation de graphiques; les *packages* **data.table** et **plyr** permettent de manipuler des tableaux de données; le *package* **car** apporte de nombreux outils pour faciliter l'analyse de modèles de régressions, etc. Ce partage des *packages* rend accessible à tous des méthodes d'analyses complexes et récentes et favorise grandement la reproductibilité de la recherche. Cependant, ce fonctionnement implique quelques désavantages : 

* il existe généralement plusieurs *packages* pour effectuer le même type d'analyse, ce qui peut devenir une source de confusion;
* certains *packages* cessent d'être mis à jour au fil des années, ce qui nécessite de leur trouver d'autres alternatives (et ainsi apprendre la syntaxe des nouveaux *packages*);
* il est impératif de s'assurer de la fiabilité des *packages* que vous souhaitez utiliser, car n'importe qui peut proposer un *package*.

Il nous semble important de relativiser d'emblée la portée du dernier point. Il est rarement nécessaire de lire et analyser le code source d'un *package* pour s'assurer de sa fiabilité. Nous ne sommes pas des spécialistes de tous les sujets et il peut être extrêmement ardu de comprendre la logique d'un code écrit par une autre personne. Nous vous recommandons donc de privilégier l'utilisation de *packages* qui :

* ont fait l'objet d'une publication dans une revue à comité de lecture ou qui ont déjà été cités dans des études ayant fait l'objet d'une publication revue par les pairs;
* font partie de projets comme [ROpensci](https://ropensci.github.io/reproducibility-guide/sections/introduction/){target="_blank"} prônant la vérification  par les pairs ou subventionnés par des organisations comme [R Consortium](https://www.r-consortium.org/){target="_blank"}.
* sont disponibles sur l'un des deux principaux répertoires de *packages* R, soit [CRAN](https://cran.r-project.org/){target="_blank"} et [Bioconductor](https://www.bioconductor.org/){target="_blank"}.

Toujours pour nuancer notre propos, il convient de distinguer *package* de *package*! Certains d'entre eux sont des ensembles très complexes de fonctions permettant de réaliser des analyses poussées alors que d'autres sont des projets plus modestes dont l'objectif principal est de simplifier le travail des utilisateurs·trices. Ces derniers ressemblent à des petites boites à outils et font généralement moins l'objet d'une vérification intensive.

Pour conclure cette section, l'illustration partagée sur Twitter par Darren L Dahly résume avec humour la force du logiciel R et de sa communauté
(figure \@ref(fig:fig03)) : R apparait clairement comme une communauté hétéroclyte, mais diversifiée et adaptable.

```{r fig03, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Métaphore sur les langages et programmes d'analyse statistique",  out.width='60%'}
knitr::include_graphics('images/introduction/softwares_and_cars.jpeg', dpi = NA)
```

Dans ce livre, nous détaillerons les **packages** utilisés dans chaque section avec un encadré spécifique, accompagné de l'icône présenté à la figure \@ref(fig:fig04).

```{r fig04, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Icône des encadrés dédiés aux packages",  out.width='20%'}
knitr::include_graphics('css/images/package.png', dpi = NA)
```

## Environnement de travail{#sect012}

Dans cette section, nous vous proposons une visite de l'environnement de travail classique  R.

### Installer R {#sect0121}

La première étape pour travailler avec R est bien sûr de l'installer. Pour ce faire, il suffit de visiter le site web de [CRAN](https://cran.r-project.org/){target="_blank"} et de télécharger la dernière version de R en fonction de votre système d'exploitation : Windows, Linux ou Mac. Une fois installé, si vous démarrez R immédiatement, vous aurez alors accès à une console, plutôt rudimentaire, attendant sagement vos instructions (figure \@ref(fig:fig05)).

```{r fig05, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="La console de base de R",  out.width='85%'}
knitr::include_graphics('images/introduction/r_console.jpeg', dpi = NA)
```

Notez que vous pouvez aussi télécharger des version plus anciennes de R en allant sur ce [lien](https://cran.r-project.org/bin/windows/base/old/){target="_blank"}. Ceci peut être intéressant lorsque vous voulez reproduire des résultats d'une autre étude ou que certains *packages* ne sont plus disponibles dans les nouvelles versions.

### L'environnement RStudio{#sect0122}

Rares sont les utilisateurs·trices de R qui préfèrent travailler directement avec la console classique. Nous vous recommandons vivement d'utiliser RStudio, soit un environnement de développement dédié à R, offrant une intégration très intéressante d'une console, d'un éditeur de texte, d'une fenêtre de visualisation des données, d'une autre pour les graphiques, d'un accès à la documentation, etc. En d'autres termes, si R est un vélo minimaliste, RStudio permet d'y rajouter des freins, des vitesses, un porte-bagage, des gardes-boues et une selle confortable. Vous pouvez [télécharger](https://rstudio.com/products/rstudio/download){target="_blank"} et installer RStudio sur Windows, Linux et Mac. La version de base est gratuite, mais l'entreprise qui développe ce logiciel propose aussi des versions commerciales du logiciel qui assurent essentiellement un support technique. Il existe d'autres environnements de développement pour travailler avec R (VisualStudio, Jupyter, Tinn-R, Radiant, RIDE, etc.), mais RStudio offre à ce jour la meilleure option en terme de facilité d'installation, de prise en main et de fonctionnalités proposées (voir l'interface de RStudio à la figure \@ref(fig:fig06)).

```{r fig06, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Environnement de base de RStudio",  out.width='85%'}
knitr::include_graphics('images/introduction/r_studio_01.jpeg', dpi = NA)
```

Avant d'aller plus loin, notez que : 

* La console actuellement ouverte dans RStudio vous informe de la version de R que vous utilisez. Vous pouvez en effet avoir plusieurs versions de R installées sur votre ordinateur et passer de l'une à l'autre avec RStudio. Pour cela, naviguez dans l'onglet *Tools / Global Options* et dans le volet  *General*, vous pouvez sélectionner la version de R que vous souhaitez utiliser.
* L'aspect de RStudio peut être modifié en navigant dans l'onglet *Tools / Global Options* et dans le volet *Appearance*. Nous avons une préférence pour le mode sombre avec le style *pastel on dark*, mais libre à chacun·e de choisir le style qui lui convient.

```{r fig07, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="RStudio avec le style pastel on dark",  out.width='85%'}
knitr::include_graphics('images/introduction/r_studio_02.jpeg', dpi = NA)
```

Une fois ces détails réglés, vous pouvez ouvrir votre première feuille de code en allant dans l'onglet *File / New File / R Script*. Votre environnement est maintenant découpé en quatre fenêtres (figure \@ref(fig:fig08)) : 

1. L'éditeur de code, vous permettant d'écrire le script que vous voulez exécuter et permettant de garder une trace de votre travail. Ce script peut être enregistré sur votre ordinateur avec l'extension **.R**, mais ce n'est qu'un simple fichier texte.
2. La console vous permettant d'exécuter votre code R et de voir les résultats s'afficher au fur et à mesure.
3. La fenêtre d'environnement vous montrant les objets, fonctions et jeux de données actuellement disponibles dans votre session (chargés dans la mémoire vive).
4. La fenêtre de l'aide, des graphiques et de l'explorateur de fichiers. Vous pouvez accéder ici à la documentation de R et des *packages* que vous utilisez, aux sorties graphiques que vous produisez et aux dossiers de votre environnement de travail.

```{r fig08, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Les quatre fenêtres de RStudio",  out.width='85%'}
knitr::include_graphics('images/introduction/r_studio_03.jpeg', dpi = NA)
```

Prenons un bref exemple, tapez la syntaxe suivante dans l'éditeur de code (fenêtre 1 à la figure \@ref(fig:fig08)) : 

```{r ma_somme}
ma_somme <- 4+4
```

Sélectionnez ensuite cette syntaxe (mettre en surbrillance avec la souris), quand vous utilisez le raccourci *Ctrl+Enter* ou cliquez sur le bouton *Run* (avec la flèche verte), cette syntaxe est envoyée à la console qui l'exécute immédiatement. Notez que rien ne se passe tant que le code n'est pas envoyé à la console. Il s'agit donc de deux étapes distinctes : écrire son code, puis l'envoyer à la console. Vous constaterez également qu'un objet *ma_somme* est apparu dans votre environnement et que sa valeur est bien 8. Votre console se "souvient" de cette valeur, elle est actuellement stockée dans votre mémoire vive sous le nom de *ma_somme* (figure \@ref(fig:fig09)).

```{r fig09, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Les quatre fenêtres de RStudio",  out.width='90%'}
knitr::include_graphics('images/introduction/r_studio_04.jpeg', dpi = NA)
```

Pour conclure cette section, nous vous invitons à enregistrer votre première syntaxe R (*File / Save As*) dans un fichier **.R** que vous pouvez appeler par exemple "mon_premier_script.R". Fermez ensuite RStudio, redémarrez le et ouvrez (*File / Open File*) votre fichier "mon_premier_script.R". Vous pouvez constater que votre code est toujours présent, mais que votre environnement est vide tant que vous n'exécutez pas votre syntaxe. En effet, lorsque vous fermez RStudio, l'environnement est vidé pour libérer de la mémoire vive. Ceci peut poser problème lorsque certains codes sont très longs à exécuter, nous verrons donc plus tard comment enregistrer l'environnement en cours pour le recharger par la suite.


### Installer et charger un *package*{#sect0123}

Dans la section sur la Philosophie de R, nous avons souligné la place centrale jouée par les *packages*. Notez que les termes **paquet** et plus rarement *librarie* sont parfois utilisés en français. Voyons ensemble comment installer un *package* intitulé **lubridate**, qui nous permettra plus tard de manipuler des données temporelles.

#### Installer un *package* depuis CRAN{#sect01231}

Pour installer un *package*, vous devez être connecté à Internet puisque R va accéder au répertoire de *packages* *CRAN* pour télécharger le *package* et l'installer sur votre machine. Cette opération est réalisée avec la fonction `install.packages`.

```{r message=FALSE, warning=FALSE, eval=FALSE}
install.packages("lubridate")
```

Notez qu'une fois que le *package* est installé, vous n'aurez plus besoin de le refaire. Le *package* est disponible localement sur votre ordinateur, à moins de le désinstaller explicitement avec la fonction `remove.packages`.

#### Installer un *package* depuis GitHub{#sect01232}

*CRAN* est le répertoire officiel des *packages* de R. Vous pouvez cependant télécharger des *packages* provenant d'autres sources. Très souvent, les *packages* sont disponibles sur le site web [GitHub](https://github.com/){target="_blank"} et l'on peut même y trouver des versions en développement avec des fonctionnalités encore non intégrées dans la version sur *CRAN*. Reprenons le cas de **lubridate**, sur GitHub, il est disponible à la page [suivante](https://github.com/tidyverse/lubridate){target="_blank"}. Pour l'installer nous devons d'abord installer un autre *package* appelé **remotes** (depuis *CRAN*).

```{r message=FALSE, warning=FALSE, eval=FALSE}
install.packages("remotes")
```

Maintenant que nous disposons de **remotes**, nous pouvons utiliser la fonction d'installation `remotes::install_github` pour directement télécharger **lubridate** depuis GitHub.

```{r message=FALSE, warning=FALSE, eval=FALSE}
remotes::install_github("tidyverse/lubridate")
```

#### Charger un *package* {#sect01233}

Maintenant que **lubridate** est installé, nous pouvons le charger dans notre session actuelle de R et accéder aux fonctions qu'il propose. Pour cela, suffit d'utiliser la fonction `library`. Conventionnellement, l'appel des *packages* se fait au tout début du script que vous rédigez. Rien ne vous empêche de le faire au fur et à mesure de votre code, mais ce dernier perd alors en lisibilité. Notez qu'à chaque nouvelle session (redémarrage de R), il faudra recharger les *packages* dont vous avez besoin.

```{r message=FALSE, warning=FALSE}
library(lubridate)
```

Si vous obtenez un message d'erreur du type : 

<span class="error_message">Error in library(mon_package) : aucun *package* nommé ‘mon_package’ n'est trouvé.</span>

Cela signifie que le *package* que vous tentez de charger n'est pas encore installé sur votre ordinateur. Dans ce cas, réessayer de l'installer avec la fonction `install.packages`. Si le problème persiste, vérifiez que vous n'avez pas fait une faute de frappe dans le nom du *package.* Vous pouvez également redémarrer RStudio et réessayer d'installer le *package*.

### Obtenir de l'aide

Lorsque vous installez des *packages* dans R, vous téléchargez aussi leur documentation. Tous les *packages* de *CRAN* disposent d'une documentation, ce n'est pas forcément vrai pour *GitHub*. Dans RStudio, vous pouvez accéder à la documentation des *packages* dans l'onglet **Packages** (figure \@ref(fig:fig010)). Vous pouvez utiliser la barre de recherche pour retrouver rapidement un *package* installé. Si vous cliquez sur le nom du *package*, vous accédez directement à sa documentation dans cette fenêtre.

```{r fig010, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Description des packages",  out.width='45%'}
knitr::include_graphics('images/introduction/rstudio_packages.jpeg', dpi = NA)
```

Vous pouvez également accéder à ces informations en utilisant la syntaxe suivante dans votre console : 

```{r eval = FALSE}
help(package = 'lubridate')
```

Souvent, vous aurez besoin d'accéder à la documentation d'une fonction spécifique d'un *package*. Affichons la documentation de la fonction `now` de **lubridate** : 

```{r eval = FALSE}
help(now, package = 'lubridate')
```

ou plus simplement :
```{r eval = FALSE}
?lubridate::now
```


Vous pouvez aussi utiliser le raccourci suivant : 

```{r eval = FALSE}
?now
```

Si vous connaissez le nom d'une fonction, mais vous ne vous souvenez plus à quel *package* elle appartient, lancez une recherche en utilisant un double point d'interrogation : 

```{r eval = FALSE}
??now
```

Vous découvrirez ainsi que la fonction `now` n'existe pas que dans **lubridate**, ce qui souligne l'importance de bien connaître les *packages* que l'on installe et que l'on charge dans notre session ! 

Maintenant que nous avons fait le tour de l'environnement de travail, nous pouvons passer aux choses sérieuses, soit les bases du langage R.

## Les bases du langage R {#sect013}

R est un langage de programmation. Il vous permet de communiquer avec votre ordinateur pour lui donner des tâches à accomplir. Dans cette section, nous aborderons les bases du langage. Ce type de section introductive à R est présente dans tous les manuels sur R ; elle est donc incontournable. À la première lecture, elle vous semblera probablement aride, et ce, d'autant plus que nous ne réalisons pas d'analyse à proprement parler. Gardez en tête que l'analyse de données requiert au préalable une phase de structuration de ces dernières, opération qui nécessite la maîtrise des notions abordées dans cette section. Nous vous recommandons une première lecture de ce chapitre pour comprendre quelles manipulations que vous pouvez effectuer avec R, la lecture des chapitres suivants dédiés aux analyses statistiques, puis de consulter à nouveau cette section au besoin. Notez aussi que la maîtrise des différents objets et opérations de base de R ne s’acquiert qu'en pratiquant. Vous gagnerez cette expertise au fil de vos prochains codes R, période durant laquelle vous pourrez consulter ce chapitre tel un guide de référence des différents objets et notions fondamentales de R.


### Hello World ! {#sect0131}

Une introduction à un langage de programmation se doit de commencer par le rite de passage **Hello World**. Il s'agit d'une forme de tradition consistant à montrer aux nouveaux utilisateurs·trices comment afficher le message "Hello World" à l'écran avec le langage en question.

```{r}
print("Hello World")
```

Bravo ! Vous venez officiellement de faire votre premier pas dans R !

### Objets et expressions {#sect0132}

Dans R, nous passons notre temps à manipuler des **objets** à l'aide d'**expressions**. Prenons un exemple concret, si vous tapez la syntaxe `4 + 3`, vous manipulez deux objets (4 et 3) au travers d'une expression indiquant que vous souhaitez obtenir la somme des deux objets.

```{r}
4 + 3
```

Cette expression est correcte, R comprend vos indications et effectue le calcul.

Il est possible d'enregistrer le résultat d'une expression et de la conserver dans un nouvel objet. On appelle cette opération déclarer une variable.

```{r}
ma_somme <- 4 + 3
```

Concrètement, nous venons de demander à R d'enregistrer le résultat de `4 + 3` dans un espace spécifique de notre mémoire vive. Si vous regardez dans votre fenêtre **Environment**, vous verrez en effet qu'un objet appelé ma_somme est actuellement en mémoire et a pour valeur 7.

Notez ici que le nom des variables ne peut être composé que de lettres, de chiffres, de points (.) et de tiret bas (_) et doit commencer par une lettre. R est sensible à la casse; en d'autre termes, les variables `Ma_somme`, `ma_sommE`, `ma_SOMME`, et `MA_SOMME` renvoient toutes à un objet différent. Attention donc aux fautes de frappe. Si vous déclarez une variable en utilisant le nom d'une variable existante, la première est écrasée par la seconde : 

```{r}
age <- 35
age

age <- 45
age
```
Portez alors attention aux noms de variables que vous utilisez et réutilisez. Réutilisons notre objet `ma_somme` dans une nouvelle expression : 

```{r}
ma_somme2 <- ma_somme + ma_somme
```

Avec cette nouvelle expression, nous indiquons à R que nous souhaitons déclarer une nouvelle variable appelée `ma_somme2`, et que cette variable aura pour valeur `ma_somme + ma_somme`, soit `7 + 7`. Sans surprise, `ma_somme2` a pour valeur 14.

Notez que la mémoire vive (l'environnement) est vidée lorsque vous fermez R. Autrement dit, R perd complètement la mémoire lorsque vous le fermez. Vous pouvez bien sûr recréer vos objets en relançant les mêmes syntaxes. C'est pourquoi vous devez conserver vos feuilles de codes et ne pas seulement travailler dans la console. La console ne garde aucune trace de votre travail. Pensez donc à bien enregistrer votre code !

Nous verrons dans un autre chapitre comment sauvegarder des objets et les recharger dans une session ultérieure de R (LIEN SECTION). Ce type d'opération est pertinent quand le temps de calcul nécessaire à la production de certains objets est très long.

### Fonctions et arguments {#sect0_133}

Dans R, nous manipulons le plus souvent nos objets avec des **fonctions**. Une fonction est elle-même un objet, mais qui a la particularité de pouvoir effectuer des opérations sur d'autres objets. Par exemple, déclarons l'objet `taille` avec une valeur de 175.897 : 

```{r}
taille <- 175.897
```

Nous allons utiliser la fonction `round` dont l'objectif est d'arrondir un nombre à virgule pour obtenir un nombre entier.

```{r}
round(taille)
```

Pour effectuer leurs opérations, les fonctions ont généralement besoin d'**arguments**. Ici, `taille` est un argument passé à la fonction `round`. Si nous regardons la documentation de `round` avec `help(round)`, nous constatons que cette fonction prend en réalité deux argments : *x* et *digits*. Le premier est le nombre que nous souhaitons arrondir et le second le nombre de décimales à conserver. On peut lire dans la documentation que la valeur par défaut de *digits* est 0, ce qui explique que `round(taille)` a produit le résultat de 176.

```{r fig011, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Arguments de la fonction round",  out.width='45%'}
knitr::include_graphics('images/introduction/help_round.jpeg', dpi = NA)
```

Réutilisons maintenant la fonction `round` mais en gardant une décimale : 

```{r}
round(taille, digits = 1)
```
Il est aussi possible que certaines fonctions ne requièrent pas d'arguments. Par exemple, la fonction `now` va indiquer la date précise (avec l'heure) et n'a besoin d'aucun argument pour le faire : 

```{r}
now()
```

Par contre, si nous essayons de lancer la fonction `round` sans argument, nous obtiendrons une erreur : 

```{r eval = FALSE}
round()
```
<span class = "error_message">Erreur : 0 arguments passed to 'round' which requires 1 or 2 arguments</span>

Le message est très clair, `round` a besoin d'au moins un argument pour fonctionner. Si au lieu d'un nombre, nous avions donné du texte à la fonction `round`, nous aurions aussi obtenu une erreur : 

```{r eval = FALSE}
round("Hello World")
```
<span class = "error_message">Error in round("Hello World") : 
  non-numeric argument to mathematical function</span>

À nouveau le message est très explicite : nous avons passé un argument non-numérique à une fonction mathématique. Lisez toujours vos messages d'erreurs qui vous permettront de repérer les coquilles et de corriger votre code !

Une fonction essentielle est la fonction `print` qui permet d'afficher la valeur d'une variable.

```{r}
print(ma_somme)
```


### Principaux types de données {#sect0134}

Depuis le début de ce chapitre, nous avons déclaré plusieurs variables et essentiellement des données numériques. Dans R, il existe trois principaux types de données de base : 

* Les données numériques, qui peuvent être des nombres entiers (appelés *integers*), ou des nombres décimaux (appelés *floats*), par exemple `15` et `15.3`.
* Les données de type texte qui sont des chaînes de caractères (appelées *strings*) et déclarées entre guillemets `"abcdefg"`.
* Les données booléennes (*booleans*) avec deux valeurs, soit vrai (`TRUE`) ou faux (`FALSE`).

Déclarons une variable pour chacun de ces types : 

```{r}
age <- 35
taille <- 175.5
adresse <- '4225 rue de la gauchetiere'
proprietaire <- TRUE
```

Si vous avez un doute sur le type de données stockées dans une variable, vous pouvez utiliser la fonction `typeof`. Par exemple, cela permet de repérer si des données qui sont supposées être numériques sont en fait stockées sous forme de texte comme dans l'exemple ci-dessous.

```{r}
typeof(age)
typeof(taille)

tailletxt <- "175.5"
typeof(tailletxt)

```


Notez également qu'il existe des types pour représenter l'absence de données : 

* pour représenter un objet vide, on utilisera l'objet `NULL`,
* pour représenter une donnée manquante, on utilisera l'objet `NA`,
* pour représenter un texte vide, on utilisera une chaîne de caractère de longueur 0, soit `""`.

```{r}
age2 <- NULL
taille2 <- NA
adresse2 <- ''
```

### Opérateurs {#sect0135}

Nous avons vu que les fonctions permettent de manipuler des objets. Nous pouvons également effectuer un grand nombre d'opérations avec des opérateurs.

#### Opérateurs mathématiques {#sect01351}

Les opérateurs mathématiques permettent d'effectuer du calcul avec des données de type numérique.

```{r tableOperateurMath, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(
        Operateur = c("`+`","`-`","`*`","`/`", "`^`", "`**`",
                      "`%%`", "`%/%`"),
        Description = c("Addition", 'Soustraction', 'Multiplication',
                        'Division', 'Exponentiel', 'Exponentiel',
                        'Reste de division', 'Division entière'),
        Syntaxe = c("`4 + 4`", "`4 - 3`", "`4 * 3`", "`12 / 4`",
                    "`4 ^ 3`", '`4 ** 3`', "`15.5 %% 2`",
                    "`15.5 %/% 2`"), 
        Resultat = c(8,1,12,3,64,64,1.5,7))

show_table(df, 
           col.names = c("Opérateur","Description","Syntaxe","Résultat"),
           caption = 'Opérateurs mathématiques'
           )

```

#### Opérateurs relationnels  {#sect01352}

Les opérateurs relationnels permettent de vérifier des conditions dans R. Ils renvoient un booléen, `TRUE` si la condition est vérifiée et `FALSE` si ce n'est pas le cas.

```{r tableOperateurRela, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(
        Operateur = c("`==`","`!=`","`>`","`<`", "`>=`", "`<=`"),
        Description = c("Égalité", 'Différence', 'Est supérieur ', 'Est inférieur', 'Est supérieur ou égal', 'Est inférieur ou égal'),
        Syntaxe = c("`4 == 4`", "`4 != 4`", "`5 > 4`", "`5 < 4`", "`5 >= 4`", '`5 <= 4`'), 
        Resultat = c(TRUE, FALSE, TRUE, FALSE, TRUE, FALSE))

show_table(df, 
           col.names = c("Opérateur","Description","Syntaxe","Résultat"),
           caption = 'Opérateurs relationnels'
           )
```

#### Opérateurs logiques {#sect01353}

Les opérateurs logiques permettent de combiner plusieurs conditions :

* L'opérateur **ET** permet de vérifier que deux conditions (l'une ET l'autre) sont TRUE. Si l'une des deux est FALSE, il renvoie FALSE.

* L'opérateur **OU** permet de vérifier que l'une des deux conditions est TRUE (l'une OU l'autre). Si les deux sont FALSE, alors il renvoit FALSE.

* L'opérateur **NOT** permet d'inverser une condition. Ainsi NOT TRUE est FALSE et NOT FALSE est TRUE.


```{r tableOperateurLogi, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(
        Operateur = c("`&`","`|`","`!`"),
        Description = c("ET", "OU", "NOT"),
        Syntaxe = c("`TRUE & FALSE`", "`TRUE | FALSE`", "`! TRUE`"), 
        Resultat = c(FALSE, TRUE, FALSE))

show_table(df, 
           col.names = c("Opérateur","Description","Syntaxe","Résultat"),
           caption = 'Opérateurs logiques'
           )
```

Prenons le temps pour un rapide exemple : 

```{r}

A <- 4 
B <- 10
C <- -5

# produit TRUE car A est bien plus petit que B et C est bien plus petit que A
A < B & C < A

# produit FALSE car si A est bien plus petit que B, 
# B est en revanche plus grand que c
A < B & B < C

# produit TRUE car la seconde condition est inversée
A < B &  ! B < C

# produit TRUE car au moins une des deux conditions est juste
A < B |  B < C

```
Notez que l'opérateur **ET** est prioritaire sur l'opérateur **OU** et que les parenthèses sont prioritaires sur tous les opérateurs : 

```{r}
# produit TRUE car on commence par tester A < B puis B < C ce qui donne FALSE
# on obtient ensuite
# FALSE |  A > C
# enfin, A est bien supérieur à C, donc l'une des deux conditions est vraie
A < B & B < C |  A > C

```
Notez qu'en arrière-plan, les opérateurs sont en réalité des fonctions déguisées. Il est donc possible de définir de nouveau comportements pour les opérateurs. Il est par exemple possible d'additionner ou comparer des objets spéciaux comme des dates, des géométries, des graphes, etc.

### Structures de données {#sect0136}

Jusqu'à présent, nous avons utilisé des objets ne comprenant qu'une seule valeur. Or, des analyses statistiques nécessitent de travailler à des volumes de données bien plus grands. Pour stocker plusieurs valeurs, nous allons travailler avec plusieurs structures de données : les vecteurs, les matrices, les tableaux de données et les listes.

#### Vecteurs {#sect01361}

Les vecteurs sont la brique élémentaire de R. Ils permettent de stocker une série de valeurs du même type dans une seule variable. Pour déclarer un vecteur, on utilise la fonction *c()* : 

```{r}
ages <- c(35,45,72,56,62)
tailles <- c(175.5,180.3,168.2,172.8,167.6)
adresses <- c('4225 rue de la gauchetiere',
              '4223 rue de la gauchetiere',
              '4221 rue de la gauchetiere',
              '4219 rue de la gauchetiere',
              '4217 rue de la gauchetiere')
proprietaires <- c(TRUE,TRUE,FALSE,TRUE,TRUE)
```

Nous venons ainsi de déclarer quatre nouvelles variables étant chacune un vecteur de longueur cinq (comprenant chacun cinq valeurs). Ces vecteurs représentent, par exemple, les réponses de plusieurs répondants à un questionnaire.

::: {.bloc_attention data-latex=""}
Il existe dans R une subtilité à l'origine de nombreux malentendus : la distinction entre un vecteur de type texte et un vecteur de type facteur. Dans l'exemple précédent, le vecteur *adresses* est un vecteur de type texte. Chaque nouvelle valeur ajoutée dans le vecteur peut être n'importe quelle nouvelle adresse. Déclarons un nouveau vecteur qui contiendrait cette fois-ci la couleur des yeux de personnes ayant répondu au questionnaire.

```{r}
couleurs_yeux <- c('marron','marron','bleu','bleu','marron','vert')
```

Contrairement aux adresses, il y a un nombre limité de couleurs que nous pouvons mettre dans ce vecteur. Il serait intéressant de fixer les valeurs possibles du vecteur pour s'assurer que de nouvelles ne soient pas ajoutées par erreur. Pour cela, nous pouvons convertir ce vecteur texte en vecteur de type facteur avec la fonction `as.factor`.

```{r}
couleurs_yeux_facteur <- as.factor(couleurs_yeux)
```

Notez qu'à présent, nous pouvons ajouter une nouvelle couleur dans le premier vecteur, mais pas dans le second.

```{r}
couleurs_yeux[7] <- "rouge"
couleurs_yeux_facteur[7] <- "rouge"
```
Le message d'erreur nous informe que nous avons tenté d'introduire une valeur invalide dans le facteur.

Les facteurs peuvent sembler restrictifs et très régulièrement, on préfère travailler avec de simples vecteurs de type texte plutôt que des facteurs. Cependant, de nombreuses fonctions d'analyse nécessitent d'utiliser des facteurs car ils assurent une certaine cohérence dans les données. Il est donc essentiel de savoir passer du texte au facteur avec la fonction `as.factor`. À l'inverse, il est parfois nécessaire de revenir à une variable de type texte avec la fonction `as.character`.

Notez que des vecteurs numériques peuvent aussi être convertis en facteurs : 

```{r}
tailles_facteur <- as.factor(tailles)
```

Cependant, si vous souhaitez reconvertir ce facteur en format numérique, il faudra passer dans un premier temps par le format texte : 

```{r}
as.numeric(tailles_facteur)
```

Comme vous pouvez le voir, convertir un facteur en valeur numérique renvoie des nombres entiers. Ceci est dû au fait que les valeurs dans un facteur sont recodées sous forme de nombres entiers, chaque nombre correspondant à une des valeurs originales (appelées niveaux). Si on convertit un facteur en valeurs numériques, on obtient donc ces nombres entiers.

```{r}
as.numeric(as.character(tailles_facteur))
```
Morale de l'histoire, ne confondez pas les données de type texte et de type facteur. Dans le doute, vous pouvez demander à R quel est le type d'un vecteur avec la fonction `class`.

```{r}
class(tailles)
class(tailles_facteur)
class(couleurs_yeux)
class(couleurs_yeux_facteur)
```
:::

Quasiment toutes les fonctions utilisent des vecteurs. Par exemple, on pourrait calculer la moyenne du vecteur *ages* en utilisant la fonction *mean* présente de base dans R.

```{r}
mean(ages)
```

Cela démontre bien que le vecteur est la brique élémentaire de R ! Toutes les variables que nous avons déclarées dans les sections précédentes sont aussi des vecteurs, mais de longueur 1 !

#### Matrices {#sect01362}

Il est possible de combiner des vecteurs pour former des matrices. Une matrice est un tableau en deux dimensions (colonnes et lignes) généralement utilisé pour représenter certaines structures de données comme des images (pixels), effectuer du calcul matriciel ou plus simplement présenter des matrices de corrélations. Vous aurez rarement à travailler directement avec des matrices, mais il est bon de savoir ce qu'elles sont. Créons deux matrices à partir de nos précédents vecteurs.

```{r}
matrice1 <- cbind(ages,tailles)
# afficher la matrice 1
print(matrice1)
# afficher les dimensions de la matrice 1 (1er chiffre : lignes; 2e chiffre : colonnes)
print(dim(matrice1))

matrice2 <- rbind(ages, tailles)
# afficher la matrice 2
print(matrice2)
# afficher les dimensions de la matrice 2
print(dim(matrice2))
```
Comme vous pouvez le constater, la fonction `cbind` permet de concaténer des vecteurs comme s'ils étaient les colonnes d'une matrice, alors que `rbind` les combine comme s'ils étaient des lignes d'une matrice. La figure \@ref(fig:fig012) présente graphiquement le passage du vecteur à la matrice.

```{r fig012, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Du vecteur à la matrice",  out.width='30%'}
knitr::include_graphics('images/introduction/vecteur_to_matrix.png', dpi = NA)
```

Notez que vous pouvez transposer une matrice avec la fonction `t`. Si nous essayons maintenant de comparer la matrice 1 et la matrice 2 nous allons avoir une erreur car elles n'ont pas les mêmes dimensions.

```{r eval = FALSE}
matrice1 == matrice2
```
<span class="error_message">Error in matrice1 == matrice2 : non-conformable arrays</span>

En revanche, on pourrait transposer la matrice 1 et refaire cette comparaison : 

```{r}
t(matrice1) == matrice2
```

Le résultat souligne bien que l'on a les mêmes valeurs dans les deux matrices. Il est aussi possible de construire des matrices directement avec la fonction `matrix`, ce que nous montrons dans la prochaine section.

#### *Arrays*  {#sect01363}

S'il est rare de travailler avec des matrices, il est encore plus rare de manipuler des *arrays*. Un *array* est une matrice spéciale qui peut avoir plus que deux dimensions. Un cas simple serait un *array* en trois dimensions : lignes, colonnes, profondeur, que l'on pourrait se représenter comme un cube divisé en sous cubes. Au delà de trois dimensions, il devient difficile de se les représenter. Cette structure de données peut être utilisée pour représenter les différentes bandes spectrales d'une image satellitaire. Les lignes et les colonnes délimiteraient les pixels de l'image, la profondeur quant à elle délimiterait les différents bandes composant l'image (figure \@ref(fig:fig012)).

```{r fig013, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Un array avec trois dimension",  out.width='15%'}
knitr::include_graphics('images/introduction/array.png', dpi = NA)
```

Créons un array en combinant trois matrices avec la fonction `array`. Chacune de ces matrices sera composée respectivement de 1, de 2 et de 3 et aura une dimension de 5 x 5. L'array final aura donc des dimensions de 5 x 5 x 3.

```{r}
mat1 <- matrix(1, nrow = 5, ncol = 5)
mat2 <- matrix(2, nrow = 5, ncol = 5)
mat3 <- matrix(3, nrow = 5, ncol = 5)

mon_array <- array(c(mat1, mat2, mat3), dim = c(5,5,3))

print(mon_array)
```


#### *DataFrames* {#sect01364}

S'il est rare de manipuler des matrices et des *arrays*, le *DataFrame* (tableau de données en français) est la structure de données la plus souvent utilisée. Dans cette structure, chaque ligne du tableau représente un individu et chaque colonne représente une caractéristique de ces individus. Ces colonnes ont des noms, ce qui permet facilement d'accéder à leurs valeurs. Créons un *DataFrame* à partir de nos quatres vecteurs et de la fonction `data.frame`.

```{r}
df <- data.frame(
  "age" = ages,
  "taille" = tailles,
  "adresse" = adresses,
  "proprietaire" = proprietaires
)
```

```{r tabfirsttable, echo=FALSE, message=FALSE, warning=FALSE}
show_table(df, 
           caption = 'Un premier DataFrame')
```

Dans Rstudio, vous pouvez visualiser votre tableau de données avec la fonction `View(df)`. Comme vous pouvez le constater, chaque vecteur est devenu une colonne de votre tableau de données *df*. La figure \@ref(fig:fig014) résume ce passage d'une simple donnée à un DataFrame en passant par un vecteur.

```{r fig014, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="De la donnée au DataFrame",  out.width='25%'}
knitr::include_graphics('images/introduction/vecteur_to_dataframe.png', dpi = NA)
```

Plusieurs fonctions de base de R fournissent des informations importantes sur un *DataFrame* : 

* `names` renvoie les noms des colonnes du DataFrame;
* `nrow` renvoie le nombre de lignes;
* `ncol` renvoie le nombre de colonnes.

```{r}
names(df)
nrow(df)
ncol(df)
```

Vous pouvez accéder à chaque colonne de *df* en utilisant le symbole `$` ou `[["nom_de_la_colonne"]]`. Recalculons ainsi la moyenne des âges :

```{r}
mean(df$age)
mean(df[["age"]])
```

#### Listes {#sect01365}

La dernière structure de données à connaître est la liste. Elle ressemble à un vecteur, au sens où elle permet de stocker un ensemble d'objets les uns à la suite des autres. Cependant, une liste peut contenir n'importe quel type d'objets. Vous pouvez ainsi construire des listes de matrices, des listes d'*arrays*, des listes mixant des vecteurs, des graphiques, des *DataFrames*, des listes de listes...

Créons ensemble une liste qui va contenir des vecteurs et des matrices à l'aide de la fonction `list.`

```{r}
ma_liste <- list(c(1,2,3,4),
                 matrix(1, ncol = 3, nrow = 5),
                 matrix(5, ncol = 3, nrow = 7),
                 'A'
                 )
```

Il est possible d'accéder aux éléments de la liste par leur position dans cette dernière en utilisant les doubles crochets `[[ ]]`: 

```{r}
print(ma_liste[[1]])
print(ma_liste[[4]])
```

Il est aussi possible de donner des noms aux éléments de la liste et d'utiliser le symbole `$` pour y accéder. Créons une nouvelle liste de vecteurs et donnons leurs des noms avec la fonction `names`.

```{r}
liste2 <- list(c(35,45,72,56,62), 
               c(175.5,180.3,168.2,172.8,167.6),
               c(TRUE,TRUE,FALSE,TRUE,TRUE)
)
names(liste2) <- c("age",'taille','proprietaire')

print(liste2$age)
```

Si vous avez bien suivi, vous devez avoir compris qu'un *DataFrame* n'est en fait rien d'autre qu'une liste de vecteurs avec des noms !

Bravo ! Vous venez de faire le tour des bases du langage R. Vous allez apprendre désormais à manipuler des données dans des *DataFrames* !

## Manipuler des données {#sect014}

Dans cette section, vous apprendrez à charger et manipuler des *DataFrames* en vue d'effectuer des opérations classiques de gestion de données.

### Charger un *DataFrame* depuis un fichier {#sect0141}

Il sera rarement nécessaire de créer vos *DataFrames* manuellement comme signalé dans la section précédente. Le plus souvent, vous disposerez de fichiers contenant vos données et utiliserez des fonctions pour les importer dans R sous forme d'un *DataFrame*. Les formats à importer les plus répandus sont : 

* *.csv*, soit un fichier texte dont chaque ligne représente une ligne du tableau de données et dont les colonnes sont séparées par un délimiteur (généralement une virgule ou un point-virgule).
* *.dbf*, ou fichier *dBase*, souvent associés à des fichiers d'information géographique au format *ShapeFile*.
* *.xls* et *.xlsx*, soit des fichiers générés par Excel.
* *.json*, soit un fichier texte utilisant la norme d'écriture propre au langage JavaScript.

Plus rarement, il se peut que vous ayez à charger des fichiers provenant de logiciels propriétaires :

* *.sas7bdat* (SAS),
* *.sav* (SPSS) et
* *.dta* (STATA).

Pour lire la plupart de ces fichiers, nous allons utiliser le *package* **foreign** dédié à l'importation d'une multitude de formats. Commencez donc par l'installer (`install.packages("foreign")`). Nous allons charger cinq fois le même jeu de données enregistré dans des formats différents (*csv*, *dbf*, *dta*, *sas7bdat* et *xlsx*). Aussi, nous mesurerons le temps nécessaire pour importer chacun de ces fichiers avec la fonction `Sys.time`.

#### Lire un fichier *csv* {#sect01411}

Pour le format *csv*, il n'y a pas besoin d'utiliser un *package* puisque R dispose d'une fonction de base pour lire ce format.

```{r message=FALSE, warning=FALSE}
t1 <- Sys.time()
df1 <- read.csv("data/priseenmain/SR_MTL_2016.csv", 
         header = TRUE, sep = ",", dec = ".",
         stringsAsFactors = FALSE)
t2 <- Sys.time()
d1 <- as.numeric(difftime(t2,t1,units="secs"))

cat("le dataframe df1 a ",nrow(df1),' observations',
    'et ',ncol(df1),"colonnes\n")
```

Rien de bien compliqué ! Notez tout de même que : 

* Lorsque vous chargez un fichier *csv*, vous devez connaître le **séparateur**, soit le caractère utilisé pour délimiter les colonnes. Dans le cas présent, il s'agit d'une virgule (spécifiez avec l'argument `sep = ","`), mais il pourrait tout aussi bien être un point virgule (`sep = ";"`) une tabulation (`sep = "    "`), etc.
* Vous devez également spécifier le caractère utilisé comme séparateur de décimales. Le plus souvent, ce sera le point (`dec = "."`), mais certains logiciels avec des paramètres régionaux de langue française (notamment Excel) exportent des fichiers *csv* avec des virgules comme séparateur de décimales (utilisez alors `dec = ","`).
* L'argument `header` indique si la première ligne (l'entête) du fichier comprend ou non les noms des colonnes du jeu de données (avec les valeurs `TRUE` ou `FALSE`). Il arrive que certains fichiers *csv* soient fournis sans entête et que les noms et descriptions des colonnes soient fournis dans un autre fichier.
* L'argument *stringsAsFactors* permet d'indiquer à R que les colonnes comportant du texte doivent être chargées comme des vecteurs de type texte et nom de type facteur.

#### Lire un fichier *dbase* {#sect01412}

Pour lire un fichier *dbase* (.dbf), nous utilisons la fonction `read.dbf` du *package* **foreign** installé précédemment : 

```{r message=FALSE, warning=FALSE}
library(foreign)

t1 <- Sys.time()
df2 <- read.dbf("data/priseenmain/SR_MTL_2016.dbf")
t2 <- Sys.time()
d2 <- as.numeric(difftime(t2,t1,units="secs"))

cat("le dataframe df2 a ",nrow(df2)," observations",
    "et ",ncol(df2),"colonnes\n")
```
Comme vous pouvez le constater, nous obtenons les mêmes résultats qu'avec le fichier *csv*.

#### Lire un fichier *dta* (Stata) {#sect01413}

Si vous travaillez avec des collègues utilisant le logiciel Stata, il se peut que ces derniers vous partagent des fichiers *dta*. Toujours en utilisant le *package* **foreign**, vous serez en mesure de les charger directement dans R.

```{r message=FALSE, warning=FALSE}
t1 <- Sys.time()
df3 <- read.dta("data/priseenmain/SR_MTL_2016.dta")
t2 <- Sys.time()
d3 <- as.numeric(difftime(t2,t1,units="secs"))

cat("le dataframe df3 a ",nrow(df3)," observations",
    "et ",ncol(df3),"colonnes\n", sep = "")
```


#### Lire un fichier *sav* (SPSS) {#sect01414}

Pour importer un fichier *sav* provenant du logiciel statistique SPSS, utilisez la fonction `read.spss` du *package* **foreign**.

```{r message=FALSE, warning=FALSE}
t1 <- Sys.time()
df4 <- as.data.frame(read.spss("data/priseenmain/SR_MTL_2016.sav"))
t2 <- Sys.time()
d4 <- as.numeric(difftime(t2,t1,units="secs"))

cat("le dataframe df4 a ",nrow(df4)," observations",
    "et ",ncol(df4),"colonnes\n", sep = "")
```

#### Lire un fichier *sas7bdat* (SAS) {#sect01415}

Pour importer un fichier *sas7bdat* provenant du logiciel statistique SAS, utilisez la fonction `read.sas7bdat` du *package* **sas7bdat**. Installez préalablement le *package* (`install.packages("sas7bdat")`) et chargez le (`library(sas7bdat)`).

```{r message=FALSE, warning=FALSE}
library(sas7bdat)

t1 <- Sys.time()
df5 <- read.sas7bdat("data/priseenmain/SR_MTL_2016.sas7bdat")
t2 <- Sys.time()
d5 <- as.numeric(difftime(t2,t1,units="secs"))

cat("le dataframe df5 a ",nrow(df5)," observations",
    "et ",ncol(df5),"colonnes\n", sep ="")
```


#### Lire un fichier *xlsx* (Excel) {#sect01416}

Lire un fichier Excel dans R n'est pas toujours une tâche facile. Généralement, nous recommandons d'exporter les fichiers en question au format *csv* dans un premier temps, puis de le lire avec la fonction `read.csv` dans un second temps (section \@ref(sect01411)). 
Il est néanmoins possible de lire directement un fichier *xlsx* avec le *package* **xlsx**. Ce dernier requiert que le logiciel JAVA soit installé sur votre ordinateur (Windows, Mac ou Linux). Si vous utilisez la version 64 bit de R, vous devrez télécharger et installer la version 64 bit de JAVA. Une fois que ce logiciel tiers est installé, il ne vous restera plus qu'à installer (`install.packages("xlsx")`) et charger (`library(xlsx)`) le *package* **xlsx**.

```{r message=FALSE, warning=FALSE}
library(xlsx)

t1 <- Sys.time()
df6 <- read.xlsx(file="data/priseenmain/SR_MTL_2016.xlsx",
                 sheetIndex = 1,
                 as.data.frame = TRUE)
t2 <- Sys.time()
d6 <- as.numeric(difftime(t2,t1,units="secs"))

cat("le dataframe df6 a ",nrow(df6)," observations",
    "et ",ncol(df6),"colonnes\n", sep = "")
```

Il est possible d'accélérer significativement la vitesse de lecture d'un fichier *xlsx* en utilisant la fonction `read.xlsx2`. Il faut cependant indiquer à cette dernière le type de données de chaque colonne. Dans le cas présent, les cinq premières colonnes contiennent des données de type texte (`character`), alors que les 43 autres sont des données numériques (`numeric`). Nous utilisons la fonction `rep` afin de ne pas avoir à écrire plusieurs fois *character* et *numeric*.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(xlsx)

t1 <- Sys.time()
df7 <- read.xlsx2(file="data/priseenmain/SR_MTL_2016.xlsx",
                  sheetIndex = 1, 
                  as.data.frame = TRUE,
                  colClasses = c(rep("character",5),rep("numeric",43))
                  )
t2 <- Sys.time()
d7 <- as.numeric(difftime(t2,t1,units="secs"))

cat("le dataframe df6 a ",nrow(df7)," observations",
    "et ",ncol(df7),"colonnes\n", sep = "")
```

Si l'on compare les temps d'exécution (tableau \@ref(tab:tableduration)), on constate que la lecture des fichiers *xlsx* peut être extrêmement longue si l'on ne spécifie pas le type des colonnes, ce qui peut devenir problématique pour des fichiers volumineux. Notez également que la lecture des fichiers *csv* devient de plus en plus laborieuse à mesure que la taille du fichier *csv* augmente. Si vous devez un jour charger des fichiers *csv* de plusieurs gigaoctets, nous vous recommandons vivement d'utiliser la fonction `fread` du *package* **data.table** qui est beaucoup plus rapide.

```{r tableduration, echo=FALSE, message=FALSE, warning=FALSE}

DureeImportation <- data.frame(
  "duree" = c(d1,d2,d3,d4,d5,d6,d7),
  "fonction" = c("read.csv","read.dbf","read.spss","read.dta",
                 'read.sas7bdat',"read.xlsx","read.xlsx2")
)

show_table(DureeImportation, 
           digits = 2, 
           col.names = c("Durée (s)","fonction"),
           caption = 'Temps nécessaire pour lire les données en fonction du type de fichiers')
```

### Manipuler un *DataFrame* {#sect0142}

Une fois le *DataFrame* chargé, voyons comment il est possible de le manipuler.

#### Un petit mot sur le **tidyverse** {#sect01421}

**Tidyverse** est un ensemble de *packages* conçus pour faciliter la structuration et la manipulation des données dans R. Avant d'aller plus loin, il est important d'aborder brièvement un débat actuel dans la Communauté R. Entre 2010 et 2020, l'utilisation du **tidyverse** s'est peu à peu répandue. Développé et maintenu par Hadley Wickham, **tidyverse** introduit une philosophie et une grammaire spécifiques qui diffèrent du langage R traditionnel. Une partie de la communauté a pour ainsi dire complètement embrassé le **tidyverse** et de nombreux *packages* en dehors du **tidyverse** ont adopté sa grammaire et sa philosophie. À l'inverse, une autre partie de la communauté est contre cette évolution ([voir l'article du blogue suivant](https://blog.ephorie.de/why-i-dont-use-the-tidyverse){target="_blank"}). Les arguments pour et contre **tidyverse** sont résumés dans le tableau suivant. 



```{r tableTidyverse, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(
          Pour = c("Simplicité d'écriture et d'apprentissage",
  "Ajout de l'opérateur `%>%` permettant d'enchaîner les traitements",
  "La meilleure librairie pour réaliser des graphiques : **ggplot2**",
  "Crée un écosystème cohérent",
  "Package en développement et de plus en plus utilisé"),
          Contre = c("Nouvelle syntaxe à apprendre",
  "Perte de lisibilité avec l'opérateur `->`",
  "Certaines fonctions de base sont remplacées par **tidyverse** lors de son chargement, pouvant créer des erreurs.",
  "Ajoute une dépendance dans le code",
  "Philosophie d'évolution agressive, aucune assurance de rétro-compatibilité")
)

show_table(df, 
      col.names = c("Avantage du tidyverse",
                  "Problème posé par le tidyverse"),
      caption = 'Avantages et inconvénients du tidyverse', 
      col.to.resize = c(1,2), 
      col.width = "7cm")
```


Le dernier point est probablement le plus problématique. Dans sa volonté d'évoluer au mieux et sans restriction, le *package* **tidyverse** n'offre aucune garantie de rétro-comptatibilité. En d'autre termes, des changements importants peuvent être introduits d'une version à l'autre rendant potentiellement obsolète votre propre code. Nous n'avons pas d'opinion tranchée sur le sujet : **tidyverse** est un outil très intéressant dans de nombreux cas; nous évitons simplement de l'utiliser systématiquement et préférons charger directement des sous-packages (comme **dplyr** ou **ggplot2**) du **tidyverse**. Notez que le *package* **data.table** offre une alternative au **tidyverse** dans la manipulation de données. Au prix d'une syntaxe généralement un peu plus complexe, le package **data.table** offre une vitesse de calcul bien supérieure au **tidyverse** et assure une bonne rétro-compatibilité.


#### Gérer les colonnes d'un *DataFrame* {#sect01422}

Repartons du *DataFrame* que nous avions chargé précédemment en important un fichier *csv*.

```{r message=FALSE, warning=FALSE}
df <- read.csv(file="data/priseenmain/SR_MTL_2016.csv", 
               header = TRUE, sep = ",", dec = ".",
               stringsAsFactors = FALSE)
```

##### Sélectionner une colonne {#sect014221}

Pour rappel, il est possible d'accéder aux colonnes dans ce *DataFrame* en utilisant le symbole dollar `$ma_colonne` ou les doubles crochets `[["ma_colonne"]]`.

```{r message=FALSE, warning=FALSE}
# Calcul de la superficie totale de l'Île de Montréal
sum(df$KM2)
sum(df[["KM2"]])
```

##### Sélectionner plusieurs colonnes {#sect014222}

Il est possible de sélectionner plusieurs colonnes d'un *DataFrame* et filtrer ainsi les colonnes inutiles. Pour cela, on peut utiliser un vecteur contenant soit les positions des colonnes (1 pour la première colonne, 2 pour la seconde et ainsi de suite), soit les noms des colonnes.

```{r message=FALSE, warning=FALSE}
# Conserver les 5 premières colonnes
df2 <- df[1:5]

# Conserver les colonnes 1, 5, 10 et 15
df3 <- df[c(1,5,10,15)]

# Cela peut aussi être utilisé pour changer l'ordre des champs
df3 <- df[c(10,15,1,5)]

# Conserver les colonnes 1 à 5, 7 à 12, 17 et 22
df4 <- df[c(1:5,7:12,17,22)]

# Conserver les colonnes avec leurs noms
df5 <- df[c("SRIDU","KM2","Pop2016","MaisonIndi","LoyerMed")]
```

##### Supprimer des colonnes {#sect014223}

Il est parfois plus intéressant et rapide de  supprimer directement des colonnes plutôt que de recréer un nouveau *DataFrame*. Pour ce faire, on attribue la valeur `NULL` à ces colonnes.

```{r message=FALSE, warning=FALSE}
# Supprimer le colonnes 2, 3 et 5
df3[c(2,3,5)] <- list(NULL)

# Supprimer une colonne avec son nom
df4$OID <- NULL

# Supprimer plusieurs colonnes par leur nom
df5[c("SRIDU","LoyerMed")] <- list(NULL)
```
Notez que si vous supprimez une colonne, vous ne pouvez pas revenir en arrière. Il faudra recharger votre jeu de données ou éventuellement relancer les calculs qui avaient produit cette colonne.

##### Renommer des colonnes {#sect014224}

Il est possible de changer le nom d'une colonne. Cette opération est importante pour faciliter la lecture du *DataFrame* ou encore s'assurer que l'exportation du *DataFrame* dans un format ne posera pas de problème.

```{r message=FALSE, warning=FALSE}
# Voici les noms des colonnes
names(df5)

# Renommer toutes les colonnes
names(df5) <- c('superficie_km2','population_2016', 'maison_individuelle_prt')
names(df5)

# Renommer avec dplyr
library(dplyr)
df4 <- rename(df4, "population_2016" = "Pop2016",
              "prs_moins_14ans_prt" = "A014",
              "prs_15_64_ans_prt" = "A1564",
              "prs_65plus_ans_prt" = "A65plus"
              )
```


#### Calculer de nouvelles variables {#sect01423}

Il est possible d'utiliser les colonnes de type numérique pour calculer de nouvelles colonnes en utilisant les opérateurs mathématiques vus dans la section \@ref(sect0135). Prenons un exemple concret : calculons la densité de population par secteur de recensement dans notre *DataFrame*, puis affichons un résumé de cette nouvelle variable.

```{r message=FALSE, warning=FALSE}
# Calcul de la densité
df$pop_density_2016 <- df$Pop2016 / df$KM2

# Statistiques descriptives
summary(df$pop_density_2016)
```

Nous pouvons aussi calculer le ratio entre le nombre de maisons et le nombre d'appartements.

```{r message=FALSE, warning=FALSE}

# Calcul du ratio
df$total_maison <- (df$MaisonIndi + df$MaisJumule + df$MaisRangee + df$AutreMais)
df$total_apt <- (df$AppDuplex + df$App5Moins + df$App5Plus)
df$ratio_maison_apt <- df$total_maison / df$total_apt
```

Retenez ici que R va appliquer le calcul à chaque ligne de votre jeu de données et stocker le résultat dans une nouvelle colonne. Cette opération est du calcul vectoriel : toute la colonne est calculée en une seule fois. R est d'ailleurs optimisé pour le calcul vectoriel.

#### Fonctions mathématiques {#sect01424}

R propose un ensemble de fonctions de base pour effectuer du calcul. Voici une liste non-exhaustive des principales fonctions : 

* `abs` calcule les valeurs absolues des valeurs d'un vecteur
* `sqrt` calcule les racines carrées des valeurs d'un vecteur
* `log` calcule les logarithmes des valeurs d'un vecteur
* `exp` calcule les exponentielles des valeurs d'un vecteur
* `factorial` calcule la factorielle des valeurs d'un vecteur
* `round` arrondit les valeurs d'un vecteur
* `ceiling`, `floor` arrondit à l'unité supérieure ou inférieure les valeurs d'un vecteur
* `sin`, `asin`, `cos`, `acos`, `tan`, `atan` sont des fonctions de trigonométrie
* `cumsum` calcule la somme cumulative des valeurs d'un vecteur.

Ces fonctions sont des fonctions vectorielles puisqu'elles s'appliquent à tous les éléments d'un vecteur. Si votre vecteur en entrée comprend cinq valeurs, le vecteur en sortie comprendra aussi cinq valeurs.

À l'inverse, les fonctions suivantes s'appliquent directement à l'ensemble d'un vecteur et ne vont renvoyer qu'une seule valeur :

* `sum` calcule la somme des valeurs d'un vecteur
* `prod` calcule le produit des valeurs d'un vecteur
* `min`, `max` renvoient les valeurs maximale et minimale d'un vecteur
* `mean`, `median` renvoient la moyenne et la médiane d'un vecteur
* `quantile` renvoit les percentiles d'un vecteur.

#### Fonctions pour manipuler des chaînes de caractères {#sect01425}

Outre les données numériques, vous aurez à travailler avec des données de type texte (`string`). Le **tidyverse** avec le *package* **stringr** offre des fonctions très intéressantes pour manipuler ce type de données. Pour un aperçu de toutes les fonctions offertes par **stringr**, référer-vous à sa [*Cheat Sheet*](https://github.com/rstudio/cheatsheets/blob/master/strings.pdf){target="_blank"}. Commençons avec un *DataFrame* assez simple comprenant des adresses et des noms de personnes.

```{r message=FALSE, warning=FALSE}
library(stringr)

df <- data.frame(
  noms = c("Jérémy Toutanplace","constant Tinople","dino Resto","Luce tancil"),
  adresses = c('15 rue Levy', '413 Blvd Saint-Laurent', '3606 rue Duké', '2457 route St Marys')
)
```

##### Majuscules et minuscules {#sect014251}

Pour harmoniser ce *dataframe*, nous allons dans un premier temps mettre des majuscules au premier caractère des prénoms et noms des individus avec la fonction `str_to_title`.

```{r message=FALSE, warning=FALSE}
df$noms_corr <- str_to_title(df$noms)
print(df$noms_corr)
```

On pourrait également tout mettre en minuscules ou tout en majuscules.
```{r message=FALSE, warning=FALSE}
df$noms_min <- tolower(df$noms)
df$noms_maj <- toupper(df$noms)
print(df$noms_min)
print(df$noms_maj)
```

##### Remplacer du texte {#sect014252}

Les adresses comprennent des caractères accentués. Ce type de caractères pose régulièrement des problèmes d'encodage. Nous pourrions alors décider de les remplacer par des caractères simples avec la fonction `str_replace_all`.

```{r message=FALSE, warning=FALSE}
df$adresses_1 <- str_replace_all(df$adresses,'é','e')
print(df$adresses_1)
```

La même fonction peut être utilisée pour remplacer les *St* par Saint et les *Blvd* par Boulevard.

```{r message=FALSE, warning=FALSE}
df$adresses_2 <- str_replace_all(df$adresses_1,' St ',' Saint ')
df$adresses_3 <- str_replace_all(df$adresses_2,' Blvd ',' Boulevard ')
print(df$adresses_3)
```

##### Découper du texte {#sect014253}

Il est parfois nécessaire de découper du texte pour en extraire des éléments. On doit alors choisir un caractère de découpage. Dans notre exemple, on pourrait vouloir extraire les numéros civiques des adresses, en utilisant le premier espace comme caractère de découpage, en utilisant la fonction `str_split_fixed`.

```{r message=FALSE, warning=FALSE}
df$num_civique <- str_split_fixed(df$adresses_3, ' ',n=2)[,1]
print(df$num_civique)
```

Pour être exact, sachez que pour notre exemple, la fonction `str_split_fixed` renvoie deux colonnes de texte : une avec le texte avant le premier espace, soit le numéro civique, et une avec le reste du texte. Le nombre de colonnes est contrôlé par l'argument `n`. Si `n = 1`, la fonction ne fait aucun découpage, avec `n = 2` la fonction va découper en deux parties le texte avec la première occurence du délimiteur, et ainsi de suite.  En ajoutant `[,1]` à la fin, nous indiquons que l'on souhaite garder seulement la première des deux colonnes.

Il est également possible d'extraire des parties de texte et de ne garder par exemple que les *N* premiers caractères ou les *N* derniers caractères : 

```{r message=FALSE, warning=FALSE}

# ne garder que les 5 premiers caractères
substr(df$adresses_3,start = 1, stop = 5)

# ne garder que les 5 derniers caractères
n_caract <- nchar(df$adresses_3)
substr(df$adresses_3, start = n_caract-4, stop = n_caract)
```
Notez que les paramètres `start` et `stop` de la fonction `substr` peuvent accepter un vecteur de valeurs. Il est ainsi possible d'appliquer une sélection de texte différente à chaque chaîne de caractères dans notre vecteur en entrée. On pourrait par exemple vouloir récupérer tout le texte avant le second espace pour garder uniquement le numéro civique et le type de rue.

```{r message=FALSE, warning=FALSE}
# étape 1 : récupérer les positions des espaces pour chaque adresses
positions <- str_locate_all(df$adresses_3, " ")

# étape 2 : récupérer les positions des seconds espaces
sec_positions <- sapply(positions, function(i){
  i[2,1]
})

# étape 3 : appliquer le découpage
substr(df$adresses_3, start = 1, stop = sec_positions-1)

```


##### Coller du texte {#sect014254}

À l'inverse du découpage, il est parfois nécessaire de concaténer des éléments de texte, ce qu'il est possible de réaliser avec la fonction `paste`.

```{r message=FALSE, warning=FALSE}
df$texte_complet <- paste(df$noms_corr, df$adresses_3, sep = " : ")
print(df$texte_complet)
```

Le paramètre `sep` permet d'indiquer le ou les caractères à intercaler entre les éléments à concaténer. Notez qu'il est possible de concaténer plus que deux éléments.

```{r message=FALSE, warning=FALSE}
df$ville <- c('Montreal','Montreal','Montreal','Montreal')
paste(df$noms_corr, df$adresses_3, df$ville, sep = ", ")
```

#### Manipuler des colonnes de type date {#sect01426}

Nous avons vu que les principaux types de données dans R sont le numérique, le texte, le booléen et le facteur. Il existe d'autres types introduits par différents *packages*. Nous abordons ici les types date et heure (*date* and *time*). Pour les manipuler, nous privilégions l'utilisation du *package* **lubridate** du **tidyverse**. Pour illuster le tout, nous l'appliquerons avec un jeu de données ouvertes de la Ville de Montréal représentant les collisions routières impliquant au moins un cycliste survenues après le 1^er^ janvier 2017.

```{r message=FALSE, warning=FALSE}
accidents_df <- read.csv(file="data/priseenmain/accidents.csv", sep = ",")
names(accidents_df)
```

Nous disposons de trois colonnes représentant respectivement l'heure, la date et le nombre de victimes impliquées dans la collision.

##### Du texte à la date {#sect014261}

Actuellement, les colonnes *HEURE_ACCDN* et *DT_ACCDN* sont au format texte. Nous pouvons afficher quelques lignes du jeu de données avec la fonction `head` pour visualiser comment elles ont été saisies.

```{r message=FALSE, warning=FALSE}
head(accidents_df, n = 5)
```

Un peu de ménage s'impose : les heures sont indiquées comme des périodes d'une heure. Nous utilisons la fonction `str_split_fixed` du *package* **stringr** pour ne garder que la première partie de l'heure (avant le tiret). Nous allons ensuite concaténer l'heure et la date avec la fonction `paste`, puis nous convertirons ce résultat en un objet *date-time*.

```{r message=FALSE, warning=FALSE}
library(lubridate)

# Étape 1 : découper la colonne Heure_ACCDN
accidents_df$heure <- str_split_fixed(accidents_df$HEURE_ACCDN, "-", n=2)[,1]

# Étape 2 : concaténer l'heure et la date
accidents_df$date_heure <- paste(accidents_df$DT_ACCDN, 
                                 accidents_df$heure,
                                 sep = ' ')

# Étape 3 : convertir au format datetime
accidents_df$datetime <- as_datetime(accidents_df$date_heure,
                                     format = "%Y/%m/%d %H:%M:%S")
```

Pour effectuer la conversion, nous avons utilisé la fonction `as_datetime` du package **lubridate**. Elle prend comme paramètre un vecteur de texte et une indication du format de ce vecteur de texte. Il existe de nombreuses façons de spécifier une date et une heure et l'argument *format* permet d'indiquer celle à utiliser. Dans cet exemple, la date est structurée comme suit : 
`année/mois/jour heure:minute:seconde`, ce qui se traduit par le format `%Y/%m/%d %H:%M:%S`.

* %Y signifie une année indiquée avec quatre caractères : 2017
* %m signifie un mois, indiqué avec deux caractères : 01, 02, 03, ... 12
* %d signifie un jour, indiqué avec deux caractères : 01, 02, 03, ... 31
* %H signifie une heure, au format 24 heures avec deux caractères : 00, 02, ... 23
* %M signifie des minutes indiquées avec deux caractères : 00, 02, ... 59
* %S signifie des secondes, indiquées avec deux caractères : 00, 02, ... 59

Notez que les caractères séparant les années, jours, heures, etc. sont aussi à indiquer dans le format. Dans notre exemple, nous utilisons des `/` pour séparer les éléments de la date et des `:` pour l'heure, et un espace pour séparer la date et l'heure.

Il existe d'autres nomenclatures pour spécifier un format *datetime* : par exemple, des mois renseignés par leur nom, l'indication AM-PM, etc. Vous pouvez vous référez à la documentation de la fonction `strptime` (`help(strptime)`) pour explorer les différentes nomenclatures et choisir celle qui vous convient. Bien évidemment, il est **nécessaire** que toutes les dates de votre colonne soient renseignées dans le même format. Sinon, la fonction renverra des valeurs `NA` aux endroits où elle a échoué à lire le format. Après toutes ces opérations, rejettons un oeil à notre *DataFrame*.

```{r message=FALSE, warning=FALSE}
head(accidents_df, n = 5)
```
##### Extraire des informations d'une date {#sect014262}

À partir de la nouvelle colonne `datetime`, nous sommes en mesure d'extraire des informations intéressantes comme : 

* le nom du jour de la semaine avec la fonction `weekdays`
```{r message=FALSE, warning=FALSE}
accidents_df$jour <- weekdays(accidents_df$datetime)
```

* la période de la journée avec les fonctions `am` et `pm` 
```{r message=FALSE, warning=FALSE}
accidents_df$AM <- am(accidents_df$datetime)
accidents_df$PM <- pm(accidents_df$datetime)
head(accidents_df[c("jour", "AM", "PM")], n=5)
```

Il est aussi possible d'accéder aux sous-éléments d'un *datetime* comme l'année, le mois, le jour, l'heure, la minute, la seconde avec les fonctions `year()`, `month()`,`day()`, `hour()`,  `minute()` et `second()`.

##### Calculer une durée entre deux *datetime* {#sect014263}

Une autre utilisation intéressante du format *datetime* est de calculer des différences de temps. Par exemple, nous pourrions utiliser le nombre de minutes écoulées depuis 7h00 le matin comme une variable dans une analyse visant à déterminer le moment critique des collisions routières durant l'heure de pointe du matin. 
Pour cela, nous devons créer un *datetime* de référence en concaténant la date de chaque observation, et le temps `07:00:00` qui sera notre point de départ.

```{r message=FALSE, warning=FALSE}
accidents_df$date_heure_07 <- paste(accidents_df$DT_ACCDN, 
                                 '07:00:00',
                                 sep = ' ')
accidents_df$ref_datetime <- as_datetime(accidents_df$date_heure_07,
                                     format = "%Y/%m/%d %H:%M:%S")
```
Il ne nous reste plus qu'à calculer la différence de temps entre la colonne *datetime* et notre temps de référence *ref_datetime*.

```{r message=FALSE, warning=FALSE}
accidents_df$diff_time <- difftime(accidents_df$datetime,
                                   accidents_df$ref_datetime,
                                   units = 'min')
```

Notez qu'ici la colonne *diff_time* est d'un type spécial : une différence temporelle (*difftime*). Il faut encore la convertir au format numérique pour pourvoir l'utiliser avec la fonction `as.numeric`. Par curiosité, réalisons rapidement un histogramme avec la fonction `hist` pour analyser rapidement cette variable d'écart de temps !

```{r fig015, fig.align='center', auto_pdf = TRUE, fig.cap="Répartition temporelle des accidents à vélo",  out.width='65%'}

accidents_df$diff_time_num <- as.numeric(accidents_df$diff_time)
hist(accidents_df$diff_time_num, breaks = 50)

```

On observe clairement deux pics, un premier entre 0 et 100 ( entre 07h00 08h30 environ) et un second plus important entre 550 et 650 (entre 16h00 et 17h30 environ), ce qui correspond sans surprise aux heures de pointe. Il est intéressant de noter que plus d'accidents se produisent à l'heure de pointe du soir qu'à celle du matin.

##### Tenir compte du fuseau horaire {#sect014264}

Lorsque l'on travaille avec des données provenant de différents endroits dans le monde ou que l'on doive tenir compte des heures d'été et d'hiver, il convient de tenir compte du fuseau horaire. Pour créer une date avec un fuseau horaire, il est possible d'utiliser le paramètre `tz` dans la fonction `as_datetime` et d'utiliser l’identifiant du fuseau approprié. Dans notre cas, les données d'accident ont été collectées à Montréal qui a un décalage de -5 heures par rapport au temps de référence UTC (+1 heure en été). Le code spécifique de ce fuseau horaire est *EDT*, il est facile de trouver ces codes avec le site web [timeanddate.com](https://www.timeanddate.com/time/map/).

```{r message=FALSE, warning=FALSE}
accidents_df$datetime <- as_datetime(accidents_df$date_heure,
                                     format = "%Y/%m/%d %H:%M:%S",
                                     tz = "EDT")
```

#### Recoder des variables {#sect01427}

Recoder des variables signifie changer la valeur d'une variable selon une condition afin d'obtenir une nouvelle variable. Si nous reprenons le jeu de données précédent sur les accidents à vélo, nous pourrions vouloir créer une nouvelle colonne nous indiquant si la collision a eu lieu en heures de pointe ou non. On obtiendrait ainsi une nouvelle variable avec seulement deux catégories plutôt que la variable numérique originale. Nous pourrions aussi définir quatre catégories avec l'heure de pointe du matin, l'heure de pointe du soir, le reste de la journée et la nuit.

##### Le cas binaire avec ifelse {#sect014271}

Si l'on ne souhaite créer que deux catégories, le plus simple est d'utiliser la fonction `ifelse`. Cette fonction va évaluer une condition (section \@ref(sect0135)) pour chaque ligne d'un *DataFrame* et produire un nouveau vecteur. Créons donc une variable binaire indiquant si une collision a eu lieu durant les heures de pointe ou hors heures de pointe. Nous devons alors évaluer les conditions suivantes : 

Est-ce que l'accident a eu lieu entre 07h00 (0) **ET** 09h00 (120), **OU** est ce que la collision a eu lieu entre 16h30 (570) **ET** 18h30 (690)?

```{r message=FALSE, warning=FALSE}
table(is.na(accidents_df$diff_time_num))
```
Notons dans un premier temps que nous avons 40 observations sans valeur pour la colonne `diff_time_num`. Il s'agit d'observations pour lesquelles nous ne disposions pas de dates au départ.

```{r message=FALSE, warning=FALSE}
Cond1 <- accidents_df$diff_time_num >= 0 & accidents_df$diff_time_num <= 120
Cond2 <- accidents_df$diff_time_num >= 570 & accidents_df$diff_time_num <= 690

accidents_df$moment_bin <- ifelse(Cond1 | Cond2,
                                  "en heures de pointe",
                                  "hors heures de pointe")
```

Comme vous pouvez le constater, la fonction `ifelse` nécessite trois arguments : 

* Une condition, pouvant être `TRUE` ou `FALSE`,
* La valeur à renvoyer si la condition est `FALSE`

Avec la fonction `table`, nous pouvons rapidement visualisuer les effectifs des deux  catégories ainsi créées : 

```{r message=FALSE, warning=FALSE}
table(accidents_df$moment_bin)

# vérifier si on a toujours seulement 40 NA
table(is.na(accidents_df$moment_bin))
```
Les heures de pointe représentent quatre heures de la journée, ce qui nous laisse neuf heures hors heures de pointe entre 07h00 et 20h00.

```{r message=FALSE, warning=FALSE}
# Ratio de collisions routières en heures de pointe
(841 / 2414) / (4 / 13)

# Ratio de collisions routières hors heure de pointe
(1573 / 2414) / (9 / 13)
```
En rapportant les collisions aux durées des deux périodes, on observe une nette surreprésentation des collisions impliquant un vélo pendant les heures de pointe d'environ 13% comparativement à la période hors des heures de pointe.

##### Le cas multiple avec la fonction *case_when* {#sect014272}

Lorsque l'on souhaite créer plus que deux catégories, il est possible soit d'enchaîner plusieurs fonctions `ifelse` (ce qui produit un code plus long et moins lisible), soit d'utiliser la fonction `case_when` du *package* **dplyr** du **tidyverse**. Reprenons notre exemple et créons quatre catégories : 

* En heures de pointe du matin
* En heures de pointe du soir
* Le reste de la journée (entre 07:00 et 20:00)
* La nuit (entre 21:00 et 07:00)

```{r message=FALSE, warning=FALSE}
library(dplyr)

accidents_df$moment_multi <- case_when(
  accidents_df$diff_time_num >= 0 & accidents_df$diff_time_num <= 120 ~ "pointe matin",
  accidents_df$diff_time_num >= 570 & accidents_df$diff_time_num <= 690 ~ "pointe soir",
  accidents_df$diff_time_num > 690 & accidents_df$diff_time_num < 780 ~ "journee",
  accidents_df$diff_time_num > 120 & accidents_df$diff_time_num < 570 ~ "journee",
  accidents_df$diff_time_num < 0 | accidents_df$diff_time_num >= 780 ~ "nuit"
)

table(accidents_df$moment_multi)

#vérifions encore les NA
table(is.na(accidents_df$moment_multi))
```
La syntaxe de cette fonction est un peu particulière. Elle accepte un nombre illimité d'arguments. Chaque argument est composé d'une condition et d'une valeur à renvoyer si la condition est vraie; ces deux éléments étant reliés par le symbole `~`. Notez que toutes les évaluations sont effectuées dans l'ordre des arguments. En d'autres termes, la fonction va d'abord tester la première condition et assigner ces valeurs, puis recommencer pour les prochaines conditions. Ainsi, si une observation (ligne du tableau de données) obtient `TRUE` à plusieurs conditions, elle obtiendra la valeur de la dernière condition qu'elle a validée.

#### Sous-sélection d'un *DataFrame* {#sect01428}

Dans cette section, nous verrons comment extraire des sous-parties d'un *DataFrame*. Il est possible de sous-sélectionner des lignes et des colonnes en se basant sur des conditions ou leurs index. Pour cela, nous allons utiliser un jeu de données fourni avec R : le jeu de données **iris** décrivant des fleurs du même nom.

```{r message=FALSE, warning=FALSE}
data("iris")

# Nombre de lignes et de colonnes
dim(iris)
```
##### Sous-sélection des lignes {#sect014281}

Sous-sélectionner des lignes par index est relativement simple. Admettons que nous souhaitons sélectionner les lignes 1 à 5, 10 à 25, 37 et 58.

```{r message=FALSE, warning=FALSE}
sub_iris <- iris[c(1:5, 10:25, 37, 58),]
nrow(sub_iris)
```
Sous-sélectionner des lignes avec une condition peut être effectué soit avec une syntaxe similaire, soit en utilisant la fonction `subset`. Sélectionnons toutes les fleurs de l'espèce Virginica.

```{r message=FALSE, warning=FALSE}
iris_virginica1 <- iris[iris$Species == "virginica",]
iris_virginica2 <- subset(iris, iris$Species == "virginica")

# Vérifions que les deux dataframes ont le même nombre de lignes
nrow(iris_virginica1) == nrow(iris_virginica2)
```

Vous pouvez utiliser dans les deux cas tous les opérateurs vus dans les sections \@ref(sect01352) et \@ref(sect01353). L'enjeu est d'arriver à un vecteur booléen final permettant d'identifier les observations à conserver.

##### Sous-sélectionner des colonnes {#sect014282}

Nous avons déjà vu comment sélectionner des colonnes en utilisant leur nom ou leur index dans la section \@ref(sect014221). Ajoutons ici un cas particulier où nous souhaiterions sélectionner des colonnes selon une condition. Par exemple, nous pourrions vouloir conserver que les colonnes comprenant le mot *Length*. Pour cela, nous utiliserons la fonction `grepl`, permettant de déterminer si des caractères sont présents dans une chaîne de caractères.

```{r message=FALSE, warning=FALSE}
nom_cols <- names(iris)
print(nom_cols)

test_nom <- grepl("Length",nom_cols, fixed = TRUE)
ok_nom <- nom_cols[test_nom]

iris_2 <- iris[ok_nom]
print(names(iris_2))
```
Il est possible d'obtenir ce résultat en une seule ligne de code, mais elle est un peu moins lisible.

```{r message=FALSE, warning=FALSE}
iris2 <- iris[names(iris)[grepl("Length",names(iris), fixed = TRUE)]]
```

##### Sélectionner des colonnes et des lignes {#sect014283}

Nous avons vu qu'avec les crochets, nous pouvons extraire les colonnes et les lignes d'un *DataFrame*. Il est possible de combiner les deux opérations simultanément. Pour ce faire, il faut indiquer en premier les indices ou la condition permettant de sélectionner une ligne, puis les indices ou la condition pour sélectionner les colonnes : `[index_lignes , index_colonnes]`. Sélectionnons cinq premières lignes et les les trois premières colonnes du jeu de données iris : 

```{r message=FALSE, warning=FALSE}
iris_5x3 <- iris[c(1,2,3,4,5),c(1,2,3)]
print(iris_5x3)
```

Combinons nos deux exemples précédents pour sélectionner uniquement les lignes avec des fleurs de l'espèce virginica, et les colonnes avec le mot Length.

```{r message=FALSE, warning=FALSE}
iris_virginica3 <- iris[iris$Species == "virginica",
                       names(iris)[grepl("Length",names(iris), fixed = TRUE)]]
head(iris_virginica3, n=5)
```

#### Fusionner des *DataFrames* {#sect01429}

Terminons cette section avec la fusion de *DataFrames* qu'il est possible de réaliser de deux façons, soit par ajout, soit par jointure.

##### Fusionner des *DataFrame* par ajout {#sect014291}

Ajouter deux *DataFrames* peut se faire en fonction de leurs colonnes, ou en fonction de leurs lignes. Dans ces deux cas, on utilisera respectivement les fonction `cbind` et `rbind`. La figure \@ref(fig:fig016) résume graphiquement le fonctionnement des deux fonctions.

```{r fig016, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Fusion de DataFrames",  out.width='30%'}
knitr::include_graphics('images/introduction/rbind_cbind.png', dpi = NA)
```

Pour que `cbind` fonctionne, il faut que les deux *DataFrames* aient le même nombre de lignes. Pour `rbind`, les deux *DataFrames* doivent avoir le même nombre de colonnes. Prenons à nouveau comme exemple le jeu de données iris. Nous allons commencer par le séparer en trois sous-jeux de données comprenant chacun une espèce d'iris. Puis, nous fusionnerons deux d'entre eux avec la fonction `rbind`.

```{r message=FALSE, warning=FALSE}
iris1 <- subset(iris, iris$Species == "virginica")
iris2 <- subset(iris, iris$Species == "versicolor")
iris3 <- subset(iris, iris$Species == "setosa")

iris_comb <- rbind(iris2,iris3)
```

Nous pourrions aussi extraire dans les deux *DataFrames* les colonnes comprenant le mot *Length* et le mot *Width*, puis les fusionner.

```{r message=FALSE, warning=FALSE}
iris_l <- iris[names(iris)[grepl("Length",names(iris), fixed = TRUE)]]
iris_w <- iris[names(iris)[grepl("Width",names(iris), fixed = TRUE)]]

iris_comb <- cbind(iris_l,iris_w)
names(iris_comb)
```
##### Joindre des *DataFrame* {#sect014292}

Une jointure est une opération un peu plus complexe qu'un simple ajout. L'idée est d'associer des informations de plusieurs *DataFrames* en utilisant une colonne (appelée une clef) présente dans les deux jeux de données. On distingue plusieurs types de jointure : 

* Les jointures internes permettant de combiner les éléments communs entre deux *DataFrames* A et B
* La jointure complète permettant de combiner les éléments présents dans A ou B
* La jointure à gauche, permettant de ne conserver que les éléments présents dans A même s'ils ne trouvent pas leur correspondance dans B.

Ces trois jointures sont présentées à la figure \@ref(fig:fig016); pour ces trois cas, la colonne commune se nomme *id*.

```{r fig017, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Jointure de DataFrames",  out.width='30%'}
knitr::include_graphics('images/introduction/merging.png', dpi = NA)
```

Vous retiendrez que les deux dernières jointures peuvent produire des valeurs manquantes. Pour réaliser ces opérations, on utilise la fonction `merge`. Prenons un exemple simple à partir d'un petit jeu de données.

```{r message=FALSE, warning=FALSE}
auteurs <- data.frame(
    name = c("Tukey", "Venables", "Tierney", "Ripley", "McNeil", "Apparicio"),
    nationality = c("US", "Australia", "US", "UK", "Australia", "Canada"),
    retired = c("yes", rep("no", 5)))
livres <- data.frame(
    aut = c("Tukey", "Venables", "Tierney", "Ripley", "Ripley", "McNeil","Wickham"),
    title = c("Exploratory Data Analysis",
              "Modern Applied Statistics ...",
              "LISP-STAT",
              "Spatial Statistics", "Stochastic Simulation",
               "Interactive Data Analysis", "R for Data Science"))
```

Nous avons donc deux *DataFrames*, le premier décrivant des auteurs et le second des livres. Effectuons une première jointure interne afin de savoir pour chaque livre la nationnalité de son auteur et si ce dernier est à la retraite.

```{r message=FALSE, warning=FALSE}
df1 <- merge(livres, auteurs, #les deux DataFrames 
             by.x = "aut", by.y = 'name', #les noms des colonnes de jointures
             all.x = FALSE, all.y = FALSE)

print(df1)
```
Cette jointure est interne car les deux paramètres *all.x* et *all.y* ont pour valeur `FALSE`. Ainsi, nous indiquons à la fonction que nous ne souhaitons ni garder tous les éléments du premier *DataFrame* ni tous les éléments du second, mais uniquement les éléments présents dans les deux. Vous noterez ainsi que le livre "R for Data Science" n'est pas présent dans le jeu de données final car son auteur "Wickham" ne fait pas partie du *DataFrame* auteurs. De même, l'auteur "Apparicio" n'apparaît pas dans la jointure, car aucun livre dans le *DataFrame* books n'a été écrit par cet auteur.

Pour conserver tous les livres, nous pouvons effectuer une jointure à gauche en renseignant `all.x = TRUE`. Nous allons ainsi forcer la fonction à garder tous les livres et mettre des valeurs vides aux informations manquantes des auteurs.

```{r message=FALSE, warning=FALSE}
df2 <- merge(livres, auteurs, #les deux DataFrames 
             by.x = "aut", by.y = 'name', #les noms des colonnes de jointures
             all.x = TRUE, all.y = FALSE)

print(df2)
```

Et pour garder tous les livres et tous les auteurs, nous pouvons faire une jointure complète en indiquant `all.x = TRUE` et `all.y = TRUE`.

```{r message=FALSE, warning=FALSE}
df3 <- merge(livres, auteurs, #les deux DataFrames 
             by.x = "aut", by.y = 'name', #les noms des colonnes de jointures
             all.x = TRUE, all.y = TRUE)

print(df3)
```


## Bien structurer un code R {#sect016}

Terminons ici avec quelques conseils sur la rédaction d’un code R. Bien rédiger son code est essentiel pour trois raisons :

1. pouvoir relire et réutiliser son code dans le futur.
2. permettre à d'autres personnes de bien lire et réutiliser votre code.
3. minimiser les risques d'erreurs.

Ne négligez pas l'importance d'un code bien rédigé et bien documenté, vous vous éviterez ainsi des migraines lorsque vous devrez exhumer du code écrit il y a plusieurs mois.

Voici quelques lignes directrices peu contraignantes, mais qui devraient vous être utiles : 

1. **Privilégier la clarté à la concision** : il vaut mieux parfois scinder une ligne de code en plusieurs sous-étapes afin de faciliter la lecture de l’opération réalisée. Par exemple, si l'on reprend une ligne de code d’une section précédente où nous sélectionnions l'ensemble des colonnes du jeu de données `iris` comprenant le mot `Length` : 
```{r message=FALSE, warning=FALSE}
iris_l <- iris[names(iris)[grepl("Length",names(iris), fixed = TRUE)]]
```
Il serait possible de simplifier la lecture de ce code en détaillant les différentes étapes comme suit : 
```{r message=FALSE, warning=FALSE}
noms_cols <- names(iris)
sel_noms <- noms_cols[grepl("Length",noms_cols, fixed = TRUE)]
iris_l <- iris[sel_noms]
```

2. **Documenter et commenter son code le plus possible** : il est possible de rajouter du texte dans un code R qui ne sera pas exécuté, ce qu’on appelle des commentaires. Typiquement, une ligne commençant par un `#` ne sera pas interprétée par le logiciel. Utilisez des commentaires le plus souvent possible pour décrire les actions que vous souhaitez effectuer avec votre code. Il sera ainsi plus facile de le relire, de naviguer dans votre code, mais également de repérer d'éventuelles erreurs. Si l'on reprend l'exemple précédent : 
```{r message=FALSE, warning=FALSE}
# récupération du nom des colonnes dans le dataframe iris
noms_cols <- names(iris)

# sélection des colonnes avec les caractères "Length"
sel_noms <- noms_cols[grepl("Length",noms_cols, fixed = TRUE)]

# extraction des colonnes sélectionnées dans un nouveau dataframe
iris_l <- iris[sel_noms]
```

3. **Éviter le code à rallonge…** : typiquement, essayez de vous limiter à des lignes de code d'une longueur maximale de 80 caractères. Au-delà de ce seuil, il est judicieux de découper votre code en plusieurs lignes.

4. **Adopter une convention d'écriture** : une convention d'écriture est un ensemble de règles strictes définissant comment un code doit être rédigé. À titre d'exemple, il est parfois recommandé d'utiliser le *lowerCamelCase*, le *UpperCamelCase*, ou encore de séparer les mots par des tirets bas *upper_camel_case*. Un mélange de ces différentes conventions peut être utilisé pour distinguer les variables, les fonctions et les classes. Il peut être difficile de réellement arrêter une telle convention, car les différents packages dans R utilisent des conventions différentes. Dans vos propres codes, il est surtout important d'avoir une certaine cohérence et ne pas changer de convention au fil de votre code.

5. **Indenter son code** : l'indentation du code permet de le rendre beaucoup plus lisible. Indenter son code signifie insérer au début de chaque ligne de code un certain nombre d'espaces permettant d'indiquer à quel niveau de profondeur on se situe. Typiquement, lorsque des accolades ou des parenthèses sont ouvertes dans une fonction, une boucle ou une condition, on rajoute deux ou quatre espaces en début de ligne. Prenons un exemple très concret, admettons que nous écrivons une fonction affichant un résumé statistique à chaque colonne d'un jeu de données si cette colonne est de type numérique. L'indentation dans cette fonction va jouer un rôle crucial dans sa lisibilité. 
Sans indentation et sans respecter la règle des 80 caractères, on obtient ceci : 

```{r message=FALSE, warning=FALSE}
summary_all_num_cols <- function(dataset){for(col in names(dataset)){if(class(dataset[[col]] == "numeric")){print(summary(dataset[[col]]))}}}
```

Avec de l'indentation et des commentaires, la syntaxe est beaucoup plus lisible puisqu’elle permet de repérer facilement trois niveaux / paliers dans le code :

```{r message=FALSE, warning=FALSE}
#définition d'une fonction
summary_all_num_cols <- function(dataset){
  #Itération sur chaque colonne de la fonction
  for(col in names(dataset)){
    # A chaque itération, testons si la colonne est de type numérique
    if(class(dataset[[col]] == "numeric")){
      # Si oui, on affiche un résumé statistique pour cette colonne
      print(summary(dataset[[col]]))
    } # ici on sort de la condition (niveau 3)
  } # ici on sort de la boucle (niveau 2)
}# ici on sort de la fonction (niveau 1)
```

6. **Adopter une structure globale pour vos scripts** : un code R peut être comparé à une recette de cuisine. Si tous les éléments sont dans le désordre et sans structure globale, la recette risque d'être très difficile à suivre. Cette structure risque de changer quelque peu en fonction de la recette ou de l'auteur·e, mais les principaux éléments restent les mêmes. Dans un code R, on peut distinguer plusieurs éléments récurrents que nous vous recommandons d'organiser de la façon suivante :
  a. Charger les différents packages **utilisés** par le script. Cela permet dès le début du code de savoir quelles sont les fonctions et méthodes qui seront employées dans le script. On limite aussi les risques d'oublier des packages qui seraient chargés plus loin dans le code.
  b. Définir les fonctions dont vous aurez besoin en plus de celles présentes dans les packages. Idem, placer ses fonctions en début de code évite d'oublier de les charger ou de les chercher quand on en a besoin.
  c. Définir le répertoire de travail avec la fonction `setwd` et charger les données nécessaires.
  d. Effectuer au besoin les opérations de manipulations sur les données.
  e. Effectuer les analyses nécessaires en scindant si possibles les différentes étapes.

Notez également que l'étape de définition des fonctions complémentaires peut être effectuée dans une feuille de code séparée, et l'ensemble de ces fonctions chargées à l'aide de la fonction `source`. De même, si la manipulation des données est conséquente, il est recommandé de l'effectuer avec un code à part, d'enregistrer les données structurées, puis de le charger directement au début de votre code dédié à l'analyse.

7. **Exploiter les commentaires délimitant les sections dans Rstudio** : il est possible d'écrire des commentaires d'une certaine façon pour que l'IDE les détecte comme des délimiteurs de sections. L'intérêt principal est que l'on peut ensuite facilement naviguer entre ces sections en utilisant Rstudio comme montré à la figure \@ref(fig:sectionsRstudio), mais aussi masquer des sections afin de faciliter la lecture du reste du code. Pour délimiter une section, il suffit d'ajouter une ligne de commentaire comprenant quatre fois les caractères `-`, `=` ou `#` à la suite : 


```{r

# Voici ma section 1 ----------------------------------

# Voici ma section 2 ==================================

# Voici ma section 3 ##################################

# Autre exemple pour mieux marquer la rupture dans un code : 

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#### Titre de ma section 4 ####
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
```
```{r sectionsRstudio, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap='Naviguer dans des sections de codes avec Rstudio',  out.width='50%'}
knitr::include_graphics('images/introduction/sections_rstudio.png', dpi = NA)
```

8. **Adopter une structure globale pour vos projets** : au-delà du script, il est nécessaire de bien structurer vos projets. Le plus important étant d'utiliser une structure commune à chaque projet pour vous faciliter le travail. Nous proposons ci-dessous un exemple de structure assez général pouvant être utilisé dans la plupart des cas. Elle sépare notamment les données originales des données structurées, ainsi que les fonctions complémentaires et la structuration des données du principal bloc d'analyse.

```{r structFolder, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Structure de dossier recommandée pour un projet avec R",  out.width='50%'}
knitr::include_graphics('images/introduction/structuration projet.png', dpi = NA)
```

**Ne négligez jamais l'importance d'un code bien écrit et documenté !**

## Conclusion et ressources pertinentes  {#sect017}
Voilà qui conclut ce chapitre sur les bases du langage R. Vous avez maintenant les connaissances nécessaires pour commencer à travailler. N'hésitez pas à revenir sur les différentes sous-sections au besoin ! Pour aller plus loin dans l'apprentissage du langage, vous pouvez également vous plonger dans le chapitre R AVANCÉ. Cependant, nous vous recommandons de faire vos premiers pas avec cette base avant de vous lancer dans cette partie davantage orientée programmation. Quelques ressources pertinentes qui pourraient vous être utiles sont aussi reportées au tableau ci-dessous. 


```{r tableRessources, echo=FALSE, message=FALSE, warning=FALSE}
if(knitr::is_latex_output()){
  df <- data.frame(
        Ressource = c("Rbloggers","CRAN packages by date", "Introduction à R et au TidyVerse", "Numyard", "Cheasheets"),
        Description = c("Un recueil de nombreux blogues sur R : parfait pour être tenu au courant des nouveautés et faire des découvertes.", "Les derniers packages publiés sur CRAN : cela permet de garder un oeil sur les nouvelles fonctionnalités de ses packages préférés.", "Une excellente ressource en français pour en apprendre plus sur le tidyverse.", "Une chaîne YouTube pour revoir les bases de R en vidéo.", "Des feuilles de triche résumant les fonctionnalités de nombreux packages."),
        Url = c("https://www.r-bloggers.com", "https://cran.r-project.org/web/packages","https://juba.github.io/tidyverse","https://www.youtube.com/user/TheLearnR","https://rstudio.com/resources/cheatsheets"))

  show_table(df,
             col.names = c("Ressource","Description","Url"),
             caption = 'Ressources pertinente pour en apprendre plus sur R',
             col.to.resize = c(2,3),
             col.width = "6cm")
}else{
  df <- data.frame(
        Ressource = c("[Rbloggers](https://www.r-bloggers.com){target='_blank'}","[CRAN packages by date](https://cran.r-project.org/web/packages/available_packages_by_date.html){target='_blank'}", "[Introduction à R et au TidyVerse](https://juba.github.io/tidyverse/index.html){target='_blank'}", "[Numyard](https://www.youtube.com/user/TheLearnR/featured){target='_blank'}", "[Cheasheets](https://rstudio.com/resources/cheatsheets){target='_blank'}"), 
        Description = c("Un recueil de nombreux blogues sur R : parfait pour être tenu au courant des nouveautés et faire des découvertes.", "Les derniers *packages* publiés sur CRAN : cela permet de garder un oeil sur les nouvelles fonctionnalités de ses *packages* préférés.", "Une excellente ressource en français pour en apprendre plus sur le tidyverse.", "Une chaîne YouTube pour revoir les bases de R en vidéo.", "Des feuilles de triche résumant les fonctionnalités de nombreux *packages*."))

  show_table(df,
             col.names = c("Ressource","Description"),
             caption = 'Ressources pertinente pour en apprendre plus sur R')
}
```


<!--chapter:end:01-priseenmainR.Rmd-->

# Modèles généralisés additifs  {#chap11}

Dans les précédents chapitres, nous avons eu l'occasion d'explorer les modèles de régression linéaire, puis les modèles généralisés et enfin les modèles généralisés à effets mixtes. Dans ce chapitre, nous abordons une nouvelle extension dans le monde des régressions : les modèles généralisés additifs (*Generalized additive model* en anglais — GAM). Cette extension a pour but de permettre de modéliser des relations non linéaires entre les variables indépendantes et la variable dépendante.

::: {.bloc_package data-latex=""}
Dans cette section, nous utiliserons principalement les packages suivants : 

À modifier 
library(mgcv)
library(segmented)
library(splines2)
library(viridis)
library(metR) # pour placer des étiquettes sur les isolignes
library(itsadug)
library(mixedup)
library(gamlss)
library(gamlss.add)

* Pour créer des graphiques :
  - **ggplot2**, le seul, l'unique
  - **ggpubr** pour combiner des graphiques et réaliser des diagrammes
  
* Pour ajuster des modèles GLM :
  - **VGAM** et **gamlss** offrent tous les deux un très large choix de distributions et de fonctions de diagnostic, mais nécessitent plusieurs manipulations manuelles
  - **mgcv** offre moins de distributions que les deux précédents mais est plus simple d'utilisation

 * Pour analyser des modèles GLM :
  - **car** essentiellement pour la fonction `vif`
  - **DHARMa** pour le diagnostic des résidus simulés 
  - **DescTools** pour les tests de Lilliefors, Shapiro-Wilk, Anderson-Darling et Jarque-Bera
  - **ROCR** et **caret** pour l'analyse de la qualité d'ajustement de modèles pour des variables qualitatives
  - **sandwich** pour générer des erreurs standards robustes pour le modèle GLM logistique binomial
:::


## Introduction {#sect111}

Puisque les modèles GAM sont une extension des modèles GLM, ils peuvent s'appliquer autant à des modèles pour des variables indépendantes qualitatives, de comptage ou continues. Nous l'appliquons ici à ce dernier type de variable indépendante. Pour rappel, la formule décrivant un modèle linéaire généralisé (GLM) assumant une distribution normale et une fonction de lien identitaire est la suivante : 

\footnotesize
\begin{equation}
\begin{aligned}
&Y \sim Normal(\mu,\sigma)\\
&g(\mu) = \beta_0 + \beta X\\
&g(x) = x
\end{aligned}
(\#eq:glm1)
\end{equation}
\normalsize

Les coefficients $\beta$ permettent de quantifier l'effet des variables indépendantes (*X*) sur la moyenne (l'espérance) ($\mu$) de la variable dépendante (*Y*). Un coefficient $\beta_k$ négatif indique que si la variable $X_k$ augmente, alors la variable *Y* tend à diminuer et inversement, si le coefficient est positif. L'inconvénient de cette formulation est que le modèle est seulement capable de capter des relations linéaires entre ces variables. Or, il existe de nombreuses situations dans lesquelles une variables indépendante aura un lien non linéaire avec une variable dépendante; voici quelques exemples : 

* Si nous mesurons le niveau de bruit émis par une source sonore (variable dépendante) à plusieurs endroits et que nous tentons de prédire l'intensité sonore en fonction de la distance à la source (variable indépendante), nous pouvons nous attendre à observer une relation non linéaire entre les deux. En effet, le son étant une énergie se dispersant selon une sphère dans l'espace, son intensité est inversement proportionnelle au carré de la distance avec la source sonore.
* La concentration de la pollution atmosphérique en ville suit généralement des patrons temporels et spatiaux influencés directement par la météorologie et les activités humaines. Autrement dit, il serait absurde d'introduire l'espace de façon linénaire (avec un gradiant nord-sud ou est-ouest), ou le moment de la journée de façon linéaire (comme si la pollution augmentait du matin au soir ou inversement). En guise d'exemple, la figure \@ref(fig:figgam1), tirée de @2020_3, illustre bien ces variations temporelles pour deux polluants (le dioxyde d'azote et l'azote).
* *Donner un autre exemple ?*

```{r figgam1, echo=FALSE, fig.align='center', fig.cap="Patron journalier du NO2 et de l'O3 à Paris", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='75%'}
library(dplyr)
knitr::include_graphics('images/gam/no2_03_patterns.PNG', dpi = NA)
```

### Non linéarité fonctionnelle

Il existe de nombreuses façons d'introduire des relations non linéaires dans un modèle. La première et la plus simple à mettre en oeuvre est de transformer la variable indépendante à l'aide d'une fonction inverse, exponentielle, logarithmique ou autre.

Prenons un premier exemple, présenté à la figure \@ref(fig:figgam2)) univarié avec une variable *Y* que nous tentons de prédire avec une variable *X*. Si nous ajustons une droite de régression à ces données (en bleu), nous constatons que l'augmentation de *X* est associée avec une augmentation de *Y*. Cependant, la droite de régression est très éloignée des données et ne capte qu'une petite partie de la relation. Une lecture attentive permet de constater que l'effet de *X* sur *Y* augmente de plus en plus rapidement à mesure que *X* augmente. Cette forme est caractéristique d'une relation exponentielle. Nous pouvons donc transformer la variable *X* avec la fonction exponentielle afin d'obtenir un meilleur ajustement (en rouge).

```{r figgam2, echo=FALSE, fig.align='center', fig.cap="Relation non linéaire exponentielle", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='50%'}
library(ggplot2)
library(ggpubr)
library(boot)

df <- data.frame(
  x1 = rnorm(1000,0,1),
  x2 = rnorm(1000,10,5),
  x3 = rnorm(1000,0,3),
  x4 = abs(rnorm(1000,0,5))
)

df$y1 <- exp(df$x1) + rnorm(1000,0,2)
df$y2 <- -log(df$x2 + rnorm(1000,0,1))
df$y3 <- inv.logit(df$x3) + rnorm(1000,0,0.3)
df$y4 <- sqrt(df$x4) + rnorm(1000,0,0.2)


ggplot(df)+
  geom_point(aes(x = x1, y = y1), size = 0.5) + 
  labs(x = "x", y = "y")  + 
  geom_smooth(aes(x = x1, y = y1),method = "lm",
              se=FALSE, color="blue", formula = y ~ x)+ 
  geom_smooth(aes(x = x1, y = y1),method = "lm",
              se=FALSE, color="red", formula = y ~ exp(x))

```


La figure \@ref(fig:figgam3) illustre trois autres situations avec les fonctions logarithmique, logistique inverse et racine carrée. Cette approche peut donner des résultats intéressants si vous disposez d'une bonne justification théorique sur la forme attendue de la relation entre *X* et *Y*. 

```{r figgam3, echo=FALSE, fig.align='center', fig.cap="Autres Relations non linéaires", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='75%'}

P1 <- ggplot(df)+
  geom_point(aes(x = x2, y = y2), size = 0.5) + 
  labs(x = "x", y = "y", subtitle = "fonction logarithmique")  + 
  geom_smooth(aes(x = x2, y = y2),method = "lm",
              se=FALSE, color="blue", formula = y ~ x)+ 
  geom_smooth(aes(x = x2, y = y2),method = "lm",
              se=FALSE, color="red", formula = y ~ log(x)) + 
  ylim(-3,2.5) + xlim(0,20)

P2 <- ggplot(df)+
  geom_point(aes(x = x3, y = y3), size = 0.5) + 
  labs(x = "x", y = "y", subtitle = "fonction logistique inverse")  + 
  geom_smooth(aes(x = x3, y = y3),method = "lm",
              se=FALSE, color="blue", formula = y ~ x)+ 
  geom_smooth(aes(x = x3, y = y3),method = "lm",
              se=FALSE, color="red", formula = y ~ inv.logit(x))

P3 <- ggplot(df)+
  geom_point(aes(x = x4, y = y4), size = 0.5) + 
  labs(x = "x", y = "y", subtitle = "fonction racine carrée")  + 
  geom_smooth(aes(x = x4, y = y4),method = "lm",
              se=FALSE, color="blue", formula = y ~ x)+ 
  geom_smooth(aes(x = x4, y = y4),method = "lm",
              se=FALSE, color="red", formula = y ~ sqrt(x))

ggarrange(P1,P2,P3, ncol = 2, nrow = 2)

```

Il existe également des cas de figure dans lesquels aucune fonction ne donne de résultats pertinents, tel qu'illustré à la figure \@ref(fig:figgam4). Nous constatons facilement qu'aucune des fonctions proposées n'est capable de bien capter la relation entre les deux variables. Puisque cette relation est complexe, il convient alors d'utiliser une autre stratégie pour la modéliser. 

```{r figgam4, echo=FALSE, fig.align='center', fig.cap="Relation non linéaire plus complexe", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='75%'}

library(mgcv)
dataset <- gamSim(eg=1,n=400,dist="normal",scale=1,verbose=F)

ggplot(dataset) + 
  geom_point(aes(y = y, x = x2), size = 1) + 
  geom_smooth(aes(x = x2, y = y, color = "lineaire"),method = "lm",
              se=FALSE, formula = y ~ x, size = 1.3) + 
  geom_smooth(aes(x = x2, y = y, color = "logarithme"),method = "lm",
              se=FALSE, formula = y ~ log(x), size = 1.3) +
  geom_smooth(aes(x = x2, y = y, color = "racine carrée"),method = "lm",
              se=FALSE, formula = y ~ sqrt(x), size = 1.3) + 
  scale_color_manual(values = c("lineaire" = "#0077b6",
                                "logarithme" = "#e63946",
                                "racine carrée" = "#2a9d8f"))+
  labs(colour = 'forme fonctionelle')

```


### Non linéarité avec des polynomiales

Nous avions vu dans le chapitre sur la régression simple (REF) qu'il est possible d'utiliser des polynomiales. Pour rappel, il s'agit simplement d'ajouter à un modèle la variable *X* à différents exposants ($X+X^2+\dots+X^k$). Chaque exposant supplémentaire (chaque ordre supplémentaire) permet au modèle d'ajuster une relation plus complexe. Rien de tel qu'un graphique pour illuster le tout (figure  \@ref(fig:figgam5)).

```{r figgam5, echo=FALSE, fig.align='center', fig.cap="Visualisation de plusieurs polynomiales", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}

p1 <- ggplot(dataset) + 
  geom_point(aes(y = y, x = x2), size = 1) + 
  geom_smooth(aes(x = x2, y = y), color = "blue" ,method = "lm",
              se=FALSE, formula = y ~ x + I(x**2), size = 1.3) + 
  labs(subtitle = "polynomiale de degré 2", x = "x", y = "y")
  
p2 <- ggplot(dataset) + 
  geom_point(aes(y = y, x = x2), size = 1) + 
  geom_smooth(aes(x = x2, y = y), color = "blue" ,method = "lm",
              se=FALSE, formula = y ~ x + I(x**2) + I(x**3), size = 1.3) + 
  labs(subtitle = "polynomiale de degré 3", x = "x", y = "y")

p3 <- ggplot(dataset) + 
  geom_point(aes(y = y, x = x2), size = 1) + 
  geom_smooth(aes(x = x2, y = y), color = "blue" ,method = "lm",
              se=FALSE, formula = y ~ x + I(x**2) + I(x**3) + I(x**4), size = 1.3) + 
  labs(subtitle = "polynomiale de degré 4", x = "x", y = "y")

p4 <- ggplot(dataset) + 
  geom_point(aes(y = y, x = x2), size = 1) + 
  geom_smooth(aes(x = x2, y = y), color = "blue" ,method = "lm",
              se=FALSE, formula = y ~ x + I(x**2) + I(x**3) + I(x**4)+ I(x**5), size = 1.3) + 
  labs(subtitle = "polynomiale de degré 5", x = "x", y = "y")

ggarrange(p1,p2,p3,p4, ncol = 2, nrow = 2)
```

L'enjeu est de sélectionner le bon nombre de degrés de la polynomiale pour le modèle. Chaque degré supplémentaire constitue une nouvelle variable dans le modèle et donc un paramètre supplémentaire. Un trop faible nombre de degrés produit des courbes trop simplistes alors qu'un nombre trop élevé conduit à un surajustement (*overfitting* en anglais) du modèle. La figure \@ref(fig:figgam6) illustre ces deux situations.

```{r figgam6, echo=FALSE, fig.align='center', fig.cap="Sur et sous-ajustement d'une polynomiale", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
ggplot(dataset) + 
  geom_point(aes(y = y, x = x2), size = 1) + 
  geom_smooth(aes(x = x2, y = y), color = "blue" ,method = "lm",
              se=FALSE, formula = y ~ x + I(x**2), size = 1.3) +
  geom_smooth(aes(x = x2, y = y), color = "red" ,method = "lm",
              se=FALSE, formula = y ~ x + poly(x,degree = 12), size = 1.3) + 
  labs(subtitle = "polynomiales de degrés 2 (bleu) et 12 (rouge)", x = "x", y = "y")
```

Un des problèmes inhérents à l'approche des polynomiales est la difficulté d'interprétation. En effet, les coefficients ne sont pas directement interprétables et seule une figure représentant les prédictions du modèle permet d'avoir une idée de l'impact de la variable *X* sur la variable *Y*.

### Non linéarité par segments

Un compromis intéressant offrant une interprétation simple et une relation potentiellement complexe consiste à découper la variable *X* en segments, puis d'ajuster un coefficient pour chacun de ces segments. On obtient ainsi une ligne brisée et des coefficients faciles à interpréter (\@ref(fig:figgam7)). 

```{r figgam7, echo=FALSE, fig.align='center', fig.cap="Régression par segment", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
library(segmented)

model <- lm(y ~ x2, data = dataset)
o <- segmented(model, seg.Z = ~x2, psi = list(x2 = c(0.25,0.5)),
  control = seg.control(display = FALSE)
)
dataset$fit <- broken.line(o)$fit

ggplot(dataset) + 
  geom_point(aes(y = y, x = x2), size = 1) + 
  geom_line(aes(x = x2, y = fit), color = 'blue', size = 1)
```
L'enjeu est alors de déterminer le nombre points et la localisation de points de rupture. L'inconvénient majeur de cette approche est qu'en réalité, peu de phénomènes sont marqués par des ruptures très nettes.

À la figure \@ref(fig:figgam7), nous avons divisé la variable *X* en trois segments ($k_1$, $k_2$ et $k_3$), définis respectivement avec les intervalles suivants : [0,00-0,22], [0,22-0,41] et [0,41-1,00]. Concrètement, cela revient à diviser la variable *X* en trois nouvelles variables $X_{k1}$, $X_{k2}$, et $X_{k3}$. La valeur de $X_{ik}$ est égale à $x_i$ si $x_i$ se trouve dans l'intervalle propre à *k*, et 0 autrement. Ici, nous obtenons trois coefficients : 

* le premier est positif, une augmentation de *X* sur le premier segment est associée à une augmentation de *Y*.
* le second est négatif, une augmentation de *X* sur le second segment est associée à une diminution de *Y*.
* le troisième est aussi négatif, une augmentation de *X* sur le troisième segment est associée à une diminution de *Y*, mais moins forte que pour le second segment.


### Non linéarité avec des *splines*

La dernière approche et certainement la plus flexible est d'utiliser ce que l'on appelle une *spline* pour capter des relations non linéaires. Une *spline* est une fonction créant des variables supplémentaires à partir d'une variable *X* et d'une fonction de base. Ces variables supplémentaires appelées bases (*basis* en anglais) sont ajoutées au modèle; la sommation de leurs valeurs multipliées par leurs coefficients permet de capter les relations non linéaires entre une variable dépendante et une variable indépendante. Le nombre de base, ainsi que leur localisation (plus souvent appelé nœuds) permettent de contrôler la complexité de la fonction non linéaire.

Prenons un premier exemple simple avec une fonction de base triangulaire (*tent basis* en anglais). Nous créons ici une *spline* avec sept nœuds répartis équitablement sur l'intervalle de valeurs de la variable *X*. Les sept bases qui en résultent sont présentées à la figure \@ref(fig:figgam8). Dans cette figure, chaque sommet d'un triangle correspond à un nœud et chaque triangle correspond à une base.


```{r figgam8, echo=FALSE, fig.align='center', fig.cap="Bases de la spline triangulaire", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
library(splines)

basis <- bs(dataset$x2, df = 7, degre = 1)
df <- data.frame(basis * 3)
df$X <- dataset$x2
df <- reshape2::melt(df, id.vars = "X")

ggplot() + 
  geom_point(aes(y = y, x = x2), size = 1, data = dataset) + 
  geom_line(aes(x = X, y = value, color = variable), size = 1, data = df) + theme(legend.position = "none") 
```


En ajoutant ces bases dans notre modèle de régression, nous pouvons ajuster un coefficient pour chacune et le représenter en multipliant ces bases par les coefficients obtenus avec une simple régression linéaire (\@ref(fig:figgam9)).

```{r figgam9, echo=FALSE, fig.align='center', fig.cap="Spline triangulaire multipliée par ces coefficients", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}

basis <- bs(dataset$x2, df = 7, degre = 1)
model <- lm(y ~ basis, data = dataset)
coefs <- coefficients(model)[2:length(coefficients(model))]

prodbase <- sweep(basis, MARGIN = 2, coefs, `*`)

df <- data.frame(prodbase)
df$X <- dataset$x2
df <- reshape2::melt(df, id.vars = "X")

ggplot() + 
  geom_point(aes(y = y, x = x2), size = 1, data = dataset) + 
  geom_line(aes(x = X, y = value, color = variable), size = 1, data = df) + theme(legend.position = "none") 
```

On remarque ainsi que les bases correspondant à des valeurs plus fortes de *Y* ont reçu des coefficients plus élevés. Pour reconstituer la fonction non linéaire, il suffit d'additionner ces bases multipliées par leurs coefficients, soit la ligne à la figure \@ref(fig:figgam10).

```{r figgam10, echo=FALSE, fig.align='center', fig.cap="Spline triangulaire", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}

basis <- bs(dataset$x2, df = 7, degre = 1)
model <- lm(y ~ basis, data = dataset)
coefs <- coefficients(model)[2:length(coefficients(model))]

prodbase <- sweep(basis, MARGIN = 2, coefs, `*`)
total <- rowSums(prodbase) + coefficients(model)[1]

df <- data.frame(prodbase)
df$X <- dataset$x2
df <- reshape2::melt(df, id.vars = "X")
df$total <- total

ggplot() + 
  geom_point(aes(y = y, x = x2), size = 1, data = dataset) + 
  geom_line(aes(x = X, y = value, group = variable), color = "grey", linetype="dashed", size = 1, data = df) +
  geom_line(aes(x = X, y = total), color = "blue", size = 1, data = df)+
  theme(legend.position = "none") 
```

La fonction de base triangulaire est intéressante pour présenter la logique qui sous-tend les *splines*, mais elle est rarement utilisée en pratique. On lui préfère généralement d'autre formes donnant des résultats plus lisses comme les *B-spline* quadratique, *B-spline* cubiques, *M-spline*, *Duchon spline*, etc.

```{r figgam11, echo=FALSE, fig.align='center', fig.cap="Comparaison de différentes bases", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='75%'}
library(mgcv)
library(splines2)

## squared spline
basis <- bSpline(dataset$x2, df = 7, degre = 2)
model <- lm(y ~ basis, data = dataset)
coefs <- coefficients(model)[2:length(coefficients(model))]

prodbase <- sweep(basis, MARGIN = 2, coefs, `*`)
total <- rowSums(prodbase) + coefficients(model)[1]

df <- data.frame(prodbase)
df$X <- dataset$x2
df <- reshape2::melt(df, id.vars = "X")
df$total <- total

P1 <- ggplot() + 
  geom_point(aes(y = y, x = x2), size = 1, data = dataset) + 
  geom_line(aes(x = X, y = value, group = variable), color = "grey", linetype="dashed", size = 1, data = df) +
  geom_line(aes(x = X, y = total), color = "blue", size = 1, data = df)+
  theme(legend.position = "none") + 
  labs(subtitle = "B-spline quadratique")+ 
  ylim(0,20)


## cubic spline
basis <- bSpline(dataset$x2, df = 7, degre = 3)
model <- lm(y ~ basis, data = dataset)
coefs <- coefficients(model)[2:length(coefficients(model))]

prodbase <- sweep(basis, MARGIN = 2, coefs, `*`)
total <- rowSums(prodbase) + coefficients(model)[1]

df <- data.frame(prodbase)
df$X <- dataset$x2
df <- reshape2::melt(df, id.vars = "X")
df$total <- total

P2 <- ggplot() + 
  geom_point(aes(y = y, x = x2), size = 1, data = dataset) + 
  geom_line(aes(x = X, y = value, group = variable), color = "grey", linetype="dashed", size = 1, data = df) +
  geom_line(aes(x = X, y = total), color = "blue", size = 1, data = df)+
  theme(legend.position = "none") + 
  labs(subtitle = "B-spline cubique")+ 
  ylim(0,20)

## M-spline
basis <- mSpline(dataset$x2, df = 7, degre = 2)
model <- lm(y ~ basis, data = dataset)
coefs <- coefficients(model)[2:length(coefficients(model))]

prodbase <- sweep(basis, MARGIN = 2, coefs, `*`)
total <- rowSums(prodbase) + coefficients(model)[1]

df <- data.frame(prodbase)
df$X <- dataset$x2
df <- reshape2::melt(df, id.vars = "X")
df$total <- total

P3 <- ggplot() + 
  geom_point(aes(y = y, x = x2), size = 1, data = dataset) + 
  geom_line(aes(x = X, y = value, group = variable), color = "grey", linetype="dashed", size = 1, data = df) +
  geom_line(aes(x = X, y = total), color = "blue", size = 1, data = df)+
  theme(legend.position = "none") + 
  labs(subtitle = "M-spline")+ 
  ylim(0,20)


## Duchon spline
smoother <- smoothCon(s(x2, bs="ds", k = 7), data = dataset, absorb.cons = T)
basis <- smoother[[1]]$X
model <- lm(y ~ basis, data = dataset)
coefs <- coefficients(model)[2:length(coefficients(model))]

prodbase <- sweep(basis, MARGIN = 2, coefs, `*`)
total <- rowSums(prodbase) + coefficients(model)[1]

df <- data.frame(prodbase)
df$X <- dataset$x2
df <- reshape2::melt(df, id.vars = "X")
df$total <- total

P4 <- ggplot() + 
  geom_point(aes(y = y, x = x2), size = 1, data = dataset) + 
  geom_line(aes(x = X, y = value, group = variable), color = "grey", linetype="dashed", size = 1, data = df) +
  geom_line(aes(x = X, y = total), color = "blue", size = 1, data = df)+
  theme(legend.position = "none") + 
  labs(subtitle = "Duchon-spline") + 
  ylim(0,20)

ggarrange(P1,P2,P3,P4, ncol = 2, nrow = 2)
```

Les approches que nous venons de décrire sont regroupées sous l'appellation de modèles additifs. Dans les prochaines sous-sections, nous nous concentrons davantage sur les splines du fait de leur plus grande flexibilité.

## Spline de régression et spline de lissage

Dans les exemples précédents, nous avons vu que la construction d'une *spline* nécessite d'effectuer deux choix importants : le nombre de nœuds et leur localisation. Un trop grand nombre de nœuds conduit à un sur-ajustement du modèle alors qu'un trop faible nombre de nœuds conduit à un sous-ajustement. Lorsque ces choix sont effectués par l'utilisateur et que les bases sont ajoutées manuellement dans le modèle tel que décrit précédemment, on parle alors de **splines de régression** (*Regression Spline* en anglais).

Une approche a été proposée pour faciliter le choix du nombre de nœuds il s'agit de **splines de lissage**  (*smoothing spline* en anglais). L'idée derrière cette approche est d'introduire dans le modèle une pénalisation associée avec le nombre de nœuds (ou degré de liberté) de la spline, dans une perspective de parcimonie : chaque noeud supplémentaire doit suffisamment contribuer au modèle pour être conservé. Il n'est nécessaire ici de rentrer dans le détail mathématique de cette pénalisation qui est un peu complexe. Retenez simplement qu'elle dépend d'un paramètre appelé $\lambda$ :

* Plus $\lambda$ tend vers 0, plus la pénalisation est faible et plus la *spline* de lissage devient une simple *spline* de régression. 
* À l'inverse, elle est forte, plus la pénalité est importante, au point que la *spline* peut se résumer à une simple ligne droite. 

Cela est illustré à la figure \@ref(fig:figgam12) comprenant trois splines avec 20 nœuds et des valeurs $\lambda$ différentes contrôlant la force de la pénalité. 

Bien évidemment, on constate qu'avec la *spline* de régression (non pénalisée), 20 nœuds conduisent à un fort surajustement du modèle. En revanche, les splines de lissage (pénalisées) permettent de corriger ce problème de surajustement. Toutefois, une valeur trop importante de $\lambda$ conduit à un sous-ajustement du modèle (ici $\lambda = 3$ et $\lambda = 100$, lignes verte et bleue).


```{r figgam12, echo=FALSE, fig.align='center', fig.cap="Pénalisation des splines", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}

## model1 : regression spline sans penalite
mod1 <- gam(y~ s(x2, k = 20, fx = T), data = dataset)

## model2 : regression spline avec penalite = 3
mod2 <- gam(y~ s(x2, k = 20, sp = 0.5), data = dataset)

## model3 : regression spline avec penalite = 10
mod3 <- gam(y~ s(x2, k = 20, sp = 3), data = dataset)

## model3 : regression spline avec penalite = 100
mod4 <- gam(y~ s(x2, k = 20, sp = 100), data = dataset)

df <- data.frame(
  y = dataset$y,
  x = dataset$x2,
  pred1 = predict(mod1),
  pred2 = predict(mod2),
  pred3 = predict(mod3),
  pred4 = predict(mod4)
)

ggplot(df) + 
  geom_point(aes(x = x, y = y),size = 1) +
  geom_line(aes(x = x, y = pred1, color = 'spline de régression'), size = 1.1) +
  geom_line(aes(x = x, y = pred2, color = 'lambda = 0.5'), size = 1.1) +
  geom_line(aes(x = x, y = pred3, color = 'lambda = 3'), size = 1.1)+
  geom_line(aes(x = x, y = pred4, color = 'lambda = 100'), size = 1.1) + 
  labs(colour = 'pénalisation')

```


Avec les splines de lissage, l'enjeu est alors de sélectionner une valeur optimale de $\lambda$. Le plus souvent, les *packages* R **estiment eux-mêmes** ce paramètre à partir des données utilisées dans le modèle. Toutefois, gardez en mémoire que vous pouvez modifier ce paramètre. Mentionnons également que les splines de lissage peuvent être reparamétrées dans un modèle pour être intégrées comme des effets aléatoires. Dans ce cas-ci, $\lambda$ est remplacé par un simple paramètre de variance directement estimé dans le modèle [@wood2004stable].


## Interpréter une spline

L'interprétation d'une *spline* se fait à l'aide graphique. En effet, puisqu'elle est composée d'un ensemble de coefficients appliqués à des bases, il est difficile d'interpréter ces derniers. On préfère alors la représenter à l'aide d'un graphique, illustrant son *effet marginal*. Ce graphique est construit en trois étapes : 

1. Créer un jeu de données fictif dans lequel l'ensemble des variables indépendantes sont fixées à leurs moyennes respectives, sauf la variable pour laquelle on souhaite représenter la *spline*. Pour cette dernière, un ensemble de valeurs allant de son minimum à son maximum sont utilisés.
2. Utiliser le modèle pour prédire les valeurs attendues de la variable dépendante pour chacune des observations fictives ainsi créées.
3. Afficher les prédictions obtenues dans un graphique.

Notez ici qu'un graphique des effets marginaux se base sur les prédictions du modèle. Si un modèle est mal ajusté, les prédictions ne seront pas fiables et il sera inutile d'interpréter la *spline* en résultant.

## Aller plus loin avec les splines

Jusqu'ici nous avons seulement présenté le cas le plus simple pour lequel une *spline* est construite à partir d'une seule variable dépendante continue, mais elles peuvent être utilisées dans de nombreux autres contextes et ont une incroyable flexibilité. Nous détaillons ici trois exemples fréquents : les splines cycliques, les splines variant par groupe et les splines multivariées.

**POURQUOI NE PAS FAIRE UN TABLEAU ICI**

### Splines cycliques

Une *spline* cyclique est une extension d'une *spline* classique dont les bases aux extrémités sont spécifiées de telle sorte que la valeur au départ de la *spline* soit la même que celle à la fin de la *spline*. Ceci permet à la *spline* de former une boucle ce qui est particulièrement intéressant pour des variables dont le 0 et la valeur maximale correspondent en réalité à la même valeur. L'exemple le plus parlant est certainement le cas d'une variable représentant la mesure d'un angle en degrés. Les valeurs de 0 et 360 indique la même valeur et les valeurs 350 et 10 sont toutes les deux à une distance de 10 degrés de 0. Un autre exemple possible serait de considérer l'heure comme une variable continue; dans ce cas, 24h et 00h signifient la même chose.

Prenons un exemple concret : nous souhaitons modéliser la concentration de dioxyde d'azote (NO^2) à Paris, mesurée par un ensemble de stations fixes. On pourrait s'attendre à ce que le NO^2 suive chaque jour un certain patron. Concrètement, à proximité d'axes routiers majeurs, on s'attendrait à observer des pics suivant les flux pendulaires. À la figure \@ref(fig:figgam13), on retrouve bien les deux pics attendus correspondant aux heures de pointe du matin et du soir. Aussi, tel qu'indiqué par la ligne rouge, la valeur prédite par la *spline* est la même à 24h et à 0h.


```{r figgam13, echo=FALSE, fig.align='center', fig.cap="Spline cyclique pour modéliser la concentration de NO2", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}

dataset <- read.csv('data/gam/NO2_airparif.csv', sep=';')
dataset <- dataset[2:nrow(dataset),]
df <- reshape2::melt(dataset, id.vars = c("heure","date"))
df$no2 <- ifelse(df$value == "n/d", NA, as.numeric(df$value))

model <- gam(no2 ~ s(heure, bs = "cp"), data = df, family = tw)
preds <- data.frame(
  heure = 1:24
)
modout <- predict(model, newdata = preds, type="link",se.fit = T)
preds$yhat <- exp(modout$fit)
preds$lower <- exp(modout$fit - 1.96*modout$se.fit)
preds$upper <- exp(modout$fit + 1.96*modout$se.fit)

subpts <- df[sample(1:nrow(df),size = 1000,replace = F),]
subpts$heure2 <- subpts$heure + runif(nrow(subpts),min = -0.5, max = +0.5)

cent_line <- preds$yhat[[1]]

ggplot() + 
  geom_point(mapping = aes(x = heure2, y = no2), size = 0.5, data = subpts, alpha = 0.5) +
  geom_ribbon(mapping = aes(x = heure, ymax = upper, ymin = lower), fill = "grey", alpha = 0.4, data = preds) +
  geom_line(mapping = aes(x = heure, y = yhat), color = "blue", size =1.1, data = preds) + 
  geom_hline(yintercept = cent_line, color = "red", linetype="dashed") +
  labs(x = "moment de la journée", y = 'concentration de NO2') + 
  scale_x_continuous(breaks = seq(4,20,4), labels = paste(seq(4,20,4),'h',sep='')) + 
  xlim(1,24)

```



### Splines par groupe

Tel qu'abordé dans les chapitres précédents, il arrive régulièrement que les observations appartiennent à des différents groupes. Dans ce cas de figure, on peut être amené à vérifier si la relation décrite par une *spline* est identique pour chacun des groupes d'observations. Il s'agit alors d'ajuster une *spline* différente par groupe. Dans l'exemple précédent, chaque valeur de NO^2 a été mesurée par une station fixe de mesure spécifique. Compte tenu du fait que l'environnement autour de chaque station est particulier, on pourrait s'attendre à ce que les valeurs de NO^2 ne présentent pas exactement les mêmes patrons journaliers pour chaque station.

À la figure \@ref(fig:figgam14), il est possible de constater que le NO^2 suit globalement le même patron temporel pour l'ensemble des stations à l'exception de trois d'entres-elles. Il s'agit en réalité de stations situées dans des secteurs ruraux de la région parisienne et donc moins impactés par le trafic routier.


```{r figgam14, echo=FALSE, fig.align='center', fig.cap="Spline cyclique variant par groupe", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}

model <- gam(no2 ~ s(heure, bs = "cp",by = variable), data = df, family = gaussian)
preds <- expand.grid(
  heure = 1:24,
  variable = unique(df$variable)
)

modout <- predict(model, newdata = preds, type="link",se.fit = T)
preds$yhat <- modout$fit
preds$lower <- modout$fit - 1.96*modout$se.fit
preds$upper <-modout$fit + 1.96*modout$se.fit

subpts <- df[sample(1:nrow(df),size = 1000,replace = F),]
subpts$heure2 <- subpts$heure + runif(nrow(subpts),min = -0.5, max = +0.5)

cent_line <- preds$yhat[[1]]

ggplot() + 
  geom_point(mapping = aes(x = heure2, y = no2), size = 0.5, data = subpts, alpha = 0.5) +
  geom_ribbon(mapping = aes(x = heure, ymax = upper, ymin = lower, group = variable), fill = "grey", alpha = 0.4, data = preds) +
  geom_line(mapping = aes(x = heure, y = yhat, color = variable), size =0.7, data = preds) + 
  labs(x = "moment de la journée", y = 'concentration de NO2', color = 'station de mesure') + 
  scale_x_continuous(breaks = seq(4,20,4), labels = paste(seq(4,20,4),'h',sep='')) + 
  xlim(1,24) + 
  theme(legend.position = 'NONE')

```


### Splines bivariées

Jusqu'ici nous n'avons considéré que des *splines* ne s'appliquant qu'à une seule variable indépendante, cependant il est possible de construire des splines multivariées s'ajustant simultanément sur plusieurs variables indépendantes. L'objectif est alors de modéliser les potentielles interactions non linéaires entre les variables indépendantes combinées dans une même *spline*. Prenons un exemple concret, dans la section sur les modèles GLM, nous avions modélisé la couverture des aires de diffusion (AD) à Montréal par des îlots de chaleur. Parmi les variables indépendantes, nous avions notamment la distance au centre-ville ainsi que la part de la surface végétalisée des AD. Nous pourrions formuler l'hypothèse que ces deux variables impactent conjointement et de façon non linéaire la surface d'îlot de chaleur dans chaque AD. Pour représenter la *spline* bivariée, on utilise alors une carte de chaleur dont la couleur représente la valeur de la variable dépendante prédite en fonction des deux variables indépendantes.

**COMPREND PAS CETTE PHRASE** MULTIPLIER PAR 100. PROPORTION

La *spline* bivariée représentée à la figure \@ref(fig:figgam15) indique que les AD avec la plus grande proportion de leur surface couverte par des îlots de chaleur sont situés à moins de 25 kilomètres du centre ville, au-delà de cette distance, cette proportion chute en bas de 0,1, soit 10%. En revanche, à proximité du centre-ville (moins d'un kilomètre), même les AD disposant d'un fort pourcentage de surface végétalisée sont tout de mêmes marquées par un fort pourcentage de surface couverte par des îlots de chaleur.

Les *splines* bivariées sont aussi fréquemment utilisées pour capturer un potentiel patron spatial dans les données. En effet, si l'on dispose des coordonnées spatiales de chaque observation (*x,y*), il est possible d'ajuster une *spline* bivariée sur ces coordonnées qui contrôlera ainsi l'effet de l'espace. 

```{r figgam15, echo=FALSE, fig.align='center', fig.cap="Spline d'interaction bivariée", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='70%'}
dataset <- read.csv("data/glm/data_chaleur.csv")
dataset$prt_hot01 <- dataset$prt_hot/100
model <- gam(prt_hot01 ~ te(prt_veg,dist_cntr), data = dataset, family = betar)
vis.gam(model, plot.type = 'contour',type = 'response', main = '', xlab = 'pourcentage de la surface végétalisée', ylab = 'distance au centre-ville', too.far = 0.1,n.grid = 150)
```


## Mise en oeuvre dans R {#sect112}

Il est possible d'ajuster des *splines* de régression dans n'importe quel package permettant d'ajuster des coefficients pour un modèle de régression. Il suffit de construire les bases des *splines* en amont à l'aide du package **splines2** et de les ajouter directement dans l'équation de régression. 
En revanche, il est nécessaire d'utiliser des packages spécialisés pour ajuster des *splines* de lissage. Parmi ceux-ci **mgcv** est probablement le plus populaire du fait de sa (très) grande flexibilité, suivi des packages **gamlss**, **gam** et **VGAM**. Nous comparerons ici les deux approches. Nous allons tenter d'améliorer le modèle que nous avions ajusté pour prédire le pourcentage de surface couverte par des îlots de chaleur dans les aires de diffusion de Montréal, dans une perspective d'équité environnementale. Pour rappel, la variable dépendante est exprimée en pourcentage et nous utilisons une distribution *beta* pour la modéliser.

```{r message=FALSE, warning=FALSE}
library(mgcv)
## Chargement des données
dataset <- read.csv("data/gam/data_chaleur.csv",fileEncoding = "utf8")
## ajustement du modèle de base
refmodel <- gam(hot ~
          A65Pct + A014Pct + PopFRPct + PopMVPct +
          poly(prt_veg, degree = 2)  + Arrond,
        data = dataset, family = betar(link = "logit"))
```

Dans notre première analyse de ces données, nous avions ajusté une polynomiale d'ordre 2 pour représenter un potentiel impact non linéaire de la végétation sur les îlots de chaleur. Nous allons à présent remplacer ce terme par une *spline* de régression en sélectionnant quatre nœuds.

```{r message=FALSE, warning=FALSE}
library(splines2)
## création des bases de la spline
basis <- bSpline(x = dataset$prt_veg, df =4, intercept = FALSE)
## ajouter les bases au dataframe
basisdf <- as.data.frame(basis)
names(basisdf) <- paste('spline',1:ncol(basisdf),sep='')
dataset <- cbind(dataset, basisdf)
## ajuster le modèle
model0 <- gam(hot ~
          A65Pct + A014Pct + PopFRPct + PopMVPct +
          spline1 + spline2 + spline3 + spline4 + Arrond,
        data = dataset, family = betar(link = "logit"))

```

Nous pouvons à présent ajuster une *spline* de lissage et laisser **mgcv** décider de son niveau de complexité.

```{r message=FALSE, warning=FALSE}
## ajustement du modèle avec une spline simple
model1 <- gam(hot ~
          A65Pct + A014Pct + PopFRPct + PopMVPct +
          s(prt_veg)  + Arrond,
        data = dataset, family = betar(link = "logit"))
```

Notez ici que la syntaxe à employer est très simple, il suffit de spécifier `s(prt_veg)` pour indiquer à la fonction `gam` que vous souhaitez ajuster une *spline* pour la variable `prt_veg`. Nous pouvons à présent comparer l'ajustement des deux modèles en utilisant la mesure AIC et représenter les effets marginaux du pourcentage de végétation dans les deux modèles.

```{r message=FALSE, warning=FALSE}
## comparaison des AIC
AIC(refmodel, model0, model1)
```

On constate que la valeur d'AIC du second modèle est plus réduite, indiquant un meilleur ajustement du modèle avec une *spline* de régression. Notons cependant que la différence avec la *spline* de lissage est anecdotique (deux points d'AIC) et que nous connaissions a priori le bon nombre de nœuds à utiliser. Pour des relations plus complexes, les *splines* de lissage ont tendance à nettement mieux performer. Voyons à présenter comme représenter ces trois termes non linéaires.

```{r message=FALSE, warning=FALSE, figgam16, fig.align='center', fig.cap="Comparaison d'une spline et d'une polynomiale", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='75%'}
## création d'un dataframe de prédiction dans lequel seule
## la variable prt_veg varie.
dfpred <- data.frame(
  prt_veg = seq(min(dataset$prt_veg), max(dataset$prt_veg), 0.5),
  A65Pct = mean(dataset$A65Pct),
  A014Pct = mean(dataset$A014Pct),
  PopFRPct = mean(dataset$PopFRPct),
  PopMVPct = mean(dataset$PopMVPct),
  Arrond = "Verdun"
)

## recréation des bases de la spline de régression
## pour les nouvelles observations
nvl_bases <- data.frame(predict(basis,newx = dfpred$prt_veg))
names(nvl_bases) <- paste('spline',1:ncol(basisdf),sep='')
dfpred <- cbind(dfpred, nvl_bases)

## définition de la fonction inv.logit, soit l'inverse de la fonction 
## de lien du modèle pour retrouver les prédictions dans l'échelle 
## originales des données
inv.logit <- function(x){exp(x)/(1+exp(x))}

## utilisation des deux modèles pour effectuer les prédictions
predref <- predict(refmodel, newdata = dfpred, type = 'link', se.fit = T)
predmod0 <- predict(model0, newdata = dfpred, type = 'link', se.fit = T)
predmod1 <- predict(model1, newdata = dfpred, type = 'link', se.fit = T)

## calcul de la valeur prédite et construction des intervalles de confiance
dfpred$polypred <- inv.logit(predref$fit)
dfpred$poly025 <- inv.logit(predref$fit - 1.96 * predref$se.fit)
dfpred$poly975 <- inv.logit(predref$fit + 1.96 * predref$se.fit)

dfpred$regsplinepred <- inv.logit(predmod0$fit)
dfpred$regspline025 <- inv.logit(predmod0$fit - 1.96 * predmod0$se.fit)
dfpred$regspline975 <- inv.logit(predmod0$fit + 1.96 * predmod0$se.fit)

dfpred$splinepred <- inv.logit(predmod1$fit)
dfpred$spline025 <- inv.logit(predmod1$fit - 1.96 * predmod1$se.fit)
dfpred$spline975 <- inv.logit(predmod1$fit + 1.96 * predmod1$se.fit)

## créer un graphique pour afficher les résultats
ggplot(dfpred) + 
  geom_ribbon(aes(x = prt_veg, ymin = poly025, ymax = poly975),
              alpha = 0.4, color = 'grey') +
  geom_ribbon(aes(x = prt_veg, ymin = spline025, ymax = spline975),
              alpha = 0.4, color = 'grey') +
  geom_ribbon(aes(x = prt_veg, ymin = regspline025, ymax = regspline975),
              alpha = 0.4, color = 'grey') +
  geom_line(aes(y = polypred, x = prt_veg, color = 'polynomiale'),
            size = 1) + 
  geom_line(aes(y = regsplinepred, x = prt_veg, color = 'spline de régression'),
            size = 1)+
  geom_line(aes(y = splinepred, x = prt_veg, color = 'spline de lissage'),
            size = 1)
```

On constate que les trois termes renvoient des prédictions très similaires et qu'une légère différence n'est observable que pour les secteurs avec les plus hauts niveaux de végétation (supérieurs à 75%).

Jusqu'ici, nous utilisions l'arrondissement dans lequel est comprise chaque aire de diffusion comme une variable nominale afin de capturer la dimension spatiale du jeu de données. Puisque nous avons abordé la notion de *splines* bivariées, il serait certainement plus efficace d'en construire une à partir des coordonnées géographiques (x,y) des centroïdes des aires de diffusion. En effet, il est plus probable que la distribution des îlots de chaleur suive un patron spatial continu sur le territoire plutôt que de suivre les délimitations arbitraires des arrondissements.

```{r message=FALSE, warning=FALSE}
## ajustement du modèle avec une spline bivariée pour l'espace
model2 <- gam(hot ~
          A65Pct + A014Pct + PopFRPct + PopMVPct +
          s(prt_veg)  + s(X,Y),
        data = dataset, family = betar(link = "logit"))
```

Notez ici que l'expression `s(X,Y)` permet de créer une *spline* bivariée à partir des coordonnées (X,Y), soit deux colonnes présentes dans le jeu de données. Ces coordonnées sont exprimées toutes deux en mètres. Si vous avez besoin d'ajuster une *spline* multivariée à des variables s'exprimant dans des unités différentes, il est nécessaire d'utiliser une autre syntaxe `te(X,Y)` ou `t2(X,Y)` faisant appel à une structure mathématique légèrement différente, soit des *tensor product smooths*.

Voyons désormais, le résumé d'un modèle GAM tel que présenté dans R.

```{r message=FALSE, warning=FALSE}
summary(model2)
```
La première partie du résumé comprend les résultats pour les effets fixes et linéaires du modèle. Ils s'interprètent comme pour ceux d'un GLM classique. La seconde partie présente les résultats pour les termes non linéaires. La valeur de *p* permet de déterminer si la *spline* a ou non un effet différent de 0. Une valeur non significative indique que la *spline* ne contribue pas au modèle. Les colonnes *edf* et *Ref.df* indiquent la complexité de la *spline* et peuvent être considérées comme une approximation du nombre de nœuds. Dans notre cas, la *spline* spatiale (`s(X,Y)`) est environ 5 fois plus complexe que la *spline* ajustée pour la végétation (`s(prt_veg)`). Cela n'est pas surprenant puisque la dimension spatiale (*spline* bivariée) du phénomène sont certainement plus complexes que l’impact de la végétation. Notez ici que des valeurs *edf* et *Ref.df* proche de 1 signaleraient que l'impact d’un prédicteur est essentiellement linéaire et qu'il n'est pas nécessaire de recourir une *spline* pour cette variable.

La dernière partie du résumé comprend deux indicateurs de qualité d'ajustement, soit le R^2 ajusté et la part de la déviance expliquée.

```{r message=FALSE, warning=FALSE}
AIC(refmodel, model1, model2)
```
On peut constater que le fait d'introduire la *spline* spatiale dans le modèle contribue à réduire encore la valeur de l'AIC et donc à améliorer le modèle. À ce stade, on pourrait tenter de forcer la *spline* à être plus complexe en augmentant le nombre de nœuds.

```{r message=FALSE, warning=FALSE}
## augmentation de la complexité de la spline spatiale
model3 <- gam(hot ~
          A65Pct + A014Pct + PopFRPct + PopMVPct +
          s(prt_veg)  + s(X,Y,k = 40),
        data = dataset, family = betar(link = "logit"))

AIC(refmodel, model1, model2, model3)
```

Cela a pour effet d'améliorer de nouveau le modèle. Pour vérifier l'augmentation du nombre nœuds est judicieuse, il est possible de représenter le résultat de deux *splines* précédentes. Pour ce faire, nous proposons de calculer les valeurs prédites de la *spline* pour chaque localisation dans notre terrain d'étude, en le découpant préalablement en pixels de 100 de côté. Pour cette prédiction, nous maintenons toutes les autres variables à leur moyenne respective afin d'évaluer uniquement l'effet de la *spline* spatiale.

```{r message=FALSE, warning=FALSE, figgam17, fig.align='center', fig.cap="Comparaison de deux splines spatiales", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='100%'}
library(viridis)
library(metR) # pour placer des étiquettes sur les isolignes

## création d'un dataframe fictif pour les prédictions
dfpred <- expand.grid(
  prt_veg =mean(dataset$prt_veg),
  A65Pct = mean(dataset$A65Pct),
  A014Pct = mean(dataset$A014Pct),
  PopFRPct = mean(dataset$PopFRPct),
  PopMVPct = mean(dataset$PopMVPct),
  X = seq(min(dataset$X),max(dataset$X),100),
  Y = seq(min(dataset$Y),max(dataset$Y),100)
)

dfpred$predicted1 <- predict(model2, newdata = dfpred, type = 'response')
dfpred$predicted2 <- predict(model3, newdata = dfpred, type = 'response')

# centrage des prédictions
dfpred$predicted1 <- dfpred$predicted1 - mean(dfpred$predicted1)
dfpred$predicted2 <- dfpred$predicted2 - mean(dfpred$predicted2)

# représentation des splines
plot1 <- ggplot(dfpred) + 
  geom_raster(aes(x = X, y = Y, fill = predicted1)) + 
  geom_point(aes(x = X, y = Y),
             size = 0.2, alpha = 0.4,
             color = 'black', data = dataset)+
  geom_contour(aes(x = X, y = Y, z = predicted1),binwidth = 0.1, 
               color = 'white', linetype = 'dashed') + 
  geom_text_contour(aes(x = X, y = Y, z = predicted1), 
                    color = 'white', binwidth = 0.1)+
  scale_fill_viridis() +
  coord_cartesian() + 
  theme(axis.title= element_blank(),
        axis.text = element_blank(),
        axis.ticks =  element_blank()
        ) + 
  labs(subtitle = 'spline de base')

plot2 <- ggplot(dfpred) + 
  geom_raster(aes(x = X, y = Y, fill = predicted2)) + 
  geom_point(aes(x = X, y = Y),
             size = 0.2, alpha = 0.4,
             color = 'black', data = dataset)+
  geom_contour(aes(x = X, y = Y, z = predicted2),binwidth = 0.1, color = 'white', linetype = 'dashed') + 
  geom_text_contour(aes(x = X, y = Y, z = predicted2), color = 'white', binwidth = 0.1)+
  scale_fill_viridis() +
  coord_cartesian()+ 
  theme(axis.title= element_blank(),
        axis.text = element_blank(),
        axis.ticks =  element_blank()
        ) + 
  labs(subtitle = 'spline plus complexe')

ggarrange(plot1, plot2, nrow = 1, ncol = 2, common.legend = TRUE, legend = 'bottom')
```

Or, il s'avère que les deux *splines* spatiales sont très similaires. Par conséquent, il est vraissemblablement plus pertinent de conserver la plus simple des deux. Notez que le Mont-Royal (cercle central) est caractérisé par des valeurs plus faibles d'îlots de chaleur alors que les quartiers centraux situés un peu plus au Nord sont au contraire marqués par des pourcentages d'îlots de chaleur supérieures de 20 points de pourcentage en moyenne.

### GAMM {#sect1121}

Bien entendu, il est possible de combiner les modèles généralisés additifs (GAM) avec les modèles à effet mixtes (GLMM) abordés dans les sections précédentes. Dénommés modèles généralisés additifs à effets mixtes (GAMM), ils peuvent facilement être mise en œuvre avec **mgcv**.

#### GAMM et interceptes aléatoires

Pour définir un intercepte aléatoire, il suffit d'utiliser la notation `s(var, bs = 're')` avec `var` une variable nominale. Reprenons l'exemple précédent, mais avec cette fois-ci les arrondissements comme un intercepte aléatoire.

```{r message=FALSE, warning=FALSE}
dataset$Arrond <- as.factor(dataset$Arrond)
model4 <- gam(hot ~
          A65Pct + A014Pct + PopFRPct + PopMVPct +
          s(prt_veg)  + s(Arrond, bs = "re"),
        data = dataset, family = betar(link = "logit"))
```

L'enjeu est ensuite d'extraire la variance propre à cet effet aléatoire ainsi que les valeurs des interceptes pour chaque arrondissement.

```{r message=FALSE, warning=FALSE}
gam.vcomp(model4)
```

On constate donc que l'écart type de l'effet aléatoire des arrondissements est de 0.39, ce qui signifie que les effets de chaque arrondissement seront compris à 95% entre -1,17 et 1,17 sur l'échelle du prédicteur linéaire. **FAUDRAIT EXPLIQUER COMMENT TU OBTIENS +-1.17**. Pour extraire les interceptes spécifiques de chaque arrondissement, on peut utiliser alors la fonction `get_random` du package **itsadug**.

```{r message=FALSE, warning=FALSE}
library(itsadug)
values <- get_random(model4)[[1]]
df <- data.frame(
  ri = as.numeric(values),
  arrond = names(values)
)

ggplot(df) + 
  geom_point(aes(x = ri, y = reorder(arrond, ri))) + 
  geom_vline(xintercept = 0, color = "red") + 
  labs(y = "Arrondissement", x = "intercepte aléatoire")
```
On constate ainsi que pour une partie des arrondissements, la densité d'îlot de chaleur est systématiquement supérieure à la moyenne régionale représentée ici par la ligne rouge (0 = effet moyen pour tous les arrondissements). Il convient alors d’améliorer ce graphique en ajoutant le niveau d'incertitude associé à chaque intercepte. Pour ce faire, nous utilisons la fonction `extract_random_effects` du package **mixedup**. Notez que ce package n'est actuellement pas disponible sur CRAN et doit être téléchargé sur **github** avec la commande suivante :

```{r message=FALSE, warning=FALSE, eval = FALSE}
remotes::install_github('m-clark/mixedup')
```

On peut ensuite procéder à l'extraction des effets aléatoires et les représenter à nouveau.

```{r message=FALSE, warning=FALSE}
library(mixedup)
df_re <- extract_random_effects(model4, re = "Arrond")

ggplot(df_re) + 
  geom_errorbarh(aes(xmin = lower_2.5, xmax = upper_97.5, y = reorder(group, value))) +
  geom_point(aes(x = value, y = reorder(group, value))) + 
  geom_vline(xintercept = 0, color = "red") + 
  labs(y = "Arrondissement", x = "intercept aléatoire")
```

Cela permet de distinguer quels écarts sont significativement différents de 0 au seuil de 95%. Puisque nous utilisons ici la distribution *beta* et une fonction de lien logistique, nous devons utiliser des prédictions pour simplifier l'interprétation des coefficients. Nous allons ici fixer toutes les variables à leur moyenne respective sauf l'arrondissement et calculer les prédictions dans l'échelle originale (0 à 1).

```{r message=FALSE, warning=FALSE}
dfpred <- data.frame(
  A65Pct = mean(dataset$A65Pct),
  A014Pct = mean(dataset$A014Pct),
  PopFRPct = mean(dataset$PopFRPct),
  PopMVPct = mean(dataset$PopMVPct),
  prt_veg = mean(dataset$prt_veg),
  Arrond = as.character(unique(dataset$Arrond))
)

# calculer les prédictions pour le prédicteur linéaire
dfpred$preds <- predict(model4, newdata = dfpred, type = "link")

# calculer l'intervalle de confiance en utilisant les valeurs
# extraites avec extract_random_effects
dfpred <- dfpred[order(dfpred$Arrond),]
df_re <- df_re[order(df_re$group),]

dfpred$lower <- dfpred$preds - 1.96*df_re$se
dfpred$upper <- dfpred$preds + 1.96*df_re$se

# il nous reste juste à reconvertir le tout dans l'unité d'origine
# en utilisant l'inverse de la fonction logistique
inv.logit <- function(x){exp(x)/(1+exp(x))}

dfpred$lower <- inv.logit(dfpred$lower)
dfpred$upper <- inv.logit(dfpred$upper)
dfpred$preds <- inv.logit(dfpred$preds)

ggplot(dfpred) + 
  geom_errorbarh(aes(xmin = lower, xmax = upper, y = reorder(Arrond, preds))) +
  geom_point(aes(x = preds, y = reorder(Arrond, preds))) + 
  geom_vline(xintercept = mean(dfpred$preds), color = "red") + 
  labs(y = "Arrondissement", x = "intercepte aléatoire")

```

On constante ainsi que, pour une hypothétique aire de diffusion moyenne, la différence de densité d'îlot de chaleur peut être de 0,32 (32% de la surface de l'AD) entre les arrondissements Verdun et Dollard-des-Ormeaux.

#### GAMM et coefficients aléatoires

En plus des interceptes aléatoires, il est aussi possible de définir des coefficients aléatoires. Reprenons notre exemple et tentons de faire varier l'effet de la variable `PopFRPct` en fonction de l'arrondissement.

```{r message=FALSE, warning=FALSE}
model5 <- gam(hot ~
          A65Pct + A014Pct + PopFRPct + PopMVPct +
          s(prt_veg)  + s(Arrond, bs = "re") + 
          s(PopFRPct, Arrond, bs = "re"),
        data = dataset, family = betar(link = "logit"))
```

Notez ici une distinction importante ! Le modèle n'assume aucune corrélation entre les coefficients aléatoires pour la variable `PopFRPct` et pour l'intercepte aléatoire. Il est assumé que ces deux effets proviennent de deux distributions normales distinctes. En d'autres termes, le modèle ne dispose pas des paramètres nécessaires pour vérifier si les arrondissements avec les interceptes les plus forts (avec des densités supérieures d'îlot de chaleur) sont aussi des arrondissements dans lesquels l'effet de la variable `PopFRPct` est plus prononcée (et vice-versa). Pour plus d'informations sur cette distinction, référez-vous à la section \@ref(sect0724).

```{r message=FALSE, warning=FALSE}
AIC(model4, model5)
```

Ce dernier modèle présente une valeur d'*AIC* plus faible et serait donc ainsi mieux ajusté que notre modèle avec seulement un intercepte aléatoire. Nous pouvons donc extraire les coefficients aléatoires et les représenter.

```{r message=FALSE, warning=FALSE}
df_re <- extract_random_effects(model5)
df_re <- subset(df_re, df_re$effect == 'PopFRPct')

ggplot(df_re) + 
  geom_errorbarh(aes(xmin = lower_2.5, xmax = upper_97.5, y = reorder(group, value))) +
  geom_point(aes(x = value, y = reorder(group, value))) + 
  geom_vline(xintercept = 0, color = "red") + 
  labs(y = "Arrondissement", x = "coefficient aléatoire")
```

**LES VALEURS NE PASSENT PAS DANS TON CODE**
On constate notamment que seuls trois arrondissements ont des coefficients aléatoires significativement différents de 0. Ainsi, pour les arrondissements Anjou et Plateau-Mont-Royal, les coefficients aléatoires sont respectivement de `r df[df$group == "Anjou",]$value[[1]]` et `r df[df$group == "Le Plateau-Mont-Royal",]$value[[1]]`, et viennent donc se retrancher à la valeur moyenne régionale de `r round(model5$coefficients[[4]],4)` qui atteint alors presque 0. Du point de vue de l'interprétation, on peut en conclure que le groupe des personnes à faible revenu ne subit pas de sur-exposition aux îlots de chaleur à l'échelle des AD dans ces arrondissements.

En revanche, dans l'arrondissement Mercier-Hochelaga-Maisonneuve, la situation est à l'inverse plus systématiquement en défaveur des populations à faible revenu, avec une taille d'effet prêt de deux fois supérieure à la moyenne régionale.

**JE NE COMPRENDS PAS LE 2 FOIS**

::: {.bloc_aller_loin data-latex=""}
**Des effets aléatoires plus complexes dans les GAMM**

Il est possible de spécifier des GAMM avec des effets aléatoires plus complexes autorisant par exemple des corrélations entre les différents effets / niveaux. Il faut pour cela utiliser la fonction `gamm` de **mgcv** ou la fonction `gamm4` du package **gamm4**. La première offre plus de flexibilité, mais la seconde est plus facile à utiliser et doit être privilégiée quand un modèle comporte un très grand nombre de groupes dans un effet aléatoire, ou lorsque la distribution du modèle n'est pas gaussienne. La fonction `gamm` permet ajuster des modèles non gaussiens, mais elle utilise une approche appelée *PQL* (*Penalized Quasi-Likelihood* en anglais) connue pour être moins stable et moins précise.

Cependant, dans l'exemple de cette section, nous utilisons un modèle GAMM avec une distribution *beta*, ce qui n'est actuellement pas supporté par les fonctions `gamm` et `gamm4`. Pour un modèle GAMM plus complexe utilisant une distribution *beta*, il serait nécessaire d'utiliser le *package* **gamlss**, mais ce dernier utilise aussi une approche de type PQL. Nous montrons tout de même ici comment ajouter un modèle qui inclurait une corrélation entre les deux effets aléatoires de l'exemple précédent. Notez ici que le terme `re` apparaissant dans la formule permet de spécifier un effet aléatoire en utilisant la syntaxe du *package* **nlme**. Plus spécifiquement, **gamlss** fait un pont avec **nlme** et utilise son algorithme d'ajustement au sein de ces propres routines. De même, le terme `pb` permet de spécifier une *spline* de lissage dans le même esprit que **mgcv**. Il est également possible d'utiliser le terme `ga` faisant le lien avec **mgcv** et de profiter de sa flexibilité dans **gamlss**.

```{r message=FALSE, warning=FALSE}
library(gamlss)
library(gamlss.add)

model6 <- gamlss(hot ~
          pb(prt_veg) + 
          re(fixed = ~ A65Pct + A014Pct + PopFRPct + PopMVPct, 
             random = ~(1 + PopFRPct)|Arrond),
        data = dataset, family = BE(mu.link = "logit"))
```

On peut ensuite accéder à la partie du modèle qui nous intéresse, soit celle concernant les effets aléatoires.

```{r message=FALSE, warning=FALSE}
randomPart <- model6$mu.coefSmo[[2]]
print(randomPart)
```

À lecture de la partie du résumé consacrée aux résultats pour les effets aléatoires de ce résumé, on constate que la corrélation entre les interceptes aléatoires et les coefficients aléatoires est de -0,65. Cela signifie que pour les arrondissements avec des interceptes élevés (plus grande proportion d'îlots de chaleur), l'effet de la variable `PopFRPct` tend à être plus faible. Autrement dit, dans les arrondissements avec beaucoup d'îlots de chaleur, les personnes à faible revenu ont tendance à être moins exposées, tel qu'illustré à la figure \@ref(fig:corr_random).

```{r corr_random, fig.align='center', fig.cap="Relation entre les effets aléatoires des arrondissements et la variable population à faible revenu", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='75%', fig.pos = "H", out.extra = "",auto_pdf = TRUE}

df <- ranef(randomPart)
df$arrond <- rownames(df)
names(df) <- c('Intercept', 'PopFRPct', 'Arrondissement')

ggplot(df) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_vline(xintercept = 0, color = "red") +
  geom_point(aes(x = Intercept, y = PopFRPct))
```


:::

<!--chapter:end:11-GAM.Rmd-->

# Annexes {#annexes}

## Tableau des valeurs critiques de khi^2^ {#annexe1}

La courte syntaxe ![](Images/Rlogo.png) ci-dessous permet de générer un tableau avec les valeurs critiques du khi^2^ pour différents degrés de signification (valeurs de p).

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(stargazer)

# vecteur pour les degrés de liberté de 1 à 30, puis 40 et 50
dl <- c(1:30, 40, 50, 100, 250, 500) 
# la fonction qchisq permet d'obtenir la valeur théorique en fonction 
# d'une valeur de p et d'un nombre de degrés de liberté
tableChi2 <- cbind(dl,
                p0.05 =  round(qchisq(p=0.95,  df=dl, lower.tail = FALSE),3),
                p0.01 =  round(qchisq(p=0.99,  df=dl, lower.tail = FALSE),3),
                p0.001 = round(qchisq(p=0.999, df=dl, lower.tail = FALSE),3))
# Impression du tableau avec la library stargazer
stargazer(tableChi2, type="text", summary=FALSE, rownames=FALSE, align = TRUE, digits = 3,
          title="Distribution des valeurs critiques du Khi2")
```


```{r tableCritiqueChi2, echo=FALSE, message=FALSE, warning=FALSE}
dl <- c(1:30, 40, 50, 100, 250, 500)
tableChi2 <- cbind(dl,
                p0.05 =  round(qchisq(p=0.95,  df=dl, lower.tail = FALSE),3),
                p0.01 =  round(qchisq(p=0.99,  df=dl, lower.tail = FALSE),3),
                p0.001 = round(qchisq(p=0.999, df=dl, lower.tail = FALSE),3))

# knitr::kable(
#   head(tableChi2, n=nrow(tableChi2)), booktabs = TRUE,
#   col.names=c("dl", "p=0,05", "p=0,01", "p=0,001"),
#   caption = 'Distribution des valeurs critiques du khi2'
#   )

show_table(tableChi2, 
           caption = "Distribution des valeurs critiques du khi2",
           col.names=c("dl", "p=0,05", "p=0,01", "p=0,001")
)
```

## Tableau des valeurs critiques de F {#annexe2}

La courte syntaxe ![](Images/Rlogo.png) ci-dessous permet de générer un tableau avec les valeurs critiques de F avec _p_=0,05.

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(stargazer)

dl1 <- c(1:10, 15, 20, 25, 30, 50, 100, 500)
dl2 <- c(1:30, 40, 50, 100)
matrice <- matrix(ncol=length(dl1), nrow=length(dl2), byrow = TRUE)
for(r in 1:length(dl1)){
  for(c in 1:length(dl2)){
    matrice[c,r] <- round(qf(p=0.05, dl1[r], dl2[c], lower.tail = FALSE),2)
  }
}

tableF_p0.05 <- as.data.frame(matrice, row.names = paste0("dl2=",dl2, sep=""))
colnames(tableF_p0.05) <- paste0("dl1=",dl1, sep="")

stargazer(tableF_p0.05, type="text", summary=FALSE, rownames=FALSE, align = TRUE, digits = 3,
          title="Distribution des valeurs critiques de F avec p=0,05")
```


```{r tableCritiqueF, echo=FALSE, message=FALSE, warning=FALSE}
dl1 <- c(1:10, 15, 20, 25, 30, 50, 100, 500)
dl2 <- c(1:30, 40, 50, 100)
matrice <- matrix(ncol=length(dl1), nrow=length(dl2), byrow = TRUE)
for(r in 1:length(dl1)){
  for(c in 1:length(dl2)){
    matrice[c,r] <- round(qf(p=0.05, dl1[r], dl2[c], lower.tail = FALSE),2)
  }
}

tableF_p0.05 <- as.data.frame(matrice, row.names = paste0("dl2=",dl2, sep=""))
colnames(tableF_p0.05) <- paste0("dl1=",dl1, sep="")

# knitr::kable(
#   head(tableF_p0.05, n=nrow(tableF_p0.05)), booktabs = TRUE,
#   row.names = TRUE,
#   caption = 'Distribution des valeurs critiques de F avec p=0,05'
# )

show_table(tableF_p0.05, 
           caption = "Distribution des valeurs critiques de F avec p=0,05")
```


## Tableau des valeurs critiques de *t* {#annexe3}

La courte syntaxe ![](Images/Rlogo.png) ci-dessous permet de générer un tableau avec les valeurs critiques de T avec _p_=0,05, 0,01 et 0,01.


```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(stargazer)

# vecteur pour les degrés de liberté de 1 à 30, puis 40 et 50
dl <- c(1:30, 40, 50, 60, 70, 80, 90, 100, 250, 500, 1000, 2500) 
# la fonction qchisq permet d'obtenir la valeur théorique en fonction 
# d'une valeur de p et d'un nombre de degrés de liberté
tableT <- cbind(dl,
                p0.10 =  round(qt(p=1 - (0.10/2),  df=dl),2),
                p0.05 =  round(qt(p=1 - (0.05/2),  df=dl),2),
                p0.01 =  round(qt(p=1 - (0.01/2),  df=dl),2),
                p0.001 = round(qt(p=1 - (0.001/2), df=dl),2))
# Impression du tableau avec la library stargazer
stargazer(tableT, type="text", summary=FALSE, rownames=FALSE, align = TRUE, digits = 2,
          title="Distribution des valeurs critiques de t")
```


```{r tableCritiqueT, echo=FALSE, message=FALSE, warning=FALSE}
# vecteur pour les degrés de liberté de 1 à 30, puis 40 et 50
dl <- c(1:30, 40, 50, 60, 70, 80, 90, 100, 250, 500, 1000, 2500) 
# la fonction qchisq permet d'obtenir la valeur théorique en fonction 
# d'une valeur de p et d'un nombre de degrés de liberté
tableT <- cbind(dl,
                p0.10 =  round(qt(p=1 - (0.10/2),  df=dl),2),
                p0.05 =  round(qt(p=1 - (0.05/2),  df=dl),2),
                p0.01 =  round(qt(p=1 - (0.01/2),  df=dl),2),
                p0.001 = round(qt(p=1 - (0.001/2), df=dl),2))

# Impression du tableau avec la library stargazer
# stargazer(tableT, type="text", summary=FALSE, rownames=FALSE, align = TRUE, digits = 3,
          # title="Distribution des valeurs critiques du t")

show_table(tableT, 
           caption = "Distribution des valeurs critiques de t")
```

<!--chapter:end:19-Annexes.Rmd-->

