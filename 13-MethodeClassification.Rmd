# Méthodes de classification non-supervisées {#chap13}

Dans le cadre de ce chapitre, nous présentons les méthodes les plus couramment utilisées pour explorer la présence de groupes homogènes au sein d'un jeu de données. Elles portenent le nom de classifications non-supervisées car elles visent à attribuer les observations similaires d'un jeu de données à des classes / groupes distincts entre eux. Non-supervisé signifie que ces classes / groupes ne sont pas connus à priori et doivent être déterminés à partir des données. Ces méthodes peuvent être vues comme une façon de réduire le nombre d'observation d'un jeu de données à un ensemble d'observations synthétiques, représentant le mieux possible la population étudiée.

::: {.bloc_package data-latex=""}
Dans ce chapitre, nous utiliserons principalement les packages suivants : 

* Pour créer des graphiques :
  - **ggplot2**, le seul, l'unique
  - **ggpubr** pour combiner des graphiques et réaliser des diagrammes
  
* Pour classifier des observations :
  - **geocmens** pour investiguer le résultat de classification floues

:::

## Un aperçu sur la multitude des méthodes de classications {#sect131}

La processus de classification d'observation est très utilisé en sciences sociales. Son objectif est d'identifier des groupes cohérents au sein d'un ensemble d'observations. Ces groupes peuvent ensuite être analysés et nous renseigner sur les caractéristiques communes partagées par les individus qui les composent. Un bon exemple est l'identification de profils de consommateur.rice.s à partir de données issues d'une enquête. En créant des groupes homogènes de consommateur.rice.s il est possible de déterminer des portraits types utilisés en marketing. Un second exemple serait de regrouper les secteurs d'une ville selon leurs caractéristiques environnementales (présence de végétation, niveau de bruit, pollution atmosphérique, accessibilité aux aménités, etc.) et socio-économiques (part de la population à faible revenu, minorités visibles, diplômée, etc.) pour identifier des types de secteurs cumulant des enjeux sociaux et environnementaux.

Il existe un grand nombre de méthodes de classification généralement regroupées dans plusieurs familles imbriquées.

La première distinction à connaître est entre les méthodes **supervisées** et **non-supervisées**. Pour les premières, les catégories / groupes / classes des observations sont connues à l'avance. Pour ces méthodes l'enjeu n'est pas de trouver les catégories (car elles sont connues), **mais de déterminer des règles ou un modèle permettant d'attribuer des observations à ces catégories**. Nous n'abordons pas ces méthodes dans ce chapitre dédié aux cas des méthodes de classification non-supervisées. Pour ces dernières, les catégories ne sont pas connues à l'avance et l'enjeu est de faire **ressortir des structures de groupes propres aux données**. Notez également qu'à la frontière entre ces deux familles se situent les méthodes de classification semi-supervisées. Il s'agit de cas spécifiques où des informations partielles sont connues sur les groupes à détecter : le groupe final de certaines observations seulement est connu, certaines observations sont supposées appartenir à un même groupe même s'il est indéfini en lui-même [@bair2013semi].

La seconde distinction à connaître est entre les méthodes **strictes** et **floues**. Les premières ont pour objectif d'assigner chaque observation à une et une seule catégorie, alors que les secondes décrivent l'appartenance de chaque observation à chaque catégorie. En termes de données, cela signifique que pour les méthodes strictes, le groupe d'appartenance d'une observation peut être contenu dans une simple variable catégorielle (une colonne d'un *dataframe*). Pour les méthodes floues, il est nécessaire de disposer de plusieurs variables continues (plusieurs colonnes numériques d'un *dataframe*), soit une par groupe, pour stoquer l'appartenance de chaque observation à chacun des groupes.

```{r floueVSstrict, echo=FALSE, fig.align='center', fig.cap="Classification stricte et floue", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='40%'}
library(dplyr)
knitr::include_graphics('images/classification/floueVSstricte.png', dpi = NA)
```

Il existe donc des méthodes supervisées floues, supervisées strictes, non-supervisées floues, non-supervisées floues et les méthodes semi-supervisées. Une partie de ces méthodes sont illustrées à la figure \@ref(fig:methoClassif) [@gelb2021apport].

```{r methoClassif, echo=FALSE, fig.align='center', fig.cap="Synthèse des principales méthodes de classification (Gelb et Apparicio 2021)", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='60%'}
library(dplyr)
knitr::include_graphics('images/classification/syntheseClassif.png', dpi = NA)
```

Parmis les méthodes de classifications supervisées floues, notez que nous avons déjà abordé la régression multinomiale dans le chapitre sur les GLM (REF).

Dans ce chapitre, nous abordons les trois méthodes les plus classiques et les plus faciles à mettre en oeuvre soit : la classification ascendante hiérarchique, les nuées dynamiques strictes (k-means et k-median) et nuées dynamiques floues (c-means et c-median). Ces méthodes de classification non-supervisées peuvent être appliquées avec deux approches théoriques différentes : exploratoire ou confirmatoire.

* exploratoire : les groupes sont inconnus à l'avance à la fois dans les données et sur le plan théorique. L'objectif de la classification est alors de "faire parler les données", d'identifier des groupes et d'ensuite construire une réflexion autours des résultats obtenus. Il est possible de faire des parallèles entre cette approche et la théorie ancrée (*grounded-theory*), inductive, visant à construire des théories à partir des données plutôt que de tester des théories établies à priori avec des données. Attention cependant à bien noter qu'une approche exploratoire ne permet pas de tester et de valider la théorie construite. En effet, il est nécessaire de collecter de nouvelles données dans une démarche cette fois-ci confirmatoire pour vérifier que la théorie établie précédemment est valide. Le risque en approche exploratoire est de construire des théories qui ne seraient pas supportées par de nouvelles données, notamment du fait de corrélations illusoires où du simple hasard. En étudiant un échantillon suffisamment grand, il est certains que des relations apparaîtront, mais se pose alors la question de la pertinence de ces relations. C'est pourquoi nous recommandons d'éviter autant que possible d'étudier des données sans cadre théorique.

* confirmatoire : les groupes sont inconnus dans les données, mais le cadre théorique de la recherche prévoit d'observer certains groupes du fait par exemple de travaux antérieur. Dans ce contexte, l'objectif est de vérifier si le cadre théorique utilisé est valide, soit que les résultats de la classification vont dans le sens de la théorie. 

## Concepts centraux en classifications

Avant d'aborder les méthodes de classification non-supervisées, il est nécessaire de définir ici deux concepts centraux, la **distance** et l'**inertie**.

### Distance

La distance en analyse de données est définie comme une fonction (*d*) permettant de déterminer à quel point deux observations sont différentes ou similaires l'une de l'autre. Elle doit satisfaire un ensemble de conditions, soit : 

* non négativité : la distance minimale entre deux objets est 0; $d(x,y) \geq 0$
* identité des indiscernables : la distance entre deux objets X et Y est 0 seulement si X = Y; $d(x,y)=0\text{ si et seulement si }x=y$
* symétrie : la distance entre X et Y est la même qu'entre Y et X; $d(x,y) = d(y,x)$
* triangle d'inégalité : passer d'un point X à un point Z est toujours égale ou plus court que de passer par Y entre X et Z; $d(x,z)\leq d(x,y)+d(y,z)$

il existe un grand nombre de distances que l'on peut utiliser pour déterminer le niveau de similarité entre des observations, nous en présentons ici les six plus fréquentes en sciences sociales, mais sachez que bien d'autres existent.

#### La distance Euclidienne

Il s'agit vraissemblablement de la distance la plus couramment utilisée, soit la longueur de la ligne droite la plus courte entre les deux objets considérés. Pour la représenter, admettons que nous nous intéressons à trois classes d'étudiant.e.s A, B et C pour lesquelles nous avons calculé la moyenne de leurs notes dans les cours de méthodes quantitatives et qualitatives. Ces deux variables sont mesurées dans la même unité et varient de 0 à 100.

Voici un graphique représentant la situation : 

```{r dist0, echo=FALSE, fig.align='center', fig.cap="Situation de base pour le calcul de distance", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='50%'}

df <- data.frame(
  "classe" = c("A","B","C"),
  "quantitative" = c(85,82,79),
  "qualitative" = c(80,78,77)
)

ggplot(df) + 
  geom_point(aes(x = quantitative, y = qualitative, color = classe), size = 2) + 
  scale_color_manual(values = c("A" = "#E73D3D", "B" = "#3CE73C", "C" = "#3C3CE7")) + 
  labs(x = "score moyen en méthodes quantitatives",
       y = "score moyen en méthode qualitatives")

```
Les distances Euclidiennes entre les classes B et C et les classes C et A sont donc : 


```{r dist1, echo=FALSE, fig.align='center', fig.cap="Représentation de la distance Euclidienne", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='50%'}

df2 <- data.frame(
  xstart = c(79,79),
  xend = c(82,85),
  ystart = c(77,77),
  yend = c(78,80)
)

ggplot(df) + 
  geom_segment(data = df2, mapping = aes(x = xstart, y = ystart, 
                                         xend = xend, yend = yend))+
  geom_point(aes(x = quantitative, y = qualitative, color = classe), size = 2) + 
  scale_color_manual(values = c("A" = "#E73D3D", "B" = "#3CE73C", "C" = "#3C3CE7")) + 
  labs(x = "score moyen en méthodes quantitatives",
       y = "score moyen en méthode qualitatives")

```
On peut constater que la distance entre les classes C et B est plus petite qu'entre les classes A et C, ce qui indiquent que les deux premières se ressemblent davantage.

La formule de la distance Euclidienne est la suivante : 

$$d(a,b) = \sqrt{\sum{}^c_{i=1}(a_i-b_i)^2}$$

soit tout simplement la racine carrée de la somme des écarts au carré pour chacune des variables décrivant les observations *a* et *b*.

Nous pouvons facilement les calculer à la main ici : 

* Distance A-C : $\sqrt{(85-79)^2+(80-77)^2} = 6.71$
* Distance B-C : $\sqrt{(82-79)^2+(78-77)^2} = 3,16$

::: {.bloc_attention data-latex=""}
**Distance et unité de mesure : ** Il est très important de garder à l'esprit que la distance entre deux observations dépend directement des unités de mesure utilisées. Ceci est problématique car il est rare que toutes les variables utilisées pour décrire des observations utilisent la même échelle. Ainsi, une variable dont les valeurs numériques seraient arbitrairement plus grandes risquerait de déséquilibrer un calcul de distance. À titre d'exemple, une variable mesurée en m plutôt qu'en km produirait des distances euclidiennes 1000 fois plus grandes.

Il est donc nécessaire de standardiser les varibales utilisées avant de calculer des distances. Cette opération permet de convertir les variables originales vers une échelle commune. Il existe plusieurs opérations possibles : 

* centrage et réduction : cette méthode revient à soustraire à chaque variable sa moyenne, puis à la diviser par son écart-type. La nouvelle variable obtenue s'exprime alors en écart-type (assi appelé score-Z). La formule de la transformation est : $f(x) = \frac{x - \bar{x}}{\sigma_x}$, avec $\bar{x}$ la moyenne de $x$ et $\sigma_x$ l'écart-type de x.
* mise à l'échelle de 0 à 1 : cette méthode permet de changer l'étendue d'une variable pour que son maximum soit 1 et que son minimum soit 0. La formule de cette transformation est : $f(x) = \frac{x-min(x)}{max(x)-min(x)}$.
* la transformation en rang : cette méthode consiste à remplacer les valeurs d'une variable par leur rang. La valeur la plus faible est remplacée par 1, et la plus forte par *n* (nombre de valeurs). Notez que cette transformation modifie la distribution de la variable originale contrairement aux deux transformations précédentes. Cette propriété peut être désirable si les écarts absolus entre les valeurs ont peu d'importance, si la variables n'a pas été mesurée avec précision ou encore si des valeurs extrèmes sont présentes.
* la transformation en percentile : cette méthode consiste à remplacer les valeurs d'une variable par leur percentile correspondant. Elle peut être vue comme une standaridsation de la transformation en rang car elle ne dépend pas du nombre d'observations.

La figure suivante montre l'impact de ces transformation sur l'histogramme d'une variable.

```{r impactTransform, echo=FALSE, fig.align='center', fig.cap="Impacts de différentes transformation sur la distribution d'une variable", fig.pos="H", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='80%'}

x <- rgamma(10000, 0.95,0.1)
dfx <- data.frame(
  x = x,
  xstd = (x - mean(x)) / sd(x),
  x_01 = (x-min(x)) / (max(x) - min(x)),
  x_rang = rank(x,ties.method = "min"),
  x_prt = trunc(rank(x,ties.method = "average"))/length(x)
)

x2 <- reshape2::melt(dfx)

x2$variable <- case_when(x2$variable == "x" ~ "1-originale",
                         x2$variable == "xstd" ~ "2-centrée-réduite",
                         x2$variable == "x_01" ~ "3-mise à l'échelle 0-1",
                         x2$variable == "x_rang" ~ "4-rangs",
                         x2$variable == "x_prt" ~ "5-percentiles",
                         )

ggplot(x2) + 
  geom_histogram(aes(x = value), bins = 50, color = "white") + 
  facet_wrap(vars(variable), ncol=2, scales = "free")

```
:::

#### La distance de Manhattan

Cette seconde distance est égalemment courramment utilisée. Elle doit son nom au réseau de rue de l'île de Manhattan suivant un plan quadrillé. La distance de Manhattan correspond à la somme des écarts absolus entre les valeurs des différentes variables décrivant les observations. La figure \@ref(fig:dist2) l'illustre avec le même exemple que précédemment.

```{r dist2, echo=FALSE, fig.align='center', fig.cap="Représentation de la distance de Manhattan", message=FALSE, warning=FALSE, auto_pdf=TRUE, out.width='50%'}

df2 <- data.frame(
  xstart = c(79,82,79,79),
  xend = c(82,82,79,85),
  ystart = c(77,77,77,80),
  yend = c(77,78,80,80)
)

ggplot(df) + 
  geom_segment(data = df2, mapping = aes(x = xstart, y = ystart, 
                                         xend = xend, yend = yend))+
  geom_point(aes(x = quantitative, y = qualitative, color = classe), size = 2) + 
  scale_color_manual(values = c("A" = "#E73D3D", "B" = "#3CE73C", "C" = "#3C3CE7")) + 
  labs(x = "score moyen en méthodes quantitatives",
       y = "score moyen en méthode qualitatives")

```
La formule de la distance de Manhattan est la suivante : 

$$d(a,b) = \sum{}^c_{i=1}(|a_i-b_i|)$$

soit tout simplement la somme des écarts absolus entre les variables décrivant les observations *a* et *b*.

La distance de Manhattan doit être privilégiée à la distance Euclidienne lorsque les données considérées ont un très grand nombre de dimensions (variables). En effet, lorsque le nombre de variables augmente (on parle ici de plus de 30 variables), la distance Euclidienne tend à être grande pour toutes les observations et à moins bien discriminer les observations proches et lointaines les unes des autres. Du fait de sa nature additive, la distance de Manhattan est moins sujette à ce problème [@aggarwal2001surprising].

### Inertie

Un concept important à saisir dans le cadre des méthodes de classification non-supervisées et le concept d'**inertie** d'un jeu de données. Il est proche du concept de variance qui a été présenté dans le chapitre sur la statistique univariée (REF).

L'inertie est une quantité permettant de décrire la dispersion des observations d'un jeu de données. Cette mesure dépend à la fois des données (nombres d'observations, nombre de variables, échelle des variables) et de la mesure de distance retenu entre deux observations.


## Combinaisons méthodes factorielles et méthodes de classications {#sect132}

On raconte quoi ici ?

## Classification ascendantes hiérachiques {#sect133}

## Nuées dynamiques  {#sect134}

Les méthodes des nuées dynamiques regroupent plusieurs algorithmes tous plus ou moin lié avec l'algorithme le plus connu : *k-means* originallement proposé par @macqueen1967some. Nous présentons également ici trois variantes du *k-means*, soit le *k-medians*, le *c-means* et le *c-medians*.

### k-means  {#sect1341}

Nous commençons ici par détailler le fonctionnement de cet algorithme afin de mieux le cerner. Au préalable l'algorithme suppose que certains éléments soit connus d'avance : 

* une matrice de données X comportant n lignes (nombre d'observations) et p colonnes (nombre de variables). Chaque variable de cette matrice doit être de type quantitative.
* 


### k-median  {#sect1342}
### Les extensions en logique floues : c-means, c-median  {#sect1343}
