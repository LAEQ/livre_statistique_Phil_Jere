# (PART) Analyse bivariée {-} 

# Analyses bivariées {#chap04}

Dans ce chapitre, nous présentons les principales méthodes exploratoires et confirmatoires bivariées permettant d'évaluer la relation entre deux variables, et ce, en fonction de leur type : deux variables quantitatives, deux variables qualitatives ou encore une variable quantitative _versus_ une variable qualitative (comprenant deux modalités ou plus de deux modalités) (figure \@ref(fig:fig1)). 

```{r fig1, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Les principales méthodes bivariées",  out.width='65%'}
knitr::include_graphics('images/bivariee/figure1.jpg', dpi = NA)
```

Plus spécifiquement, nous présentrons puis mettrons en œuvre dans le logiciel ![](images/logos/Rlogo.png) les méthodes suivantes : covariance, corrélation et régression linéaire simple (entre deux variables quantitatives, section \@ref(sect041)), tableau de contingence et test du khi^2^ (entre deux variables qualitatives, section \@ref(sect042)), t de student (test *t*) et test de Wilcoxon (entre une variable quantitative et une variable qualitative comprenant deux modalités, section \@ref(sect043)), et analyse de variance et test de Kruskal-Wallis (entre une variable quantitative et une variable qualitative comprenant plus de deux modalités, section \@ref(sect044)).

::: {.bloc_package .bloc_package_png data-latex="{blocs/package}"}
Dans cette section, nous utiliserons principalement les *packages* suivants : 

* Pour créer des graphiques :
  - **ggplot2**, le seul, l'unique
  - **ggpubr**, pour combiner des graphiques et réaliser des diagrammes quantiles-quantiles
* Pour manipuler des données : 
  - **dplyr**, avec les fonctions *group_by*, *summarize* et les pipes *%>%*
* Pour les corrélations (section \@ref(sect0411)) : 
  - **correlation**, de l'ensemble de package **easy_stats**, offrant une large gamme de méthodes de corrélations
  - **boot** pour réaliser des corrélations avec *bootstrap* 
  - **Hmisc** pour calculer des corrélations de Pearson et Spearman
  - **ppcor**, notamment pour des corrélations partielles
  - **psych** pour obtenir une matrice de  corrélation (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de p.
  - **stargazer** pour créer des beaux tableaux d’une matrice de corrélation en Html ou en LaTeX ou en ASCII.
  - **corrplot**, pour créer des graphiques de matrices de corrélation
* Pour le tableau de contignence (section \@ref(sect0412)) :
  - **gmodels**, pour construire des tableaux de contingence et calculer les tests *t* et ses différentes variantes ((section \@ref(sect0424)))
  - **vcd**, pour construire un graphique pour un tableau de contigence ((section \@ref(sect0424)))
* Pour les test *t* : 
  - **sjstats** pour réaliser des test *t* pondérés
  - **effectsize**, pour calculer les tailles d'effet de tests de *t*
* Pour la section sur les ANOVA (section \@ref(sect0441)) : 
  - **car**, pour les ANOVA classiques
  - *lmtest* pour le test de Breusch-Pagan d'homogénéité des variances 
  - **rstatix**, intégrant de nombreux tests classiques (comme le test de Shapiro) avec **tidyverse**
:::

## Relation linéaire entre deux variables quantitatives {#sect041}

::: {.bloc_objectif .bloc_objectif_png data-latex="{blocs/objectif}"}
**Deux variables continues varient-elles dans le même sens ou bien en sens contraire ?** Répondre à cette question est une démarche exploratoire classique en sciences sociales puisque les données socioéconomiques sont souvent associées linéairement. En d'autres termes, lorsque l'une des deux variables tant à augmenter, la seconde augmente également ou diminue systématiquement.

En études urbaines, on pourrait vouloir vérifier si certaines variables socioéconomiques sont associées positivement ou négativement à des variables environnementales jugées positives (comme la couverture végétale ou des mesures d’accessibilité spatiale aux parcs) ou négatives (pollutions atmosphériques et sonores). 

Par exemple, au niveau des secteurs de recensement d’une ville canadienne ou américaine, on pourrait vouloir vérifier si le revenu médian des ménages ou encore le coût moyen du loyer varient dans le même sens que la couverture végétale ; ou au contraire, en sens inverse des niveaux moyens de dioxyde d’azote ou de bruit routier.

Pour évaluer la linéarité entre deux variables continues, deux statistiques descriptives sont utilisées : la **covariance**\index{covariance} (section \@ref(sect0412)) et la **corrélation**\index{corrélation} (section \@ref(sect0413)).
:::


### Bref retour sur le postulat de la relation linéaire {#sect0411}

Vérifier le postulat de la linéarité consiste à évaluer si deux variables quantitatives varient dans le même sens ou bien en sens contraire. Toutefois, la relation entre deux variables quantitatives n’est pas forcément linéaire. En guise d'illustration, la figure \@ref(fig:fig2) permet de distinguer quatre types de relations :

* le cas **a** illustre une relation linéaire positive entre les deux variables puisqu’elles vont dans le même sens. Autrement dit, quand les valeurs de *X* augmentent, celles de *Y* augmentent aussi. En guise d'exemple, pour les secteurs de recensement d'une métropole donnée, il est fort probable que le coût moyen du loyer soit associé positivement avec le revenu médian des ménages. Graphiquement parlant, il est clair qu'une droite dans ce nuage de points résumerait efficacement la relation entre ces deux variables.

* le cas **b** illustre une relation linéaire négative entre les deux variables puisqu’elles vont en sens inverse. Autrement dit, quand les valeurs de *X* augmentent, celles de *Y* diminuent, et inversement. En guise d'exemple, pour les secteurs de recensement d'une métropole donnée, il est fort probable que le revenu médian des ménages soit associé négativement avec le taux de chômage. De nouveau, une droite résumerait efficacement cette relation.

* pour le cas **c**,  il y a une relation entre les deux variables, mais qui n’est pas linéaire. Le nuage de points entre les deux variables prend d’ailleurs une forme parabolique qui traduit une relation curvilinéaire. Concrètement, on observe une relation positive jusqu'à un certain seuil, puis une relation négative. 
 
* pour le cas **d**,  la relation entre les deux variables est aussi curvilinéaire; d'abord négative, puis positive.


```{r fig2, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Relations linéaires et curvilinéaires entre deux variables continues",  out.width='85%'}
knitr::include_graphics('images/bivariee/figure2.jpg', dpi = NA)
```

Prenons un exemple concret. Dans une étude portant sur l'équité environnementale et la végétation à Montréal, Pham *et al.* [-@PhamApparicioSeguin2012] ont montré qu'il existe une relation curvilinéaire entre l'âge médian des bâtiments résidentiels (axe des abscisses) et les couvertures végétales (axes des ordonnées) :

* la couverture de la végétation totale et celle des arbres augmentent quand l'âge médian des bâtiments croît jusqu'à atteindre un pic autour de 60 ans (autour de 1950). On peut supposer que les secteurs récemment construits, surtout ceux dans les banlieues, présentent des niveaux de végétation plus faibles. Au fur et au fur que le quartier vieillit, les arbres plantés lors du développement résidentiel deviennent matures — canopée plus importante –, d'où l'augmentation des valeurs de la couverture végétale totale et de celle des arbres.
*  Par contre, dans les secteurs développés avant les années 1950, la densité du bâti est plus forte, laissant ainsi moins de place pour la végétation, ce qui explique une diminution des variables relatives à la couverture végétale (figure \@ref(fig:fig3)).

```{r fig3, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Exemples de relations curvilinéaires",  out.width='65%'}
knitr::include_graphics('images/bivariee/figure3.jpg', dpi = NA)
```

Dans les sous-sections suivantes, nous décrirons deux statistiques descriptives et exploratoires – la covariance (section \@ref(sect0412)) et la corrélation (section \@ref(sect0413)) – utilisées pour évaluer la **relation linéaire** entre deux variables continues. Ces deux mesures permettent de mesurer le degré d'association entre deux variables, sans que l'une soit la variable dépendante (variable à expliquer) et l'autre, la variable dépendante (variable explicative). Puis, nous décrirons la régression linéaire simple (section \@ref(sect0414))  qui permet justement de prédire une variable dépendante (_Y_) à partir d'une variable indépendante (_X_).

### Covariance {#sect0412}

#### Formulation {#sect04121} 

La covariance\index{covariance} (eq. \@ref(eq:cov)), écrite $cov(x,y)$, est égale à la moyenne du produit des écarts des valeurs des deux variables par rapport à leurs moyennes respectives :


\begin{equation} 
cov(x,y) = \frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1} = \frac{covariation}{n-1}
(\#eq:cov)
\end{equation} 

avec $n$ étant le nombre d’observations; $\bar{x}$ et $\bar{y}$ (prononcez x et y barre) étant les moyennes respectives des variables *X* et *Y*.

#### Interprétation {#sect04122} 

Le numérateur de l'équation \@ref(eq:cov) représente la covariation\index{covariance}, soit la somme du produit des déviations des valeurs $x_{i}$ et $y_{i}$ par rapport à leurs moyennes respectives ($\bar{x}$ et $\bar{y}$). La covariance est donc la covariation divisée par le nombre d’observations, soit la moyenne de la covariation. Sa valeur peut être positive ou négative :  

* positive quand les deux variables varient dans le même sens, c'est-à-dire que lorsque les valeurs de la variable _X_ s'éloignent de la moyenne, les valeurs de _Y_ s'éloignent aussi dans le même sens; et négative pour une situation inverse. 
 * Quand la covariance est égale à 0, il n’y a pas de relation entre les variables _X_ et _Y_. Plus sa valeur absolue est élevée, plus la relation entre les deux variables *X* et *Y* est importante. 
 
Ainsi, la covariance\index{covariance} correspond à un centrage des variables, c’est-à-dire à soustraire à chaque valeur de la variable sa moyenne correspondante. L'inconvénient majeur de l'utilisation de la covariance est qu'elle est tributaire des unités de mesure des deux variables. Par exemple, si nous calculons la covariance entre le pourcentage de personnes à faible revenu et la densité de population (habitants au km^2^) au niveau des secteurs de recensement de la région métropolitaine de Montréal, nous obtenons une valeur de covariance de 34934. En revanche, si la densité de population est exprimée en milliers d'habitants au km^2^, la valeur de la covariance sera de 34,934, alors que la relation linéaire entre les deux variables reste la même tel qu'illustré à la figure \@ref(fig:fig4). Pour rémédier à ce problème, on privilégie l'utilisation du coefficient de corrélation.

```{r fig4, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Covariance et unités de mesure', out.width='75%'}
library("ggplot2")
library("ggpubr")
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
cov1 <- round(cov(df$FaibleRev,df$HabKm2),0)
cor1 <- round(cor(df$FaibleRev,df$HabKm2),3)
cov2 <- round(cov(df$FaibleRev,df$Hab1000Km2),3)
cor2 <- round(cor(df$FaibleRev,df$Hab1000Km2),3)
plot1 <- ggplot(data = df, mapping = aes(x=FaibleRev,y=HabKm2))+
  geom_point(colour="red")+
  labs(title=paste0("covariance = ", cov1), 
       subtitle = paste0("corrélation = ", cor1),
       caption = "Les traits pointillés indiquent les moyennes.")+
  xlab("Personnes à faible revenu (%)")+
  ylab(expression("Densité de population : habitants au"~km^{2}))+
  geom_vline(xintercept = mean(df$FaibleRev), colour="black", linetype="dashed", size=.5) +
  geom_hline(yintercept = mean(df$HabKm2), colour="black", linetype="dashed", size=.5) +
  stat_smooth(method="lm", se=FALSE)
plot2 <- ggplot(data = df, mapping = aes(x=FaibleRev,y=Hab1000Km2))+
  geom_point(colour="red")+
  labs(title=paste0("covariance = ", cov2), 
       subtitle = paste0("corrélation = ", cor2),
       caption = "Les traits pointillés indiquent les moyennes.")+
  xlab("Personnes à faible revenu (%)")+
  ylab(expression("Densité de population : 1000 habitants au"~km^{2}))+
  geom_vline(xintercept = mean(df$FaibleRev), colour="black", linetype="dashed", size=.5) +
  geom_hline(yintercept = mean(df$Hab1000Km2), colour="black", linetype="dashed", size=.5) +
  stat_smooth(method="lm", se=FALSE)
ggarrange(plot1, plot2, ncol = 2, nrow = 1)
```


### Corrélation {#sect0413}

#### Formulation {#sect04131} 
Le coefficient de corrélation de Pearson ($r$) est égal à la covariance (numérateur) divisée par le produit des écart-types des deux variables *X* et *Y* (dénominateur). Il représente une standardisation de la covariance. Autrement dit, le coefficient de corrélation repose sur un centrage (moyenne = 0) et une réduction (variance = 1) des deux variables, c’est-à-dire à soustraire à chaque valeur sa moyenne correspondante et à la diviser par son écart-type. Il correspond ainsi à la moyenne du produit des deux variables centrées réduites. Il s'écrit alors :

\begin{equation} 
r_{xy} = \frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{(n-1)\sqrt{\sum_{i=1}^n(x_i - \bar{x})^2(y_i - \bar{y})^2}}=\sum_{i=1}^n\frac{ZxZy}{n-1}
(\#eq:cor)
\end{equation}

La syntaxe ![](images/Rlogo.png) ci-dessous démontre que le coefficient de corrélation de Pearson est bien égal à la moyenne du produit de deux variables centrées-réduites.

```{r message=FALSE, warning=FALSE}
library("MASS")
N <- 1000      # nombre d'observations
moy_x <- 50   # moyenne de x
moy_y <- 40   # moyenne de y
sd_x <- 10    # écart-type de x
sd_y <- 8     # écart-type de y
rxy <- .80 # corrélation entre X et Y
## création de deux variables fictives normalement distribuées et corrélées entre elles
# Création d'une matrice de covariance
cov <- matrix(c(sd_x^2,  rxy*sd_x*sd_y, rxy*sd_x*sd_y, sd_y^2), nrow=2)
# Création du tableau de données avec deux variables
df <- as.data.frame(mvrnorm(N, c(moy_x, moy_y), cov))
# Centrage et réduction des deux variables
df$zV1 <- scale(df$V1, center = TRUE, scale = TRUE)
df$zV2 <- scale(df$V2, center = TRUE, scale = TRUE)
# Corrélation de Pearson
cor1 <- cor(df$V1, df$V2)
# Moyenne du produit des variables centrées-réduites
cor2 <- sum(df$zV1*df$zV2) / (nrow(df)-1)
cat("Corrélation de Pearson = ",round(cor1,5),
    "\nMoyenne du produit des variables centrées-réduites =", round(cor2,5))
```


#### Interprétation {#sect04132} 

Le coefficient de corrélation $r$ varie de −1 à 1 avec :

* 0 quand il n’y a pas de relation linéaire entre les variables _X_ et _Y_
* −1 quand il y relation linéaire négative parfaite
* et 1 quand il y a une relation linéaire positive parfaite. 


```{r fig5, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Relations entre deux variables continues et coefficients de corrélation de Pearson', out.width='75%'}
library("MASS")
library("ggplot2")
library("ggpubr")
N <- 1000      # nombre d'observations
moy_x <- 50   # moyenne de x
moy_y <- 40   # moyenne de y
sd_x <- 10    # écart-type de x
sd_y <- 8     # écart-type de y
rxy <- c(.90,-.85,0.01) # corrélation entre X et Y
 # Matrice de covariance
cov1 <- matrix(c(sd_x^2,  rxy[1]*sd_x*sd_y, rxy[1]*sd_x*sd_y, sd_y^2), nrow=2)
cov2 <- matrix(c(sd_x^2,  rxy[2]*sd_x*sd_y, rxy[2]*sd_x*sd_y, sd_y^2), nrow=2) 
cov3 <- matrix(c(sd_x^2,  rxy[3]*sd_x*sd_y, rxy[3]*sd_x*sd_y, sd_y^2), nrow=2) 
data1 <- mvrnorm(N, c(moy_x, moy_y), cov1)
data2 <- mvrnorm(N, c(moy_x, moy_y), cov2)
data3 <- mvrnorm(N, c(moy_x, moy_y), cov3)
plot1 <- ggplot(mapping = aes(x=data1[,1],y=data1[,2]))+
  geom_point(colour="red")+
  ggtitle("a. Relation linéaire positive", subtitle = paste0("Corrélation = ",round(cor(data1)[1,2],3)))+
  xlab("Variable X")+ylab("Variable Y")+
  stat_smooth(method="lm", se=FALSE)
plot2 <- ggplot(mapping = aes(x=data2[,1],y=data2[,2]))+
  geom_point(colour="red")+
  ggtitle("b. Relation linéaire négative", subtitle = paste0("Corrélation = ",round(cor(data2)[1,2],3)))+
  xlab("Variable X")+ylab("Variable Y")+
  stat_smooth(method="lm", se=FALSE)
plot3 <- ggplot(mapping = aes(x=data3[,1],y=data3[,2]))+
  geom_point(colour="red")+
  ggtitle("c. Absence de relation linéaire", subtitle = paste0("Corrélation = ",round(cor(data3)[1,2],3)))+
  xlab("Variable X")+ylab("Variable Y")+
  stat_smooth(method="lm", se=FALSE)
ggarrange(plot1, plot2, plot3, ncol = 2, nrow = 2)
```

Concrètement, le signe du coefficient de corrélation indique si la relation est positive ou négative et la valeur absolue du coefficient indique le degré d’association entre les deux variables. Reste à savoir comment déterminer qu’une valeur de corrélation est faible, moyenne ou forte. En sciences sociales, on utilise habituellement les intervalles de valeurs reportées au tableau \@ref(tab:tableIntervallesCorrelation). Toutefois, ces seuils sont tout à fait arbitraires. En effet, dépendamment de la discipline de recherche (sciences sociales, sciences de la santé, sciences physiques, etc.), et des variables à l’étude, l’interprétation d’une valeur de corrélation peut varier. Par exemple, en sciences sociales, une valeur de corrélation de 0,2 sera considérée comme très faible alors qu’en sciences de la santé, elle pourrait être considérée comme intéressante. À l’opposé, une valeur de 0,9 en sciences physiques pourrait être considérée comme faible. Il convient alors d’utiliser ces intervalles avec précaution.

```{r tableIntervallesCorrelation, echo=FALSE, message=FALSE, warning=FALSE}
df <- data.frame(
        Correlation = c("Faible","Moyenne", "Forte"), 
        Negative = c("de −0,3 à 0,0","de −0,5 à −0,3", "de −1,0 à −0,5"), 
        Positive = c("de 0,0 à 0,3","de 0,3 à 0,5", "de 0,5 à 1,0"))
knitr::kable(
  head(df, 3), booktabs = TRUE,
  col.names = c("Corrélation","Négative","Positive"),
  caption = 'Intervalles pour l’interprétation du coefficient de corrélation habituellement utilisés en sciences sociales'
)
```

Le coefficient de corrélation mis au carré représente le coefficient de détermination et indique la proportion de la variance de la variable _Y_ expliquée par la variable _X_ et inversement. Par exemple, un coefficient de corrélation de −0,70 signale que 49% de la variance de la variable de _Y_ est expliquée par _X_ (figure \@ref(fig:fig6)).

```{r fig6, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Coefficient de corrélation et proportion de la variance expliquée', out.width='75%'}
library("ggplot2")
R <- seq(-1, 1, by=0.01)
R2 <- R^2
 ggplot(mapping = aes(y=R2,x=R)) +
   geom_rect(xmin=-1,xmax=-.5, ymin=0,ymax=1, size=0, fill="#91bfdb")+
   geom_rect(xmin=-.5,xmax=-.3, ymin=0,ymax=1, size=0, fill="#e0f3f8")+ 
   geom_rect(xmin=-.3,xmax=.3, ymin=0,ymax=1, size=0, fill="#ffffbf")+
   geom_rect(xmin=.3,xmax=.5, ymin=0,ymax=1, size=0, fill="#fee090")+  
   geom_rect(xmin=.5,xmax=1, ymin=0,ymax=1, size=0, fill="#fc8d59")+
   geom_point() +
   ggtitle("Corrélation et coefficient de détermination")+
   xlab("Corrélation de Pearson – R")+
   ylab(expression("Coefficient de détermination –"~ R^{2}))+
   geom_vline(xintercept = -1, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = -0.5, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = -0.3, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = 0.0, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = 0.3, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = 0.5, colour="black", linetype="dashed", size=.5)+
   geom_vline(xintercept = 1, colour="black", linetype="dashed", size=.5) +
   
   annotate(geom="text", x =-.75, y=0.8, label="Forte et \nnégative", color="black", hjust = 0.5, size = 3.5)+
   annotate(geom="text", x =.75, y=0.8, label="Forte et \npositive", color="black", hjust = 0.5, size = 3.5)+
   annotate(geom="text", x =-.4, y=0.8, label="Modérée \n et négative", color="black", hjust = 0.5, size = 3.5)+  
   annotate(geom="text", x =.4, y=0.8, label="Modérée \n et positive", color="black", hjust = 0.5, size = 3.5)+   
   annotate(geom="text", x =-.15, y=0.8, label="Faible et \n négative", color="black", hjust = 0.5, size = 3.5)+   
   annotate(geom="text", x =.15, y=0.8, label="Faible et \n positive", color="black", hjust = 0.5, size = 3.5)
```

**Condition d'application.** L'utilisation du coefficient de corrélation de Pearson nécessite que les deux variables continues soient normalement distribuées et qu'elles ne comprennent pas de valeurs aberrantes (extrêmes). D’ailleurs, plus le nombre d’observations sera réduit, plus la présence de valeurs aberrantes aura un impact important sur le résultat du coefficient de corrélation de Pearson. En guise d’exemple, dans le nuage de points à gauche de la figure \@ref(fig:fig7), il est possible d’identifier des valeurs extrêmes qui se démarquent nettement dans le jeu de données : six observations avec une densité de population supérieure à 20 000 habitants au km^2^ et deux observations avec un pourcentage de 65 ans et plus supérieur à 55%. Si l'on supprime ces observations (ce qui est défendable dans ce contexte) – soit moins d'un pourcent des observations du jeu de données initial –, la valeur du coefficient de corrélation passe de −0,158 à −0,194, signalant une augmentation du degré d'association entre les deux variables.

```{r fig7, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Illustation de l’effet des valeurs extrêmes sur le coefficient de Pearson', out.width='75%'}
library("ggplot2")
library("ggpubr")
library("moments")
df1 <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
cor1 <- cor(df1$HabKm2, df1$A65plus, method = "pearson")
df2 <- subset(df1, HabKm2 < 20000 & A65plus < 50)
cor2 <- cor(df2$HabKm2, df2$A65plus, method = "pearson")
plot1 <- ggplot(data=df1, mapping = aes(x=A65plus,HabKm2))+
  geom_point(colour="red")+
  ggtitle(paste0("N = ", nrow(df1)), subtitle = paste0("Corrélation = ",round(cor1,3)))+
  xlab("65 ans et plus (%)")+
  ylab(expression("Densité de population : 1000 habitants au"~km^{2}))+
  stat_smooth(method="lm", se=FALSE)+
  annotate(geom="text", x =60, y=50000, label=paste0("Skewness = ", round(skewness(df1$HabKm2),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=48000, label=paste0("Kurtosis = ", round(kurtosis(df1$HabKm2),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=46000, label=paste0("Shapiro = ", round(shapiro.test(df1$HabKm2)$statistic,3)), color="black", hjust = 1, size = 3.5)
plot2 <- ggplot(data=df2, mapping = aes(x=A65plus,HabKm2))+
  geom_point(colour="red")+
  ggtitle(paste0("N = ", nrow(df2)), subtitle = paste0("Corrélation = ",round(cor2,3)))+
  xlab("65 ans et plus (%)")+
  ylab(expression("Densité de population : 1000 habitants au"~km^{2}))+
  stat_smooth(method="lm", se=FALSE)+
  annotate(geom="text", x =60, y=20000, label=paste0("Skewness = ", round(skewness(df2$HabKm2),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=19200, label=paste0("Kurtosis = ", round(kurtosis(df2$HabKm2),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=18400, label=paste0("Shapiro = ", round(shapiro.test(df2$HabKm2)$statistic,3)), color="black", hjust = 1, size = 3.5)
ggarrange(plot1, plot2, ncol = 2, nrow = 1)
```


#### Corrélations pour des variables anormalement distribuées (coefficients de Spearman, Tau de kendall) {#sect04133} 

Lorsque les variables sont fortement anormalement distribuées, le coefficient de corrélation de Pearson est peu adapté pour analyser leurs relations linéaires. Il est alors conseillé d'utiliser deux statistiques non-paramétriques : principalement, le coefficient de corrélation de Spearman (_rho_) et secondairement, le coefficient de Kendall ($\tau$, prononcez Tau), qui varient aussi tous deux de −1 à 1. 
Calculé sur les rangs des deux variables, **le coefficient de Spearman** est le rapport entre la covariance des deux variables de rangs sur les écart-types des variables de rangs. En d'autres termes, il représente simplement le coefficient de Pearson calculé sur les rangs des deux variables :

\begin{equation} 
r_{xy} = \frac{cov(rg_{x},rg_{y})}{\sigma_{rg_{x}}\sigma_{rg_{y}}}
(\#eq:spearman)
\end{equation}

La syntaxe ![](images/Rlogo.png) ci-dessous démontre clairement que le coefficient de Spearman est bien le coefficient de Pearson calculé sur les rangs (\@ref(sect04131)).


```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
# Transformation des deux variables en rangs
df$HabKm2_rang <- rank(df$HabKm2)
df$A65plus_rang <- rank(df$A65plus)
# Coefficient de Spearman avec la fonction cor et la méthode spearman
cat("Coefficient de Spearman = ", 
    round(cor(df$HabKm2, df$A65plus, method = "spearman"),5))
# Coefficient de Pearson sur les variables transformées en rangs
cat("Coefficient de Pearson calculé sur les variables transformées en rangs = ", 
    round(cor(df$HabKm2_rang, df$A65plus_rang, method = "pearson"),5))
# Vérification avec l'équation
cat("Covariance divisée par le produit des écart-types sur les rangs :",
    round(cov(df$HabKm2_rang, df$A65plus_rang) / (sd(df$HabKm2_rang)*sd(df$A65plus_rang)),5))
```

Le **coefficient de Kendall** est une autre mesure non-paramétrique calculée comme suit :

\begin{equation} 
\tau = \frac{n_{c}-n_{d}}{\frac{1}{2}n(n-1)}
(\#eq:tau)
\end{equation}

avec $n_{c}$ et $n_{d}$ qui sont respectivement les nombres de paires d'observations **c**oncordantes et **d**iscordantes; et le dénominateur étant le nombre totale de paires d'observations. Des paires sont dites corcondantes quand les valeurs des deux observations vont dans les même sens pour les deux variables ($x_{i}>x_{j}$ et $y_{i}>y_{j}$ ou  $x_{i}<x_{j}$ et $y_{i}<y_{j}$), et discordantes quand elles vont en sens contraire ($x_{i}>x_{j}$ et $y_{i}<y_{j}$ ou $x_{i}<x_{j}$ et $y_{i}>y_{j}$). Contrairement au calcul du coefficient de Spearman, celui de Kendall peut être chronophage : plus le nombre d'observations sera élevé, plus les temps de calcul et la mémoire utilisée sont importants. En effet, avec _n_=1000, le nombre de paires d'observations (${0.5*n(n-1)}$) sera de 499500, contre près de 50 millions avec _n_=10000 (49 995 000).


```{r fig8, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Comparaison des coefficients de Pearson, Spearman et Kendall sur deux variables anormalement distribuées', out.width='75%'}
library("moments")
library("ggpubr")
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
df$HabKm2 <- df$HabKm2 / 1000
p <- round(cor(df$HabKm2, df$A65plus, method = "pearson"),3)
s <- round(cor(df$HabKm2, df$A65plus, method = "spearman"),3)
k <- round(cor(df$HabKm2, df$A65plus, method = "kendall"),3)
Plot1 <- ggplot(data =df, mapping = aes(x=HabKm2))+
  geom_histogram(color="white",fill="#B22222", aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$HabKm2), sd = sd(df$HabKm2)), color="blue",size=1.2)+
  labs(title="Histogramme")+
  xlab(expression("1000 habitants au"~km^{2}))+
  ylab("Densité")+
  annotate(geom="text", x =60, y=0.130, label=paste0("Skewness = ", round(skewness(df$HabKm2,na.rm=TRUE),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=0.124, label=paste0("Kurtosis = ", round(kurtosis(df$HabKm2,na.rm=TRUE),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=0.118, label=paste0("Shapiro = ", round(shapiro.test(df$HabKm2)$statistic,3)), color="black", hjust = 1, size = 3.5)
Plot2 <- ggplot(data = df, mapping = aes(x=A65plus))+
  geom_histogram(color="white",fill="#B22222", aes(y=..density..))+
  stat_function(fun = dnorm, args = list(mean = mean(df$A65plus), sd = sd(df$A65plus)), color="blue",size=1.2)+
  labs(title="Histogramme")+
  xlab("65 ans et plus (%)")+
  ylab("Densité")+
  annotate(geom="text", x =60, y=0.072, label=paste0("Skewness = ", round(skewness(df$A65plus,na.rm=TRUE),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=0.069, label=paste0("Kurtosis = ", round(kurtosis(df$A65plus,na.rm=TRUE),3)), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=0.066, label=paste0("Shapiro = ", round(shapiro.test(df$A65plus)$statistic,3)), color="black", hjust = 1, size = 3.5)
Plot3 <- ggplot(data=df, mapping = aes(x=A65plus,HabKm2))+
  geom_point(colour="red")+
  labs(title="Nuage de points")+
  xlab("65 ans et plus (%)")+
  ylab(expression("1000 habitants au"~km^{2}))+
  stat_smooth(method="lm", se=FALSE, cor.coef = TRUE, cor.method = "pearson")+
  annotate(geom="text", x =60, y=50, label=paste0("Pearson = ", p), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=48, label=paste0("Spearman = ", s), color="black", hjust = 1, size = 3.5)+
  annotate(geom="text", x =60, y=46, label=paste0("Kendall = ", k), color="black", hjust = 1, size = 3.5)  
ggarrange(Plot1, Plot2, Plot3, ncol=3,nrow=1)
```

À la lecture des deux histogrammes ci-dessus, il est clair que les deux variables *densité de population* et *pourcentage de personnes ayant 65 ou plus* sont très anormalement distribuées. Dans ce contexte, l'utilisation du coefficient de Pearson peut nous amener à mésestimer la relation existant entre les deux variables. Notez que les coefficients de Spearman et de Kendall sont tous les deux plus faibles.

#### Corrélations robustes (*Biweight midcorrelation*, *Percentage bend correlation* et la corrélation *pi* de Shepherd) {#sect04134}

Dans l'exemple donné à la figure \@ref(fig:fig7), nous avions identifié des valeurs aberrantes que nous avons retirées du jeu de données. Cette pratique peut tout à fait se justifier quand les données sont erronées (un capteur de pollution renvoyant une valeur négative, un questionnaire rempli par un mauvais plaisantin, etc.), mais parfois, les cas extrêmes font partie du phénomène à analyser. Dans ce contexte, les identifier et les retirer peut paraître arbitraire. Une solution plus élégante est d'utiliser des méthodes dites **robustes**, c'est à dire moins sensibles aux valeurs extrêmes. Pour les corrélations, la *Biweight midcorrelation* [@wilcox1994percentage] est au coefficient de Pearson ce que la médiane est à la moyenne. Il est donc pertinent de l'utiliser dans des jeux de données présentant potentiellement des valeurs extrêmes. Elle est calculée comme suit : 


\begin{equation}
\begin{split}
&u_{i} = \frac{x_{i} - med(x)}{9 * (med(|x_{i} - med(x)|))} \text{ et } v_{i} = \frac{y_{i} - med(y)}{9 * (med(|y_{i} - med(y)|))}\\
&w_{i}^{(x)} = (1 - u_{i}^2)^2 I(1 - |u_{i}|) \text{ et } w_{i}^{(y)} = (1 - v_{i}^2)^2 I(1 - |v_{i}|)\\
&I(x) = 
\begin{cases}
1, \text{si} x = 1\\
0, \text{sinon}
\end{cases}\\

&\tilde{x}_{i} = \frac{(x_{i} - med(x))w_{i}^{(x)}}{\sqrt{(\sum_{j=1}^m)[(x_{j} - med(x))w_{j}^{(x)}]^2}} \text{ et } \tilde{y}_{i} = \frac{(y_{i} - med(y))w_{i}^{(y)}}{\sqrt{(\sum_{j=1}^m)[(y_{j} - med(y))w_{j}^{(y)}]^2}}\\
&bicor(x,y) = \sum_{i=1}^m \tilde{x_i}\tilde{y_i}\\
\end{split}
(\#eq:bicor)

\end{equation}

Comme le souligne l'équation \@ref(eq:bicor), la *Biweight midcorrelation* est basée sur les écarts à la médiane, plutôt que sur les écarts à la moyenne.

Assez proche de la *Biweight midcorrelation*, la *Percentage bend correlation* se base également sur la médiane des variables *X* et *Y*. Le principe général est de donner un poids plus faible dans le calcul de cette corrélation à un certain pourcentage des observations (20% est généralement recommandé) dont la valeur est éloignée de la médiane. Pour une description complète de la méthode, vous pourrez lire l'article de @wilcox1994percentage.

Enfin, une autre option est l'utilisation de la corrélation $pi$ de Sherphred [@Schwarzkopf2012]. Il s'agit simplement d'une méthode en deux étapes. Premièrement, les valeurs abberantes sont identifiées à l'aide d'une approche par *bootstrap* utilisant la distance de Mahalanobis (calculant les écarts multivariés entre les observations). Ensuite, le coefficient de *Spearman* est calculé sur les observations restantes.

Appliquons ces corrélations aux données précédentes. Notez que ce simple code d'une dizaine de lignes permet d'explorer rapidement la corrélation entre deux variables selon six mesures de corrélations.

```{r robcorr, echo=TRUE, message=FALSE, warning=FALSE, out.width='75%'}
library("correlation")
df1 <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
methods <- c("pearson","spearman","biweight","percentage","shepherd")
rs <- lapply(methods,function(m){
  test <- correlation::cor_test(data = df1, x="Hab1000Km2",y="A65plus",method = m, ci=0.95)
  return(c(test$r,test$CI_low, test$CI_high))
  })
dfCorr <- data.frame(do.call(rbind,rs))
names(dfCorr) <- c("r","IC_05","CI_95")
dfCorr$method <- methods
kableExtra::kable(dfCorr,digits = 2, 
                  caption = 'Comparaison de différentes corrélations pour les variables densité de population et pourcentage de personnes ayant 65 ans et plus',
                  col.names=c("r","IC 5%","IC 95%", "Méthode"))
```

Il est intéressant de mentionner que ces trois corrélations sont rarement utilisées malgré leur pertinence dans de nombreux cas d'application. Nous faisons face ici à un cercle vicieux dans la recherche : les méthodes les plus connues sont les plus utilisées car elles sont plus facilement acceptées par les autres chercheurs. Des méthodes plus élaborées nécessitent davantage de justification et de discussion, ce qui peut conduire à de multiples sessions de corrections/resoumissions pour qu'un article soit accepté, malgré le fait qu'elles puissent être plus adaptées au jeu de données à l'étude.

#### Significativité des coefficients de corrélation {#sect04135} 

Quelle que soit la méthode utilisée, il convient de vérifier si le coefficient de corrélation est ou non statistiquement différent de 0. En effet, nous travaillons la plupart du temps avec des données d'échantillonage, et très rarement avec des populations complètes. En collectant un nouvel échantillon, aurions-nous obtenu des résultats différents ? Le calcul de ce degré de significativité nous permet de quantifier notre niveau de certitude quant à l'existance d'une corrélation entre nos deux variables, positive ou négative. Cet objectif est réalisé en calculant la valeur de _t_ et le nombre de degrés de liberté : $t=\sqrt{\frac{n-2}{1-r^2}}$ et $dl = n-2$ avec $r$ et $n$ étant le coefficient de corrélation et le nombre d'observations. De manière classique, on utilisera la table des valeurs critiques de la distribution de $t$ : si la valeur de $t$ est supérieure à la valeur critique (avec  _p_ = 0,05 et le nombre de degré de liberté), alors le coefficient est significatif à 5%. En d'autres termes, si la vraie corrélation entre les deux variables (calculable uniquement à partir des populations complètes) était 0, alors la probabilité de collecter notre échantillon aurait été inférieure à 5%. Dans ce contexte, on peut raisonnablement rejeter l'hypothèse nulle (corrélation réelle de 0). 

La courte syntaxe ![](images/Rlogo.png) illustre comment calculer la valeur de $t$, le nombre de degrés de liberté et la valeur de _p_ pour une corrélation donnée.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
r <- cor(df$A65plus, df$LogTailInc)     # Corrélation
n <- nrow(df)                           # Nombre d'observations
dl <- nrow(df)-2                        # degrés de liberté
t <-  r*sqrt((n-2)/(1-r^2))             # Valeur de T
p <- 2*(1-pt(abs(t),dl))                # Valeur de p
cat("\nCorrélation =", round(r, 4),       
    "\nValeur de t =", round(t, 4),
    "\nDegrés de liberté =", dl,
    "\np=", round(p, 4))        
```

Plus simplement, la fonction `cor.test` permet d'obtenir en une seule ligne de code le coefficient de corrélation, l'intervalle de confiance à 95% et les valeurs de _t_ et de _p_, tel qu'illustré dans la syntaxe ![](images/Rlogo.png) ci-dessous. Si l'intervalle de confiance est à cheval sur 0, c'est-à-dire que la borne inférieure est négative et la borne supérieure positive, alors le coefficient de corrélation n'est pas significatif au seuil choisi (95% habituellement). Dans l'exemple ci-dessous, le relation linéaire entre les deux variables est significativement négative avec une corrélation de Pearson de −0,158 (P=0,000) et un intervalle de confiance de [−0,219 −0,095].

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Intervalle de confiance à 95%
cor.test(df$HabKm2, df$A65plus, conf.level = .95)
# Vous pouvez accéder à chaque sortie de la fonction cor.test comme suit :
p <- cor.test(df$HabKm2, df$A65plus)
cat("Valeur de corrélation = ", round(p$estimate,3), "\n",
    "Intervalle à 95% = [", round(p$conf.int[1],3), " ", round(p$conf.int[2],3), "]", "\n",
    "Valeur de t = ", round(p$statistic,3), "\n",
    "Valeur de p = ", round(p$p.value,3), sep="")
# Corrélation de Spearman
cor.test(df$HabKm2, df$A65plus, method = "spearman")
# Corrélation de Kendall
cor.test(df$HabKm2, df$A65plus, method="kendall")
```

On pourra aussi modifier l'intervalle de confiance, par exemple à 90% ou 99%. Le choix de l'intervalle de confiance et du seuil de significativité doivent être définis avant l'étude. Il doit s'appuyer sur les standards de la littérature du domaine étudié, du niveau de preuve attendu et de la quantité de données.
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Intervalle à 90%
cor.test(df$HabKm2, df$A65plus, method ="pearson", conf.level = .90)
# Intervalle à 99%
cor.test(df$HabKm2, df$A65plus, method ="pearson", conf.level = .99)
```

**Corrélation et _bootstrap_.** Dans le premier chapitre (LIEN BOOTSTRAP), nous avons abordé la notion de _bootstrap_, soit des méthodes d'inférence statistique basées sur des réplications des données initiales par rééchantillonnage. Il est possible d'appliquer la même méthode aux corrélations afin d'obtenir un intervale de confiance avec _r_ réplications, tel qu'illustré à partir de la syntaxe ![](images/Rlogo.png) ci-dessous.


```{r fig9, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Histogramme pour les valeurs de corrélation issues du Bootstrap', out.width='75%'}
library("boot")
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
# Fonction pour la corrélation
correlation <- function(df, i, X, Y, cor.type="pearson"){
  # Paramètres de la fonction :
  # data : dataframe
  # X et Y : noms des variables X et Y
  # cor.type : type de corrélation : c("pearson","spearman","kendall")
  # i : indice qui sera utilisé par les réplications (à ne pas modifier)
  cor(df[[X]][i], df[[Y]][i], method=cor.type)
}
# Calcul du Bootstrap avec 5000 réplications
corBootstraped <- boot(data=df, # nom du tableau
                     statistic = correlation, # appel de la fonction à répliquer 
                     R=5000, # nombre de réplications
                     X = "A65plus",
                     Y ="HabKm2", 
                     cor.type="pearson")
# Histogramme pour les valeurs de corrélation issues du Bootstrap
plot(corBootstraped)
# Corrélation "bootstrapée"
corBootstraped
# Intervalle de confiance du bootstrap à 95%
boot.ci(boot.out = corBootstraped, conf = 0.95, type = "all")
# Comparaison de l'intervalle classique basé sur la valeur de T
p <- cor.test(df$HabKm2, df$A65plus)
cat(round(p$estimate,5), " [", round(p$conf.int[1],4), " ",round(p$conf.int[2],4), "]", sep="")
```

Le _bootstrap_ renvoie un coefficient de corrélation de Pearson de −0,158. Les intervalles de confiance obtenues à partir des différentes méthodes d'estimation (normale, basique, pourcentage et bca) ne sont pas à cheval sur 0, indiquant que le coefficient est significatif à 5%.

#### Corrélation partielle {#sect04136} 

::: {.bloc_objectif .bloc_objectif_png data-latex="{blocs/objectif}"}

**Quelle est la relation entre deux variables continues une fois pris en compte une ou plusieurs variables dites de contrôle ?** En études urbaines, on pourrait vouloir vérifier si deux variables sont ou non associées une fois contrôlée la densité de population ou encore la distance au centre-ville.

La corrélation partielle\index{corrélation partielle} permet d'évaluer la relation linéaire entre deux variables quantitatives continues, une fois contrôlé une ou plusieurs autres variables quantitatives (dites variables de contrôle).


:::

Le coefficient de corrélation partielle peut être calculé pour les trois méthodes (Pearson, Spearman et Kendall). Variant aussi de −1 à 1, il est calculé comme suit :

\begin{equation} 
r_{ABC} = \frac{r_{AB}-r_{AC}r_{BC}}{\sqrt{(1-r_{AC}^2)(1-r_{BC}^2)}}
(\#eq:corpartielle)
\end{equation}

avec _A_ et _B_ étant les deux variables pour lesquelles on souhaite évaluer la relation linéaire, une fois contrôlée la variable _C_; $r$ étant le coefficient de corrélation (Pearson, Spearman ou Kendall) entre deux variables.

Dans l'exemple ci-dessous, nous voulons estimer la relation linéaire entre le pourcentage de personnes à faible revenu et la couverture végétale au niveau des îlots de l'île de Montréal, une fois contrôlée la densité de population. En effet, plus cette dernière sera forte, plus la couverture végétale sera faible ($r$ de Pearson = −0,603). La valeur du $r$ de Pearson s'élève à −0,546 entre le pourcentage de personnes à faible revenu dans la population totale de l'îlot et la couverture végétale. Une fois la densité de population contrôlée, il chute à −0,316. Pour calculer la corrélation partielle, on pourra utiliser la fonction `pcor.test` du package **ppcor**.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("foreign")
library("ppcor")
dfveg <- read.dbf("data/bivariee/IlotsVeg2006.dbf")
# Corrélation entre les trois variables
round(cor(dfveg[, c("VegPct", "Pct_FR","LogDens")], method="p"), 3)
# Corrélation partielle entre :
# la couverture végétale de l'îlot (%) et
# le pourcentage de personnes à faible revenu
# une fois contrôlée la densité de population
pcor.test(dfveg$Pct_FR, dfveg$VegPct, dfveg$LogDens, method="p")
# Calcul de la corrélation partielle avec la formule :
corAB <- cor(dfveg$VegPct, dfveg$Pct_FR, method = "p")
corAC <- cor(dfveg$VegPct, dfveg$LogDens, method = "p")
corBC <- cor(dfveg$Pct_FR, dfveg$LogDens, method = "p")
CorP  <- (corAB - (corAC*corBC)) / sqrt((1-corAC^2)*(1-corBC^2))
cat("Corr. partielle avec ppcor  = ", 
    round(pcor.test(dfveg$Pct_FR,  dfveg$VegPct, dfveg$LogDens, method="p")$estimate,5),
    "\nCorr. partielle (formule)  = ", round(CorP, 5))
```

#### Mise en œuvre dans ![](images/Rlogo.png) {#sect04137}

Comme vous l'aurez compris, il est possible d'arriver au même résultat dans ![](images/Rlogo.png) par différents moyens. Pour calculer les corrélations, nous avons utilisé jusqu'à présent les fonctions de base `cor` et `cor.test` . Il est aussi possible de recourir à des fonctions d'autres _packages_, dont notamment :

* **Hmisc** dont la fonction `rcorr` permet de calculer des corrélations de Pearson et Spearman (mais non celle de Kendall) avec la valeur de _p_.
* **psych** dont la fonction `corr.test` permet d'obtenir la matrice de corrélation (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de _p_.
* **stargazer** pour créer des beaux tableaux d'une matrice de corrélation en *Html* ou en *LaTeX* ou en ASCII.
* **apaTables**  pour créer un tableau avec une matrice de corrélation dans un fichier Word.
* **correlation** pour aller plus loin et explorer les corrélations bayesiennes, robustes, non-linéaires ou multiniveaux.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
library("Hmisc")
library("stargazer")
library("apaTables")
library("dplyr")
# Corrélations de Pearson et Spearman et valeurs de p 
# avec la fonction rcorr de Hmisc pour deux variables
Hmisc::rcorr(df$RevMedMen, df$Locataire, type="pearson")
Hmisc::rcorr(df$RevMedMen, df$Locataire, type="spearman")
# Matrice de corrélation avec la fonction rcorr de Hmisc pour plus de variables
# On crée un vecteur avec les noms des variables à sélectionner
Vars <- c("RevMedMen","Locataire", "LogTailInc","A65plus","ImgRec", "HabKm2", "FaibleRev")
Hmisc::rcorr(df[, Vars] %>% as.matrix())
# # Avec la fonction corr.test de psych pour avoir la matrice de corrélation
# # (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de p
# print(psych::corr.test(df[, Vars], 
#              method = "kendall", 
#              ci=TRUE, alpha = 0.05), short=FALSE) 
# Création d'un tableau pour une matrice de corrélation
# changer le paramètre type pour 'html' or 'LaTex'
p <- cor(df[, Vars], method="pearson")
stargazer(p, title="Correlation Matrix", type = "text")
# stargazer(p, title="Correlation Matrix", type = "html")
# stargazer(p, title="Correlation Matrix", type = "latex")
# Créer un tableau avec la matrice de corrélation 
# dans un fichier Word (.doc)
apaTables::apa.cor.table(df[, c("RevMedMen","Locataire","LogTailInc")], 
                         filename = "data/bivariee/TitiLaMatrice.doc",
                         show.conf.interval = TRUE,
                         landscape = TRUE)
```


::: {.bloc_astuce .bloc_astuce_png data-latex="{blocs/astuce}"}
**Une image vaut mille mots, surtout pour une matrice de corrélation !** Le package **corrplot** vous permet justement de construire de belles figures avec une matrice de corrélation (figures \@ref(fig:figcorrplot1) et \@ref(fig:figcorrplot2)). L'intérêt de ce type de figure est de repérer rapidemment des associations intéressantes lorsque l'on calcule les corrélations entre un grand nombre de variables.
:::

```{r figcorrplot1, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Matrice de corrélation avec corrplot (chiffres)', out.width='60%'}
library("corrplot")
library("ggpubr")
df <- read.csv("data/bivariee/sr_rmr_mtl_2016.csv")
Vars <- c("RevMedMen","Locataire", "LogTailInc","A65plus","ImgRec", "HabKm2", "FaibleRev")
p <- cor(df[, Vars], method="pearson")
couleurs <- colorRampPalette(c("#053061", "#2166AC","#4393C3", "#92C5DE",
                               "#D1E5F0", "#FFFFFF", "#FDDBC7", "#F4A582",
                               "#D6604D", "#B2182B", "#67001F"))
corrplot(p, addrect = 3, method="number", diag=FALSE, col=couleurs(100))
```

```{r figcorrplot2, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Matrice de corrélation avec corrplot (chiffres et ellipses)', out.width='60%'}
fig2 <- corrplot.mixed(p, lower="number", lower.col = "black", 
                      upper = "ellipse", upper.col=couleurs(100))
```


#### Comment rapporter des valeurs de corrélations? {#sect04138}

Bien qu'il n'y ait pas qu'une seule manière de reporter des corrélations, voici quelques lignes directrices pour vous guider : 

* Signaler si la corrélation est faible, modérée ou forte.
* Indiquer si la corrélation est positive ou négative. Toutefois, ce n'est pas une obligation car l'on peut rapidement le constater avec le signe du coefficient.
* Mettre le *r* et le *p* en italique et en minuscules.
* Deux décimales uniquement pour le $r$ (sauf si une plus grande précision se justifie dans le domaine d'étude).
* Trois décimales pour la valeur de *p*. Si elle est inférieure à 0,001, écrire plutôt *p* < 0,001.
* Indiquer éventuellement le nombre de degrés de liberté, soit $r(dl)=...$

Voici des exemples :

* La corrélation entre les variables revenu médian des ménages et pourcentage de locataire est fortement négative (*r* = −0,78, *p* < 0,001).
* La corrélation entre les variables revenu médian des ménages et pourcentage de locataire est forte (*r*(949) = −0,78, *p* < 0,001).
* La corrélation entre les variables densité de population et pourcentage de logements de taille est modérée (*r* = 0,48, *p* < 0,001).
* La corrélation entre les variables densité de population et pourcentage de 65 ans et plus n'est pas significative (*r* = −0,08, *p* = 0,07).

Pour un texte en anglais, référez-vous à : [https://www.socscistatistics.com/tutorials/correlation/default.aspx](https://www.socscistatistics.com/tutorials/correlation/default.aspx){target="_blank"}.

### Régression linéaire simple  {#sect0414}

::: {.bloc_objectif .bloc_objectif_png data-latex="{blocs/objectif}"}

**Comment expliquer et prédire une variable continue en fonction d'une autre variable?** Répondre à cette question relève de la statistique inférentielle. Il s'agit en effet d'établir une équation simple du type $Y = a + bX$, pour expliquer et prédire les valeurs d'une variable dépendante (*Y*) à partir d'une variable indépendante (*X*). L'équation de la régression est construite grâce à un jeu de données (un échantillon). À partir de cette équation, il est possible de prédire la valeur attendue de *Y* pour n'importe quelle valeur de *X*. On appelle cette équation un modèle, car elle cherche à représenter la réalité de façon simplifiée.

La régression linéaire simple se distingue ainsi de la **covariance** (section \@ref(sect0412)) et de la **corrélation** (section \@ref(sect0413)), relevant de la statistique bivariée descriptive et exploratoire. 

Par exemple, la régression linéaire simple pourrait être utilisée pour expliquer les notes d'un groupe d'étudiants à un examen (variable dépendante *Y*) en fonction du nombre d'heures qu'ils ont consacrés à la révision des notes de cours (variable indépendante *X*). Une fois l'équation de régression déterminée et si le modèle est efficace, nous pourrons prédire les notes des étudiants inscrits au cours la session suivante en fonction du temps qu'ils prévoient de passer à étudier, et ce, avant même qu'ils aient passé l'examen. 

Formulons un exemple d'application de la régression linéaire simple en études urbaines. Dans le cadre d'une étude sur les îlots de chaleur urbains, la température de surface (variable dépendante) pourrait être expliquée par la proportion de la superficie de l'îlot couverte par de la végétation (variable indépendante). On supposerait alors que plus cette proportion est importante, plus la température est faible et inversement, soit une relation linéaire négative. Si le modèle est efficace, nous pourrions prédire la température moyenne des îlots d'une autre municipalité pour laquelle nous ne disposons pas d'une carte de température, et repérer ainsi les îlots de chaleur potentiels. Bien entendu, il est peu probable que nous arrivions à prédire efficacement la température moyenne des îlots avec uniquement la couverture végétale comme variable explicative. En effet, bien d'autres caractéristiques de la forme urbaine peuvent influencer ce phénomène comme la densité du bâti, la couleur des toits, les occupations du sol présentes, l'effet des canyons urbains, etc. Il faudrait alors inclure non pas une, mais plusieurs variables explicatives (indépendantes).

Ainsi, on distinguera la **régression linéaire simple** (une variable indépendante, explicative) de la **régression linéaire multiple** (plusieurs variables indépendantes); cette dernière sera largement abordée au chapitre \@ref(chap05).

:::

Dans cette section, nous décrirons succinctement la régression linéaire simple. Concrètement, nous verrons comment déterminer la droite de régression, interpréter ses différents paramètres du modèle et comment évaluer la qualité d'ajustement du modèle. Nous n'aborderons pas les hypothèses liées au modèle de régression linéaire des moindres carrés ordinaires (MCO), ni les conditions d'application. Ces éléments seront expliqués au chapitre \@ref(chap05) consacré à la régression linéaire multiple.

::: {.bloc_attention .bloc_attention_png data-latex="{blocs/attention}"}
**Corrélation, régression simple et causalité : attention aux raccourcis !**

Si une variable *X* explique et prédit efficacement une variable *Y*, cela ne veut pas dire pour autant que *X* cause *Y*. Autrement dit, la corrélation, l'association entre deux variables ne signifie qu'il existe un lien de causalité entre elles.

Premièrement, la variable explicative (*X*, indépendante) doit absolument précéder la variable à expliquer (*Y*, dépendante). Par exemple, l'âge (*X*) peut influencer le sentiment de sécurité (*Y*). Mais, le sentiment de sécurité ne peut en aucun cas influencer l'âge. Par conséquent, l'âge ne peut conceptuellement pas être la variable dépendante dans cette relation.

Deuxièmement, bien qu'une variable puisse expliquer efficacement une autre variable, elle peut être un **facteur confondant**. Prenons deux exemples bien connus :

* Avoir les doigts jaunes est associé au cancer du poumon. Bien entendu, les doigts jaunes ne causent pas le cancer : c'est un facteur confondant puisque fumer augmente les risques du cancer du poumon et jaunit aussi les doigts.

* Dans un article intitulé *Chocolate Consumption, Cognitive Function, and Nobel Laureates*, Messerli [-@Messerli] a trouvé une corrélation positive entre la consommation de chocolat par habitant et le nombre de prix Nobel pour dix millions d'habitants pour 23 pays. Ce résultat a d'ailleurs été rapporté par de nombreux médias ([Radio Canada](https://ici.radio-canada.ca/nouvelle/582457/chocolat-consommateurs-nobels), [La Presse]("https://www.lapresse.ca/vivre/sante/nutrition/201210/11/01-4582347-etude-plus-un-pays-mange-de-chocolat-plus-il-a-de-prix-nobel.php"), [Le Point](https://www.lepoint.fr/insolite/le-chocolat-dope-aussi-l-obtention-de-prix-nobel-12-10-2012-1516159_48.php), etc.), sans pour autant que Messerli [-@Messerli] et les journalistes concluent à un lien de causalité entre les deux variables. Tout chercheur sait que la consommation de chocolat ne permet pas d'obtenir des résultats intéressants et de publier dans des revues prestigieuses; c'est plutôt le café ! Plus sérieusement, il est probable que les pays les plus riches investissent davantage dans la recherche et obtiennent ainsi plus de prix Nobel. Dans les pays les plus riches, il est aussi probable que l'on consomme plus de chocolat, considéré comme un produit de luxe dans les pays les plus pauvres.

Pour approfondir le sujet sur la confusion entre *corrélation, régression simple et causalité*, vous pourrez visionner cette courte vidéo ludique de vulgarisation.

<iframe width="560" height="315" src="https://www.youtube.com/embed/A-_naeATJ6o" frameborder="0" allowfullscreen></iframe>

 
L'association entre deux variables peut aussi être simplement le fruit du hasard. Si l'on explore de très grandes quantités de données (avec un nombre impressionnant d'observations et de variables), soit une démarche relevant du *data mining*, le hasard fera que l'on risque d'obtenir des corrélations surprenantes entre certaines variables. Prenons un exemple concret, admettons que l'on ait collecté 100 variables et que l'on calcule les corrélations entre chaque paire de variables. On obtient une matrice de corrélation de 100 x 100, à laquelle on peut enlever la diagonale et une moitié de la matrice, ce qui nous laisse un total de 4950 corrélations différentes. Admettons que l'on choisisse un seuil de significativité de 5%, on doit alors s'attendre à ce que le hasard produise des résultats significatifs dans 5% des cas. Sur 4950 corrélations, cela signifie qu'environ 247 corrélations seront significatives, et ce, indépendamment de la nature des données. Nous pouvons aisément l'illustrer avec la syntaxe ![](images/Rlogo.png) suivante :


```{r corraleatoire, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Corrélations significatives obtenues aléatoirement', out.width='60%'}
library("Hmisc")
nbVars <- 100 # nous utilisons 100 variables générées aléatoirement pour l'expérience
nbExperiment <- 1000 # nous reproduirons 1000 fois l'expérience avec les 100 variables
# Le nombre de variables significatives par expérience est enregistrée dans Results
Results <- c()
# iterons pour chaque expérimentation (1000 fois)
for(i in 1:nbExperiment){
  Datas <- list()
  # générons 100 variables aléatoires normalement distribuées
  for (j in 1:nbVars){
    Datas[[j]] <- rnorm(150)
  }
  DF <- do.call("cbind",Datas)
  # calculons la matrice de corrélation pour les 100 variables
  cor_mat <- rcorr(DF)
  # comptons combien de fois les corrélations étaient significatives
  Sign <- table(cor_mat$P<0.05)
  NbPairs <- Sign[["TRUE"]]/2
  # ajoutons les résultats dans Results
  Results <- c(Results,NbPairs)
}
# transformons Results en un dataframe
df <- data.frame(Values = Results)
# affichons le résultat
ggplot(df, aes(x = Values)) + 
  geom_histogram(aes(y =..density..), 
                 colour = "black", 
                 fill = "white") +
  stat_function(fun = dnorm, args = list(mean = mean(df$Values), 
                sd = sd(df$Values)),color="blue")+
  geom_vline(xintercept = mean(df$Values),color="red", size=1.2)+
  annotate("text", x=250, y = 0.028, 
           label = paste("Nombre moyen de corrélations significatives\n 
                         sur 1000 replications :",
           round(mean(df$Values),0),sep=""), hjust="left")+
  xlab("Nombre de corrélations significatives")+
  ylab("densité")
```
:::
#### Principe de base de la régression linéaire simple {#sect04141}
La régression linéaire simple vise à déterminer une droite (une fonction linéaire) qui résume le mieux la relation linéaire entre une variable dépendante (*Y*) et une variable indépendante (*X*) :
\begin{equation} 
\widehat{y_i} = \beta_{0} + \beta_{1}x_{i}
(\#eq:regsimple)
\end{equation}

avec $\widehat{y_i}$ et $x_{i}$ qui sont respectivement la valeur prédite de la variable indépendante et la valeur de la variable dépendante pour l'observation $i$. $\beta_{0}$ est la constante (*intercept* en anglais), soit la valeur prédite de la variable $Y$ quand $X$ est égale à 0. $\beta_{1}$ est le coefficient de régression pour la variable *X*, soit la pente de la droite. Ce coefficient nous informe sur la relation entre les deux variables : s’il est positif, la relation est positive; s’il est négatif, la relation est négative, et proche de 0, la relation est nulle (la droite sera alors horizontale). Plus la valeur absolue de $\beta_{1}$ est élevée, plus la pente est forte, et plus la variable *Y* varie à chaque changement d’une unité de la variable *X*.

Considérons un exemple fictif de dix municipalités d'une région métropolitaine pour lesquelles nous disposons de deux variables : le pourcentage d'actifs occupés se rendant au travail principalement à vélo et la distance de la municipalité au centre-ville (tableau \@ref(tab:regfictives)).


```{r regfictives, echo=FALSE, message=FALSE, warning=FALSE, out.width='85%'}
df <- read.csv("data/bivariee/Reg.csv", stringsAsFactors = F)
df1 <- df[1:5,]
df2 <- df[6:10,]
knitr::kable(list(df1, df2),
   format.args = list(decimal.mark = ",", big.mark = " "),           
   col.names = c("Municipalité","Vélo","KMCV"),
   caption = "Données fictives sur l'utilisation du vélo par municipalité",
   booktabs = TRUE, valign = 't', row.names = FALSE)
```

D'emblée, à la lecture du nuage de points (figure \@ref(fig:figreg)), on décèle une forte relation linéaire négative entre les deux variables : plus la distance au centre-ville augmente, plus le pourcentage de cyclistes est faible, ce qui est confirmé par le coefficient de corrélation (*r* = −0,86). La droite de régression (en rouge à la figure \@ref(fig:figreg)) qui résume le mieux la relation entre `Vélo` (variable dépendante) et `KmCV` (variable indépendante) s'écrit alors : **Vélo = 30,603 − 1,448 x KmCV**.

La valeur du coefficient de régression ($\beta_{1}$) est de −1,448. Le signe de ce coefficient décrit une relation négative entre les deux variables. Ainsi, à chaque ajout d'une unité de la distance au centre-ville (exprimée en kilomètres), le pourcentage de cyclistes diminue de 1,448. Retenez que l'unité de mesure de la variable dépendante est très importante pour bien interpréter le coefficient de régression. En effet, si la distance au centre-ville n'était pas exprimée en kilomètres, mais plutôt en mètres, $\beta_1$ sera égal à 0,001448. Dans la même optique, l'ajout de 10 km de distance entre une municipalité et le centre-ville fait diminuer le pourcentage de cyclistes de −14,48 points de pourcentage.

Avec, cette équation de régression, il est possible de prédire le pourcentage de cyclistes pour n'importe quelle municipalité de la région métropolitaine. Par exemple, pour des distances de 5, 10 ou 20 kilomètres, les pourcentages de cyclistes seraient de :

* $\widehat{y_i} = 21.873 + (1.448 \times 5 km) = 23.363$
* $\widehat{y_i} = 21.873 + (1.448 \times 10km) = 8.883$
* $\widehat{y_i} = 21.873 + (1.448 \times 20km) = 1.643$

```{r figreg, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Relation linéaire entre l'utilisation du vélo et la distance au centre-ville", out.width='65%'}
library("ggplot2")
modele <- lm(Velo~KmCV, df)
b0 <- round(modele$coefficients[1],3)
b1 <- round(modele$coefficients[2],3)
eq <- paste0("Vélo = ", b0, " ", b1, " x KmCV")
r2 <- round(summary(modele)$r.squared,3)
pearson <- round(cor(df$Velo, df$KmCV),3)
ggplot(df, aes(x=KmCV, y=Velo)) +
  stat_smooth(method="lm", se=FALSE, col="red")+
  labs(title= eq,
     subtitle = paste0("r2 = ", r2, ", r =", pearson), sep="")+
  xlab("Distance au centre-ville (km)")+
  ylab("Vélo (%)")+
  geom_point(colour="black", size=5)
```  


#### Formulation de la droite de régression des moindres carrés ordinaires {#sect04142}
Reste à savoir comment sont estimés les différents paramètres de l'équation, soit $\beta_0$ et $\beta_1$. À la figure \@ref(fig:figreg2), les points noirs représentent les valeurs observées ($y_i$) et les points bleus les valeurs prédites ($\widehat{y_i}$) par l'équation du modèle. Les traits noirs verticaux représentent pour chaque observation $i$, l'écart entre la valeur observée et la valeur prédite, dénommé résidu ($\epsilon_i$, prononcez epsilon de _i_ ou plus simplement le résidu pour _i_, ou encore le terme d'erreur de _i_). Si un point est au-dessus de la droite de régression, la valeur observée sera alors supérieure à la valeur prédite ($y_i > \widehat{y_i}$) et inversement, si le point est au-dessous de la droite ($y_i < \widehat{y_i}$). Plus cet écart ($\epsilon_i$) est important, plus l'observation s'éloigne de la prédiction du modèle, et par extension moins bon est le modèle. Au tableau \@ref(tab:regfictives2), vous constaterez que la somme des résidus est égale à zéro. La méthode des moindres carrés ordinaires (MCO) vise à minimiser les écarts au carré entre les valeurs observées ($y_i$) et prédites ($\beta_0+\beta_1 x_i$, soit $\widehat{y_i}$) :

\begin{equation} 
min\sum_{i=1}^n{(y_i-(\beta_0+\beta_1 x_i))^2}
(\#eq:mco)
\end{equation}

Pour minimiser ces écarts, le coefficient de régression $\beta_1$ représente le rapport entre la covariance entre *X* et *Y* et la variance de *Y* (équation \@ref(eq:b1)), tandis que la constante $\beta_0$ est la moyenne de la variable *Y* moins le produit de la moyenne de *X* et de son coefficient de régression (équation \@ref(eq:b0)).

\begin{equation} 
\beta_1 = \frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2} = \frac{cov(X,Y)}{var(X)}
(\#eq:b1)
\end{equation}

\begin{equation} 
\beta_0 = \widehat{Y}-\beta_1 \widehat{X}
(\#eq:b0)
\end{equation}

```{r figreg2, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Droite de régression, valeurs observées, prédites et résidus", out.width='65%'}
df <- read.csv("data/bivariee/Reg.csv", stringsAsFactors = F)
modele <- lm(Velo~KmCV, df)
df$ypredit <- round(modele$fitted.values,3)
df$residus <- round(residuals(modele),3)
df$sqrtRes <- round(residuals(modele),3)^2
b0 <- round(modele$coefficients[1],3)
b1 <- round(modele$coefficients[2],3)
eq <- paste0("Vélo = ", b0, " ", b1, " x KmCV")
r2 <- round(summary(modele)$r.squared,3)
pearson <- round(cor(df$Velo, df$KmCV),3)
seg <- data.frame(x1 = df[3,3], x2 = 12, y1 = df[2,3], y2 = df[2,3]+5)
ggplot(df) +
  aes(
    x = KmCV,
    y = Velo,
    ymin = Velo,
    ymax = ypredit
  ) +
  stat_smooth(method="lm", se=FALSE, col="red")+
  labs(title= eq,
       subtitle = paste0("r2 = ", r2, ", r =", pearson), sep="")+
  xlab("Distance au centre-ville (km)")+
  ylab("Vélo (%)")+
  geom_segment(aes(xend = df[9,3]+.2, x = df[9,3]+2,
                   yend = df[9,2], y = df[9,2]+3),
                   size=1.2, linetype="solid", colour="brown4",
                   arrow = arrow(length = unit(0.2, "inches")))+
  geom_segment(aes(xend = df[9,3]+.05, x = df[9,3]+2,
                   yend = df[9,2]-abs(df[9,5]/2),
                   y = df[9,2]-abs(df[9,5]/2)),
                   size=1.2, linetype="solid", colour="brown4",
                   arrow = arrow(length = unit(0.2, "inches")))+
  geom_segment(aes(xend = df[9,3], x =df[9,3]-.5,
                   yend = df[9,4]-.5, y = df[9,4]-9),
                   size=1.2, linetype="solid", colour="brown4",
                   arrow = arrow(length = unit(0.2, "inches")))+
  annotate(geom="text", x =df[9,3]+2.2, y= df[9,2]+4,
           label="Valeur observée pour la municipalité I", color="black", hjust = 0, size = 5)+
  annotate(geom="text", x =df[9,3]+2.2, y=df[9,2]+3,
           label="x =5,225 km, et y = 25,3%", color="black", hjust = 0, size = 5)+
  annotate(geom="text", x =df[9,3]+2.2, df[9,2]-abs(df[9,5]/2)+.2,
           label="résidu = 25,3 - 23,038 = 2,262", color="black", hjust = 0, size = 5)+
  annotate(geom="text", x =df[9,3], y= df[9,4]-9.5,
           label="Valeur prédite pour I", color="black", hjust = 0.5, size = 5)+
  
    annotate(geom="text", x =df[9,3], y= df[9,4]-10.5,
           label="30,603 - 1,448 x 5,225 = 23,038", color="black", hjust = 0.5, size = 5)+
  geom_pointrange(colour="black", size=.8, linetype="solid", shape=16)+
  geom_point(colour="blue", size=3, aes(x=KmCV, y=ypredit))
```  

```{r regfictives2, echo=FALSE, message=FALSE, warning=FALSE}
df$sqrtRes <- round(df$residus^2,3)
sqrtres <- round(sum(df$sqrtRes),3)
df[11,1] <- "Somme"
df[11,5] <- 0
df[11,6] <- sqrtres
df1 <- rbind(df[1:5,],df[11,])
df2 <- rbind(df[6:10,],df[11,])
opts <- options(knitr.kable.NA = "--")
knitr::kable(
  head(df, nrow(df)), booktabs = TRUE, valign = 't', row.names = FALSE,
   format.args = list(decimal.mark = ",", big.mark = " "),           
  col.names = c("Municipalité","Vélo","KMCV","Valeur prédite","Résidu", "Résidu au carré"),
  caption = "Valeurs observées, prédites et résidus")
``` 

#### Mesurer la qualité d'ajustement du modèle {#sect04143}
Les trois mesures les plus courantes pour évaluer la qualité d'ajustement d'un modèle de régression linéaire simple sont l'erreur quadratique moyenne (en anglais, *root-mean-square error*, *RMSE*), le coefficient de détermination ($R^2$) et la statistique *F* de Fisher. Pour mieux appréhender le calcul de ces trois mesures, rappelons que l'équation de régression s'écrit : 

\begin{equation} 
y_i = \beta_0 + \beta_1 x_1+ \epsilon_i \Rightarrow Y= \beta_0 + \beta_1 X + \epsilon
(\#eq:reg2)
\end{equation}

Elle comprend ainsi une partie de *Y* qui est expliquée par le modèle et une autre partie non expliquée : $\epsilon$ appelé habituellement le terme d'erreur. Ce terme d'erreur pourrait représenter d'autres variables explicatives qui n'ont pas été prises en compte pour prédire la variable indépendante ou une forme de variation aléatoire inexplicable présente lors de la mesure.

\begin{equation} 
Y  = \underbrace{\beta_0 + \beta_1 X}_{\mbox{partie expliquée par le modèle}}+ \underbrace{\epsilon}_{\mbox{partie non expliquée}}
(\#eq:reg3)
\end{equation}

Par exemple, pour la municipalité *A* au tableau \@ref(tab:regfictives2), nous avons : $y_A = \widehat{y}_A - \epsilon_A \Rightarrow 12.5 = 10.138+2.362$. Souvenez-vous que la variance d'une variable est la somme des écarts à la moyenne, divisée par le nombre d'observations. Par extension, il est alors possible de décomposer la variance de *Y* comme suit :

\begin{equation} 
\underbrace{\sum_{i=1}^n (y_{i}-\bar{y})^2}_{\mbox{variance de Y}} = \underbrace{\sum_{i=1}^n (\widehat{y}_i-\bar{y})^2}_{\mbox{var. expliquée}} + \underbrace{\sum_{i=1}^n (y_{i}-\widehat{y})^2}_{\mbox{var. non expliquée}} \Rightarrow 
SCT = SCE + SCR
(\#eq:reg4)
\end{equation}

avec :

* *SCT* est la somme des écarts au carré des valeurs observées à la moyenne (en anglais, _total sum of squares_)
* *SCE* est la somme des écarts au carré des valeurs prédites à la moyenne (en anglais, _regression sum of squares_)
* *SCR* est la somme des carrés des résidus (en anglais, _sum of squared errors_).

Autrement dit, la variance totale est égale à la variance expliquée plus la variance non expliquée. Au tableau \@ref(tab:computeR), vous pouvez repérer les valeurs de *SCT*, *SCE* et *SCR* et constater que 279,30 = 227,04 + 52,26 et 27,93 = 22,70 + 5,23.

```{r computeR, echo=FALSE, message=FALSE, warning=FALSE, out.width='85%'}
data <- read.csv("data/bivariee/Reg.csv", stringsAsFactors = F)
modele <- lm(Velo~KmCV, data)
data$yi <- data$Velo
data$ypredit <- modele$fitted.values
data$ei <- modele$residuals
data$Velo <- NULL
data$KmCV <- NULL
n <- nrow(data)
moy_y <- mean(data$yi)
sumy <- sum(data$yi)
data$yi_ymean2 <- (data$yi - moy_y)^2
data$ypredit_ymean2 <- (data$ypredit - moy_y)^2
data$ei2 <- (data$ei)^2
df <- data
df[11,1] <- "N"
df[11,2] <- n
df[12,1] <- "Somme"
df[13,1] <- "Moyenne"
for (i in c(2,4:7))
{
  df[12,i] <- sum(df[1:10,i])
  df[13,i] <- mean(df[1:10,i])
  df[i] <- round(df[i], 2)
}
df[3] <- round(df[3], 2)
colnames(df) <- c("Municipalité", "$y_i$", "$\\widehat{y}_i$", "$\\epsilon_i$","$(y_i-\\bar{y})^2$","$(\\widehat{y}_i-y_i)^2$", "$\\epsilon_i^2$")
opts <- options(knitr.kable.NA = "--")
knitr::kable(head(df, nrow(df)),
   format.args = list(decimal.mark = ",", big.mark = " "),           
   caption = "Calcul du coefficient de détermination",
   booktabs = TRUE, valign = 't', row.names = FALSE)
```


**Calcul de l'erreur quadratique moyenne**

La somme des résidus au carré (*SCR*) divisée par le nombre d'observations représente donc le carré moyen des erreurs (en anglais, *mean square error - MSE*), soit la variance résiduelle du modèle ($52,26 / 10 = 5,23$). Plus sa valeur sera faible, plus le modèle sera efficace pour prédire la variable indépendante. L'erreur quadratique moyenne (en anglais, *root-mean-square error - RMSE*) est simplement la racine carrée de la somme des résidus au carré divisée par le nombre d'observations ($n$) :

\begin{equation} 
RMSE = \frac{\sqrt{\sum_{i=1}^n (y_{i}-\widehat{y})^2}}{n}
(\#eq:reg5)
\end{equation}

Elle représente ainsi une **mesure absolue des erreurs** qui est exprimée dans l'unité de mesure de la variable dépendante. Dans le cas présent, on a : $\sqrt{5,23}=2,29$. Cela signifie qu'en moyenne, l'écart absolu (ou erreur absolue) entre les valeurs observées et prédites est de 2,29 points de pourcentage. De nouveau, une plus faible valeur de **RMSE** indique un meilleur ajustement du modèle. Mais surtout, le RMSE permet d'évaluer avec quelle précision le modèle prédit la variable dépendante. Il est donc particulièrement important si l'objectif principal du modèle est de prédire des valeurs sur un échantillon d'observations pour lequel la variable dépendante est inconnue.

**Calcul du coefficient de détermination**

Nous avons largement démontré que la variance totale est égale à la variance expliquée plus la variance non expliquée. La qualité du modèle peut donc être évaluée avec le coefficient de détermination ($R^2$), soit le rapport entre les variances expliquée et totale : 

\begin{equation} 
R^2 = \frac{SCE}{SCT} \mbox{ avec } R^2 \in \left[0,1\right]
(\#eq:reg6)
\end{equation}

Comparativement au RMSE qui est une mesure absolue, le coefficient de détermination est une **mesure relative** qui varie de 0 à 1. Il exprime la proportion de la variance de *Y* qui est expliquée par la variable *X*; autrement dit, plus sa valeur est élevée, plus *X* influence / est capable de prédire *Y*. Dans le cas présent, on a : $R^2 = 227.04 / 279.3 = 0.8129$, ce qui signale que 81,3% de la variance du pourcentage de cyclistes est expliquée par la distance au centre-ville. Tel que signalé dans la section \@ref(sect04132), la racine carrée du coefficient de détermination ($R^2$) est égale au coefficient de corrélation ($r$) entre les deux variables. 

**Calcul de la statistique _F_ de Fisher**

La statistique _F_ de Fisher permet de vérifier la significativité globale du modèle.

\begin{equation} 
F = (n-2)\frac{R^2}{1-R^2} = (n-2)\frac{SCE}{SCR}
(\#eq:reg7)
\end{equation}

L'hypothèse nulle (h<sub>0</sub> avec $\beta_1=0$) est rejetée si la valeur calculée de *F* est supérieure à la valeur critique de la table *F* avec *(1, n-2)* degrés de liberté et un seuil $\alpha$ (*p*=0,05 habituellement) (voir le tableau des valeurs critiques de F, section \@ref(annexe2)). Notez qu'on utilise rarement la table *F* puisqu'avec la fonction `pf(f obtenu, 1, n-2, lower.tail = FALSE)` l'on obtient directement la valeur de *p* associée à la valeur de *F*. Concrètement, si le test _F_ est significatif (avec *p*<0,05), plus la valeur de *F* sera élevée, plus le modèle sera efficace (et plus le $R^2$ sera également élevé).

Notez que la fonction *summary* renvoie les résultats du modèle, dont notamment le test _F_ de Fisher.

```{r echo=TRUE, message=FALSE, warning=FALSE, out.width='85%'}
# utiliser la fonction summary
summary(modele)
```

Dans le cas présent, $F = (10 - 2)\frac{0.8129}{1-0.8129} = (10-2)\frac{227.04}{52.26} = 34.75$ avec une valeur de $p < 0.001$. Par conséquent, le modèle est significatif.

#### Mise en œuvre dans ![](images/Rlogo.png) {#sect04144}
Comment calculer une régression linéaire simple dans ![](images/Rlogo.png)? Rien de plus simple avec la fonction `lm(formula = y ~ x, data= dataframe)`.

```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/Reg.csv", stringsAsFactors = F)
## Création d'un objet pour le modèle
monmodele <- lm(Velo ~ KmCV, df)
## Sorties du modèle avec la fonction summary
summary(monmodele)
## Calcul du MSE et du RMSE
MSE <- mean(monmodele$residuals^2)
RMSE <- sqrt(MSE)
cat("MSE=", round(MSE, 2), "; RMSE=", round(RMSE,2), sep="")
```

#### Comment rapporter une régression linéaire simple {#sect04145}

Nous avons calculé une régression linéaire simple pour prédire le pourcentage d'actifs occupés utilisant le vélo pour se rendre au travail en fonction de la distance au centre-ville (en kilomètres). Le modèle obtient un *F* de Fisher significatif (*F*(1,8)= 34,75, *p* < 0,001) et un $R^2$ de 0,813. Le pourcentage de cyclistes peut être prédit par l'équation suivante : 30,603 - 1,448 x (distance au centre-ville en km).

## Relation entre deux variables qualitatives {#sect042}

::: {.bloc_objectif .bloc_objectif_png data-latex="{blocs/objectif}"}
**Deux variables qualitatives sont-elles associées entre elles?** Plus spécifiquement, certaines modalités d'une variable qualitative sont-elles associées significativement à certaines modalités d'une variable qualitative.

Prenons l'exemple de deux variables qualitatives : l'une intitulée *groupe d'âge* comprenant trois modalités (15 à 29 ans, 30 à 44 ans, 45 à 64 ans); l'autre intitulée *mode de transport habituel pour se rendre au travail* comprenant quatre modalités (véhicule motorisé, transport en commun, vélo, marche). 

Comparativement aux deux autres groupes, on pourrait supposer que les jeunes se déplacent proportionnellement plus en modes de transport actifs (vélo et marche) et en transports en commun. À l'inverse, il est possible que les 45 à 64 ans se déplacent majoritairement en véhicule motorisé.

Pour vérifier l'existence d'associations significatives entre les modalités de deux variables qualitatives, il est possible de construire un **tableau de contingence**\index{tableau de contingence} (section \@ref(sect0421)), puis de réaliser le **test du khi^2^**\index{khi2} (section \@ref(sect0422)).
:::


### Construction de tableau de contingence {#sect0421}

Les données du tableau de contingence ci-dessous décrivent 279 projets d'habitation à loyer modique (HLM) dans l'ancienne ville de Montréal, croisant les modalités de la période de construction (en colonnes) et de la taille (en ligne) des projets HLM [@TheseApparicio]. Les différents éléments du tableau sont décrits ci-dessous.

* **Les fréquences observées**, nommées communément $f_{ij}$, correspondent aux observations appartenant à la fois à la *i^ème^* modalité de la variable en ligne et à la *j^ème^* modalité de la variable en colonne. À titre d’exemple, on compte 14 HLM construits entre 1985 et 1989 comprenant moins de 25 logements.

* **Les marges** du tableau sont les totaux pour chaque modalité en ligne ($n_{i.}$) et en colonne ($n_{j.}$). En guise d’exemple, sur les 279 projets HLM, 53 comprennent de 25 à 49 logements et 56 ont été construites entre 1968 et 1974. Bien entenu, la somme des marges en ligne ($n_{i.}$) est égale au nombre total d'observations ($n_{ij}$), tout comme la somme de marges en colonne ($n_{.j}$).

* **Trois pourcentages** sont disponibles (total, en ligne, en colonne). Ils sont respectivement la fréquence observée divisée par le nombre d'observations ($f_{ij}/n_{ij}×100$), par la marge en ligne ($f_{ij}/n_{i.}×100$) et en colonne ($f_{ij}/n_{.j}100$). En guise d'exemple, 5% des 279 projets HLM ont été construits entre 1985 et 1989 et comprennent moins de 25 logements (pourcentage total, soit 14/279×100). Aussi, plus de la moitié des habitations de moins de 25 logements ont été construits entre 1990 et 1994 (pourcentage en ligne, 41/80×100). Finalement, près de 36% des logements construits avant 1975 ont 100 logements et plus (20/56×100).

* **Les fréquences théoriques**, représentent les valeurs que l'on devrait observer théoriquement s'il y avait indépendance entre les modalités des deux variables : si la répartition des deux modalités des deux variables étaient dûes au hasard. Pour le croisement de deux modalités, la fréquence théorique est égale au produit des marges divisé par le nombre total d'observations ($ft_{ij} = (n_{i.}n_{.j})/n_{ij}$). Par exemple, la fréquence théorique pour le croisement des modalité *moins de 25 logements* et *avant 1975* est égale à : (80×56)/279 = 16,06. Nous observons ici que la valeur théorique (16,06) est bien supérieure à la valeur réelle (6). On observe donc moins de HLM de moins de 25 logements avant 1975 que ce que l'on pourrait attendre du hasard.

* **La déviation** est la différence entre la fréquence observée et la fréquence théorique ($f_{ij}-ft_{ij}$). Plus la déviation est grande, plus on s'écarte d'une situation d'indépendance entre les deux modalités *i* et *j*. La somme des déviations sur une ligne ou sur une colonne est nulle. Si la déviation *ij* est nulle, la fréquence théorique est égale à la fréquence observée, ce qui signifie qu’il y a indépendance entre les modalités *i* et *j*. Une déviation positive traduit, quant à elle, une attraction entre les modalités *i* et *j*, ou autrement dit, une surreprésentation du phénomène *ij*; tandis qu’une déviation négative renvoie à une répulsion entre les modalités *i* et *j*, soit une sous-représentation du phénomène *ij*. Dans le cas précédent, on observait 6 habitations de moins de 25 logements construits avant 1975 et une fréquence théorique de 16,0. La déviation est donc -10,06, soit une sous-représentation du phénomène.


* **La contribution au khi^2^** est égale à la déviation au carré divisée par la fréquence théorique : $\chi_{ij}^2 = (f_{ij}-ft_{ij})^2/ft_{ij}$. Plus sa valeur est forte, plus il y a association entre les deux modalités. La somme des contributions au khi^2^ représente le *khi^2^* total pour l'ensemble du tableau de contingence (ici à 63,54) que nous abordons dans la section suivante.


```{r tablecontingence, echo=FALSE, message=FALSE, warning=FALSE, out.width='75%'}
library("gmodels")
TabKhi2 <- read.csv("data/bivariee/hlm.csv")
# Création d'un facteur pour les modalités de la période de construction
TabKhi2$Periode <- factor(TabKhi2$Periode, 
                              levels = c(1,2,3,4,5), 
                              labels = c("Av. 1975", "1975-79", "1980-84", "1985-89", "1990-94"))
# Création d'un facteur pour les modalités de la taille
TabKhi2$Taille <- factor(TabKhi2$Taille, 
                             levels = c(1,2,3,4), 
                             labels = c("< 25 log.", "25-49", "50-99", "100 et +"))
# Tableau de contingence en utilisant la fonction CrossTable du package gmodels
CrossTable(TabKhi2$Taille, TabKhi2$Periode,
           expected=TRUE, chisq = TRUE, resid = TRUE, digits = 2, format="SPSS")
```

### Test du khi^2^ {#sect0422}
Avec le test du khi^2^, on postule qu'il y a indépendance entre les modalités des deux variables qualitatives, soit l'hypothèse nulle (h<sub>0</sub>). Puis, on cacule le nombre de degrés de liberté : $DL = (n-1)(l-1)$ avec $l$ et $n$ étant respectivement les nombres de modalités en ligne et en colonne. Pour notre tableau de contingence, nous avons 12 degrés de liberté : $(4-1)(5-1)=12$. À partir du nombre de degré de liberté et d'un seuil critique de significativité (prenons 5% ici), nous pouvons trouver la valeur critique de khi^2^ dans le tableau des valeurs critiques du khi^2^ : 21,03 section \@ref(annexe1)). Puisque la valeur du khi^2^ calculé dans le tableau de contingence est bien supérieure à celle obtenue dans le tableau des valeurs critiques (63,54), on peut rejeter l'hypothèse d'indépendance au seuil de 5%. Autrement dit, si les deux variables n'étaient pas associées, nous aurions eu moins de 5% de chances de collecter des données avec ce niveau d'association, ce qui nous permet de rejeter l'hypothèse nulle (absence d'association). Notez que le test reste significatif avec des seuils de 1% (*p*=0,01) et 0,1% (*p*=0,001) puisque les valeurs critiques sont de 26,22 et 32,91.

Bien entendu, une fois que l'on connait le nombre de degrés de liberté, on peut directement calculer les valeurs critiques pour différents seuils de signification et éviter ainsi de recourir à la table ci-dessus. Dans la même veine, on peut aussi calculer la valeur de *p* d'un tableau de contingence en spécifiant le nombre de degrés de liberté et la valeur du khi^2^ obtenue.

```{r echo=TRUE, message=FALSE, warning=FALSE}
cat("Valeurs critiques du khi2 avec le nombre de degrés de liberté", "\n",
    round(qchisq(p=0.95,  df=12, lower.tail = FALSE),3), "avec p=0,05", "\n",
    round(qchisq(p=0.99,  df=12, lower.tail = FALSE),3), "avec p=0,01", "\n",
    round(qchisq(p=0.999, df=12, lower.tail = FALSE),3), "avec p=0,0001")
cat("Valeurs de p du Khi2 obtenu (63.54291) avec 12 degrés de liberté :", "\n",
    pchisq(q=63.54291, df=12, lower.tail = FALSE))
```

::: {.bloc_aller_loin .bloc_aller_loin_png data-latex="{blocs/aller_loin}"}
Outre le khi^2^, d'autres mesures d'association permettent de mesurer le degré d'association entre deux variables qualitatives. Les plus courantes sont reportées au tableau ci-dessous. À des fins de comparaison, le khi^2^ décrit précédemment est aussi reporté sur la première ligne du tableau.

Statistique | Formule | Propriété et interprétation
-------- | ------- | -----------------------------
khi^2^  | $\chi^2 = \sum \frac{(f_{ij}-ft_{ij})^2}{ft_{ij}}$ | Mesure classique du Khi^2^ calculé à partir des différences entre les fréquences observées et attendues. Valeur de *p* disponible.
Ratio de vraissemblance du  khi^2^ | $G^2 = 2 \sum f_{ij} \ln{(\frac{f_{ij}}{ft_{ij}})}$ | Calculé à partir du ratio entre les fréquences observées et attendues. Valeur de *p* disponible.
khi^2^ de Mantel-Haenszel | $Q_{MH}=(N−1)r^2$ | avec $r$ étant le coefficient de corrélation entre les deux variables qualitatives; par exemple, entre les valeurs des modalités de 1 à 5 de la variable *période de construction* et celles de 1 à 4 de la variable *taille du projet* HLM. Ce coefficient est très utile quand les deux variables qualitatives ne sont pas nominales, mais **ordinales**. Valeur de *p* disponible.
Corrélation polychorique | obtenue itérativement par maximum de vraissemblance | Dans le même esprit que le khi^2^ de Mantel-Haenszel, la corrélation polychorique s'applique à deux variables ordinales. Plus spécifiquement, elle formule le postulat que deux variables théoriques normalement distribuées ont été mesurées de façon approximative avec deux échelles ordinales. Par exemple, en psychologie, le sentiment de bien être et le sentiment de sécurité peuvent être conceptualisés comme deux variables continues normalement distribuées. Cependant, les mesurer directement est très difficile, on a donc recours à des échelles de Likert allant de 1 à 10. Pour cet exemple, il serait pertinent d'utiliser la corrélation polychorique. Comme une corrélation de Pearson, la corrélation polychorique varie de -1 à 1, une valeur négative indiquant une relation inverse entre les deux variables théoriques et inversement. Une valeur de *p* peut être obtenue.
Coefficient Phi | $\phi=\sqrt{\frac{\chi^2}{n}}$ | Simplement le Khi^2^ divisé par le nombre d'observations. Si les deux variables qualitatives comprennent deux modalités chacune (tableau 2x2 dimensions) alors $\phi$ varie de −1 à 1; sinon de 0 à $min(\sqrt{c-1}, \sqrt{l-1})$ avec $c$ et $l$ étant le nombre de modalités en colonne et en ligne. Par conséquent, ce coefficient est peu utile pour les tableaux de plus de 2x2 dimensions. Pas de valeur de *p* disponible.
V de Cramer | $V=\sqrt{\frac{\chi^2/n}{min(c-1,l-1)}}$ | Il représente un ajustement du coefficient Phi et varie de 0 à 1. Plus sa valeur est forte plus les deux variables sont associées. À la lecture des deux formules, vous constaterez que pour un tableau de 2 x 2, la valeur du V de Carmer sera égale à celle du Coefficient Phi. Pas de valeur de *p* disponible.
:::

### Mise en œuvre dans ![](images/Rlogo.png) {#sect0423}

Pour calculer le Khi^2^ entre deux variables qualitatives, on utilise la fonction de base : `chisq.test(x = ..., y = ...)` qui renvoie le nombre de degré de liberté, les valeurs du Khi^2^ et de *p*.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Importation du csv
dataHLM <- read.csv("data/bivariee/hlm.csv")
# Calcul du Khi2 avec la fonction de base chisq.test
chisq.test(x = dataHLM$Taille, y = dataHLM$Periode)
```

Pour la construction du tableau de contingence, deux options sont possibles dépendamment de la structuration de votre tableau de données initial. Premier cas de figure, votre tableau comprend une ligne par observation avec les différentes modalités dans deux colonnes (ici *Periode* et *Taille*). Dans la syntaxe ci-dessous, pour chacune des deux variables qualitatives, on crée un facteur afin de spécifier un intitulé à chaque modalité (`factor(levels =c(....), labels = c(..)`). Puis, on utilise la fonction `CrossTable` du package **gmodels**. Pour obtenir les fréquences théoriques, les contributions locales au Khi^2^ et les déviations, on spécifie les options suivantes : `expected=TRUE, chisq=TRUE, resid=TRUE`.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("gmodels")
#Premiers enregistrements du tableau
head(dataHLM)
# La variable Periode comprend 5 modalités (de 1 à 5)
table(dataHLM$Periode)
# La variable Periode comprend 4 modalités (de 1 à 4)
table(dataHLM$Taille)
#Création d'un facteur pour les cinq modalités de la période de construction
dataHLM$Periode <- factor(dataHLM$Periode, 
                          levels = c(1,2,3,4,5), 
                          labels = c("<1975", 
                                     "1975-1979", 
                                     "1980-1984", 
                                     "1985-1989", 
                                     "1990-1994"))
#Création d'un facteur pour les quatre modalités de la taille des habitations
dataHLM$Taille <- factor(dataHLM$Taille, 
                         levels = c(1,2,3,4), 
                         labels = c("<25 log.", 
                                    "25-49", 
                                    "50-99", 
                                    "100 et +"))
# Pour construire un tableau de contingence on utilise la fonction CrossTable 
# (package gmodels) les deux lignes ci-dessous sont mises en commentaire 
# pour ne pas répéter le tableau
#CrossTable(x=dataHLM$Taille, y=dataHLM$Periode, digits = 2,
#           expected=TRUE, chisq = TRUE, resid = TRUE, format="SPSS")
```

Deuxième cas de figure, vous disposez déjà d'un tableau de contingence, soit les fréquences observées ($f_{ij}$). On n'utilise donc pas la fonction `CrossTable`, mais directement la fonction `chisq.test`.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Importation des données
df <-  read.csv("data/bivariee/data_transport.csv", stringsAsFactors = FALSE)
df # Visualisation du tableau
Matrice <- as.matrix(df[, c("Homme","Femme")])
dimnames(Matrice) <- list(unique(df$ModeTransport), Sexe=c("Homme","Femme"))
# Notez que vous pouvez saisir vos données directement si vous avez peu d'observations
Femme <- c(689400, 21315, 181435, 43715, 24295, 8395) # Vecteur de valeurs pour les femmes
Homme <- c(561830, 40010, 238330, 54360, 13765, 6970) # Vecteur de valeurs pour les hommes
Matrice <- as.table(cbind(Femme, Homme)) # Création du tableau
# Nom des deux variables et de leurs modalités respectives
dimnames(Matrice) <- list(transport=c("Automobile (conducteur)",
                                      "Automobile (passager)",
                                      "Transport en commun",                            
                                      "A pied",
                                      "Bicyclette",
                                      "Autre moyen"),
                          sexe=c("Homme","Femme"))
# Test du Khi2
test <- chisq.test(Matrice)
print(test)
# Fréquences observées (Fij)
test$observed
# Fréquences théoriques (FTij)
round(test$expected,0)
# Déviations (Fij - FTij)
round(test$observed-test$expected,0)
# Contributions au Khi2
round((test$observed-test$expected)^2/test$expected,2)
# Marges en lignes et en colonnes
colSums(Matrice)
rowSums(Matrice)
# Grand total
sum(Matrice)
# Pourcentages
round(Matrice/sum(Matrice)*100,2)
# Pourcentages en ligne
round(Matrice/rowSums(Matrice)*100,2)
# Pourcentages en colonne
round(Matrice/colSums(Matrice)*100,2)
```

Pour obtenir les autres mesures d'association, on pourra utiliser la syntaxe ![](images/Rlogo.png) suivante :

```{r echo=TRUE, message=FALSE, warning=FALSE}
df <- read.csv("data/bivariee/hlm.csv")
# Fonction pour calculer les autres mesures d'association
AutresMesuresKhi2 <- function(x, y){
  testChi2 <- chisq.test(x, y) # Calcul du Chi2
  n <- sum(testChi2$observed)  # Nombre d'observations
  c <- ncol(testChi2$observed) # Nombre de colonnes
  l <- nrow(testChi2$observed) # Nombre de lignes
  dl <- (c-1)*(l-1)            # Nombre de degrés de libertés
  chi2 <- testChi2$statistic   # Khi2
  Pchi2 <- testChi2$p.value    # P pour le Khi2
  
  #Ratio de vraissemblance du khi2
  G <- 2*sum(testChi2$observed*log(testChi2$observed/testChi2$expected)) # G2
  PG <- pchisq(G, df=dl, lower.tail = FALSE) # P pour le G22
  
  # khi2 de Mantel-Haenszel avec la librarie DescTools
  MHTest <- DescTools::MHChisqTest(testChi2$observed)
  MH <- MHTest$statistic
  PMH <- MHTest$p.value
  
  # Coefficient de correlation Polychorique
  df <- data.frame("x" = as.factor(x),
                 "y" = as.factor(y))
  polychoricCorr <- correlation::cor_test(df,"x","y",method = "polychoric")
  polyR <- polychoricCorr$rho
  polyP <- polychoricCorr$p
  
  # Coefficient Phi et V de Cramer
  phi <- sqrt(chi2/n)
  vc <- sqrt(chi2/(n*min(c-1,l-1)))
  
  # Tableau pour les sorties
  dfsortie <- data.frame(
        Statistique = c("Khi2", 
                        "Ratio de vraissemblance du  khi", 
                        "Khi2 de Mantel-Haenszel",
                        "Corrélation Polychoric",
                        "Coefficient de Phi",
                        "V de Cramer"), 
        Valeur = round(c(Pchi2, G, MH, polyR, phi, vc),3), 
        P = round(c(Pchi2, PG, PMH, polyP , NA, NA),10))
  return(dfsortie)
}
kableExtra::kable(AutresMesuresKhi2(df$Periode, df$Taille),
          digits = 3,
          caption="Mesures d'association entre deux variables qualitatives")
```

### Interprétation d'un tableau de contingence {#sect0424}

Nous vous proposons une démarche très simple pour vérifier l'association entre deux variables qualitatives avec les étapes suivantes :

* On pose l'hypothèse nulle (h<sub>0</sub>), soit l'indépendance entre les deux variables. Si le Khi^2^ total du tableau de contingence est inférieur à la valeur critique du Khi^2^ avec *p*=0,05 et le nombre de degrés de liberté de la table *T*, alors il y a bien indépendance. La valeur de *p* sera alors supérieure à 0,05. L'analyse s'arrête donc là ! Autrement dit, il n'est pas nécessaire d'analyser le contenu de votre tableau de contingence puisqu'il n'y pas d'associations significatives entre les modalités des deux variables. Vous pouvez simplement signaler que : selon les résultats du test du Khi^2^, il n'y a pas d'association significative entre les deux variables ($\chi$ = ... avec *p*= ...).

* S'il y a dépendance ($khi_{observé}^2 > khi_{critique}^2$), trouver les cellules *ij* où les contributions au Khi^2^ sont les plus fortes, c'est-à-dire où les liens entre les modalités *i* de la variable en ligne et les modalités *j* de la variable en colonne sont les plus marqués. Pour ces cellules, le phénomène *ij* est surreprésenté si la déviation est positive ou sous-représenté si la déviation est négative. Commentez ces associations et utilisez les pourcentages en lignes ou en colonnes pour appuyer vos propos.

::: {.bloc_astuce .bloc_astuce_png data-latex="{blocs/astuce}"}
Pour repérer rapidement les cellules où les contributions au Khi^2^ sont les plus fortes, vous pouvez construire un graphique avec la fonction **mosaic** du *package* **vcd**. À la figure \@ref(fig:figVDC), la taille des rectangles représentent les effectifs entre les deux modalités tandis que les associations sont représentées comme suit : 
en gris lorsqu'elles ne sont pas significatives, en rouge pour des déviations significatives et négatives et en bleu pour des déviations significatives et positives.

```{r figVDC, echo=TRUE, fig.align='center', fig.cap='Graphique pour un tableau de contingence', message=FALSE, warning=FALSE, out.width='65%'}
library(vcd)
mosaic(~ Taille+Periode, data=dataHLM,shade=TRUE, legend=TRUE)
```
:::

**Exemple d'interprétation.** « Les résultats du test du khi^2^ signale qu'il existe des associations entre les modalités de la taille et de la période de construction des projets d'habitation ($\chi$ =63,5, *p* < 0,001). Les fortes contributions au khi^2^ et le signe positif ou négatif des déviations correspondantes permettent de repérer cinq associations majeures entre les modalités de taille et de période de construction des projets HLM : **1)** la répulsion entre les projets d’habitation de moins de 25 logements et la période de construction 1964-1974; **2)** l’attraction entre les projets d’habitation de 100 logements et plus et la période de construction de 1969-1974; **3)** l’attraction entre les projets d’habitation de moins de 25 logements et la période de construction de 1990-1994; **4)** la répulsion entre les projets d’habitation de 50 à 99 logements et la période de construction 1990-1994; **5)** la répulsion entre les projets d’habitation de 100 logements et plus et la période de construction 1990-1994.
On observe donc une tendance bien marquée dans l’évolution du type de construction entre 1970 et 1994 : entre 1969 et 1974, on construit habituellement de grandes habitations dépassant souvent 100 logements; du milieu des années 1970 à la fin des années 1980, on privilégie la construction d’habitations de taille plus modeste, entre 50 et 100 logements; tandis qu’au début des années 1990, on opte plutôt pour des habitations de taille réduite (moins de 50 logements). Quelques chiffres à l’appui : sur les 56 habitations réalisées entre 1969 et 1974, 20 ont plus de 100 logements, 20 comprennent entre 50 et 99 logements et seules 10 ont moins de 25 logements. Près de la moitié des habitations construites entre 1975 et 1989 regroupent 50 à 99 logements (43,8% pour la période 1975-1979, 45,8% pour 1980-1984 et 44,7% pour 1985-1989). Par contre, 51% des logements érigés à partir de 1990 disposent de moins de 25 logements » (Apparicio, 2002, 117-118). Notez que cette évolution décroissante est aussi soutenue par le coefficient négatif de la corrélation polychorique.

Vous pouvez aussi construire un graphique pour appuyer vos constats, soit avec les pourcentages en ligne ou en colonne (figure \@ref(fig:fighlm) tirée de @apparicio2006).

```{r fighlm, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Taille des ensembles HLM selon la période de construction",  out.width='80%'}
knitr::include_graphics('images/bivariee/figurehlm.png', dpi = NA)
```

**Comment rapporter succinctement les résultats d'un Test du Khi^2^?**

Le test du Khi^2^ a été réalisé pour examiner la relation entre la taille et la période de construction des habitations HLM. Cette relation est significative : $\chi^2$(12, N = 279) = 63,5, *p* < 0,001. Plus les projets ont été construits récemment, plus ils sont de taille réduite.

Pour un texte en anglais, vous pourrez consulter [https://www.socscistatistics.com/tutorials/chisquare/default.aspx](https://www.socscistatistics.com/tutorials/chisquare/default.aspx){target="_blank"}.



## Relation entre une variable quantitative et une variable qualitative à deux modalités {#sect043}

::: {.bloc_objectif .bloc_objectif_png data-latex="{blocs/objectif}"}

**Les moyennes de deux groupes de population sont-elles significativement différentes?** On souhaite ici comparer deux groupes de population en fonction d'une variable continue. 
Par exemple, pour deux échantillons respectivement d'hommes et de femmes travaillant dans le même secteur d'activité, on pourrait souhaiter vérifier si les moyennes des salaires des hommes et des femmes sont différentes et ainsi vérifier la présence ou l'absence d'une iniquité systématique. En études urbaines, dans le cadre d'une étude sur un espace public, on pourrait vouloir vérifier si la différence des moyennes du sentiment de sécurité des femmes et des hommes est significative (c'est-à-dire différente de 0).

**Pour un même groupe, la moyenne de la différence d'un phémonène donné mesuré à deux moments est-elle ou non égale à zéro?** Autrement dit, on cherche à comparer un même groupe d'individus avant et après une expérimentation, ou dans deux contextes différents. Prenons un exemple d'application en études urbaines. Dans le cadre d'une étude sur la perception des risques associés à la pratique du vélo en ville, 50 individus utilisant habituellement l'automobile pour se rendre au travail sont recrutés. L'expérimentation pourrait consister à leur donner une formation sur la pratique du vélo en ville et à les accompagner quelques jours durant leurs déplacements domicile-travail. On évaluera la différence de leurs perceptions des risques associés à la pratique du vélo sur une échelle de 0 à 100 avant et après l'expérimentation. On pourrait supposer que la moyenne des différences est significativement négative, ce qui indiquerait que la perception du risque a diminué après l'expérimentation; autrement dit, la perception du risque serait plus faible en fin de période. 
:::


### Test *t* et ses différentes variantes {#sect0431}

Le **t de student**, appelé aussi **test _t_** (*t-test* en anglais), est un test paramétrique permettant de comparer les moyennes de deux groupes (échantillons), qui peuvent être indépendantes ou non :

* **Échantillons indépendants (dits non appariés)**, les observations de deux groupes qui n'ont aucun lien entre eux. Par exemple, on souhaite vérifier si les moyennes du sentiment de sécurité des hommes et des femmes, ou encore si, les moyennes des loyers entre deux villes sont statistiquement différentes. Ainsi, les tailles des deux échantillons peuvent être différentes ($n_a \neq n_b$).

* **Échantillons dépendants (dits appariés)**, les individus des deux groupes sont les mêmes et  sont donc associés par paires. Autrement dit, on a deux séries de valeurs de taille identique $n_a = n_b$ et $n_{ai}$ est le même individu que $n_{bi}$. Ce type d'analyse est souvent utilisée en études cliniques : pour $n$ individus, on dispose d'une mesure quantitative de leur état de santé pour deux séries (l'une avant le traitement, l'autre une fois le traitement terminé). Cela permet de comparer les mêmes individus avant et après un traitement, une expérimentation; on parle alors d'étude, d'expérience et d'analyse pré-post. Concrètement, on cherche à savoir si la moyenne des différences des observations avant et après est significativement différente de 0. Si c'est le cas, on peut en conclure que l'expérimentation a eu un impact sur le phénomène mesuré (variable continue). Ce type d'analyse pré-post peut aussi être utilisé pour évaluer l'impact du réaménagement d'un espace public (rue commerciale, place publique, parc, etc.). Par exemple, on pourrait questionner le même échantillon de commerçants ou d'usagers avant et après le réaménagement d'une artère commerciale.

**Condition d'application**. Pour utiliser les tests de Student et de Welch, la variable continue doit être normalement distribuée. Si elle est fortement anormale, on utilisera le test non paramétrique de Wilcoxon (section \@ref(sect0432)). Il existe trois principaux tests pour comparer les moyennes de deux groupes :

* Test de Student (test *t*) avec échantillons indépendants et variances similaires (méthode *pooled*).  Les variances de deux groupes sont semblables quand leur ratio varie de 0,5 à 2 ($0,5< (S^2_{X_A}/S^2_{X_B})<2$).
* Test de Welch (appelé aussi Satterthwaite) avec échantillons indépendants quand les variances des deux groupes sont dissemblables.
* Test de Student (test *t*) avec échantillons dépendants.

Il s'agit de vérifier si les moyennes des deux groupes sont statistiquement différentes avec les étapes suivantes :

* On pose l'hypothèse nulle (H<sub>0</sub>), soit que les moyennes des deux groupes *A* et *B* ne sont pas différentes ($\bar{X}_{A}=\bar{X}_{B}$) ou autrement dit, la différence des deux moyennes est nulle ($\bar{X}_{A}-\bar{X}_{B}=0$). L'hypothèse alternative (H<sub>1</sub>) est donc $\bar{X}_{A}\ne\bar{X}_{B}$.
* On calcule la valeur de *t* et le nombre de degrés de liberté. La valeur de *t* sera négative quand la moyenne du groupe A est inférieure au groupe B et inversement.
* On compare la valeur absolue de *t* ($\mid T \mid$) avec celle issue de la table des valeurs critiques T avec le bon nombre de degrés de liberté et en choisissant un degré de signification (habituellement, p = 0,05). Si ($\mid t \mid$) est supérieure à la valeur *t* critique, alors les moyennes sont statistiquement différentes au degré de signification retenu.
* Si les moyennes sont statistiquement différentes, on peut calculer la taille de l'effet.


**Cas 1. Test de student pour des échantillons indépendants avec variances égales (méthode *pooled*).** La valeur de *t* est le ratio entre la différence des moyennes des deux groupes (numérateur) et l'erreur-type groupée des deux échantillons (dénominateur) :

$t = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sqrt{\frac{S^2_p}{n_A}+\frac{S^2_p}{n_B}}}$ avec 
$S^2_p = \frac{(n_A-1)S^2_{X_A}+(n_B-1)S^2_{X_B}}{n_A+n_B-2}$

avec $n_A$,$n_B$, $S^2_{X_A}$ et $S^2_{X_B}$ étant respectivement les nombres d'observations et les variances pour les groupes *A* et *B*, $S^2_p$ étant la variance groupée des deux échantillons et $n_A+n_B-2$ étant le nombre de degrés de liberté.

**Cas 2. Test de Welch pour des échantillons indépendants (avec variances différentes).** Le test de Welch est très similaire au test de student; seul le calcul de la valeur de _T_ est différent, pour tenir compte des variances respectives des groupes :

$t = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sqrt{\frac{S^2_A}{n_A}+\frac{S^2_B}{n_B}}}$ et $dl = \frac{ \left( \frac{S^2_{X_A}}{n_A}+\frac{S^2_{X_B}}{n_B} \right)^2} {\frac{S^4_{X_A}}{n^2_A(n_A-1)}+\frac{S^4_{X_B}}{n^2_B(n_B-1)}}$

Dans la syntaxe ![](images/Rlogo.png) ci-dessous, nous avons écrit une fonction dénommée `test_independants` permettant de calculer les deux tests pour des échantillons indépendants. Dans cette fonction, vous pourrez repérer comment sont calculés les moyennes, nombres d'observations et variances pour les deux groupes, le nombre de degrés de liberté, les valeurs de *t* et de *p* pour les deux tests. Puis, nous avons créé aléatoirement deux jeux de données relativement à la vitesse de déplacement de cyclistes utilisant un vélo personnel ou un vélo en libre service (généralement plus lourd et moins utilisé par des cyclistes expérimentés) :

* Au cas 1, 60 cyclistes utilisant un vélo personnel roulant en moyenne à 18 km/h (écart-type de 1,5) et 50 utilisateurs du système de vélo partage avec une vitesse moyenne de 15 km/h (écart-type de 1,5).

* Au cas 2, 60 cyclistes utilisant un vélo personnel roulant en moyenne à 16 km/h (écart-type de 3) et 50 utilisateurs du système de vélo partage avec une vitesse moyenne de 15 km/h (écart-type de 1,5). Ce faible écart des moyennes, combiné à une plus forte variance va réduire la significativité de la différence entre les deux groupes.

D'emblée, l'analyse visuelle des boîtes à moustaches (figure \@ref(fig:figttest1)) signale qu'au cas 1 contrairement au cas 2, les groupes sont plus homogènes (boîtes plus compactes) et les moyennes semblent différentes (les boîtes sont centrées différemment sur l'axe des ordonnées). Cela est confirmé par les résultats des tests.

```{r figttest1, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Boîtes à moustaches sur des échantillons fictifs non appariés', out.width='75%'}
library("ggplot2")
library("ggpubr")
# fonction ------------------
tstudent_independants <- function(A, B){
    x_a <- mean(A)           # Moyenne du groupe A
    x_b <- mean(B)           # Moyenne du groupe B
    var_a <- var(A)          # Variance du groupe A
    var_b <- var(B)          # Variance du groupe B
    sd_a <- sqrt(var_a)      # Écart-type du groupe A
    sd_b <- sqrt(var_b)      # Écart-type du groupe B
    ratio_v <- var_a / var_b # ratio des variances
    n_a <- length(A)         # nombre d'observation du groupe A
    n_b <- length(B)         # nombre d'observation du groupe B
    
    # T-test (variances égales)
    dl_test <- n_a+n_b-2       # degrés de liberté
    PooledVar <- (((n_a-1)*var_a)+((n_b-1)*var_b))/dl_test
    t_test <- (x_a-x_b) / sqrt(((PooledVar/n_a)+(PooledVar/n_b)))
    p_test <-  2*(1-(pt(abs(t_test), dl_test)))     
    # Test Welch-Sattherwaite (variances inégales)
    t_welch <- (x_a-x_b) / sqrt( (var_a/n_a) + (var_b/n_b))
    dl_num = ((var_a/n_a) + (var_b/n_b))^2
    dl_dem = ((var_a/n_a)^2/(n_a-1))  + ((var_b/n_b)^2/(n_b-1))
    dl_welch = dl_num / dl_dem # degrés de liberté
    p_welch <- 2*(1-(pt(abs(t_welch), dl_welch)))     
    
    cat("\n groupe A (n = ", n_a,"), moy = ", round(x_a,1),", 
           variance = ", round(var_a,1),", écart-type = ", round(sd_a,1),
        "\n groupe B (n = ", n_b,"), moy = ", round(x_b,1),", 
          variance = ", round(var_b,1),", écart-type = ", round(sd_b,1),
        "\n ratio variance = ",round(ratio_v,2),
        "\n t-test (variances égales): t(dl = ", dl_test, ") = ",round(t_test,4),
         ", p = ", round(p_test,6),
         "\n t-Welch (variances inégales): t(dl = ", round(dl_welch,3), ") = ",
        round(t_welch,4), ", p = ", round(p_welch,6),  sep="")    
  
    if (ratio_v > 0.5 && ratio_v < 2)  {
      cat("\n Variances similaires. Utilisez le test de Student !")
      p <- p_test
    } else {
      cat("\n Variances dissemblables. Utilisez le test de Welch-Satterwaithe !")
      p <- p_welch
    }
    
    if (p <=.05){
      cat("\n Les moyennes des deux groupes sont significativement différentes.")
    } else {
      cat("\n Les moyennes des deux groupes ne sont pas significativement différentes.")
    }
}
# CAS 1 : données fictives ------------------
# Création du groupe A : 60 observations avec une vitesse moyenne de 18 et un écart-type de 1,5
Velo1A <- rnorm(60,18,1.5)
# Création du groupe B : 50 observations avec une vitesse moyenne de 15 et un écart-type de 1,5
Velo1B <- rnorm(50,15,1.5)
df1 <- data.frame(
  vitesse = c(Velo1A,Velo1B), 
  type = c(rep("Vélo personnel",length(Velo1A)), rep("Vélo partage",length(Velo1B)))
)
boxplot1 <- ggplot(data=df1, mapping=aes(x=type,y=vitesse, colour=type)) +
  geom_boxplot(width=0.2)+
  ggtitle("Données fictives (cas 1)")+
  xlab("Type de vélo")+
  ylab("Vitesse de déplacement (km/h)")+
  theme(legend.position = "none")
# CAS 2 : données fictives ------------------
# Création du groupe A : 60 observations avec une vitesse moyenne de 18 et un écart-type de 3
Velo2A <- rnorm(60,16,3)
# Création du groupe B : 50 observations avec une vitesse moyenne de 15 et un écart-type de 1,5
Velo2B <- rnorm(50,15,1.5)
df2 <- data.frame(
  vitesse = c(Velo2A,Velo2B), 
  type = c(rep("Vélo personnel",length(Velo2A)), rep("Vélo partage",length(Velo2B)))
)
boxplot2 <- ggplot(data=df2, mapping=aes(x=type,y=vitesse, colour=type)) +
  geom_boxplot(width=0.2)+
  ggtitle("Données fictives (cas 2)")+
  xlab("Type de vélo")+
  ylab("Vitesse de déplacement (km/h)")+
  theme(legend.position = "none")
ggarrange(boxplot1, boxplot2, ncol = 2, nrow = 1)
# Appel de la fonction pour le cas 1
tstudent_independants(Velo1A, Velo1B)
# Appel de la fonction pour le cas 2
tstudent_independants(Velo2A, Velo2B)
```


#### Principe de base et formulation pour des échantillons dépendants (appariés) {#sect04311}

Nous disposons de plusieurs individus pour lesquelles nous avons mesuré un phénomène (variable continue) à deux temps différents : généralement avant et après une expérimentation (analyse pré-post). Il s'agit de vérifier si la moyenne des différences des observations avant et après la période est différente de 0. Pour ce faire, on réalise les étapes suivantes :

* On pose l'hypothèse nulle (H<sub>0</sub>), soit que la moyenne des différences entre les deux séries est égale à 0 ($\bar{D} = 0$ avec $d = {x}_{t_1}- {x}_{t_2}$). L'hypothèse alternative (H<sub>1</sub>) est donc $\bar{D} \ne 0$. Notez que l'on peut tester une autre valeur que 0.
* On calcule la valeur de *t* et le nombre de degrés de liberté. La valeur de *t* sera négative quand la moyenne des différences entre ${X}_{t_1}$ et ${X}_{t_2}$ est négative et inversement.
* On compare la valeur absolue de *t* ($\mid T \mid$) avec celle issue de la table des valeurs critiques T avec le bon nombre de degrés de liberté et en choisissant un degré de signification (habituellement, p = 0,05). Si ($\mid t \mid$) est supérieure à la valeur *t* critique, alors les moyennes sont statistiquement différentes au degré de signification retenu.

Pour le test de student avec des échantillons appariées, la valeur de *t* se calcule comme suit :

$$t = \frac{\bar{D}-\mu_0}{\sigma_D / \sqrt{n}}$$
avec $\bar{D}$ étant la moyenne des différences entre les observations appariées de la série A et de la série B, $\sigma_D$ l'écart des différences, *n* le nombre d'observations, et finalement $\mu_0$ la valeur de l'hypothèse nulle que l'on veut tester (habituellement 0). Bien entendu, il est possible fixer une autre valeur pour $\mu_0$ : par exemple, avec $\mu_0 = 10$, on chercherait ainsi à vérifier si la moyenne des différences est significativement différente de 10. Le nombre de degrés de liberté sera égal à $n-1$.

Dans la syntaxe ![](images/Rlogo.png) ci-dessous, nous avons écrit une fonction dénommée `tstudent_dependants` permettant de réaliser le test de student pour des échantillons appariés. Dans cette fonction, vous pourrez repérer comment sont calculés la différence entre les observations pairées, la moyenne et l'écart-type de cette différence, puis le nombre de degrés de liberté, les valeurs de *t* et de *p* pour les deux tests.

Pour illustrer l'utilisation de la fonction, nous avons créé aléatoirement deux jeux de données. Imaginons que ces données décrivent 50 personnes utilisant habituellement l'automobile pour se rendre au travail. Pour ces personnes, nous avons généré des valeurs du risque perçu de l'utilisation du vélo (de 0 à 100), et ce, avant et après une période de 20 jours ouvrables durant lesquels ils devaient impérativement se rendre au travail à vélo.

* Au cas 1, les valeurs de risque ont une moyenne de 70 avant l'expérimentation et de 50 après l'expérimentation, avec des écarts types de 5.
* Au cas 2, les valeurs de risque ont une moyenne de 70 avant et 66 après, avec des écarts types de 5.

D'emblée, l'analyse visuelle des boîtes à moustaches (figure \@ref(fig:figttest2)) pairées montrent que la perception du risque semble avoir nettement diminé après l'expérimentation pour le cas 1 mais pas pour le cas 2. Cela est confirmé par les résultats des tests.


```{r figttest2, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Boites à moustaches sur des échantillons fictifs appariées', out.width='75%'}
library("ggplot2")
library("ggpubr")
tstudent_dependants <- function(A, B, mu=0){
  d <- A-B           # différences entre les observations pairées
  moy <- mean(d)     # Moyenne des différences
  e_t <- sd(d)       # Écart-type des différences
  n   <- length(A)   # nombre d'observations
  dl  <- n-1         # nombre de degrés de liberté (variances égales)
  
  t <- (moy- mu) / (e_t/sqrt(n)) # valeur de t
  p <-  2*(1-(pt(abs(t), dl)))
  
  cat("\n groupe A : moy = ", round(mean(A),1),", var = ", 
         round(var(A),1),", sd = ", round(sqrt(var(A)),1),
      "\n groupe B : moy = ", round(mean(B),1),", var = ", 
         round(var(B),1),", sd = ", round(sqrt(var(B)),1),
      "\n Moyenne des différences = ", round(mean(moy),1),
      "\n Ecart-type des différences = ", round(mean(e_t),1),
      "\n t(dl = ", dl, ") = ",round(t,2),
      ", p = ", round(p,3),  sep="")
  
  if (p <=.05){
    cat("\n La moyenne des différences entre les échantillons est significative")
  }
  else{
    cat("\n La moyenne des différences entre les échantillons n'est pas significative")
  }
}
# CAS 1 : données fictives ------------------
Avant1 <- rnorm(50,70,5)
Apres1 <- rnorm(50,50,5)
df1 <- data.frame(Avant=Avant1, Apres=Apres1)
boxplot1 <- ggpaired(df1, cond1 = "Avant", cond2 = "Apres", fill = "condition", 
                     palette = "jco", title = "Données fictives (cas 1)")
# CAS 2 : données fictives ------------------
Avant2 <- rnorm(50,70,5)
Apres2 <- rnorm(50,66,5)
df2 <- data.frame(Avant=Avant2, Apres=Apres2)
boxplot2 <- ggpaired(df2, cond1 = "Avant", cond2 = "Apres", fill = "condition", 
                     palette = "jco", title = "Données fictives (cas 2)")
ggarrange(boxplot1, boxplot2, ncol = 2, nrow = 1)
# Test t : appel de la fonction tstudent_dependants
tstudent_dependants(Avant1, Apres1, mu=0)
tstudent_dependants(Avant2, Apres2, mu=0)
```


#### Mesurer la taille de l'effet {#sect04312}

Rappelons que la taille de l'effet permet d'évaluer la magnitude (force) de l'effet d'une variable (ici la variable qualitative à deux modalités) sur une autre (ici la variable continue). Dans le cas de comparaisons de moyennes (avec des échantillons pairées ou non), pour mesurer la taille d'effet, on utilise habituellement le *d* de Cohen ou encore le *g* de Hedges; le second étant un ajustement du premier. Notez que nous analyserons la taille de l'effet uniquement si le test student ou de Welch s'est révélé significatif (p<0,05).

**Pourquoi utiliser le *d* de cohen?** Deux propriétés en font une mesure particulièrement intéressante. Premièrement, elle est facile à calculer puisque *d* est le ratio entre la différence de deux moyennes de groupes (A, B) et l'écart-type combiné des deux groupes. Deuxièmement, *d* représente ainsi une mesure standardisée de la taille de l'effet ; elle permet ainsi l'évaluation de la taille d'effet indépendamment de l'unité de mesure de la variable continue. Concrètement, cela signifie que quelle que soit l'unité de mesure de la variable continue *X*, elle est toujours exprimée en unité d'écart-type de *X*. Cette propriété facilite ainsi grandement les comparaisons entre des valeurs de *d* calculées sur différentes combinaisons de variables (au même titre que le coefficient de variation ou le coefficient de corrélation par exemple). Pour des échantillons indépendants de tailles différentes, il s'écrit : 

$$d = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sqrt{\frac{(n_A-1)S^2_A+(n_B-1)S^2_B}{n_A+n_B-2}}}$$
avec $n_A$,$n_B$, $S^2_{X_A}$ et $S^2_{X_B}$ étant respectivement les nombres d'observations et les variances pour les groupes *A* et *B*, $S^2_p$.

Si les échantillons sont de tailles identiques ($n_A=n_B$), alors *d* peut s'écrire :
$$d = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sqrt{(S^2_A+\S^2_A)/2}} = \frac{\bar{X}_{A}-\bar{X}_{B}}{(\sigma_A+\sigma_B)/2}$$
avec $\sigma_A$ et $\sigma_B$ étant les écart-types des deux groupes (rappel : l'écart-type est la racine carrée de la variance !).

Le *g* de Hedge est simplement une correction de *d*, particulièrement importante quand les échantillons sont de taille réduite.
$$g = d- \left(1- \frac{3}{4(n_A+n_B)-9} \right)$$

Moins utilisé en sciences sociales, mais surtout en études cliniques, le delta de Glass est simplement la différence des moyennes des groupes indépendants (numérateur) sur l'écar-type du deuxième groupe (démoninateur). Dans une étude clinique, on a habituellement un groupe qui subit un traitement (groupe de traitement) et un groupe qui a reçu un placebo (groupe de contrôle ou groupe témoin). L'effet de taille est ainsi évalué par rapport au groupe de contrôle : 
$$\Delta = \frac{\bar{X}_{A}-\bar{X}_{B}}{\sigma_B}$$


Finalement, pour des échantillons dépendants (pairés), il s'écrit simplement $d = \bar{D}/{\sigma_D}$ avec $\bar{D}$ et $\sigma_D$ étant la moyenne et l'écart-type des différences entre les observations.

**Commment interpréter le *d* de cohen ?** Un effet sera considéré comme faible avec $\lvert d \rvert$ à 0,2, modéré à 0,50 et fort à 0,80 [@cohen1992]. Notez que ces seuils ne sont que des conventions pour vous guider à interpréter la mesure de Cohen. D'ailleurs, dans son livre intitulé *Statistical power analysis for the behavioral sciences*, il écrit : « all conventions are arbitrary. One can only demand of them that they not be unreasonable » [@cohen2013]. Plus récemment, [@sawilowsky2009] a ajouté d'autres seuils à ceux proposés par Cohen (tableau \@ref(tab:tableconvcohen)).


```{r tableconvcohen, echo=FALSE, message=FALSE, warning=FALSE}
df <- data.frame(
        Sawilowsky = c("0,1 : Très faible", "0,2 : Faible", "0,5 : Moyen","0,8 : Fort", "1,2 : Très fort", "2,0 : Énorme"), 
        Cohen = c("", "0,2 : Faible", "0,5 : Moyen","0,8 : Fort", "", ""))
knitr::kable(
  head(df, nrow(df)), booktabs = TRUE,
  caption = 'Conventions pour l’interprétation du d de Cohen'
)
```


#### Mise en œuvre dans ![](images/Rlogo.png) {#sect04313}

Nous avons écrit précédemment les fonctions `tstudent_independants` et `tstudent_dependants` uniquement pour décomposer les différentes étapes de calcul des tests de Student et de Welch. Il existe des fonctions de base (`t.test` et `var.test`) qui permettent de réaliser l'un ou l'autre de ces deux tests avec une seule ligne de code.

La fonction `t.test` permet ainsi de calculer les test de Student et de Welch :

* `t.test(x ~ y, data=, mu = 0, paired = FALSE, var.equal = FALSE,  conf.level = 0.95)` ou `t.test(x =, y =, mu = 0, paired = FALSE, var.equal = FALSE,  conf.level = 0.95)`. 
* Le paramètre `paired` sera utilisé pour spécifier si les échantillons sont dépendants (`paired=TRUE`) ou indépendants (`paired=FALSE`).
* Le paramètre *var.equal* sera utilisé pour spécifier si les variances sont égales pour le test de Student (`var.equal=TRUE`) ou dissemblables pour le test de Welch (`var.equal=FALSE`).
* `var.test(x, y)` ou `var.test(x ~ y, data=)` pour vérifier au préalable si les variances sont égales ou non et choisir ainsi un t de Student ou un t de Welch.

Les fonctions `cohens_d` et `hedges_g` du *package* **effectsize** renvoient respectivement les mesures de *d* de Cohen et du *g* de Hedge :

* `cohens_d(x ~ y, data = dataframe, paired = FALSE, pooled_sd = TRUE)` ou  `cohens_d(x, y, data = dataframe, paired = FALSE, pooled_sd = TRUE)` ou
* `hedges_g(x ~ y, data = dataframe, paired = FALSE, pooled_sd = TRUE)` ou `hedges_g(x, y, data = dataframe, paired = FALSE, pooled_sd = TRUE)`

* `glass_delta(x ~ y, data = dataframe, paired = FALSE, pooled_sd = TRUE)` ou `glass_delta(x, y, data = dataframe, paired = FALSE, pooled_sd = TRUE)`


Notez que pour toutes ces fonctions deux écritures sont possibles :

* `x ~ y, data=` avec un `dataframe` dans lequel `x` est une variable continue et `y` et un facteur binaire
* `x, y` qui sont tous deux des vecteurs numériques (variable continue).

**Exemple de test pour des échantillons indépendants**

La figure \@ref(fig:figlocataires) représente la cartographie du pourcentage de locataires par secteur de recensement (SR) pour la région métropolitaine de recensement de Montréal (RMR) en 2016, soit une variable continue. L'objectif est de vérifier si la moyenne de ce pourcentage des SR de l'agglomération de Montréal est significativement différente de celles de SR hors de l'agglomération. 

Les résultats de la syntaxe ![](images/Rlogo.png) ci-dessous signalent que le pourcentage de locataires par SR est bien supérieur dans l'agglomération (moyenne = 59,7%; écart-type = 21,4%) qu'en dehors de l'agglomération de Montréal (moyenne = 27,3%; écart-type = 20,1%); cette différence de 32,5 points de pourcentage est d'ailleurs significative et très forte (t = -23,95; p ˂ 0,001, d de Cohen = 1,54).


```{r figlocataires, fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), auto_pdf = TRUE, fig.cap="Pourcentage de locataires par secteur de recensement, RMR de Montréal, 2016",  out.width='95%'}
knitr::include_graphics('images/bivariee/FigureLocataires.jpg', dpi = NA)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("foreign")
library("effectsize")
library("ggplot2")
library("dplyr")
# Importation du fichier
dfRMR <- read.dbf("data/bivariee/SRRMRMTL2016.dbf")
# Définition d'un facteur binaire                  
dfRMR$Montreal <- factor(dfRMR$Montreal, 
                           levels=  c(0,1), 
                           labels = c("Hors de Montréal","Montréal"))
# Comparaison des moyennes ------------------------
#Boite à moustaches (boxplot)
ggplot(data = dfRMR, mapping=aes(x=Montreal,y=Locataire,colour=Montreal)) +
  geom_boxplot(width=0.2)+
  theme(legend.position="none")+
  xlab("Zone")+ ylab("Pourcentage de locataires")+
  ggtitle("Locataires par secteur de recensement", subtitle="RMR de Montréal, 2006")
# nombre d'observations, moyennes et écart-types pour les deux échantillons
group_by(dfRMR, Montreal) %>%
  summarise(
    n = n(),
    moy = mean(Locataire, na.rm = TRUE),
    ecarttype = sd(Locataire, na.rm = TRUE)
  )
# On vérifie si les variances sont égales avec la fonction var.test
# quand la valeur de P est inférieure à 0,05 alors les variances diffèrent
v <- var.test(Locataire ~ Montreal, alternative='two.sided', conf.level=.95, data=dfRMR)
print(v)
```
Le test indique que nous n'avons aucune raison de rejeter l'hypothèse nulle selon laquelle les variances sont égales. Pour l'île de Montréal, l'écart-type est de 21,4 et de 20,1 hors de l'île, soit une différence négligeable.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Calcul du T de Student ou du T de Welch
p <- v$p.value
if(p >= 0.05){
  cat("\n Les variances ne diffèrent pas !",
     "\n On utilise le test de student avec l'option var.equal=TRUE", sep="")
    t.test(Locataire ~ Montreal,  # variable continue ~ facteur binaire 
           data=dfRMR,            # nom du dataframe
           conf.level=.95,        # intervalle de confiance pour la valeur de t
           paired = FALSE,        # échantillons non pairés (indépendants)
           var.equal=TRUE)        # variances égales
} else {
  cat("\n Les variances diffèrent !",
      "\n On utilise le test de Welch avec l'option var.equal=FALSE", sep="")
  t.test(Locataire ~ Montreal,   # variable continue ~ facteur binaire 
         data=dfRMR,             # nom du dataframe
         conf.level=.95,         # intervalle de confiance pour la valeur de t
         paired = FALSE,         # échantillons non pairés (indépendants)
         var.equal=FALSE)        # variances différentes
}
# Effet de taille à analyser uniquement si le test est significatif
cohens_d(Locataire ~ Montreal, data = dfRMR, paired = FALSE)
hedges_g(Locataire ~ Montreal, data = dfRMR, paired = FALSE)
```
Notez que le *d* de Cohen et le *g* de Hedge sont très proches ici, rappelons que le second est une correction du premier pour des échantillons de taille réduite. Avec 951 observations, nous disposons d'un échantillon suffisamment grand pour que cette correction soit négligeable.

**Exemple de syntaxe ![](images/Rlogo.png) pour un test de Student pour des échantillons dépendants**

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("ggpubr")
library("dplyr")
Pre <- c(79,71,81,83,77,74,76,74,79,70,66,85,69,69,82,69,81,70,83,68,77,76,77,70,68,80,65,65,75,84)
Post <- c(56,47,40,45,49,51,54,47,44,54,42,56,45,45,48,55,59,58,56,41,56,51,45,55,49,49,48,43,60,50)
# Première façon de faire un tableau : avec deux colonnes Avant et Après
df1 <- data.frame(Avant=Pre, Apres=Post)
head(df1)
ggpaired(df1, cond1 = "Avant", cond2 = "Apres", fill = "condition", palette = "jco")
# Nombre d'observations, moyennes et écart-types
cat(nrow(df1), " observations",
    "\nPOST. moy = ", round(mean(df1$Avant),1), ", e.t. = ", round(sd(df1$Avant),1),
    "\nPRE.  moy = ", round(mean(df1$Apres),1), ", e.t. = ", round(sd(df1$Apres),1), sep="")
t.test(Pre, Post, paired = TRUE)
# Deuxième façon de faire un tableau : avec une colonne pour la variable continue et une autre pour la variable qualitative
n <- length(Pre)*2
df2 <- data.frame(
       id=(1:n),
       participant=(1:length(Pre)),
       risque=c(Pre,Post)
       )
df2$periode <- ifelse(df2$id <= length(Pre), "Pré", "Post")
head(df2)
# nombre d'observations, moyennes et écart-types pour les deux échantillons
group_by(df2, periode) %>%
  summarise(
    n = n(),
    moy = mean(risque, na.rm = TRUE),
    et = sd(risque, na.rm = TRUE)
    )
ggpaired(data=df2, x= "periode", y="risque", fill = "periode")
t.test(risque ~ periode, data=df2, paired = TRUE)
```

#### Comparer des moyennes pondérées {#sect04314}

::: {.bloc_objectif .bloc_objectif_png data-latex="{blocs/objectif}"}
En études urbaines et en géographie, le recours aux données agrégées (non individuelles) est fréquent, par exemple au niveau des secteurs de recensement (comprenant généralement entre 2500 à 8000 habitants). Dans ce contexte, un secteur de recensement plus peuplé devrait avoir un poids plus important dans l'analyse. Il est possible d'utiliser les versions pondérées des tests présentés précédemment. Prenons deux exemples pour illustrer le tout :

* Pour chaque secteur de recensement des îles de Montréal et de Laval, nous avons calculé la distance au parc le plus proche à travers le réseau de rues avec un Système d'Information Géographique (SIG). On souhaite vérifier si les enfants âgés de moins de 15 ans résidant sur l'île de Montréal bénéficient en moyenne d'une meilleure accessibilité au parc.

* Dans une étude pour sur la concentration de polluants atmosphérique dans l'environnement autour des écoles primaires montréalaises, Carrier *et al.* [-@carrier2014] souhaitaient vérifier si les élèves fréquentant les écoles les plus défavorisés sont plus exposés au dioxyde d'azote (NO<sub>2</sub>) dans leur milieu scolaire. Pour ce faire, ils ont réalisé un test _t_ sur un tableau avec comme observations les écoles primaires et trois variables : la moyenne NO<sub>2</sub> (variable continue), les quintiles extrêmes d'un indice de défavorisation (premier et dernier quintiles, variable qualitative) et le nombres d'élèves inscrits par école (variable pour la pondération).

Pour réaliser un test *t* pondéré, nous pouvons utiliser la fonction `weighted_ttest` du package **sjstats**.

:::

En guise d'exemple appliqué, dans la syntaxe ![](images/Rlogo.png) ci-dessous, nous avons refait le même test *t* que précédemment (`Locataire ~ Montreal`) en pondérant chaque secteur de recensement par le nombre de logements qu'il comprend.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("sjstats")
library("dplyr")
# Calcul des statistiques pondérées
group_by(dfRMR, Montreal) %>%
  summarise(
    n = sum(Logement),
    MoyPond = weighted_mean(Locataire, Logement),
    ecarttypePond = weighted_sd(Locataire, Logement)
  )
# Test t non pondéré
t.test(Locataire ~ Montreal, dfRMR, 
               paired = FALSE, var.equal = TRUE, conf.level=.95)
# Test t pondérée
weighted_ttest(Locataire ~ Montreal + Logement, dfRMR, 
               paired = FALSE, ci.lvl=.95)
               
```

#### Comment rapporter un test de student ou de Welch? {#sect04315}

Pour les différentes versions du test, il est important de rapporter la valeur de *t*, la valeur de _p_ et les moyennes des groupes. Voici quelques exemples :

**Test de Student ou de Welch pour échantillons indépendants**

* Dans la région métropolitaine de Montréal en 2005, le revenu total des femmes  (moyenne = 29117 dollars; écart-type = 258022) est bien inférieur à celui des homme (moyenne = 44463; écart-type = 588081). La différence entre les moyennes des deux sexes (-15345) en faveur des hommes est d’ailleurs significative (t = -27,09; *p* ˂ 0,001).
* Il y un effet significatif selon le sexe (t = -27,09; *p* ˂ 0,001), le revenu total des hommes (moyenne= 44463; écart-type = 588081) étant bien supérieur à celui des femmes (moyenne = 29 117; écart-type = 258 022).
* 50 personnes se rendrent au travail à vélo (moyenne = 33,7, écart-type = 8,5) contre 60 en automobile (moyenne = 34, écart-type = 8,7); il n'y a pas de différence significative entre les moyennes d'âge des deux groupes (t(108) = -0,79, *p* = 0,427).


**Test de Student échantillons dépendants (pairés)**

* On constaste une diminution significative de la perception du risque après l'activité (moyenne = 49,9, écart-type = 5,7) comparativement à avant (moyenne = 74,8, écart-type = 6,1), avec une différence de -24,8 (t(29) = -18,7, *p* < 0,001).
* Les résultats du pré-test (moyenne = 49,9, écart-type = 5,7) et du post-test (moyenne = 74,8, écart-type = 6,1) montrent qu'il y une diminution significative de la perception du risque (t(29) = -18,7, *p* < 0,001).

Pour un texte en anglais, vous pourrez consulter
[https://www.socscistatistics.com/tutorials/ttest/default.aspx](https://www.socscistatistics.com/tutorials/ttest/default.aspx){target="_blank"}.


### Test non paramétrique de Wilcoxon {#sect0432}

::: {.bloc_objectif .bloc_objectif_png data-latex="{blocs/objectif}"}
Si la variable continue est fortement anormalement distribuée, il est déconseillé d'utiliser les tests de Student et de Welch. On privilégiera le test des rangs signés de Wilcoxon (*Wilcoxon rank-sum test* en anglais). Attention, il est aussi appelé test U de Mann-Whitney. Ce test permet alors de vérifier si les deux groupes présentent des médianes différentes.

Pour ce faire, on utilise la fonction `wilcox.test` dans laquelle le paramètre `paired` permettra de spécifier si les échantillons sont indépendants ou non (`FALSE` ou `TRUE`).
:::

Dans l'exemple suivant, nous analysons le pourcentage de locataires dans les secteurs de recensements de la région métropolitaine de Montréal. Plus spécifiquement, nous comparons ce pourcentage entre les secteurs présents sur l'île et les secteurs hors de l'île. Il s'agit donc d'un test avec des échantillons indépendants.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library("foreign")
library("dplyr")
###############################
# Échantillons indépendants
###############################
dfRMR <- read.dbf("data/bivariee/SRRMRMTL2016.dbf")
# Définition d'un facteur binaire                  
dfRMR$Montreal <- factor(dfRMR$Montreal, 
                           levels=  c(0,1), 
                           labels = c("Hors de Montréal","Montréal"))
# Calul du nombre d'observations, moyennes et écart-types des rangs pour les deux échantillons
group_by(dfRMR, Montreal) %>%
  summarise(
    n = n(),
    moy_rang = mean(rank(Locataire), na.rm = TRUE),
    med_rang = median(rank(Locataire), na.rm = TRUE),
    ecarttype_rang = sd(rank(Locataire), na.rm = TRUE)
  )
# Test des rangs signés de Wilcoxon sur des échantillons indépendants
wilcox.test(Locataire ~ Montreal, dfRMR, paired = FALSE)
```

Nous observons bien ici une différence significative entre le pourcentage de locataires des secteurs de recensement sur l'île (rang médian = 216) et hors de l'île (rang médian = 261).

Pour le second exemple, nous générons deux jeux de données au hasard représentant une mesure d'une variable pré-traitement (*pre*) et post-traitement (*post*) pour un même échantillon.
```{r echo=TRUE, message=FALSE, warning=FALSE}
###############################
# Échantillons dépendants
###############################
pre <- sample(60:80, 50, replace=T)
post <- sample(30:65, 50, replace=T)
df1 <- data.frame(Avant=pre, Apres=post)
# Nombre d'observations, moyennes et écart-types
cat(nrow(df1), " observations",
    "\nPOST. median = ", round(median(df1$Avant),1), 
             ", moy = ", round(mean(df1$Avant),1),
    "\nPRE.  median = ", round(median(df1$Apres),1), 
             ", moy = ", round(mean(df1$Apres),1), sep="")
wilcox.test(df1$Avant, df1$Apres, paired = TRUE)
```

À nouveau, nous obtenons une différence significative entre les deux variables.

**Comment rapporter un test de Wilcoxon?**

Lorsque l'on rapporte les résultats d'un test de Wilcoxon, il est important de signaler la valeur du test (W), le degré de signification (valeur de p) et éventuellement la médiane des rangs ou de la variable originale pour les deux groupes. Voici quelques exemples :

* Les résultats du test des rangs signés de Wilcoxon signalent que les rangs de l'île de Montréal sont significativement plus élevés que ceux de l'île de Laval (W = 1223, p ˂ 0,001).
* Les résultats du test de Wilcoxon signalent que les rangs post-tests sont significativement plus faibles que ceux pré-test (W = 1273,5, p ˂ 0,001).
* Les résultats du test de Wilcoxon signalent que la médiane des rangs pré-tests (médiane = 69) est significativement plus forte que celle du post-test (médiane = 50,5) (W = 1273,5, p ˂ 0,001).


## Relation entre une variable quantitative et une variable qualitative à plus de deux modalités {#sect044}

::: {.bloc_objectif .bloc_objectif_png data-latex="{blocs/objectif}"}

**Existe-il une relation entre une variable continue et une variable qualitative comprenant plus de deux modalités?** Pour répondre à cette question, on pourra recourir à deux méthodes : l’analyse de variance – **ANOVA**, _**AN**alysis Of **VA**riance_ en anglais – et le test non paramétrique de Kruskal-Wallis. La première permet de vérifier si les moyennes de plusieurs groupes d'une population donnée sont ou non significativement différentes; la seconde si leurs médianes sont différentes.
:::

### Analyse de variance {#sect0441}

L’analyse de variance (ANOVA) est largement utilisée en psychologie, médecine et pharmacologie. Prenons un exemple classique en pharmacologie pour tester l'efficacité d'un médicament. Quatre groupes de population sont constitués :

* un premier groupe d'individus pour lequel on administre un placebo (un médicament sans substance active), soit le groupe de contrôle ou le groupe témoin;
* un second groupe auquel l'on administre le médicament avec un faible dosage;
* un troisième avec un dosage moyen;
* un quatrième avec un dosage élevé.

La variable continue permettra d'évaluer l'évolution de l'état de santé des individus (par exemple, la variation du taux de globules rouges dans le sang avant et après le traitement). Si le traitement est efficace, on s'attendrait alors à ce que les moyennes des deuxième, troisième et quatrième groupes soient plus élevées que celle du groupe de contrôle. Les différences de moyennes entre les second, troisième et quatrième groupes permettront aussi de repérer quel dosage est le plus efficace. Si nous n'observons aucune différence significative entre les groupes, cela signifie que l'effet du médicamment ne diffère pas de l'effet d'un placébo.

L'ANOVA est aussi très utilisée en études urbaines, principalement pour vérifier si un phénomène urbain varie selon plusieurs groupes d'une population donnée ou régions géographiques. En guise d'exemple, le recours à l'ANOVA permettrait de répondre aux questions suivantes :

* les moyennes des niveaux d'exposition à un polluant atmosphérique (variable continue) varient-elles significativement selon le mode de transport utilisé (automobile, vélo, transport en commun) pour des trajets similaires en heures de pointe?

* pour une métropole donnée, les moyennes des loyers (variable continue) sont-elles différentes entre les logements de la ville centre versus ceux localisés dans la première couronne et ceux de la seconde couronnes?


#### Le calcul des trois variances pour l'ANOVA {#sect04411}

L'ANOVA repose sur le calcul de trois variances :

* **la variance totale** (*VT*) de la variable dépendante continue, soit la somme des carrés des écarts à la moyenne de l'ensemble de la population (équation \@ref(eq:anova1);

* la **variance intergroupe** (*Var~inter~*) ou variance expliquée (*VE*), soit la somme des carrés des écarts entre la moyenne de chaque groupe et la moyenne de l’ensemble du jeu de données multipliées par le nombre d’individus appartenant à chacun des groupes (équation \@ref(eq:anova2);

* la **variance intragroupe**  (*Var~intra~*) ou variance non expliquée (*VNE*), soit la somme des variances des groupes de la variable indépendante (équation \@ref(eq:anova3).

\begin{equation} 
VT=\sum_{i=1}^n (y_{i}-\overline{y})^2
(\#eq:anova1)
\end{equation}

\begin{equation} 
Var_{inter} \mbox{ ou } VE=\sum_{i\in{g_1}}(\overline{y_{g_1}}-\overline{y})^2 + \sum_{i\in{g_2}}(\overline{y_{g_2}}-\overline{y})^2 + ... + \sum_{i\in{g_n}}(\overline{y_{g_k}}-\overline{y})^2
(\#eq:anova2)
\end{equation}

\begin{equation} 
Var_{intra} \mbox{ ou } VNE=\sum_{i\in{g_1}}(y_{i}-\overline{y_{g_1}})^2 + \sum_{i\in{g_2}}(y_{i}-\overline{y_{g_2}})^2 + ... + \sum_{i\in{g_n}}(y_{i}-\overline{y_{g_k}})^2 
(\#eq:anova3)
\end{equation}

avec $\overline{y}$ est la moyenne de l'ensemble de la population; $\overline{y_{g_1}}$, $\overline{y_{g_1}}$, $\overline{y_{g_k}}$ sont respectivements les moyennes des groupes 1 à _k_ (_k_ étant le nombre de modalités de la variables qualitative) .

La variance totale (*VT*) est égale à la somme de la variance intergroupe (expliquée) et la variance intragroupe (non expliquée) (équation \@ref(eq:anova5)). Le ratio entre la variance intergroupe (expliquée) et la variance totale est dénommé *Eta^2^* (équation \@ref(eq:anova6)). Il varie de 0 à 1 et exprime la proportion de la variance de la variable continue qui est expliquée par les différentes modalités de la variable qualitative.

\begin{equation} 
VT = Var_{inter} + Var_{intra} \mbox{ ou } VT = VNE + VE
(\#eq:anova5)
\end{equation}

\begin{equation} 
\eta^2= \frac{Var_{inter}}{VT} \mbox{ ou }  \eta^2= \frac{VE}{VT}
(\#eq:anova6)
\end{equation}


::: {.bloc_astuce .bloc_astuce_png data-latex="{.bloc_astuce}"}
**La décomposition de la variance totale** – égale à la somme des variances intragroupe et intergroupe – est fondamentale en statistique. Nous verrons qu'elle est aussi utilisée pour évaluer la qualité d'une partition d'une population dans le chapitre sur les méthodes de classification (LIEN A METTRE PLUS TARD). En ANOVA, on retiendra que :

* plus la variance intragroupe est faible, plus les différents groupes sont homogènes;
* plus la variance intergroupe est forte, plus les moyennes des groupes sont différentes et donc plus les groupes sont dissembables. 

Autrement dit, plus la variance intergroupe (**dissimilarité** des groupes) est maximisée et corollairement plus la variance intragroupe (**homogénéité** de chacun des groupes) est minimisée, plus les groupes sont clairement distincts et plus l'ANOVA sera performante.
:::

Examinons un premier jeu de données fictif sur la vitesse de déplacements de cyclistes (variable continue exprimée en km/h) et une variable qualitative comprenant trois groupes de cyclistes utilisant soit un vélo personnel (*n~A~* = 5), soit en libre service (*n~B~* = 7), soit électrique (*n~C~* = 6) (tableau \@ref(tab:aovfictive1)). D'emblée, on note que les moyennes de vitesse des trois groupes sont différentes : 17,6 km/h pour les cyclistes avec leur vélo personnel, 12,3  km/h les utilisateurs des vélos en libre service et 23,1 km/h pour les cyclistes avec un vélo électrique. 

Pour chaque observation, la troisième colonne du tableau représente les écarts à la moyenne globale mis au carré, tandis que les colonnes suivantes représentent la déviation au carré de chaque observation à la moyenne de son groupe d'appartenance. Ainsi, pour la première observation, on a : $(16,900 - 17,339)^2 = 0,193$ et $(16,900 - 17,580)^2~ = 0,46$. La variance totale (*VT*) est donc égale à la somme de la troisième colonne (424,663), tandis que la variance intragroupe (non expliquée, VNE) est égale à $11,228+21,537+13,993=46,758$. Quant à la variance intergroupe (expliquée, VE), elle est égale à $5\times(17,580-17,339)^2+7\times(12,257-17,339)^2+6\times(23,067-17,339)^2 = 377,904$.

On a donc $VT = Var_{inter} + Var_{intra}$, soit $424,663 = 377,904 + 46,758$ et $\eta_2 = 377,904 / 424,663 = 0,89$. Cela signale que 89% de la variance de la vitesse des cyclistes est expliquée par le type de vélo utilisé.

```{r aovfictive1, echo=FALSE, message=FALSE, warning=FALSE}
VeloA <- c(16.9, 20.4, 16.1, 17.7, 16.8)
VeloB <- c(13.4, 11.3, 14.0, 12.4, 13.7, 8.5, 12.5)
VeloC <- c(22.9, 26.0, 23.6, 21.0, 22.3, 22.6)
moyA <- mean(VeloA)
moyB <- mean(VeloB)
moyC <- mean(VeloC)
grandmoy <- mean(c(VeloA,VeloB,VeloC))
nA <- length(VeloA)
nB <- length(VeloB)
nC <- length(VeloC)
n <- nA + nB + nC
VT <- sum((c(VeloA,VeloB, VeloC)-grandmoy)^2)
VNE_A <- sum((VeloA-moyA)^2)
VNE_B <- sum((VeloB-moyB)^2)
VNE_C <- sum((VeloC-moyC)^2)
VNE <- VNE_A + VNE_B + VNE_C
VE <- nA*(moyA-grandmoy)^2 + nB*(moyB-grandmoy)^2 + nC*(moyC-grandmoy)^2
Eta2 <- round(VE / VT, 4)
  
df <- data.frame(
  velo = c(rep("A. personnel",length(VeloA)), 
           rep("B. libre service",length(VeloB)),
           rep("C. électrique",length(VeloC))),
  kmh = c(VeloA,VeloB, VeloC))
df$VT <- (df$kmh - grandmoy)^2
df$VNE_A <- ifelse(df$velo == "A. personnel", (df$kmh - moyA)^2, NA)
df$VNE_B <- ifelse(df$velo == "B. libre service", (df$kmh - moyB)^2, NA)
df$VNE_C <- ifelse(df$velo == "C. électrique", (df$kmh - moyC)^2, NA)
df_cas1 <- df
tabl <- df
tabl$velo <- as.character(tabl$velo)
tabl[19,1] <- "grande moyenne"
tabl[20,1] <- "moyenne groupe A"
tabl[21,1] <- "moyenne groupe B"
tabl[22,1] <- "moyenne groupe C "
tabl[19,2] <- round(grandmoy,3)
tabl[20,2] <- round(moyA,3)
tabl[21,2] <- round(moyB,3)
tabl[22,2] <- round(moyC,3)
tabl[23,1] <- "Variance totale"
tabl[24,1] <- "Variance intragroupe"
tabl[23,3] <- sum(tabl$VT, na.rm = TRUE)
tabl[24,4] <- sum(tabl$VNE_A, na.rm = TRUE)
tabl[24,5] <- sum(tabl$VNE_B, na.rm = TRUE)
tabl[24,6] <- sum(tabl$VNE_C, na.rm = TRUE)
tabl$VT <- round(tabl$VT,3)
tabl$VNE_A <- round(tabl$VNE_A,3)
tabl$VNE_B <- round(tabl$VNE_B,3)
tabl$VNE_C <- round(tabl$VNE_C,3)
opts <- options(knitr.kable.NA = "--")
knitr::kable(
  head(tabl, nrow(tabl)), booktabs = TRUE, valign = 't', row.names = FALSE,
   format.args = list(decimal.mark = ",", big.mark = " "),           
  col.names = c("Type de vélo",
                "km/h",
                "$(y_{i}-\\overline{y})^2$",
                "$(y_{i}-\\overline{y_{A}})^2$",
                "$(y_{i}-\\overline{y_{B}})^2$",
                "$(y_{i}-\\overline{y_{C}})^2$"
                ),
  caption = "Données fictives et calcul des trois variances (cas 1)")
```

Examinons un deuxième jeu de données fictives pour lequel le type de vélo utilisé n'aurait que peu d'effet sur la vitesse des cyclistes (tableau \@ref(tab:aovfictive2)). D'emblée, les moyennes des trois groupes semblent très similaires (19,3, 17,9 et 18,7). Les valeurs des trois variances sont les suivantes :

* la variance totale est égale à 121,756.
* la variance intragroupe (non expliquée, VNE) est égale à $9,140+50,254+56,275 = 115,669$
* la variance intragroupe (expliquée, VE) est égale à $5\times(19,300-18,528)^2+7\times(17,871-18,528)^2+6\times(18,650-18,528)^2 = 6,087$.

On a donc $VT = Var_{inter} + Var_{intra}$, soit $121,756 = 6,087 + 115,669$ et $\eta_2 = 6,087 / 121,756 = 0,05$. Cela signale que 5% de la variance de la vitesse des cyclistes est uniquement expliquée par le type de vélo utilisé.

```{r aovfictive2, echo=FALSE, message=FALSE, warning=FALSE}
VeloA <- c(17.5, 19.0, 19.7, 18.7, 21.6)
VeloB <- c(13.7, 20.8, 15.1, 18.8, 21.5, 16.5, 18.7)
VeloC <- c(16.6, 16.3, 15.6, 20.0, 24.6, 18.8)
moyA <- mean(VeloA)
moyB <- mean(VeloB)
moyC <- mean(VeloC)
grandmoy <- mean(c(VeloA,VeloB,VeloC))
nA <- length(VeloA)
nB <- length(VeloB)
nC <- length(VeloC)
n <- nA + nB + nC
VT <- sum((c(VeloA,VeloB, VeloC)-grandmoy)^2)
VNE_A <- sum((VeloA-moyA)^2)
VNE_B <- sum((VeloB-moyB)^2)
VNE_C <- sum((VeloC-moyC)^2)
VNE <- VNE_A + VNE_B + VNE_C
VE <- nA*(moyA-grandmoy)^2 + nB*(moyB-grandmoy)^2 + nC*(moyC-grandmoy)^2
Eta2 <- round(VE / VT, 4)
  
df <- data.frame(
  velo = c(rep("A. personnel",length(VeloA)), 
           rep("B. libre service",length(VeloB)),
           rep("C. électrique",length(VeloC))),
  kmh = c(VeloA,VeloB, VeloC))
df$VT <- (df$kmh - grandmoy)^2
df$VNE_A <- ifelse(df$velo == "A. personnel", (df$kmh - moyA)^2, NA)
df$VNE_B <- ifelse(df$velo == "B. libre service", (df$kmh - moyB)^2, NA)
df$VNE_C <- ifelse(df$velo == "C. électrique", (df$kmh - moyC)^2, NA)
df_cas2 <- df
tabl <- df
tabl$velo <- as.character(tabl$velo)
tabl[19,1] <- "grande moyenne"
tabl[20,1] <- "moyenne groupe A"
tabl[21,1] <- "moyenne groupe B"
tabl[22,1] <- "moyenne groupe C "
tabl[19,2] <- round(grandmoy,3)
tabl[20,2] <- round(moyA,3)
tabl[21,2] <- round(moyB,3)
tabl[22,2] <- round(moyC,3)
tabl[23,1] <- "Variance totale"
tabl[24,1] <- "Variance intragroupe"
tabl[23,3] <- sum(tabl$VT, na.rm = TRUE)
tabl[24,4] <- sum(tabl$VNE_A, na.rm = TRUE)
tabl[24,5] <- sum(tabl$VNE_B, na.rm = TRUE)
tabl[24,6] <- sum(tabl$VNE_C, na.rm = TRUE)
tabl$VT <- round(tabl$VT,3)
tabl$VNE_A <- round(tabl$VNE_A,3)
tabl$VNE_B <- round(tabl$VNE_B,3)
tabl$VNE_C <- round(tabl$VNE_C,3)
opts <- options(knitr.kable.NA = "--")
knitr::kable(
  head(tabl, nrow(tabl)), booktabs = TRUE, valign = 't', row.names = FALSE,
   format.args = list(decimal.mark = ",", big.mark = " "),           
  col.names = c("Type de vélo",
                "km/h",
                "$(y_{i}-\\overline{y})^2$",
                "$(y_{i}-\\overline{y_{A}})^2$",
                "$(y_{i}-\\overline{y_{B}})^2$",
                "$(y_{i}-\\overline{y_{C}})^2$"
                ),
  caption = "Données fictives et calcul des trois variances (cas 2)")
```


#### Le test de Fisher {#sect04412}

Pour vérifier si les moyennes sont satistiquement différentes (autrement dit, si leur différence est significativement différente de 0), on a recours au test *F* de Fisher. Pour ce faire, on pose l'hypothèse nulle (H~0~), soit que les moyennes des groupes sont égales; autrement dit que la variable qualitative n'a pas d'effet sur la variable continue (indépendance entre les deux variables). L'hypothèse alternative (H~1~) est donc que les moyennes sont différentes. Pour nos deux jeux de données fictives ci-dessus comprenant trois groupes, H~0~ signifie que $\overline{y_{A}}=\overline{y_{B}}=\overline{y_{C}}$. La statistique *F* se calcule comme suit :


\begin{equation} 
F = \frac{\frac{Var{inter}}{k-1}}{\frac{Var{intra}}{n-k}}\mbox{ ou } F = \frac{\frac{VE}{k-1}}{\frac{VNE}{n-k}}
(\#eq:anovaF)
\end{equation}

avec $n$ et $k$ étant respectivement les nombres d'observations et de modalités de la variable qualitative. L'hypothèse nulle (les moyennes sont égales) sera rejetée si la valeur du *F* calculé est supérieure à la valeur critique de la table *F* avec les degrés de libertés *(k-1, n-k)* et un seuil 
$\alpha$ (*p*=0,05 habituellement) (voir le tableau des valeurs critiques de *F*, section \@ref(annexe2)). Notez qu'on utilise rarement la table *F* puisqu'avec la fonction `aov` on calcule directement la valeur *F* et celle de *p* qui lui est associée. Concrètement, si le test _F_ est significatif (avec *p*<0,05), plus la valeur de *F* sera élevée, plus la différence entre les moyennes sera élevée.

Appliquons rapidement la démarche du test *F* à nos deux jeux de données fictives qui comprennent 3 modalités pour la variable qualitative et 18 observations. Avec $\alpha$=0,05, 2 degrés de liberté (3-1) au numérateur et 15 au dénominateur (18-3), la valeur critique de F est de 3,68. On en conclut alors que :

* pour le cas A, le F calculé est égal à $F = (377,904 /2) / (46,758 / 15) = 60,62$. Il est  supérieur à la valeur F critique; les moyennes sont donc statistiquement différentes au seuil 0,05. Autrement dit, nous aurions eu moins de 5% de chance d'obtenir un échantillon produisant ces résultats si en réalité la différence entre les moyenne était de 0.
* pour le cas B, le F calculé est égal à $F = (6,087/2)/ (115,669 / 15) = 0,39$. Il est inférieur à la valeur F critique; les moyennes ne sont donc pas statistiquement différentes au seuil 0,05.

#### Conditions d'application de l'ANOVA et solutions alternatives {#sect04414}
Trois conditions d'application doivent être vérifiées avant d'effectuer une analyse de variance sur un jeu de données :

* **Normalité des groupes.** Le test de Fisher repose sur le postulat que les échantillons (groupes) sont normalement distribués. Pour le vérifier, on a recours au test de normalité de Shapiro–Wilk (REF CHAPITRE 1 à METTRE PLUS TARD). Notez toutefois que ce test est très restrictif, surtout pour des grands échantillons.

* **Homoscédasticité**. La variance dans les échantillons doit être la même (homogénéité des variances). Pour vérifier cette condition, on utilisera les tests de Levene, de Bartlett ou de Breusch-Pagan.

* **Indépendance des observations (pseudo-réplication).** Chaque individu doit appartenir à un et un seul groupe. En d'autres termes, les observations ne sont pas indépendantes si plusieurs mesures (variable continue) sont faites sur un même individu. Si c'est le cas, on utilisera alors une analyse de variance sur des mesures répétées (voir le bloc à la fin du chapitre).

**Quelles sont les conséquences si les conditions d'application ne sont pas respectées ?** La non vérification des conditions d'application cause deux problèmes distincts : elle affecte la puissance du test (sa capacité à détecter un effet, si celui-ci existe réellement) et le taux d'erreur de type 1 (la probabilité de trouver un résultat significatif alors qu'aucune relation n'existe réellement, soit un faut-positif) [@glass1972consequences; @lix1996consequences].

* Si la distribution est asymétrique plutôt que centrée (comme pour une distribution normale), la puissance et le taux d'erreur de type 1 sont tous les deux peu affectés car le test est non-orienté (la différence de moyennes peut être négative ou positive).
* Si la distribution est leptocurtique (pointue, avec des extrémités de la distribution plus importantes, le taux d'erreur de type 1 est peu affecté, en revanche la puissance du test est réduite. L'inverse s'observe si la distribution est platicurtique (aplatie, c'est-à-dire avec des extrémités de la distribution plus réduites.
* Si les groupes ont des variances différentes, le taux d'erreur de type 1 augmente légèrement.
* Si les observations ne sont pas indépendantes, à la fois le taux d'erreurs de type 1 et la puissance du test sont fortement affectés.
* Si les échantillons sont petits, les effets présentés ci-dessus sont démultipliés.
* Si plusieurs conditions ne sont pas respectées, les conséquences présentées ci-dessus s'additionnent, voir se combinent.

**Que faire quand les conditions d'application relatives à la normalité ou à l'homoscédaticité ne sont vraiment pas respectées ?** Signalons d'emblée que le non respect de ces conditions ne change rien à la décomposition de la variance (VT=V~intra~+V~inter~). Cela signifie que vous pouvez toujours calculer Eta^2^. Par contre, le test de Fisher ne peut pas être utilisé car il sera biaisé comme décrit précédemment. Quatre solutions sont envisageables :

* Lorsque les échantillons sont fortement anormalement distribués, certains auteurs vont simplement transformer leur variable en appliquant une fonction logarithme (le plus souvent) ou racine carré, inverse ou exponentielle, et reporter le test de fisher calculé sur cette transformation. Attention toutefois ! Transformer une variable ne va pas systématiquement la rapprocher d'une distribution normale et complique l'interprétation finale des résultats. Par conséquent, avant de recalculer votre test *F*, il convient de réaliser un test de normalité de Shapiro–Wilk et un test d'homoscédasticité (Levene, Bartlett ou/et Breusch-Pagan) sur la variable continue transformée.

* Détecter les observations qui contribuent le plus à l'anormalité et l'hétéroscédasticité, dites valeurs aberrantes (*outliers* en anglais). Supprimez les et refaites votre ANOVA en vous assurant que les conditions sont désormais respectées. Notez que supprimer des observations peut être une pratique éthiquement questionnable en statistique. Si vos échantillons sont bien constitués et que la mesure collectée n'est pas erronée, pourquoi donc la supprimer? Si vous optez pour cette solution, prenez soin de comparer les résultats avant et après la suppression des valeurs aberrantes. Si les conditions sont respectées après suppression et que les résultats de l'ANOVA (Eta^2^ et test *F* de Fisher) sont très semblables, conservez donc les résultats de l'ANOVA intitiale et signalez que vous avez procédez aux deux tests.

* Lorsque les variances des groupes sont dissemblables, vous pouvez utiliser le test de Welch pour l'ANOVA au lieu du test *F* de Fisher. 

* Dernière solution, lorsque les deux conditions ne sont vraiment pas respectées, utilisez le test non paramétrique de Kruskal-Wallis. Par analogie au *t* de student, il correspond au test des rangs signés de Wilcoxon. Ce test est décrit dans la section suivante.

Vous l'aurez compris, dans de nombreux cas en statistique, les choix méthodologiques dépendent de la subjectivité du chercheur. Il faut s'adapter au jeu de données et à la culture statistique en vigueur dans votre champs d'étude. N'hésitez pas à réaliser plusieurs tests différents pour évaluer la robustesse de vos conclusions et fiez-vous en premier lieu à ceux pour lesquels votre jeu de données est le plus adapté.

### Test non paramétrique de Kruskal-Wallis {#sect0442}

Le test non paramétrique de Kruskal-Wallis est une alternative à l'ANOVA classique lorsque le jeu de données présente de graves problèmes de normalité et d'hétéroscédaticité. Cette méthode représente une ANOVA appliquée à une variable continue transformée préalablement en rangs. Du fait de la transformation en rangs, on ne vérifie plus si les moyennes sont différentes, mais bel et bien si les médianes de la variable continue sont différentes. Pour ce faire, on utilisera la fonction `kruskal.test`.

### Mise en œuvre dans ![](images/Rlogo.png) {#sect0443}

Dans une étude récente, Apparicio *et al.* [-@apparicio2018exposure] ont comparé les expositions au bruit et à la pollution atmosphérique aux heures de pointe à Montréal en fonction du mode de transport utilisé. Pour ce faire, trois équipes de trois personnes ont été constituées : un cycliste, un automobiliste et un utilisateur du transport en commun, équipés de capteurs de pollution, de sonomètres, de vêtements biométriques et d’une montre GPS. Chaque matin, à huit heures précises, les membres de chaque équipe ont réalisé un trajet d'un quartier périphérique de Montréal vers un pôle d'enseignement (université) ou d'emploi localisé au centre-ville. Le trajet inverse était réalisé le soir à 17h. Au total, une centaine de trajets ont ainsi été réalisés. Des analyses de variance ont ainsi permis de comparer entre les trois modes (automobile, vélo et transport en commun) : les temps de déplacement, les niveaux d'exposition au bruit, les niveaux d'exposition au dioxyde d'azote et la dose totale inhalée de dioxyde d'azote. Nous vous proposons ici d'analyser une partie de ces données.


#### Première ANOVA : différences entre les temps de déplacement
Comme première analyse de variance, nous allons vérifier si les moyennes des temps de déplacement sont différentes entre les trois modes de transport. 

Dans un premier temps, nous pouvons calculer les moyennes des différents groupes. On peut alors constater que les moyennes sont très sembables : 37,7 minutes pour l'automobile versus 38,4 et 41,6 pour le vélo et le transport en commun. Aussi, les variances des trois groupes sont relativement similaires.


```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
# chargement des dataframes
load("data/bivariee/dataPollution.RData")
# Statistiques descriptives pour les groupes (moyenne et écart-type)
df_TrajetsDuree %>%                                 # Nom du dataframe
    group_by(Mode) %>%                                # Variable qualitative
    get_summary_stats(DureeMinute, type = "mean_sd")  # Variable continue 
```

Pour visualiser la distribution des données pour les trois groupes, vous pouvez créer des graphiques de densité et en violons (figure \@ref(fig:figAnova1a)). La juxtaposition des trois distributions montre que les distributions des valeurs pour les trois groupes sont globalement similaires. Cela est corroboré par le fait que les boîtes du graphique en violons sont situées à la même hauteur. Autrement dit, à la lecture des deux graphiques, ils ne semblent pas y avoir de différences significatives entre les trois groupes en terme de temps de déplacement.

```{r figAnova1a, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Graphiques de densité et en violons', out.width='75%'}
library("ggplot2")
library("ggpubr")
# Graphique de densité
GraphDens <- ggplot(data = df_TrajetsDuree, 
  mapping=aes(x=DureeMinute,colour=Mode,fill=Mode)) +
  geom_density(alpha=0.55,mapping=aes(y=..scaled..))+
    labs(title="a. graphique de densité",
         x = "Densité",
         y = "Durée du trajets (en minutes)")
# Graphique en violons
GraphViolon <- ggplot(df_TrajetsDuree, aes(x=Mode, y=DureeMinute)) +
  geom_violin(fill="white") +
  geom_boxplot(width=0.1, aes(x=Mode, y=DureeMinute,fill=Mode))+
  labs(title="b. Graphique en violons",
       x = "Mode de transport",
       y = "Durée du trajets (en minutes)")+
  theme(legend.position = "none")
ggarrange(GraphDens, GraphViolon)
``` 

Nous pouvons vérifier si les échantillons sont normalement distribués avec la fonction `shapiro_test` du package **rstatix**. À titre de rappel, l'hypothèse nulle (h<sub>0</sub>) de ce test est que la distribution est normale. Par conséquent, quand la valeur de *P* associée à la statistique de Shapiro est supérieure à 0,05 alors on ne peut rejeter l'hypothèse d'une distribution normale (autrement dit, la distribution est anormale). À la lecture des sorties ci-dessous, seul le groupe des utilisateurs en transport en commun présente une distribution proche de la normalité (p=0,0504). Ce test étant très restrictif, il est fortement conseillé de visualiser le diagramme quantile-quantile pour chaque groupe (graphique QQ plot) (figure \@ref(fig:figQqplot)). Ces graphiques sont utilisés pour déterminer visuellement si une distribution empirique (observées sur des données), s'approche d'une distribution théorique (ici la loi normale). Si effectivement les deux distributions sont proches, les points du diagramme devraient tous tomber sur une ligne droite parfaite. Un intervale de confiance (représenté ici en gris) peut être construit pour obtenir une interprétation plus nuancée. Dans notre cas, seules deux observations pour le vélo et deux autres pour l'automobile s'éloignent vraiment de la ligne droite. On peut considérer que ces trois distributions s'approchent d'une distribution normale.

```{r figQqplot, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='QQ Plot pour les groupes', out.width='75%'}
library("dplyr")
library("ggpubr")
library("rstatix")
# Condition 1 : normalité des échantillons
# Test pour la normalité des échantillons (groupes) : test de Shapiro
 df_TrajetsDuree %>%          # Nom du dataframe
   group_by(Mode) %>%         # Variable qualitative
   shapiro_test(DureeMinute)  # Variable continue 
# Graphiques qqplot pour les groupes
ggqqplot(df_TrajetsDuree, "DureeMinute", facet.by = "Mode")
``` 

Pour vérifier l’hypothèse d’homogénéité des variances, vous pouvez utiliser les tests de Levene, de Bartlett ou de Breusch-Pagan. Les valeurs de *P*, toutes supérieures à 0,05, signalent que la condition d'homogénéité des variances est respectée.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
library("lmtest")
library("car")
# Condition 2 : homogénéité des variances (homocédasticité)
leveneTest(DureeMinute ~ Mode, data = df_TrajetsDuree)
bartlett.test(DureeMinute ~ Mode, data = df_TrajetsDuree)
bptest(DureeMinute ~ Mode, data = df_TrajetsDuree)
```

Deux fonctions peuvent être utilisées pour calculer l'analyse de variance : la fonction de base `aov(variable continue ~ variable qualitative, data = votre dataframe)` ou bien la fonction `anova_test(variable continue ~ variable qualitative, data = votre dataframe)` du package **rstatix**. Comparativement à `aov`, l'avantage de la fonction `anova_test` est qu'elle calcule aussi le Eta^2^.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
library("lmtest")
library("car")
# ANOVA avec la fonction aov
aov1 <- aov(DureeMinute ~ Mode, data = df_TrajetsDuree)
summary(aov1)
# calcul de Eta2 avec la fonction eta_sq du package lmtest
eta_sq(aov1)
# ANOVA avec la fonction anova_test du package rstatix
anova_test(DureeMinute ~ Mode, data = df_TrajetsDuree)
```

La valeur de *P* associée à la statistique *F* (0,444) nous permet de conclure qu'il n'y a pas de différences significatives entre les moyennes des temps de déplacements des trois modes de transport.

#### Deuxième ANOVA : différences entre les niveaux d'exposition au bruit

Dans ce second exercice, nous allons analyser les différences d'exposition au bruit. D'emblée, les statistiques descriptives révèlent que les moyennes sont dissemblables : 66,8 dB(A) pour l’automobile versus 68,8 et 74 pour le vélo et le transport en commun. Aussi, la variance du transport en commun est très différente des autres.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
# chargement des dataframes
setwd("data/bivariee")
load("dataPollution.RData")
# Statistiques descriptives pour les groupes (moyenne et écart-type)
 df_Bruit %>%                                 # Nom du dataframe
   group_by(Mode) %>%                                # Variable qualitative
   get_summary_stats(laeq, type = "mean_sd")  # Variable continue 
```

À la lecture des graphiques de densité et en violon (figure \@ref(fig:figAnova1b)), il semble clair que les niveaux d'exposition au bruit sont plus faibles pour les automobilistes et plus élevés pour les cyclistes et surtout les utilisateurs du transport en commun. En outre, la distribution des valeurs d'exposition au bruit dans le transport en commun semble bimodale. Cela s'explique par le fait que les niveaux de bruit sont beaucoup élevés dans le métro que dans les autobus.

```{r figAnova1b, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='Graphique de densité et en violons', out.width='75%'}
library("ggplot2")
library("ggpubr")
# Graphique en densité
GraphDens <- ggplot(data = df_Bruit, 
  mapping=aes(x=laeq,colour=Mode,fill=Mode)) +
  geom_density(alpha=0.55,mapping=aes(y=..scaled..))+
  labs(title="a. graphique de densité",
       x="Exposition au bruit (dB(A))")
# Graphique en violons
GraphViolon <- ggplot(df_Bruit, aes(x=Mode, y=laeq)) +
  geom_violin(fill="white") +
  geom_boxplot(width=0.1, aes(x=Mode, y=laeq,fill=Mode))+
  labs(title="b. Graphique en violons",
       x = "Mode de transport",
       y="Exposition au bruit (dB(A))")+
  theme(legend.position = "none")
ggarrange(GraphDens, GraphViolon)
``` 


Le test de Shapiro et les graphiques QQ plot (figure \@ref(fig:figQqplot2)) rélèvent que les distributions des trois groupes sont anormales. Ce résultat n'est pas surprenant si l'on tient compte de la nature logarithmique de l'échelle décibel.

```{r figQqplot2, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.cap='QQ Plot pour les groupes', out.width='75%'}
library("dplyr")
library("ggpubr")
library("rstatix")
# Condition 1 : normalité des échantillons
# Test pour la normalité des échantillons (groupes) : test de Shapiro
df_Bruit %>%          # Nom du dataframe
  group_by(Mode) %>%         # Variable qualitative
  shapiro_test(laeq)  # Variable continue 
# Graphiques qqplot pour les groupes
ggqqplot(df_Bruit, "laeq", facet.by = "Mode")
``` 

En outre, selon les valeurs des tests de Levene, de Bartlett ou de Breusch-Pagan, les variances ne sont pas égales.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
library("lmtest")
library("car")
# Condition 2 : homogénéité des variances (homocédasticité)
leveneTest(laeq ~ Mode, data = df_Bruit)
bartlett.test(laeq ~ Mode, data = df_Bruit)
bptest(laeq ~ Mode, data = df_Bruit)
```

Étant donné que les deux conditions (normalité et homogénéité des variances) ne sont pas respectées, il serait préférable d'utiliser un test non paramétrique de Kruskal-Wallis. Calculons toutefois préalablement l'ANOVA classique et l'ANOVA de Welch puisque les variances ne sont pas égales). Les valeurs de *P* des deux tests (Fisher et Welch) signalent que les moyennes d'exposition au bruit sont statistiquement différentes entre les trois modes de transport.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library("rstatix")
# ANOVA avec la fonction anova_test du package rstatix
anova_test(laeq ~ Mode, data = df_Bruit)
# ANOVA avec le test de Welch puisque les variances ne sont pas égales
welch_anova_test(laeq ~ Mode, data = df_Bruit)
```

Une fois démontré que les moyennes sont différentes, le test de Tukey est particulièrement intéressant puisqu'il nous permet de repérer les différences de moyennes significatives deux à deux, tout en ajustant les valeurs de *P* obtenues en fonction du nombre de comparaisons effectuées. Ci-dessous, on constate que toutes les paires sont statistiquement différentes et que la différence de moyennes entre les automobilistes et les cyclistes est de 1,9 dB(A) et surtout de 7,1 dB(A) entre les automobilistes et les usagers du transport en commun.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
aov2 <- aov(laeq ~ Mode, data = df_Bruit)
# Test de Tukey pour comparer les moyennes entre elles
TukeyHSD(aov2, conf.level = 0.95)
```

Le calcul de test non paramétrique de Kruskal-Wallis avec la fonction `kruskal.test` démontre aussi que les médianes des groupes sont différentes (*p*< 0,001). De manière comparable au test de Tukey, la fonction `pairwise.wilcox.test` permet aussi de repérer les différences significatives entre les paires de groupes. Pour conclure, tant l'ANOVA que le test non paramétrique de Kruskal-Wallis indiquent que les trois modes de transport sont significativement différents quant à l'exposition au bruit, avec des valeurs plus faibles pour les automobilistes comparativement aux cyclistes et aux usagers du transport en commun.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Test de Kruskal-Wallis
kruskal.test(laeq ~ Mode, data = df_Bruit)
# Calcul de la moyenne des rangs pour les trois groupes
df_Bruit$laeqRank <- rank(df_Bruit$laeq)
df_Bruit %>%
  group_by(Mode) %>%
  get_summary_stats(laeqRank, type = "mean")
# Compiraison des groupes avec la fonction pairwise.wilcox.test
pairwise.wilcox.test(df_Bruit$laeq, df_Bruit$Mode, p.adjust.method = "BH")
```

### Comment rapporter les résultats d'une ANOVA et du test de Kruskal-Wallis {#sect0444}

Plusieurs éléments doivent être reportés pour détailler les résultats d'une ANOVA ou d'un test de Kruskal-Wallis : la valeur de *F*, de *W* (dans le cas d'une ANOVA de Welch) ou du χ2 (Kruskal-Wallis), les valeurs de *P*, les moyennes ou médianes respectives des groupes et éventuellement un tableau détaillant les écarts intergroupes obtenus avec les test de Tuckey ou Wilcoxon par paires.

* Les résultats de l'analyse de variance à un facteur démontrent que le mode de transport utilisé n'a pas d'effet significatif sur le temps de déplacement en heures de pointe à Montréal (*F*(2,96)=0,82, *p*=0,444). En effet, pour des trajets de dix kilomètres entre un quartier périphérique et le centre-ville, les cyclistes (Moy=38,4, ET=15,2) arrivent en moyenne moins d'une minute après les automobilistes (Moy=37,7, ET=12,8) et les usagers du transport en commun moins de quatre minutes (Moy=41,6, ET=11,4). 

* Les résultats de l'analyse de variance à un facteur démontrent que le mode transport utilisé a un impact significatif sur le niveau d'exposition en heures de pointe à Montréal (*F*(2,96)=544, *p*<0,001 et *Welch*(2,96)=446, *p*<0,001). En effet, les usagers du transport en commun (Moy=74,0, ET=6,79) et les cyclistes (Moy=68,8, ET=4,3) sont significativement plus exposés au bruit que les automobilistes (Moy=66,8, ET=4,56). 

* Les résultats du test de Kruskal-Wallis démontre qu'il existe des différences significatives d'exposition au bruit entre les trois modes de transport (χ2(2) = 784,74, p<0,001) avec des moyennes de rangs de 1094 pour l'automobile, 1124 pour le vélo et 1207 pour le transport en commun.

::: {.bloc_aller_loin .bloc_aller_loin_png data-latex="{blocs/aller_loin}"}
Nous avons vu que l'ANOVA permet de comparer les moyennes d'une variable continue à partir d'une variable qualitative comprenant plusieurs modalités (facteur) pour des observations indépendantes. Il y a un donc une seule variable dépendante (continue) et une seule variable indépendante. Sachez qu'il existe de nombreuses extensions de l'ANOVA classique : 

* **une ANOVA à deux facteurs**, soit avec une variable dépendante continue et deux variables indépendantes qualitatives (_two-way ANOVA_ en anglais). On évalue ainsi les effets des deux variables (*a*, *b*) et de leur interaction (*ab*) sur une variable continue.

* **une ANOVA multifacteur** avec une variable dépendante continue et plus de deux variables indépendantes qualitatives. Par exemple, avec trois variables qualitatives pour expliquer la variable continue, on inclut les effets de chaque variable qualitative (*a*, *b*, *c*) ainsi que de leurs interactions (*ab*, *ac*, *bc*, *abc*).

* **L'analyse de covariance** (**ANCOVA**, **AN**alysis of **COVA**riance en anglais) comprend une variable dépendante continue, une variable indépendante qualitative (facteur) et plusieurs variables indépendantes continues dites covariables. L'objectif est alors de vérifier si les moyennes d'une variable dépendante sont différentes pour plusieurs groupes d'une population donnée, après avoir contrôlé l'effet d'une ou plusieurs variables continues. Par exemple, pour une métropole donnée, on pourra vouloir comparer les moyennes de loyers entre la ville-centre et ceux de la première et de la seconde couronnes (facteur), une fois contrôlée la taille de ces derniers (variable covariée continue). En effet, une partie de la variance des loyers s'explique certainement par la taille des logements.

* **L'analyse de variance multivariée** (**MANOVA**, _**M**ultivariate **AN**alysis of **VA**riance_ en anglais) comprend deux variables dépendantes continues ou plus et une variable indépendante qualitative (facteur). Par exemple, on souheterait comparer les moyennes d'exposition au bruit et à différents polluants (dioxyde d'azote, particules fines, ozone) (variables dépendantes continues) selon le mode de transport utilisé (automobile, vélo, transport en commun, soit le facteur).

* **L'analyse de covariance multivariée** (**MANCOVA**, **M**ultivariate **AN**alysis of **COVA**riance en anglais), soit une analyse qui comprend deux variables dépendantes continues ou plus (comme la MANOVA) et une variable qualitative comme variable indépendante (facteur) et un covariable continue ou plus.

Pour le test *t*, nous avons vu qu'il peut s'appliquer soit à deux échantillons indépendants (non appariés), soit à deux échantillons dépendants (appariés). Notez qu'il existe aussi des extensions de l'ANOVA pour des échantillons pairés. On parle alors d'**analyse de variance sur des mesures répétées**. Par exemple, on pourrait évaluer la perception du sentiment de sécurité relativement à la pratique vélo d'hiver pour un échantillon de cyclistes ayant décidé de l'adopter récemment, et ce, à plusieurs moments : avant leur première saison, à la fin de leur premier hiver, à la fin de leur second hiver. Autre exemple, on pourrait sélectionner un échantillon d'individus (100 par exemple) pour lesquels on évaluerait leurs perceptions de l'environnement sonore dans différents lieux de la ville. Comme pour l'ANOVA classique (échantillons non appariés), il existe des extensions de l'ANOVA sur des mesures répétées permettant d'inclure plusieurs facteurs (groupes de population); on mesure alors une variable continue pour plusieurs groupes d'individus à différents moments ou conditions différentes. Il est aussi possible de réaliser une ANOVA pour des mesures répétées avec une ou plusieurs covariables continues. 

Bref, si l'ANOVA était un roman, elle serait certainement « un monde sans fin » de Ken Follett!
:::
